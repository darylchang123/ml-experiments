{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import utils.ml_utils as ml_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init seeds and env variables (makes training deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.init_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start off testing two batch sizes - small (32) and large (256). We would expect the large batch size to achieve similar training loss but poorer validation loss. This generalization gap is described in https://arxiv.org/pdf/1609.04836.pdf. We would also expect the large batch size to train somewhat faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/1000\n",
      "    582/Unknown - 25s 43ms/step - loss: 0.6860 - accuracy: 0.5435\n",
      "Saving weights for epoch 0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66267, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 31s 54ms/step - loss: 0.6860 - accuracy: 0.5435 - val_loss: 0.6627 - val_accuracy: 0.6279\n",
      "Epoch 2/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5153\n",
      "Epoch 00002: val_loss did not improve from 0.66267\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6911 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5099\n",
      "Epoch 3/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.5262\n",
      "Epoch 00003: val_loss did not improve from 0.66267\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6900 - accuracy: 0.5264 - val_loss: 0.6677 - val_accuracy: 0.6131\n",
      "Epoch 4/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6271\n",
      "Epoch 00004: val_loss improved from 0.66267 to 0.63571, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6494 - accuracy: 0.6271 - val_loss: 0.6357 - val_accuracy: 0.6464\n",
      "Epoch 5/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6255 - accuracy: 0.6537\n",
      "Epoch 00005: val_loss improved from 0.63571 to 0.62085, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6255 - accuracy: 0.6538 - val_loss: 0.6208 - val_accuracy: 0.6582\n",
      "Epoch 6/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.6749\n",
      "Saving weights for epoch 5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62085 to 0.59614, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6082 - accuracy: 0.6749 - val_loss: 0.5961 - val_accuracy: 0.6930\n",
      "Epoch 7/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.6868\n",
      "Epoch 00007: val_loss improved from 0.59614 to 0.57699, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5909 - accuracy: 0.6867 - val_loss: 0.5770 - val_accuracy: 0.7096\n",
      "Epoch 8/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7032\n",
      "Epoch 00008: val_loss improved from 0.57699 to 0.57602, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5729 - accuracy: 0.7031 - val_loss: 0.5760 - val_accuracy: 0.7016\n",
      "Epoch 9/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.7107\n",
      "Epoch 00009: val_loss improved from 0.57602 to 0.55248, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5585 - accuracy: 0.7104 - val_loss: 0.5525 - val_accuracy: 0.7197\n",
      "Epoch 10/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5436 - accuracy: 0.7197\n",
      "Epoch 00010: val_loss improved from 0.55248 to 0.53118, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5435 - accuracy: 0.7195 - val_loss: 0.5312 - val_accuracy: 0.7341\n",
      "Epoch 11/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7294\n",
      "Saving weights for epoch 10\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53118\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5364 - accuracy: 0.7294 - val_loss: 0.5351 - val_accuracy: 0.7489\n",
      "Epoch 12/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7368\n",
      "Epoch 00012: val_loss did not improve from 0.53118\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5233 - accuracy: 0.7365 - val_loss: 0.5556 - val_accuracy: 0.7197\n",
      "Epoch 13/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7462\n",
      "Epoch 00013: val_loss improved from 0.53118 to 0.48511, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5132 - accuracy: 0.7459 - val_loss: 0.4851 - val_accuracy: 0.7700\n",
      "Epoch 14/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5011 - accuracy: 0.7511\n",
      "Epoch 00014: val_loss improved from 0.48511 to 0.48095, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5013 - accuracy: 0.7510 - val_loss: 0.4809 - val_accuracy: 0.7689\n",
      "Epoch 15/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.7579\n",
      "Epoch 00015: val_loss did not improve from 0.48095\n",
      "582/582 [==============================] - 13s 23ms/step - loss: 0.4926 - accuracy: 0.7577 - val_loss: 0.5318 - val_accuracy: 0.7377\n",
      "Epoch 16/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4798 - accuracy: 0.7699\n",
      "Saving weights for epoch 15\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.48095 to 0.46978, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4800 - accuracy: 0.7698 - val_loss: 0.4698 - val_accuracy: 0.7820\n",
      "Epoch 17/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.7708\n",
      "Epoch 00017: val_loss did not improve from 0.46978\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4731 - accuracy: 0.7707 - val_loss: 0.4790 - val_accuracy: 0.7741\n",
      "Epoch 18/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.7743\n",
      "Epoch 00018: val_loss improved from 0.46978 to 0.45018, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4685 - accuracy: 0.7741 - val_loss: 0.4502 - val_accuracy: 0.7900\n",
      "Epoch 19/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.7764\n",
      "Epoch 00019: val_loss did not improve from 0.45018\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4628 - accuracy: 0.7762 - val_loss: 0.4516 - val_accuracy: 0.7850\n",
      "Epoch 20/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4569 - accuracy: 0.7830\n",
      "Epoch 00020: val_loss improved from 0.45018 to 0.43949, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4569 - accuracy: 0.7828 - val_loss: 0.4395 - val_accuracy: 0.7883\n",
      "Epoch 21/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4494 - accuracy: 0.7860\n",
      "Saving weights for epoch 20\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.43949 to 0.43566, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4494 - accuracy: 0.7859 - val_loss: 0.4357 - val_accuracy: 0.7947\n",
      "Epoch 22/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4451 - accuracy: 0.7884\n",
      "Epoch 00022: val_loss did not improve from 0.43566\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4453 - accuracy: 0.7882 - val_loss: 0.4603 - val_accuracy: 0.7827\n",
      "Epoch 23/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.7938\n",
      "Epoch 00023: val_loss did not improve from 0.43566\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4356 - accuracy: 0.7938 - val_loss: 0.4388 - val_accuracy: 0.7911\n",
      "Epoch 24/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.7917\n",
      "Epoch 00024: val_loss improved from 0.43566 to 0.43554, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4373 - accuracy: 0.7917 - val_loss: 0.4355 - val_accuracy: 0.8001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.7961\n",
      "Epoch 00025: val_loss did not improve from 0.43554\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4307 - accuracy: 0.7961 - val_loss: 0.4403 - val_accuracy: 0.7865\n",
      "Epoch 26/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8017\n",
      "Saving weights for epoch 25\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43554\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4252 - accuracy: 0.8015 - val_loss: 0.4356 - val_accuracy: 0.7913\n",
      "Epoch 27/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8078\n",
      "Epoch 00027: val_loss improved from 0.43554 to 0.41058, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4189 - accuracy: 0.8077 - val_loss: 0.4106 - val_accuracy: 0.8072\n",
      "Epoch 28/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8046\n",
      "Epoch 00028: val_loss did not improve from 0.41058\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4213 - accuracy: 0.8042 - val_loss: 0.4330 - val_accuracy: 0.7936\n",
      "Epoch 29/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8075\n",
      "Epoch 00029: val_loss did not improve from 0.41058\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4118 - accuracy: 0.8071 - val_loss: 0.4518 - val_accuracy: 0.7853\n",
      "Epoch 30/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8134\n",
      "Epoch 00030: val_loss did not improve from 0.41058\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4044 - accuracy: 0.8132 - val_loss: 0.4123 - val_accuracy: 0.8091\n",
      "Epoch 31/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4015 - accuracy: 0.8119\n",
      "Saving weights for epoch 30\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.41058\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4017 - accuracy: 0.8119 - val_loss: 0.4116 - val_accuracy: 0.8093\n",
      "Epoch 32/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3990 - accuracy: 0.8151\n",
      "Epoch 00032: val_loss improved from 0.41058 to 0.40087, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3991 - accuracy: 0.8150 - val_loss: 0.4009 - val_accuracy: 0.8119\n",
      "Epoch 33/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3951 - accuracy: 0.8181\n",
      "Epoch 00033: val_loss improved from 0.40087 to 0.39197, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3952 - accuracy: 0.8180 - val_loss: 0.3920 - val_accuracy: 0.8192\n",
      "Epoch 34/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8234\n",
      "Epoch 00034: val_loss improved from 0.39197 to 0.38782, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3907 - accuracy: 0.8235 - val_loss: 0.3878 - val_accuracy: 0.8224\n",
      "Epoch 35/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8242\n",
      "Epoch 00035: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3867 - accuracy: 0.8242 - val_loss: 0.4230 - val_accuracy: 0.8001\n",
      "Epoch 36/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8223\n",
      "Saving weights for epoch 35\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3831 - accuracy: 0.8219 - val_loss: 0.4248 - val_accuracy: 0.7960\n",
      "Epoch 37/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3849 - accuracy: 0.8238\n",
      "Epoch 00037: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3848 - accuracy: 0.8237 - val_loss: 0.3934 - val_accuracy: 0.8201\n",
      "Epoch 38/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8277\n",
      "Epoch 00038: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3766 - accuracy: 0.8275 - val_loss: 0.3940 - val_accuracy: 0.8181\n",
      "Epoch 39/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.8298\n",
      "Epoch 00039: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3721 - accuracy: 0.8298 - val_loss: 0.4245 - val_accuracy: 0.7962\n",
      "Epoch 40/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8348\n",
      "Epoch 00040: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3667 - accuracy: 0.8347 - val_loss: 0.4021 - val_accuracy: 0.8141\n",
      "Epoch 41/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8343\n",
      "Saving weights for epoch 40\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3636 - accuracy: 0.8341 - val_loss: 0.4226 - val_accuracy: 0.8037\n",
      "Epoch 42/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8328\n",
      "Epoch 00042: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3650 - accuracy: 0.8326 - val_loss: 0.3992 - val_accuracy: 0.8160\n",
      "Epoch 43/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8349\n",
      "Epoch 00043: val_loss did not improve from 0.38782\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3646 - accuracy: 0.8346 - val_loss: 0.4001 - val_accuracy: 0.8106\n",
      "Epoch 44/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.8372\n",
      "Epoch 00044: val_loss improved from 0.38782 to 0.38450, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3574 - accuracy: 0.8368 - val_loss: 0.3845 - val_accuracy: 0.8239\n",
      "Epoch 45/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8343\n",
      "Epoch 00045: val_loss improved from 0.38450 to 0.37366, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3610 - accuracy: 0.8340 - val_loss: 0.3737 - val_accuracy: 0.8233\n",
      "Epoch 46/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8396\n",
      "Saving weights for epoch 45\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.37366\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3513 - accuracy: 0.8395 - val_loss: 0.3925 - val_accuracy: 0.8209\n",
      "Epoch 47/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8426\n",
      "Epoch 00047: val_loss did not improve from 0.37366\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3523 - accuracy: 0.8424 - val_loss: 0.3995 - val_accuracy: 0.8164\n",
      "Epoch 48/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3423 - accuracy: 0.8475\n",
      "Epoch 00048: val_loss improved from 0.37366 to 0.36908, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3422 - accuracy: 0.8476 - val_loss: 0.3691 - val_accuracy: 0.8261\n",
      "Epoch 49/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3463 - accuracy: 0.8438\n",
      "Epoch 00049: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3462 - accuracy: 0.8440 - val_loss: 0.3781 - val_accuracy: 0.8214\n",
      "Epoch 50/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8490\n",
      "Epoch 00050: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3397 - accuracy: 0.8486 - val_loss: 0.3985 - val_accuracy: 0.8126\n",
      "Epoch 51/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8477\n",
      "Saving weights for epoch 50\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3431 - accuracy: 0.8475 - val_loss: 0.3889 - val_accuracy: 0.8222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3328 - accuracy: 0.8495\n",
      "Epoch 00052: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3328 - accuracy: 0.8495 - val_loss: 0.3731 - val_accuracy: 0.8265\n",
      "Epoch 53/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8559\n",
      "Epoch 00053: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3283 - accuracy: 0.8558 - val_loss: 0.3992 - val_accuracy: 0.8132\n",
      "Epoch 54/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8540\n",
      "Epoch 00054: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3260 - accuracy: 0.8539 - val_loss: 0.3845 - val_accuracy: 0.8194\n",
      "Epoch 55/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8551\n",
      "Epoch 00055: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3278 - accuracy: 0.8552 - val_loss: 0.3840 - val_accuracy: 0.8246\n",
      "Epoch 56/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.8555\n",
      "Saving weights for epoch 55\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3281 - accuracy: 0.8557 - val_loss: 0.4000 - val_accuracy: 0.8147\n",
      "Epoch 57/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8591\n",
      "Epoch 00057: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3231 - accuracy: 0.8590 - val_loss: 0.4176 - val_accuracy: 0.8102\n",
      "Epoch 58/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8585\n",
      "Epoch 00058: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3202 - accuracy: 0.8583 - val_loss: 0.4105 - val_accuracy: 0.8136\n",
      "Epoch 59/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8616\n",
      "Epoch 00059: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3139 - accuracy: 0.8615 - val_loss: 0.3849 - val_accuracy: 0.8199\n",
      "Epoch 60/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8618\n",
      "Epoch 00060: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3153 - accuracy: 0.8617 - val_loss: 0.3995 - val_accuracy: 0.8145\n",
      "Epoch 61/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.8614\n",
      "Saving weights for epoch 60\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3169 - accuracy: 0.8611 - val_loss: 0.3813 - val_accuracy: 0.8184\n",
      "Epoch 62/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8639\n",
      "Epoch 00062: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3083 - accuracy: 0.8638 - val_loss: 0.3823 - val_accuracy: 0.8261\n",
      "Epoch 63/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8606\n",
      "Epoch 00063: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3130 - accuracy: 0.8607 - val_loss: 0.3831 - val_accuracy: 0.8280\n",
      "Epoch 64/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8661\n",
      "Epoch 00064: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3108 - accuracy: 0.8659 - val_loss: 0.3875 - val_accuracy: 0.8259\n",
      "Epoch 65/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8698\n",
      "Epoch 00065: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3010 - accuracy: 0.8698 - val_loss: 0.4109 - val_accuracy: 0.8100\n",
      "Epoch 66/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8682\n",
      "Saving weights for epoch 65\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3039 - accuracy: 0.8682 - val_loss: 0.3982 - val_accuracy: 0.8218\n",
      "Epoch 67/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8650\n",
      "Epoch 00067: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3049 - accuracy: 0.8649 - val_loss: 0.3802 - val_accuracy: 0.8261\n",
      "Epoch 68/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8717\n",
      "Epoch 00068: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2974 - accuracy: 0.8716 - val_loss: 0.3890 - val_accuracy: 0.8220\n",
      "Epoch 69/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8703\n",
      "Epoch 00069: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3003 - accuracy: 0.8703 - val_loss: 0.3763 - val_accuracy: 0.8289\n",
      "Epoch 70/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8676\n",
      "Epoch 00070: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2943 - accuracy: 0.8676 - val_loss: 0.3852 - val_accuracy: 0.8289\n",
      "Epoch 71/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8733\n",
      "Saving weights for epoch 70\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2963 - accuracy: 0.8732 - val_loss: 0.3878 - val_accuracy: 0.8227\n",
      "Epoch 72/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8765\n",
      "Epoch 00072: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2837 - accuracy: 0.8764 - val_loss: 0.3824 - val_accuracy: 0.8141\n",
      "Epoch 73/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.8749\n",
      "Epoch 00073: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2903 - accuracy: 0.8746 - val_loss: 0.4015 - val_accuracy: 0.8123\n",
      "Epoch 74/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8749\n",
      "Epoch 00074: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2880 - accuracy: 0.8750 - val_loss: 0.3712 - val_accuracy: 0.8248\n",
      "Epoch 75/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.8768\n",
      "Epoch 00075: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2824 - accuracy: 0.8765 - val_loss: 0.3849 - val_accuracy: 0.8222\n",
      "Epoch 76/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8799\n",
      "Saving weights for epoch 75\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2900 - accuracy: 0.8801 - val_loss: 0.4006 - val_accuracy: 0.8166\n",
      "Epoch 77/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8829\n",
      "Epoch 00077: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2743 - accuracy: 0.8826 - val_loss: 0.4092 - val_accuracy: 0.8141\n",
      "Epoch 78/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8762\n",
      "Epoch 00078: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2867 - accuracy: 0.8761 - val_loss: 0.3956 - val_accuracy: 0.8166\n",
      "Epoch 79/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.8825\n",
      "Epoch 00079: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2770 - accuracy: 0.8825 - val_loss: 0.3796 - val_accuracy: 0.8237\n",
      "Epoch 80/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8831\n",
      "Epoch 00080: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2785 - accuracy: 0.8832 - val_loss: 0.3818 - val_accuracy: 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8852\n",
      "Saving weights for epoch 80\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2687 - accuracy: 0.8852 - val_loss: 0.4812 - val_accuracy: 0.7915\n",
      "Epoch 82/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2799 - accuracy: 0.8803\n",
      "Epoch 00082: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2802 - accuracy: 0.8802 - val_loss: 0.3905 - val_accuracy: 0.8222\n",
      "Epoch 83/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8825\n",
      "Epoch 00083: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2728 - accuracy: 0.8823 - val_loss: 0.4001 - val_accuracy: 0.8156\n",
      "Epoch 84/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.8805\n",
      "Epoch 00084: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2747 - accuracy: 0.8807 - val_loss: 0.4112 - val_accuracy: 0.8134\n",
      "Epoch 85/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.8904\n",
      "Epoch 00085: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2583 - accuracy: 0.8904 - val_loss: 0.3929 - val_accuracy: 0.8310\n",
      "Epoch 86/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.8893\n",
      "Saving weights for epoch 85\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2658 - accuracy: 0.8891 - val_loss: 0.3949 - val_accuracy: 0.8181\n",
      "Epoch 87/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2631 - accuracy: 0.8887\n",
      "Epoch 00087: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2630 - accuracy: 0.8887 - val_loss: 0.3986 - val_accuracy: 0.8229\n",
      "Epoch 88/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8892\n",
      "Epoch 00088: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2604 - accuracy: 0.8892 - val_loss: 0.3974 - val_accuracy: 0.8289\n",
      "Epoch 89/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2630 - accuracy: 0.8923\n",
      "Epoch 00089: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2629 - accuracy: 0.8922 - val_loss: 0.3888 - val_accuracy: 0.8203\n",
      "Epoch 90/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.8856\n",
      "Epoch 00090: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2608 - accuracy: 0.8854 - val_loss: 0.4328 - val_accuracy: 0.8009\n",
      "Epoch 91/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8887\n",
      "Saving weights for epoch 90\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2616 - accuracy: 0.8888 - val_loss: 0.3941 - val_accuracy: 0.8237\n",
      "Epoch 92/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.8932\n",
      "Epoch 00092: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2557 - accuracy: 0.8930 - val_loss: 0.4210 - val_accuracy: 0.8117\n",
      "Epoch 93/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.8919\n",
      "Epoch 00093: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2539 - accuracy: 0.8919 - val_loss: 0.3901 - val_accuracy: 0.8246\n",
      "Epoch 94/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8952\n",
      "Epoch 00094: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2506 - accuracy: 0.8954 - val_loss: 0.4062 - val_accuracy: 0.8216\n",
      "Epoch 95/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.8962\n",
      "Epoch 00095: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2441 - accuracy: 0.8962 - val_loss: 0.4021 - val_accuracy: 0.8184\n",
      "Epoch 96/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8962\n",
      "Saving weights for epoch 95\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2512 - accuracy: 0.8961 - val_loss: 0.4149 - val_accuracy: 0.8138\n",
      "Epoch 97/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.8977\n",
      "Epoch 00097: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2482 - accuracy: 0.8976 - val_loss: 0.4469 - val_accuracy: 0.8042\n",
      "Epoch 98/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.8928\n",
      "Epoch 00098: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2530 - accuracy: 0.8927 - val_loss: 0.4063 - val_accuracy: 0.8188\n",
      "Epoch 99/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9015\n",
      "Epoch 00099: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2378 - accuracy: 0.9014 - val_loss: 0.3968 - val_accuracy: 0.8248\n",
      "Epoch 100/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.8957\n",
      "Epoch 00100: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2451 - accuracy: 0.8955 - val_loss: 0.3981 - val_accuracy: 0.8285\n",
      "Epoch 101/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9038\n",
      "Saving weights for epoch 100\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2433 - accuracy: 0.9038 - val_loss: 0.3978 - val_accuracy: 0.8205\n",
      "Epoch 102/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8973\n",
      "Epoch 00102: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2444 - accuracy: 0.8972 - val_loss: 0.3724 - val_accuracy: 0.8267\n",
      "Epoch 103/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.8999\n",
      "Epoch 00103: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2408 - accuracy: 0.9000 - val_loss: 0.4003 - val_accuracy: 0.8282\n",
      "Epoch 104/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8985\n",
      "Epoch 00104: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2446 - accuracy: 0.8983 - val_loss: 0.3905 - val_accuracy: 0.8237\n",
      "Epoch 105/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.8977\n",
      "Epoch 00105: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2410 - accuracy: 0.8976 - val_loss: 0.3931 - val_accuracy: 0.8248\n",
      "Epoch 106/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9014\n",
      "Saving weights for epoch 105\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2402 - accuracy: 0.9013 - val_loss: 0.3990 - val_accuracy: 0.8231\n",
      "Epoch 107/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9025\n",
      "Epoch 00107: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2352 - accuracy: 0.9025 - val_loss: 0.4107 - val_accuracy: 0.8175\n",
      "Epoch 108/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.8999\n",
      "Epoch 00108: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2404 - accuracy: 0.8997 - val_loss: 0.3955 - val_accuracy: 0.8252\n",
      "Epoch 109/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9019\n",
      "Epoch 00109: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2333 - accuracy: 0.9018 - val_loss: 0.4964 - val_accuracy: 0.7818\n",
      "Epoch 110/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2294 - accuracy: 0.9061\n",
      "Epoch 00110: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2298 - accuracy: 0.9060 - val_loss: 0.4123 - val_accuracy: 0.8102\n",
      "Epoch 111/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9050\n",
      "Saving weights for epoch 110\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2355 - accuracy: 0.9049 - val_loss: 0.4054 - val_accuracy: 0.8209\n",
      "Epoch 112/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9025\n",
      "Epoch 00112: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2343 - accuracy: 0.9026 - val_loss: 0.4177 - val_accuracy: 0.8134\n",
      "Epoch 113/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9006\n",
      "Epoch 00113: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2356 - accuracy: 0.9006 - val_loss: 0.4196 - val_accuracy: 0.8123\n",
      "Epoch 114/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9081\n",
      "Epoch 00114: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2269 - accuracy: 0.9081 - val_loss: 0.3873 - val_accuracy: 0.8270\n",
      "Epoch 115/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9041\n",
      "Epoch 00115: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2325 - accuracy: 0.9041 - val_loss: 0.4112 - val_accuracy: 0.8128\n",
      "Epoch 116/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9071\n",
      "Saving weights for epoch 115\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2239 - accuracy: 0.9070 - val_loss: 0.4048 - val_accuracy: 0.8190\n",
      "Epoch 117/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9081\n",
      "Epoch 00117: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2300 - accuracy: 0.9080 - val_loss: 0.4072 - val_accuracy: 0.8199\n",
      "Epoch 118/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9046\n",
      "Epoch 00118: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2335 - accuracy: 0.9047 - val_loss: 0.4057 - val_accuracy: 0.8248\n",
      "Epoch 119/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9110\n",
      "Epoch 00119: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2256 - accuracy: 0.9109 - val_loss: 0.3924 - val_accuracy: 0.8212\n",
      "Epoch 120/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9099\n",
      "Epoch 00120: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2243 - accuracy: 0.9099 - val_loss: 0.4066 - val_accuracy: 0.8242\n",
      "Epoch 121/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2237 - accuracy: 0.9080\n",
      "Saving weights for epoch 120\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2242 - accuracy: 0.9079 - val_loss: 0.4191 - val_accuracy: 0.8031\n",
      "Epoch 122/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9072\n",
      "Epoch 00122: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2270 - accuracy: 0.9073 - val_loss: 0.4296 - val_accuracy: 0.8149\n",
      "Epoch 123/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.9049\n",
      "Epoch 00123: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2274 - accuracy: 0.9047 - val_loss: 0.4143 - val_accuracy: 0.8126\n",
      "Epoch 124/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2262 - accuracy: 0.9070\n",
      "Epoch 00124: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2264 - accuracy: 0.9067 - val_loss: 0.4199 - val_accuracy: 0.8212\n",
      "Epoch 125/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.9096\n",
      "Epoch 00125: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2173 - accuracy: 0.9096 - val_loss: 0.4367 - val_accuracy: 0.8166\n",
      "Epoch 126/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9098\n",
      "Saving weights for epoch 125\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2219 - accuracy: 0.9098 - val_loss: 0.4128 - val_accuracy: 0.8203\n",
      "Epoch 127/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9086\n",
      "Epoch 00127: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2228 - accuracy: 0.9086 - val_loss: 0.3948 - val_accuracy: 0.8214\n",
      "Epoch 128/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2213 - accuracy: 0.9110\n",
      "Epoch 00128: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.2213 - accuracy: 0.9109 - val_loss: 0.4025 - val_accuracy: 0.8220\n",
      "Epoch 129/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9096\n",
      "Epoch 00129: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2221 - accuracy: 0.9095 - val_loss: 0.4138 - val_accuracy: 0.8126\n",
      "Epoch 130/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9120\n",
      "Epoch 00130: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2206 - accuracy: 0.9120 - val_loss: 0.4581 - val_accuracy: 0.7926\n",
      "Epoch 131/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9153\n",
      "Saving weights for epoch 130\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.2090 - accuracy: 0.9154 - val_loss: 0.4406 - val_accuracy: 0.8141\n",
      "Epoch 132/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.9095\n",
      "Epoch 00132: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2188 - accuracy: 0.9095 - val_loss: 0.4174 - val_accuracy: 0.8162\n",
      "Epoch 133/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2216 - accuracy: 0.9108\n",
      "Epoch 00133: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2214 - accuracy: 0.9109 - val_loss: 0.4459 - val_accuracy: 0.8018\n",
      "Epoch 134/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9161\n",
      "Epoch 00134: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2111 - accuracy: 0.9160 - val_loss: 0.4128 - val_accuracy: 0.8209\n",
      "Epoch 135/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9175\n",
      "Epoch 00135: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2101 - accuracy: 0.9175 - val_loss: 0.4206 - val_accuracy: 0.8160\n",
      "Epoch 136/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9105\n",
      "Saving weights for epoch 135\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2171 - accuracy: 0.9104 - val_loss: 0.4186 - val_accuracy: 0.8218\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/582 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9155\n",
      "Epoch 00137: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2106 - accuracy: 0.9155 - val_loss: 0.4696 - val_accuracy: 0.8031\n",
      "Epoch 138/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9107\n",
      "Epoch 00138: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2137 - accuracy: 0.9107 - val_loss: 0.4039 - val_accuracy: 0.8173\n",
      "Epoch 139/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2140 - accuracy: 0.9121\n",
      "Epoch 00139: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2140 - accuracy: 0.9122 - val_loss: 0.4210 - val_accuracy: 0.8149\n",
      "Epoch 140/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2097 - accuracy: 0.9140\n",
      "Epoch 00140: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2097 - accuracy: 0.9140 - val_loss: 0.4335 - val_accuracy: 0.8192\n",
      "Epoch 141/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9125\n",
      "Saving weights for epoch 140\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2192 - accuracy: 0.9123 - val_loss: 0.4096 - val_accuracy: 0.8128\n",
      "Epoch 142/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9187\n",
      "Epoch 00142: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2048 - accuracy: 0.9188 - val_loss: 0.4300 - val_accuracy: 0.8102\n",
      "Epoch 143/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9144\n",
      "Epoch 00143: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2082 - accuracy: 0.9143 - val_loss: 0.4247 - val_accuracy: 0.8153\n",
      "Epoch 144/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2148 - accuracy: 0.9120\n",
      "Epoch 00144: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2150 - accuracy: 0.9119 - val_loss: 0.4260 - val_accuracy: 0.8020\n",
      "Epoch 145/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2097 - accuracy: 0.9166\n",
      "Epoch 00145: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2101 - accuracy: 0.9164 - val_loss: 0.4286 - val_accuracy: 0.8164\n",
      "Epoch 146/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9159\n",
      "Saving weights for epoch 145\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2113 - accuracy: 0.9160 - val_loss: 0.4109 - val_accuracy: 0.8153\n",
      "Epoch 147/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9142\n",
      "Epoch 00147: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2081 - accuracy: 0.9143 - val_loss: 0.4378 - val_accuracy: 0.8227\n",
      "Epoch 148/1000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9173\n",
      "Epoch 00148: val_loss did not improve from 0.36908\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2049 - accuracy: 0.9173 - val_loss: 0.4219 - val_accuracy: 0.8237\n",
      "Epoch 00148: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/1000\n",
      "     73/Unknown - 23s 320ms/step - loss: 0.6919 - accuracy: 0.5222\n",
      "Saving weights for epoch 0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69192, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 30s 407ms/step - loss: 0.6919 - accuracy: 0.5222 - val_loss: 0.6919 - val_accuracy: 0.4936\n",
      "Epoch 2/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.5616\n",
      "Epoch 00002: val_loss improved from 0.69192 to 0.69152, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6859 - accuracy: 0.5620 - val_loss: 0.6915 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5649\n",
      "Epoch 00003: val_loss improved from 0.69152 to 0.68244, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6816 - accuracy: 0.5654 - val_loss: 0.6824 - val_accuracy: 0.5492\n",
      "Epoch 4/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6830 - accuracy: 0.5627\n",
      "Epoch 00004: val_loss did not improve from 0.68244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6830 - accuracy: 0.5624 - val_loss: 0.6937 - val_accuracy: 0.5028\n",
      "Epoch 5/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5707\n",
      "Epoch 00005: val_loss improved from 0.68244 to 0.67961, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6789 - accuracy: 0.5710 - val_loss: 0.6796 - val_accuracy: 0.5505\n",
      "Epoch 6/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6784 - accuracy: 0.5740\n",
      "Saving weights for epoch 5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67961 to 0.67790, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6783 - accuracy: 0.5743 - val_loss: 0.6779 - val_accuracy: 0.5587\n",
      "Epoch 7/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.5887\n",
      "Epoch 00007: val_loss improved from 0.67790 to 0.66576, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6701 - accuracy: 0.5889 - val_loss: 0.6658 - val_accuracy: 0.5946\n",
      "Epoch 8/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6665 - accuracy: 0.5959\n",
      "Epoch 00008: val_loss improved from 0.66576 to 0.66408, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6666 - accuracy: 0.5956 - val_loss: 0.6641 - val_accuracy: 0.5950\n",
      "Epoch 9/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6611 - accuracy: 0.6044\n",
      "Epoch 00009: val_loss improved from 0.66408 to 0.65806, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6610 - accuracy: 0.6047 - val_loss: 0.6581 - val_accuracy: 0.6081\n",
      "Epoch 10/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6550 - accuracy: 0.6153\n",
      "Epoch 00010: val_loss improved from 0.65806 to 0.65538, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6551 - accuracy: 0.6150 - val_loss: 0.6554 - val_accuracy: 0.6129\n",
      "Epoch 11/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6491 - accuracy: 0.6257\n",
      "Saving weights for epoch 10\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.65538\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6489 - accuracy: 0.6257 - val_loss: 0.6570 - val_accuracy: 0.6002\n",
      "Epoch 12/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6431 - accuracy: 0.6323\n",
      "Epoch 00012: val_loss improved from 0.65538 to 0.64609, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6428 - accuracy: 0.6322 - val_loss: 0.6461 - val_accuracy: 0.6221\n",
      "Epoch 13/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.6434\n",
      "Epoch 00013: val_loss improved from 0.64609 to 0.63980, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6338 - accuracy: 0.6430 - val_loss: 0.6398 - val_accuracy: 0.6309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.6528\n",
      "Epoch 00014: val_loss did not improve from 0.63980\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6209 - accuracy: 0.6521 - val_loss: 0.6407 - val_accuracy: 0.6232\n",
      "Epoch 15/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.6560\n",
      "Epoch 00015: val_loss improved from 0.63980 to 0.63811, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6175 - accuracy: 0.6551 - val_loss: 0.6381 - val_accuracy: 0.6311\n",
      "Epoch 16/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6066 - accuracy: 0.6699\n",
      "Saving weights for epoch 15\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.63811 to 0.62297, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6066 - accuracy: 0.6699 - val_loss: 0.6230 - val_accuracy: 0.6505\n",
      "Epoch 17/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5972 - accuracy: 0.6764\n",
      "Epoch 00017: val_loss improved from 0.62297 to 0.61592, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5971 - accuracy: 0.6761 - val_loss: 0.6159 - val_accuracy: 0.6515\n",
      "Epoch 18/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.6697\n",
      "Epoch 00018: val_loss improved from 0.61592 to 0.60535, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6005 - accuracy: 0.6694 - val_loss: 0.6053 - val_accuracy: 0.6601\n",
      "Epoch 19/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.6828\n",
      "Epoch 00019: val_loss improved from 0.60535 to 0.59074, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5874 - accuracy: 0.6826 - val_loss: 0.5907 - val_accuracy: 0.6694\n",
      "Epoch 20/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5764 - accuracy: 0.6925\n",
      "Epoch 00020: val_loss improved from 0.59074 to 0.57050, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5767 - accuracy: 0.6924 - val_loss: 0.5705 - val_accuracy: 0.6874\n",
      "Epoch 21/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5965 - accuracy: 0.6764\n",
      "Saving weights for epoch 20\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.57050\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5972 - accuracy: 0.6759 - val_loss: 0.5826 - val_accuracy: 0.6825\n",
      "Epoch 22/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.6986\n",
      "Epoch 00022: val_loss did not improve from 0.57050\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5714 - accuracy: 0.6983 - val_loss: 0.5743 - val_accuracy: 0.6816\n",
      "Epoch 23/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7049\n",
      "Epoch 00023: val_loss improved from 0.57050 to 0.54388, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5635 - accuracy: 0.7047 - val_loss: 0.5439 - val_accuracy: 0.7122\n",
      "Epoch 24/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7088\n",
      "Epoch 00024: val_loss did not improve from 0.54388\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5604 - accuracy: 0.7085 - val_loss: 0.5680 - val_accuracy: 0.6866\n",
      "Epoch 25/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5534 - accuracy: 0.7132\n",
      "Epoch 00025: val_loss improved from 0.54388 to 0.53876, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5538 - accuracy: 0.7127 - val_loss: 0.5388 - val_accuracy: 0.7193\n",
      "Epoch 26/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7229\n",
      "Saving weights for epoch 25\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.53876 to 0.52839, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5467 - accuracy: 0.7223 - val_loss: 0.5284 - val_accuracy: 0.7300\n",
      "Epoch 27/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7259\n",
      "Epoch 00027: val_loss improved from 0.52839 to 0.52167, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5399 - accuracy: 0.7254 - val_loss: 0.5217 - val_accuracy: 0.7319\n",
      "Epoch 28/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5343 - accuracy: 0.7265\n",
      "Epoch 00028: val_loss did not improve from 0.52167\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5344 - accuracy: 0.7263 - val_loss: 0.5547 - val_accuracy: 0.7023\n",
      "Epoch 29/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7342\n",
      "Epoch 00029: val_loss improved from 0.52167 to 0.50743, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5279 - accuracy: 0.7341 - val_loss: 0.5074 - val_accuracy: 0.7405\n",
      "Epoch 30/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5216 - accuracy: 0.7366\n",
      "Epoch 00030: val_loss did not improve from 0.50743\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5218 - accuracy: 0.7365 - val_loss: 0.5099 - val_accuracy: 0.7444\n",
      "Epoch 31/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5139 - accuracy: 0.7442\n",
      "Saving weights for epoch 30\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.50743 to 0.49959, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5144 - accuracy: 0.7437 - val_loss: 0.4996 - val_accuracy: 0.7562\n",
      "Epoch 32/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7440\n",
      "Epoch 00032: val_loss improved from 0.49959 to 0.48564, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5109 - accuracy: 0.7438 - val_loss: 0.4856 - val_accuracy: 0.7670\n",
      "Epoch 33/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5052 - accuracy: 0.7499\n",
      "Epoch 00033: val_loss improved from 0.48564 to 0.48291, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5062 - accuracy: 0.7492 - val_loss: 0.4829 - val_accuracy: 0.7610\n",
      "Epoch 34/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.7537\n",
      "Epoch 00034: val_loss did not improve from 0.48291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5025 - accuracy: 0.7537 - val_loss: 0.4840 - val_accuracy: 0.7599\n",
      "Epoch 35/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4990 - accuracy: 0.7527\n",
      "Epoch 00035: val_loss improved from 0.48291 to 0.47135, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4996 - accuracy: 0.7521 - val_loss: 0.4713 - val_accuracy: 0.7758\n",
      "Epoch 36/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4916 - accuracy: 0.7619\n",
      "Saving weights for epoch 35\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47135 to 0.46267, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4920 - accuracy: 0.7614 - val_loss: 0.4627 - val_accuracy: 0.7810\n",
      "Epoch 37/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7655\n",
      "Epoch 00037: val_loss did not improve from 0.46267\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4877 - accuracy: 0.7652 - val_loss: 0.4793 - val_accuracy: 0.7614\n",
      "Epoch 38/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4797 - accuracy: 0.7674\n",
      "Epoch 00038: val_loss did not improve from 0.46267\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4798 - accuracy: 0.7672 - val_loss: 0.4774 - val_accuracy: 0.7616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7708\n",
      "Epoch 00039: val_loss did not improve from 0.46267\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4767 - accuracy: 0.7706 - val_loss: 0.4737 - val_accuracy: 0.7668\n",
      "Epoch 40/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7688\n",
      "Epoch 00040: val_loss improved from 0.46267 to 0.45228, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4771 - accuracy: 0.7686 - val_loss: 0.4523 - val_accuracy: 0.7855\n",
      "Epoch 41/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.7729\n",
      "Saving weights for epoch 40\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.45228\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4723 - accuracy: 0.7728 - val_loss: 0.4632 - val_accuracy: 0.7764\n",
      "Epoch 42/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.7760\n",
      "Epoch 00042: val_loss did not improve from 0.45228\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4680 - accuracy: 0.7758 - val_loss: 0.4647 - val_accuracy: 0.7728\n",
      "Epoch 43/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7745\n",
      "Epoch 00043: val_loss improved from 0.45228 to 0.43373, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4688 - accuracy: 0.7742 - val_loss: 0.4337 - val_accuracy: 0.7988\n",
      "Epoch 44/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.7806\n",
      "Epoch 00044: val_loss improved from 0.43373 to 0.42726, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4583 - accuracy: 0.7803 - val_loss: 0.4273 - val_accuracy: 0.8074\n",
      "Epoch 45/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4523 - accuracy: 0.7841\n",
      "Epoch 00045: val_loss did not improve from 0.42726\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4523 - accuracy: 0.7842 - val_loss: 0.4882 - val_accuracy: 0.7603\n",
      "Epoch 46/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4499 - accuracy: 0.7860\n",
      "Saving weights for epoch 45\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.42726\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4498 - accuracy: 0.7858 - val_loss: 0.4635 - val_accuracy: 0.7732\n",
      "Epoch 47/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7888\n",
      "Epoch 00047: val_loss improved from 0.42726 to 0.42429, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4458 - accuracy: 0.7885 - val_loss: 0.4243 - val_accuracy: 0.8022\n",
      "Epoch 48/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.7886\n",
      "Epoch 00048: val_loss improved from 0.42429 to 0.42371, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4418 - accuracy: 0.7884 - val_loss: 0.4237 - val_accuracy: 0.8007\n",
      "Epoch 49/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4479 - accuracy: 0.7897\n",
      "Epoch 00049: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4478 - accuracy: 0.7897 - val_loss: 0.4274 - val_accuracy: 0.7982\n",
      "Epoch 50/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4419 - accuracy: 0.7940\n",
      "Epoch 00050: val_loss improved from 0.42371 to 0.41216, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4421 - accuracy: 0.7939 - val_loss: 0.4122 - val_accuracy: 0.8063\n",
      "Epoch 51/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.7964\n",
      "Saving weights for epoch 50\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.41216\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4334 - accuracy: 0.7966 - val_loss: 0.4311 - val_accuracy: 0.7975\n",
      "Epoch 52/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.7951\n",
      "Epoch 00052: val_loss improved from 0.41216 to 0.41060, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4366 - accuracy: 0.7949 - val_loss: 0.4106 - val_accuracy: 0.8132\n",
      "Epoch 53/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4358 - accuracy: 0.7951\n",
      "Epoch 00053: val_loss did not improve from 0.41060\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4356 - accuracy: 0.7951 - val_loss: 0.4445 - val_accuracy: 0.7915\n",
      "Epoch 54/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4283 - accuracy: 0.7990\n",
      "Epoch 00054: val_loss improved from 0.41060 to 0.40884, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4279 - accuracy: 0.7992 - val_loss: 0.4088 - val_accuracy: 0.8078\n",
      "Epoch 55/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.7981\n",
      "Epoch 00055: val_loss improved from 0.40884 to 0.40861, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4281 - accuracy: 0.7979 - val_loss: 0.4086 - val_accuracy: 0.8095\n",
      "Epoch 56/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8016\n",
      "Saving weights for epoch 55\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.40861\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4249 - accuracy: 0.8016 - val_loss: 0.4116 - val_accuracy: 0.8100\n",
      "Epoch 57/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8035\n",
      "Epoch 00057: val_loss improved from 0.40861 to 0.40308, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4205 - accuracy: 0.8035 - val_loss: 0.4031 - val_accuracy: 0.8141\n",
      "Epoch 58/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8037\n",
      "Epoch 00058: val_loss did not improve from 0.40308\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4218 - accuracy: 0.8039 - val_loss: 0.4218 - val_accuracy: 0.7997\n",
      "Epoch 59/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8072\n",
      "Epoch 00059: val_loss did not improve from 0.40308\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4161 - accuracy: 0.8071 - val_loss: 0.4107 - val_accuracy: 0.8083\n",
      "Epoch 60/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4173 - accuracy: 0.8049\n",
      "Epoch 00060: val_loss did not improve from 0.40308\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4172 - accuracy: 0.8048 - val_loss: 0.4060 - val_accuracy: 0.8106\n",
      "Epoch 61/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8077\n",
      "Saving weights for epoch 60\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.40308\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4143 - accuracy: 0.8080 - val_loss: 0.4141 - val_accuracy: 0.8025\n",
      "Epoch 62/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4123 - accuracy: 0.8087\n",
      "Epoch 00062: val_loss improved from 0.40308 to 0.40253, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4120 - accuracy: 0.8088 - val_loss: 0.4025 - val_accuracy: 0.8130\n",
      "Epoch 63/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4099 - accuracy: 0.8091\n",
      "Epoch 00063: val_loss did not improve from 0.40253\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4094 - accuracy: 0.8093 - val_loss: 0.4245 - val_accuracy: 0.7992\n",
      "Epoch 64/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8115\n",
      "Epoch 00064: val_loss improved from 0.40253 to 0.38912, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4082 - accuracy: 0.8114 - val_loss: 0.3891 - val_accuracy: 0.8224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8109\n",
      "Epoch 00065: val_loss did not improve from 0.38912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4072 - accuracy: 0.8112 - val_loss: 0.4063 - val_accuracy: 0.8085\n",
      "Epoch 66/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8136\n",
      "Saving weights for epoch 65\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.38912 to 0.38887, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3999 - accuracy: 0.8136 - val_loss: 0.3889 - val_accuracy: 0.8194\n",
      "Epoch 67/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8189\n",
      "Epoch 00067: val_loss improved from 0.38887 to 0.38125, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3968 - accuracy: 0.8186 - val_loss: 0.3813 - val_accuracy: 0.8282\n",
      "Epoch 68/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8174\n",
      "Epoch 00068: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3970 - accuracy: 0.8176 - val_loss: 0.3887 - val_accuracy: 0.8222\n",
      "Epoch 69/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.8177\n",
      "Epoch 00069: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3985 - accuracy: 0.8181 - val_loss: 0.3896 - val_accuracy: 0.8171\n",
      "Epoch 70/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8185\n",
      "Epoch 00070: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3949 - accuracy: 0.8182 - val_loss: 0.4265 - val_accuracy: 0.8007\n",
      "Epoch 71/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8201\n",
      "Saving weights for epoch 70\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3922 - accuracy: 0.8200 - val_loss: 0.4131 - val_accuracy: 0.8042\n",
      "Epoch 72/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8194\n",
      "Epoch 00072: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3941 - accuracy: 0.8192 - val_loss: 0.4137 - val_accuracy: 0.8031\n",
      "Epoch 73/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3920 - accuracy: 0.8189\n",
      "Epoch 00073: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3919 - accuracy: 0.8191 - val_loss: 0.3981 - val_accuracy: 0.8141\n",
      "Epoch 74/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8230\n",
      "Epoch 00074: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3872 - accuracy: 0.8232 - val_loss: 0.4020 - val_accuracy: 0.8130\n",
      "Epoch 75/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3871 - accuracy: 0.8218\n",
      "Epoch 00075: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3869 - accuracy: 0.8218 - val_loss: 0.3827 - val_accuracy: 0.8218\n",
      "Epoch 76/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3882 - accuracy: 0.8227\n",
      "Saving weights for epoch 75\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.38125\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3882 - accuracy: 0.8227 - val_loss: 0.4046 - val_accuracy: 0.8106\n",
      "Epoch 77/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3845 - accuracy: 0.8244\n",
      "Epoch 00077: val_loss improved from 0.38125 to 0.37876, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3843 - accuracy: 0.8243 - val_loss: 0.3788 - val_accuracy: 0.8244\n",
      "Epoch 78/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8252\n",
      "Epoch 00078: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3806 - accuracy: 0.8255 - val_loss: 0.3833 - val_accuracy: 0.8222\n",
      "Epoch 79/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8264\n",
      "Epoch 00079: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3799 - accuracy: 0.8267 - val_loss: 0.3824 - val_accuracy: 0.8209\n",
      "Epoch 80/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8287\n",
      "Epoch 00080: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3746 - accuracy: 0.8287 - val_loss: 0.4094 - val_accuracy: 0.8095\n",
      "Epoch 81/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8248\n",
      "Saving weights for epoch 80\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3813 - accuracy: 0.8248 - val_loss: 0.3840 - val_accuracy: 0.8220\n",
      "Epoch 82/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8307\n",
      "Epoch 00082: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3676 - accuracy: 0.8311 - val_loss: 0.3894 - val_accuracy: 0.8209\n",
      "Epoch 83/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8331\n",
      "Epoch 00083: val_loss did not improve from 0.37876\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3687 - accuracy: 0.8333 - val_loss: 0.3825 - val_accuracy: 0.8244\n",
      "Epoch 84/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8298\n",
      "Epoch 00084: val_loss improved from 0.37876 to 0.37690, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3732 - accuracy: 0.8298 - val_loss: 0.3769 - val_accuracy: 0.8252\n",
      "Epoch 85/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3741 - accuracy: 0.8302\n",
      "Epoch 00085: val_loss did not improve from 0.37690\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3749 - accuracy: 0.8299 - val_loss: 0.3938 - val_accuracy: 0.8138\n",
      "Epoch 86/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8337\n",
      "Saving weights for epoch 85\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.37690\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3649 - accuracy: 0.8339 - val_loss: 0.4018 - val_accuracy: 0.8121\n",
      "Epoch 87/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.8349\n",
      "Epoch 00087: val_loss did not improve from 0.37690\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3695 - accuracy: 0.8346 - val_loss: 0.4209 - val_accuracy: 0.7982\n",
      "Epoch 88/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8330\n",
      "Epoch 00088: val_loss did not improve from 0.37690\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3652 - accuracy: 0.8334 - val_loss: 0.3934 - val_accuracy: 0.8169\n",
      "Epoch 89/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3659 - accuracy: 0.8334\n",
      "Epoch 00089: val_loss improved from 0.37690 to 0.37034, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3649 - accuracy: 0.8341 - val_loss: 0.3703 - val_accuracy: 0.8345\n",
      "Epoch 90/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.8398\n",
      "Epoch 00090: val_loss did not improve from 0.37034\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3591 - accuracy: 0.8402 - val_loss: 0.3773 - val_accuracy: 0.8259\n",
      "Epoch 91/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.8392\n",
      "Saving weights for epoch 90\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.37034\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3591 - accuracy: 0.8392 - val_loss: 0.3729 - val_accuracy: 0.8287\n",
      "Epoch 92/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8381\n",
      "Epoch 00092: val_loss improved from 0.37034 to 0.36505, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3583 - accuracy: 0.8383 - val_loss: 0.3651 - val_accuracy: 0.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8364\n",
      "Epoch 00093: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3609 - accuracy: 0.8363 - val_loss: 0.3892 - val_accuracy: 0.8196\n",
      "Epoch 94/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8412\n",
      "Epoch 00094: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3534 - accuracy: 0.8416 - val_loss: 0.3842 - val_accuracy: 0.8207\n",
      "Epoch 95/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8396\n",
      "Epoch 00095: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3573 - accuracy: 0.8395 - val_loss: 0.3750 - val_accuracy: 0.8267\n",
      "Epoch 96/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8388\n",
      "Saving weights for epoch 95\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3565 - accuracy: 0.8390 - val_loss: 0.3706 - val_accuracy: 0.8295\n",
      "Epoch 97/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8422\n",
      "Epoch 00097: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3448 - accuracy: 0.8426 - val_loss: 0.3693 - val_accuracy: 0.8336\n",
      "Epoch 98/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3494 - accuracy: 0.8421\n",
      "Epoch 00098: val_loss did not improve from 0.36505\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3491 - accuracy: 0.8419 - val_loss: 0.3720 - val_accuracy: 0.8310\n",
      "Epoch 99/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8432\n",
      "Epoch 00099: val_loss improved from 0.36505 to 0.36191, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3501 - accuracy: 0.8427 - val_loss: 0.3619 - val_accuracy: 0.8360\n",
      "Epoch 100/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8477\n",
      "Epoch 00100: val_loss did not improve from 0.36191\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3453 - accuracy: 0.8476 - val_loss: 0.3764 - val_accuracy: 0.8214\n",
      "Epoch 101/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8473\n",
      "Saving weights for epoch 100\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.36191 to 0.36018, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3392 - accuracy: 0.8478 - val_loss: 0.3602 - val_accuracy: 0.8356\n",
      "Epoch 102/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8470\n",
      "Epoch 00102: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3424 - accuracy: 0.8473 - val_loss: 0.3712 - val_accuracy: 0.8298\n",
      "Epoch 103/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8492\n",
      "Epoch 00103: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3383 - accuracy: 0.8486 - val_loss: 0.3676 - val_accuracy: 0.8332\n",
      "Epoch 104/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8467\n",
      "Epoch 00104: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3418 - accuracy: 0.8469 - val_loss: 0.3622 - val_accuracy: 0.8349\n",
      "Epoch 105/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8523\n",
      "Epoch 00105: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3324 - accuracy: 0.8522 - val_loss: 0.3638 - val_accuracy: 0.8338\n",
      "Epoch 106/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.8462\n",
      "Saving weights for epoch 105\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3368 - accuracy: 0.8467 - val_loss: 0.3698 - val_accuracy: 0.8302\n",
      "Epoch 107/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8510\n",
      "Epoch 00107: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3308 - accuracy: 0.8512 - val_loss: 0.3663 - val_accuracy: 0.8340\n",
      "Epoch 108/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8495\n",
      "Epoch 00108: val_loss did not improve from 0.36018\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3333 - accuracy: 0.8498 - val_loss: 0.3678 - val_accuracy: 0.8353\n",
      "Epoch 109/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8491\n",
      "Epoch 00109: val_loss improved from 0.36018 to 0.35746, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3344 - accuracy: 0.8494 - val_loss: 0.3575 - val_accuracy: 0.8371\n",
      "Epoch 110/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8519\n",
      "Epoch 00110: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3299 - accuracy: 0.8521 - val_loss: 0.3590 - val_accuracy: 0.8347\n",
      "Epoch 111/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8551\n",
      "Saving weights for epoch 110\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3285 - accuracy: 0.8553 - val_loss: 0.3590 - val_accuracy: 0.8422\n",
      "Epoch 112/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3296 - accuracy: 0.8529\n",
      "Epoch 00112: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3298 - accuracy: 0.8528 - val_loss: 0.3698 - val_accuracy: 0.8330\n",
      "Epoch 113/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8548\n",
      "Epoch 00113: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3308 - accuracy: 0.8546 - val_loss: 0.3755 - val_accuracy: 0.8276\n",
      "Epoch 114/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8543\n",
      "Epoch 00114: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3270 - accuracy: 0.8544 - val_loss: 0.3728 - val_accuracy: 0.8340\n",
      "Epoch 115/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8567\n",
      "Epoch 00115: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3224 - accuracy: 0.8568 - val_loss: 0.3816 - val_accuracy: 0.8351\n",
      "Epoch 116/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8569\n",
      "Saving weights for epoch 115\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3239 - accuracy: 0.8566 - val_loss: 0.3847 - val_accuracy: 0.8214\n",
      "Epoch 117/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8588\n",
      "Epoch 00117: val_loss did not improve from 0.35746\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3185 - accuracy: 0.8590 - val_loss: 0.3706 - val_accuracy: 0.8306\n",
      "Epoch 118/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8576\n",
      "Epoch 00118: val_loss improved from 0.35746 to 0.35639, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3212 - accuracy: 0.8575 - val_loss: 0.3564 - val_accuracy: 0.8377\n",
      "Epoch 119/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8588\n",
      "Epoch 00119: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3191 - accuracy: 0.8586 - val_loss: 0.3597 - val_accuracy: 0.8358\n",
      "Epoch 120/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8584\n",
      "Epoch 00120: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3150 - accuracy: 0.8582 - val_loss: 0.3635 - val_accuracy: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8639\n",
      "Saving weights for epoch 120\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3103 - accuracy: 0.8638 - val_loss: 0.3673 - val_accuracy: 0.8358\n",
      "Epoch 122/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8598\n",
      "Epoch 00122: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3143 - accuracy: 0.8595 - val_loss: 0.4005 - val_accuracy: 0.8123\n",
      "Epoch 123/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8559\n",
      "Epoch 00123: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3223 - accuracy: 0.8560 - val_loss: 0.3604 - val_accuracy: 0.8364\n",
      "Epoch 124/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8610\n",
      "Epoch 00124: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3122 - accuracy: 0.8612 - val_loss: 0.3910 - val_accuracy: 0.8261\n",
      "Epoch 125/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8609\n",
      "Epoch 00125: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3130 - accuracy: 0.8610 - val_loss: 0.3809 - val_accuracy: 0.8272\n",
      "Epoch 126/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8648\n",
      "Saving weights for epoch 125\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3040 - accuracy: 0.8645 - val_loss: 0.3890 - val_accuracy: 0.8173\n",
      "Epoch 127/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8656\n",
      "Epoch 00127: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3025 - accuracy: 0.8659 - val_loss: 0.3661 - val_accuracy: 0.8375\n",
      "Epoch 128/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8712\n",
      "Epoch 00128: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2979 - accuracy: 0.8712 - val_loss: 0.3766 - val_accuracy: 0.8323\n",
      "Epoch 129/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8678\n",
      "Epoch 00129: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3023 - accuracy: 0.8679 - val_loss: 0.3637 - val_accuracy: 0.8328\n",
      "Epoch 130/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8644\n",
      "Epoch 00130: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3048 - accuracy: 0.8648 - val_loss: 0.3693 - val_accuracy: 0.8338\n",
      "Epoch 131/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8662\n",
      "Saving weights for epoch 130\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3049 - accuracy: 0.8663 - val_loss: 0.3736 - val_accuracy: 0.8323\n",
      "Epoch 132/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8706\n",
      "Epoch 00132: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3005 - accuracy: 0.8706 - val_loss: 0.3631 - val_accuracy: 0.8323\n",
      "Epoch 133/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8695\n",
      "Epoch 00133: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2998 - accuracy: 0.8697 - val_loss: 0.3585 - val_accuracy: 0.8390\n",
      "Epoch 134/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8735\n",
      "Epoch 00134: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2935 - accuracy: 0.8732 - val_loss: 0.3770 - val_accuracy: 0.8300\n",
      "Epoch 135/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8690\n",
      "Epoch 00135: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2962 - accuracy: 0.8693 - val_loss: 0.3650 - val_accuracy: 0.8351\n",
      "Epoch 136/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8719\n",
      "Saving weights for epoch 135\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2975 - accuracy: 0.8720 - val_loss: 0.3625 - val_accuracy: 0.8399\n",
      "Epoch 137/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8728\n",
      "Epoch 00137: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2893 - accuracy: 0.8728 - val_loss: 0.3702 - val_accuracy: 0.8317\n",
      "Epoch 138/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8713\n",
      "Epoch 00138: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2952 - accuracy: 0.8715 - val_loss: 0.3831 - val_accuracy: 0.8244\n",
      "Epoch 139/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8723\n",
      "Epoch 00139: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2922 - accuracy: 0.8724 - val_loss: 0.3818 - val_accuracy: 0.8263\n",
      "Epoch 140/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.8744\n",
      "Epoch 00140: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2886 - accuracy: 0.8739 - val_loss: 0.3707 - val_accuracy: 0.8360\n",
      "Epoch 141/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8727\n",
      "Saving weights for epoch 140\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2884 - accuracy: 0.8728 - val_loss: 0.3868 - val_accuracy: 0.8248\n",
      "Epoch 142/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8737\n",
      "Epoch 00142: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2893 - accuracy: 0.8738 - val_loss: 0.3679 - val_accuracy: 0.8358\n",
      "Epoch 143/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.8771\n",
      "Epoch 00143: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2842 - accuracy: 0.8772 - val_loss: 0.3729 - val_accuracy: 0.8263\n",
      "Epoch 144/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.8784\n",
      "Epoch 00144: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2840 - accuracy: 0.8786 - val_loss: 0.3719 - val_accuracy: 0.8291\n",
      "Epoch 145/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.8784\n",
      "Epoch 00145: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2793 - accuracy: 0.8786 - val_loss: 0.3644 - val_accuracy: 0.8362\n",
      "Epoch 146/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8744\n",
      "Saving weights for epoch 145\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2820 - accuracy: 0.8744 - val_loss: 0.4105 - val_accuracy: 0.8196\n",
      "Epoch 147/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.8754\n",
      "Epoch 00147: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2897 - accuracy: 0.8753 - val_loss: 0.3620 - val_accuracy: 0.8373\n",
      "Epoch 148/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8804\n",
      "Epoch 00148: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2770 - accuracy: 0.8801 - val_loss: 0.3608 - val_accuracy: 0.8379\n",
      "Epoch 149/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.8816\n",
      "Epoch 00149: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2757 - accuracy: 0.8817 - val_loss: 0.3727 - val_accuracy: 0.8321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.8773\n",
      "Epoch 00150: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2797 - accuracy: 0.8774 - val_loss: 0.3724 - val_accuracy: 0.8293\n",
      "Epoch 151/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8792\n",
      "Saving weights for epoch 150\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2779 - accuracy: 0.8793 - val_loss: 0.3671 - val_accuracy: 0.8321\n",
      "Epoch 152/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8864\n",
      "Epoch 00152: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2664 - accuracy: 0.8867 - val_loss: 0.3967 - val_accuracy: 0.8259\n",
      "Epoch 153/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2740 - accuracy: 0.8802\n",
      "Epoch 00153: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2736 - accuracy: 0.8800 - val_loss: 0.3717 - val_accuracy: 0.8317\n",
      "Epoch 154/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.8824\n",
      "Epoch 00154: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2699 - accuracy: 0.8826 - val_loss: 0.3973 - val_accuracy: 0.8188\n",
      "Epoch 155/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.8845\n",
      "Epoch 00155: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2696 - accuracy: 0.8846 - val_loss: 0.3836 - val_accuracy: 0.8216\n",
      "Epoch 156/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.8843\n",
      "Saving weights for epoch 155\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2644 - accuracy: 0.8846 - val_loss: 0.3663 - val_accuracy: 0.8358\n",
      "Epoch 157/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8855\n",
      "Epoch 00157: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2677 - accuracy: 0.8854 - val_loss: 0.3612 - val_accuracy: 0.8399\n",
      "Epoch 158/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.8880\n",
      "Epoch 00158: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2609 - accuracy: 0.8881 - val_loss: 0.3830 - val_accuracy: 0.8282\n",
      "Epoch 159/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8817\n",
      "Epoch 00159: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2673 - accuracy: 0.8814 - val_loss: 0.3942 - val_accuracy: 0.8214\n",
      "Epoch 160/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.8897\n",
      "Epoch 00160: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2578 - accuracy: 0.8898 - val_loss: 0.3623 - val_accuracy: 0.8401\n",
      "Epoch 161/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.8869\n",
      "Saving weights for epoch 160\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2617 - accuracy: 0.8868 - val_loss: 0.3759 - val_accuracy: 0.8304\n",
      "Epoch 162/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.8883\n",
      "Epoch 00162: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2572 - accuracy: 0.8886 - val_loss: 0.3646 - val_accuracy: 0.8381\n",
      "Epoch 163/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.8891\n",
      "Epoch 00163: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2614 - accuracy: 0.8890 - val_loss: 0.3792 - val_accuracy: 0.8278\n",
      "Epoch 164/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.8907\n",
      "Epoch 00164: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2585 - accuracy: 0.8907 - val_loss: 0.3838 - val_accuracy: 0.8244\n",
      "Epoch 165/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.8901\n",
      "Epoch 00165: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2510 - accuracy: 0.8904 - val_loss: 0.3922 - val_accuracy: 0.8298\n",
      "Epoch 166/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.8868\n",
      "Saving weights for epoch 165\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2636 - accuracy: 0.8873 - val_loss: 0.4100 - val_accuracy: 0.8188\n",
      "Epoch 167/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.8927\n",
      "Epoch 00167: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2517 - accuracy: 0.8929 - val_loss: 0.3733 - val_accuracy: 0.8383\n",
      "Epoch 168/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.8950\n",
      "Epoch 00168: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2459 - accuracy: 0.8948 - val_loss: 0.3687 - val_accuracy: 0.8332\n",
      "Epoch 169/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.8913\n",
      "Epoch 00169: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2529 - accuracy: 0.8914 - val_loss: 0.3760 - val_accuracy: 0.8356\n",
      "Epoch 170/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.8933\n",
      "Epoch 00170: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2491 - accuracy: 0.8936 - val_loss: 0.3696 - val_accuracy: 0.8416\n",
      "Epoch 171/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8938\n",
      "Saving weights for epoch 170\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2501 - accuracy: 0.8942 - val_loss: 0.3957 - val_accuracy: 0.8246\n",
      "Epoch 172/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.8945\n",
      "Epoch 00172: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2473 - accuracy: 0.8948 - val_loss: 0.3716 - val_accuracy: 0.8394\n",
      "Epoch 173/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.8978\n",
      "Epoch 00173: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2416 - accuracy: 0.8977 - val_loss: 0.3621 - val_accuracy: 0.8422\n",
      "Epoch 174/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.8967\n",
      "Epoch 00174: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2432 - accuracy: 0.8962 - val_loss: 0.3934 - val_accuracy: 0.8244\n",
      "Epoch 175/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.8946\n",
      "Epoch 00175: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2482 - accuracy: 0.8942 - val_loss: 0.3892 - val_accuracy: 0.8272\n",
      "Epoch 176/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.8982\n",
      "Saving weights for epoch 175\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2378 - accuracy: 0.8984 - val_loss: 0.3709 - val_accuracy: 0.8351\n",
      "Epoch 177/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.8930\n",
      "Epoch 00177: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2446 - accuracy: 0.8931 - val_loss: 0.4164 - val_accuracy: 0.8149\n",
      "Epoch 178/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9000\n",
      "Epoch 00178: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2391 - accuracy: 0.9001 - val_loss: 0.3788 - val_accuracy: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.8970\n",
      "Epoch 00179: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2424 - accuracy: 0.8972 - val_loss: 0.3812 - val_accuracy: 0.8366\n",
      "Epoch 180/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.8990\n",
      "Epoch 00180: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2395 - accuracy: 0.8993 - val_loss: 0.3691 - val_accuracy: 0.8368\n",
      "Epoch 181/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9035\n",
      "Saving weights for epoch 180\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2302 - accuracy: 0.9035 - val_loss: 0.3982 - val_accuracy: 0.8235\n",
      "Epoch 182/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.8984\n",
      "Epoch 00182: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2338 - accuracy: 0.8986 - val_loss: 0.3843 - val_accuracy: 0.8295\n",
      "Epoch 183/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9012\n",
      "Epoch 00183: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2324 - accuracy: 0.9015 - val_loss: 0.3998 - val_accuracy: 0.8201\n",
      "Epoch 184/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9012\n",
      "Epoch 00184: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2316 - accuracy: 0.9015 - val_loss: 0.3830 - val_accuracy: 0.8310\n",
      "Epoch 185/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9029\n",
      "Epoch 00185: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2280 - accuracy: 0.9032 - val_loss: 0.3699 - val_accuracy: 0.8390\n",
      "Epoch 186/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9039\n",
      "Saving weights for epoch 185\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2281 - accuracy: 0.9039 - val_loss: 0.3736 - val_accuracy: 0.8383\n",
      "Epoch 187/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9038\n",
      "Epoch 00187: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2310 - accuracy: 0.9040 - val_loss: 0.4120 - val_accuracy: 0.8199\n",
      "Epoch 188/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9061\n",
      "Epoch 00188: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2222 - accuracy: 0.9062 - val_loss: 0.3874 - val_accuracy: 0.8250\n",
      "Epoch 189/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.8991\n",
      "Epoch 00189: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2278 - accuracy: 0.8994 - val_loss: 0.4103 - val_accuracy: 0.8252\n",
      "Epoch 190/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.9079\n",
      "Epoch 00190: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2230 - accuracy: 0.9081 - val_loss: 0.3887 - val_accuracy: 0.8306\n",
      "Epoch 191/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9042\n",
      "Saving weights for epoch 190\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2233 - accuracy: 0.9044 - val_loss: 0.3816 - val_accuracy: 0.8317\n",
      "Epoch 192/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9053\n",
      "Epoch 00192: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2222 - accuracy: 0.9056 - val_loss: 0.3858 - val_accuracy: 0.8315\n",
      "Epoch 193/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9088\n",
      "Epoch 00193: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2187 - accuracy: 0.9085 - val_loss: 0.3847 - val_accuracy: 0.8368\n",
      "Epoch 194/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9046\n",
      "Epoch 00194: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2205 - accuracy: 0.9048 - val_loss: 0.3869 - val_accuracy: 0.8244\n",
      "Epoch 195/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9043\n",
      "Epoch 00195: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2252 - accuracy: 0.9045 - val_loss: 0.3834 - val_accuracy: 0.8298\n",
      "Epoch 196/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9102\n",
      "Saving weights for epoch 195\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2132 - accuracy: 0.9103 - val_loss: 0.3804 - val_accuracy: 0.8356\n",
      "Epoch 197/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9088\n",
      "Epoch 00197: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2181 - accuracy: 0.9092 - val_loss: 0.3911 - val_accuracy: 0.8310\n",
      "Epoch 198/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9106\n",
      "Epoch 00198: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2183 - accuracy: 0.9107 - val_loss: 0.4204 - val_accuracy: 0.8156\n",
      "Epoch 199/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9090\n",
      "Epoch 00199: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2144 - accuracy: 0.9091 - val_loss: 0.3895 - val_accuracy: 0.8267\n",
      "Epoch 200/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9116\n",
      "Epoch 00200: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2131 - accuracy: 0.9117 - val_loss: 0.3822 - val_accuracy: 0.8274\n",
      "Epoch 201/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.9132\n",
      "Saving weights for epoch 200\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2128 - accuracy: 0.9131 - val_loss: 0.3825 - val_accuracy: 0.8345\n",
      "Epoch 202/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9137\n",
      "Epoch 00202: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2105 - accuracy: 0.9139 - val_loss: 0.3937 - val_accuracy: 0.8308\n",
      "Epoch 203/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.9097\n",
      "Epoch 00203: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2122 - accuracy: 0.9096 - val_loss: 0.4007 - val_accuracy: 0.8209\n",
      "Epoch 204/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9130\n",
      "Epoch 00204: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2087 - accuracy: 0.9132 - val_loss: 0.4052 - val_accuracy: 0.8239\n",
      "Epoch 205/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9144\n",
      "Epoch 00205: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2051 - accuracy: 0.9146 - val_loss: 0.3984 - val_accuracy: 0.8295\n",
      "Epoch 206/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9091\n",
      "Saving weights for epoch 205\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2181 - accuracy: 0.9093 - val_loss: 0.3835 - val_accuracy: 0.8371\n",
      "Epoch 207/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9168\n",
      "Epoch 00207: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2037 - accuracy: 0.9168 - val_loss: 0.3919 - val_accuracy: 0.8343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9148\n",
      "Epoch 00208: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2048 - accuracy: 0.9147 - val_loss: 0.3933 - val_accuracy: 0.8293\n",
      "Epoch 209/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9186\n",
      "Epoch 00209: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1967 - accuracy: 0.9191 - val_loss: 0.4282 - val_accuracy: 0.8276\n",
      "Epoch 210/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9137\n",
      "Epoch 00210: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2073 - accuracy: 0.9142 - val_loss: 0.3977 - val_accuracy: 0.8317\n",
      "Epoch 211/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9198\n",
      "Saving weights for epoch 210\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1918 - accuracy: 0.9199 - val_loss: 0.3984 - val_accuracy: 0.8280\n",
      "Epoch 212/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9163\n",
      "Epoch 00212: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1985 - accuracy: 0.9167 - val_loss: 0.4198 - val_accuracy: 0.8246\n",
      "Epoch 213/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9169\n",
      "Epoch 00213: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2030 - accuracy: 0.9171 - val_loss: 0.4572 - val_accuracy: 0.8119\n",
      "Epoch 214/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9181\n",
      "Epoch 00214: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2027 - accuracy: 0.9182 - val_loss: 0.4234 - val_accuracy: 0.8207\n",
      "Epoch 215/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9199\n",
      "Epoch 00215: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1961 - accuracy: 0.9203 - val_loss: 0.4204 - val_accuracy: 0.8222\n",
      "Epoch 216/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9153\n",
      "Saving weights for epoch 215\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2041 - accuracy: 0.9155 - val_loss: 0.3983 - val_accuracy: 0.8272\n",
      "Epoch 217/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9179\n",
      "Epoch 00217: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1992 - accuracy: 0.9181 - val_loss: 0.4140 - val_accuracy: 0.8270\n",
      "Epoch 218/1000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9192\n",
      "Epoch 00218: val_loss did not improve from 0.35639\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1982 - accuracy: 0.9191 - val_loss: 0.4010 - val_accuracy: 0.8328\n",
      "Epoch 00218: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 256]\n",
    "model_state_by_batch_size_trial_1 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    mc = keras.callbacks.ModelCheckpoint(\n",
    "        'pickled_objects/batch_size_{}_best_weights_trial_1.h5'.format(batch_size),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    model_state_by_batch_size_trial_1[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=1000,\n",
    "        extra_callbacks=[es, mc],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml_utils.plot_loss_by_param(model_state_by_batch_size_trial_1, 'batch size', 'batch_size_loss_trial_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed the large batch size trains faster (~8s vs ~13s). However, the larger batch size actually achieves a better validation loss than the small batch size! One possible cause is that the learning rate is too high - in general, the accepted rule of thumb is to scale learning rate linearly with batch size. Let's try a smaller learning rate and see.\n",
    "\n",
    "But before we do that, let's also evaluate two claims made by the paper linked above:\n",
    "1) Large batch size runs converge to weights that tend to be closer to the initial weights.\n",
    "2) Large batch size runs find sharp minimizers (minima that vary sharply as you move away from them), whereas small batch sizes find flat minimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_state(model_state_by_batch_size_trial_1, 'model_state_by_batch_size_trial_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml_utils.build_model()\n",
    "initial_weights = model.get_weights()\n",
    "model.load_weights('pickled_objects/batch_size_32_best_weights_trial_1.h5')\n",
    "batch_32_weights = model.get_weights()\n",
    "model.load_weights('pickled_objects/batch_size_256_best_weights_trial_1.h5')\n",
    "batch_256_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: 0\n",
      "\tDistance from batch 32 to initial:  7.0809193\n",
      "\tDistance from batch 256 to initial:  4.9866548\n",
      "Parameter set: 1\n",
      "\tDistance from batch 32 to initial:  1.2448872\n",
      "\tDistance from batch 256 to initial:  0.72424906\n",
      "Parameter set: 2\n",
      "\tDistance from batch 32 to initial:  3.9699647\n",
      "\tDistance from batch 256 to initial:  3.505941\n",
      "Parameter set: 3\n",
      "\tDistance from batch 32 to initial:  1.7034668\n",
      "\tDistance from batch 256 to initial:  0.6810975\n",
      "Parameter set: 4\n",
      "\tDistance from batch 32 to initial:  6.8601875\n",
      "\tDistance from batch 256 to initial:  3.4972727\n",
      "Parameter set: 5\n",
      "\tDistance from batch 32 to initial:  1.5244346\n",
      "\tDistance from batch 256 to initial:  0.7644117\n",
      "Parameter set: 6\n",
      "\tDistance from batch 32 to initial:  8.274157\n",
      "\tDistance from batch 256 to initial:  4.067089\n",
      "Parameter set: 7\n",
      "\tDistance from batch 32 to initial:  2.392697\n",
      "\tDistance from batch 256 to initial:  1.2040703\n",
      "Parameter set: 8\n",
      "\tDistance from batch 32 to initial:  11.903623\n",
      "\tDistance from batch 256 to initial:  5.58949\n",
      "Parameter set: 9\n",
      "\tDistance from batch 32 to initial:  2.611147\n",
      "\tDistance from batch 256 to initial:  1.207383\n",
      "Parameter set: 10\n",
      "\tDistance from batch 32 to initial:  16.674477\n",
      "\tDistance from batch 256 to initial:  6.0734477\n",
      "Parameter set: 11\n",
      "\tDistance from batch 32 to initial:  1.5521587\n",
      "\tDistance from batch 256 to initial:  0.63440675\n",
      "Parameter set: 12\n",
      "\tDistance from batch 32 to initial:  6.00555\n",
      "\tDistance from batch 256 to initial:  1.8845519\n",
      "Parameter set: 13\n",
      "\tDistance from batch 32 to initial:  2.6692934\n",
      "\tDistance from batch 256 to initial:  0.9389192\n",
      "Parameter set: 14\n",
      "\tDistance from batch 32 to initial:  1.0635282\n",
      "\tDistance from batch 256 to initial:  1.4008852\n",
      "Parameter set: 15\n",
      "\tDistance from batch 32 to initial:  0.6041774\n",
      "\tDistance from batch 256 to initial:  0.25417367\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(initial_weights)):\n",
    "    print(\"Parameter set:\", i)\n",
    "    print(\"\\tDistance from batch 32 to initial: \", np.linalg.norm(batch_32_weights[i] - initial_weights[i]))\n",
    "    print(\"\\tDistance from batch 256 to initial: \", np.linalg.norm(batch_256_weights[i] - initial_weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_initial_weights = np.concatenate([x.flatten() for x in initial_weights])\n",
    "flattened_32_weights = np.concatenate([x.flatten() for x in batch_32_weights])\n",
    "flattened_256_weights = np.concatenate([x.flatten() for x in batch_256_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from flattened batch 32 weights to initial weights:  25.83745\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Distance from flattened batch 32 weights to initial weights: \",\n",
    "    np.linalg.norm(flattened_32_weights - flattened_initial_weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from flattened batch 256 weights to initial weights:  12.0583\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Distance from flattened batch 256 weights to initial weights: \",\n",
    "    np.linalg.norm(flattened_256_weights - flattened_initial_weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed, the larger batch size converged to weights that were closer to the initial weights (distance of 12 vs 25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing alpha:  -1.0\n",
      "     19/Unknown - 1s 40ms/step - loss: 3.9624 - accuracy: 0.5969\n",
      "Testing alpha:  -0.868421052631579\n",
      "     19/Unknown - 1s 36ms/step - loss: 2.4595 - accuracy: 0.6101\n",
      "Testing alpha:  -0.736842105263158\n",
      "     19/Unknown - 1s 36ms/step - loss: 1.5223 - accuracy: 0.6305\n",
      "Testing alpha:  -0.6052631578947368\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.9792 - accuracy: 0.6808\n",
      "Testing alpha:  -0.4736842105263158\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.6811 - accuracy: 0.7330\n",
      "Testing alpha:  -0.3421052631578948\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.5017 - accuracy: 0.7863\n",
      "Testing alpha:  -0.21052631578947367\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.4040 - accuracy: 0.8201\n",
      "Testing alpha:  -0.07894736842105265\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.3639 - accuracy: 0.8310\n",
      "Testing alpha:  0.05263157894736836\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.3717 - accuracy: 0.8201\n",
      "Testing alpha:  0.18421052631578938\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.4202 - accuracy: 0.8014\n",
      "Testing alpha:  0.3157894736842104\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.5002 - accuracy: 0.7676\n",
      "Testing alpha:  0.4473684210526314\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.5863 - accuracy: 0.7165\n",
      "Testing alpha:  0.5789473684210527\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.6426 - accuracy: 0.6629\n",
      "Testing alpha:  0.7105263157894737\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.6419 - accuracy: 0.6636\n",
      "Testing alpha:  0.8421052631578947\n",
      "     19/Unknown - 1s 35ms/step - loss: 0.5241 - accuracy: 0.7496\n",
      "Testing alpha:  0.9736842105263157\n",
      "     19/Unknown - 1s 35ms/step - loss: 0.3668 - accuracy: 0.8332\n",
      "Testing alpha:  1.1052631578947367\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.3887 - accuracy: 0.8343\n",
      "Testing alpha:  1.236842105263158\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.7635 - accuracy: 0.7790\n",
      "Testing alpha:  1.3684210526315788\n",
      "     19/Unknown - 1s 36ms/step - loss: 2.4680 - accuracy: 0.7236\n",
      "Testing alpha:  1.5\n",
      "     19/Unknown - 1s 36ms/step - loss: 7.2103 - accuracy: 0.7079"
     ]
    }
   ],
   "source": [
    "alpha_values = np.linspace(-1, 1.5, 20)\n",
    "losses = []\n",
    "for alpha in alpha_values:\n",
    "    print(\"\\nTesting alpha: \", alpha)\n",
    "    target_weights = [batch_256_weights[i]*alpha + batch_32_weights[i]*(1-alpha) for i in range(len(batch_32_weights))]\n",
    "    model.set_weights(target_weights)\n",
    "    loss, accuracy = model.evaluate(validation)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwcdZ3/8ddn7sydZCZ3IAmEhCQYCBGDgIAIEpRDwAO8YFmQ3dWfrrqu1yog+9vVn+vtqoiKByIqoIgGROUQ5DDhSsIdSEhmMpnJJD2Z+/z8/qjq0AxzdE+mu6Z73s/Hox59VHXVp7qr61P1re+3vubuiIiIpCIv6gBERCT7KHmIiEjKlDxERCRlSh4iIpIyJQ8REUmZkoeIiKRMyWMEZrbAzNzMCjKwrLvN7B/TvZwRln+/mR2V5LRuZoeO97TpYGZnmdkvMri8K8zsZ8OMO8nMdmQojq1m9qZMLCtVZvYBM/taktNeZ2ZXj/e06WJmD5vZ8nGc33fN7D/Ge9rxMObkYWYXmtl6M2szs51mts7Mjh/P4LJdunacZrYs/O73hsOfzGxZwvh/M7NNZtZqZi+a2b+NMr8zgVZ3f3S8Y80EM5tmZreYWbuZbTOzC+Pj3P1WYIWZvSbCEA9YOg9kzOwtZnafmcXMrMHMvm9mFQnjrzOznvC/Hh/yE8aXmtn/mtluM2sxs3tHWFYR8Fng/433emSCma0wszvCdR2qkdyXgavGa3nufrm7f2G8px0PY0oeZvZR4GvA/wVmAgcB/wucPX6hjb9MnEFkSD1wPjANqAFuBRKPrg14HzAVOB34oJm9a4T5XQ78ND2hZsS3gR6CbfHdwHcGHf3dAFwWRWBZogq4GpgDHA7M49U79y+5e3nC0J8w7hqCbfHw8PFfR1jW2cDT7l43btFnVi/wS+CSYcbfCpxsZrMzF1JE3D2lgWBDawPePsI0xQTJpT4cvgYUh+NOAnYAnwAagZ3AOcAZwLPAHuDTCfO6Avg1cCPQCjwCrEwY/0lgSzjuSeBtCeMuAu4HvhrO9+rw/X8AngL2AncABw+zHgsAJ9jx1Iexfixh/DHAA0AsHPctoCgcd2/42fbw+3pn+P7ZwGPAvjDu08P37wa+EMbbCvwRqEni9ygA/gXoGGGabwDfHGZcEdAJzEtmvcLxDhwaPr8O+C5wZxj3PYnfZzjt5cBz4ff9bcDCcYcAfwGagd3A9UB1ittjGUHiOCzhvZ8C/53w+jjgxRHmMdo2dB/BEeVe4EVgbcL4heE6t4bfwbeAnw2znJMItv1Ph+u7FXh3wvi3AI+G28Z24IqEcS+F32VbOBwbvn8pwbYcj31V+P5W4OPAE0ALwf+nJMnv9FxgY8Lr6wj/O0NMuySMtzLJef8Q+Oyg934FNIRx3gssH2rZSXx/14Xb1+/D7+Mh4JCE8V8Pv9d9wAbghFS2tUExHwr4MOPuBN4/zLiLeHmfFANeAF4fvr+dYJ/4fh95/T/Gy/vOi0eZNtn97Ij/+SHXZQxf2ulAH1AwwjRXAQ8CM4Ba4G/AFxJWqg/4HFBIsPE3AT8HKoDlQBewKJz+CoJsf344/ccJ/sCF4fi3Exwx5QHvJNhZz074ofqADxHsZKeEX+DzBEdJBQSn0H8bZj0WEPxhbyDYSR0RxvqmcPzRwJpwPgsI/sQfSfj8/p1swg/UApwaxjsXWBqOu5tgB3ZYGOfdJOwAh4kvFq7fAIP+kAnTGMEO6fJhxi8H2ge9l/R6EWywrcAbCA4avg7cN2ja24BqgjPUJl5OmIeG30VxuJ3cC3wt4bO3hes41HBbOM1RQOeg+D8O/C7h9bQwjiF3cElsQ70E22k+8E8EBxLxBPgA8JVwHd4QfhcjJY++hOlPDJe1JGH8EWEcrwF2AecM2hYLBsVdB7w2/J0PJUzcBDvWh8P1mhb+hkNuA0PE+TXgF4N2SnvCYQNwXsK49wEbCXaGu8Pn540w778z6MCT4GCugpcPOh8bYYc40vcXj/MYgm33+kHr8R5gejjuYwQJqyQcdyHDb2sx4KBBMY+UPL4BfGWYcReF63BxuD1dTXBg8O1wnU4Lt6HyEdb/KoJ94RlABzB1hGmT3c+O+J8fcl2S2ZgGrfy7gYZRptkCnJHw+s3A1oSV6gTyw9cVBH+K1yVMv4GX/zRXAA8mjMsjyIxDHjUQHNWfnfBDvTRo/DrgkkHz62CIsw9e/sMuTXjvS8APhln2R4BbEl4PTh7fA746zGfvJiEBAP8M3J7E71EWTvuWYcZfCTxOeOY3xPjjkvg9h12vcINN/IOWA/3A/IRpj08Y/0vgk8Ms5xzg0RS3xxMGxx/+Ue5OeF0YxnFQkvMcvA09nzCuNJzXLIJk2AeUJYz/OaMnj8Tpfwn8xzDTfy2+vTB08rgD+PAwn90KvGfQdvvdJNb9VIIzrMQzuVW8vNM9g2Dndlw47tNhXFcQnMWeSHBmdPgw83+O8OBhmPHV4fyqEravwTvEIb+/cNprE8adQVBENtyy9pJQipHidjdS8vhP4IfDjLsIeC7h9RHh+s5MeK8ZOHKY9e8ctA00AmtGmDap/ewQcb7iPz/UMJZrHs1AzSjXD+YA2xJebwvf2z8Pf7nMtDN83JUwvpNgJxS3Pf7E3QcITsfmAJjZ+8zssfBiXwxYQXAd4FWfDR0MfD1h+j0ER21zR1ifxHnsXxczO8zMbgsvMu4juAZUM9QMQvMJEutwGhKed/DK72BI7t5OUGz0EzObkTjOzD5IcGT4FnfvHmYWewk2rMTPpbpeib9PG8F3mvh7D7leZjbDzH5hZnXhcn42ynKG0gZUDnqvkmAHFxdfv9hQM0hiG9ofv7t3hE/LCdZxb/gbxCVu90MZavr49vQ6M7vLzJrMrIWguC9j25OZrSFIfue7+7Px9939EXdvdvc+d/8DwRH9ueHoToIzs6vdvcfd7wHuIjiCHsortjczyzez/zazLeE2sDUcNdx6D/v9hYZdZzP7mJk9FV7UjxEUwae6vSWjgmG2tdDgfR3uPtL+L1Gzu/clvB7pd016PzuG//yYkscDBKc754wwTT3BTjruoPC9sZoff2JmeQQX9OrN7GDg+8AHgenuXg1sIkgGcT5oXtuBD7h7dcIwxd3/lszyB63Ld4CngcXuXklwFGYMbztBOf94yyM4It6fAM3sHwjK8k9x95Gqhz4XTG6JyTPV9Ur8fcoJikmS+b3/i+D3eU24nPckLieswdc2zLAunOxZoMDMFifMdyWwOeH14QRnvvsGB5DkNjScncBUMytLeO+gUT4z1PTx7+rnBBdc57t7FcFBQTyOwdsxjOP2ZEE17VuBf3D3P48yuSfE9USKi3qCoGg27kKC64BvItiZL4iHNMznR/r+hmVmJwD/DryDoJinmqAI2cLx7x5hW2szs9F+10SHE5ztZ5NU//OpJw93byEoR/u2mZ0TVtMrNLO1ZvalcLIbgM+aWa2Z1YTTD1n3PUlHm9m54dnOR4BugmsqZQQbchOAmV1McNQ4ku8Cn4rXxjGzKjN7+yif+Y9wPZcTlFXeGL5fQXDxrc3MlhKUhyfaBSxKeP0D4GIzO8XM8sxsbvi5lJjZqWZ2VHjUVklQBryXoJwSM3s3wZHDqe7+wkjzcvde4E8ExQ1xo63XYGeY2fFhNcwvAA+5++AzvqFUEJw5xMLk9Yoqxe6+1l9ZwydxWBtO0w7cDFxlZmVmdhzBziix9tiJBMWVQxnLNhSPbxuwHrjSzIosqKp+ZhIfjU9/AvBWggvGEHwfe9y9y8yOIdixxjURXNtK3J6uBT5uZkdb4NAwGabEzFYAtwMfcvffDTH+fDMrD7fZ0wiS/K3h6HsJyuw/ZWYF4fd/EkGR2lD+wKu3tW6CEo1Sgu12NMN9fyOpICjyaiI42PgcCWes7n79CNtaubu/FH4XZmYlBEV0mFmJmRXH5xM+P5rgonk2SfU/P7aquu7+FeCjBBebmwiOgD4I/Cac5GqCP9UTBBfQHgnfG6vfElzI3Au8FzjX3Xvd/UngfwjOhnYRlB/eP0rstwBfBH4Rnp5tAtaOsvx7CC6y/xn4srv/MXz/4wR/8FaCo9cbB33uCuDHYXHIO9z9YYLk81WCo557eOUZWrKqCRJ0C0GxxaEE5chd4firCcqo/55w5PTdEeb3PYLvNW609Rrs58DnCYqrjia4LpaMKwnK01sIasjcnOTnBvtngkoGjQTfyz+5e+KZxwUE6/gqY9mGBrkQeB3Bun8e+Mko0zcQbMf1BMU/l7v70wnrcZWZtRIccP0yIc4OgrL0+8PtaY27/yp87+cEv9VvCM76UvUxggoLP0jYXhK/vw8TXJiPEVThvdTd7w7j6iVI1mcQ/I7fB96XsE6D/Q5YambxoqafEBQ91RHUFntwlFhH+v5GcgfBAcSz4fK6eHWRdjIOJijuiX8/ncAzCePPIrjediAlLVFI9T+/v8bIhGVmVxBcnH1P1LHkMjO7j+DIM6WGgmZ2HbDD3T+blsAOkAUNIN/r7u+IOhYJmNllwDJ3/0iKnzuJoDLCvLQENg7M7CGCCjmboo4l3XKl0ZwcIHfPybsDhMUwryqKkei4+zVRx5Au7v66qGPIFN3bSkREUjbhi61ERGTi0ZmHiIikbEJd86ipqfEFCxZEHYaISNbYsGHDbnevzfRyJ1TyWLBgAevXr486DBGRrGFmo93VIC1UbCUiIilT8hARkZQpeYiISMqUPEREJGVKHiIikjIlDxERSZmSh4iIpEzJQ0QkQnc+uYvv3TNSh5ATk5KHiEiE1m3ayU8eiKSd3wFR8hARiVB9rJM51SVRh5EyJQ8RkQjVx7qYUz0l6jBSlrbkYWZLzOyxhGGfmaXUc5iISC4bGHB2tnRmZfJI240R3f0Z4EgAM8sn6KP4lnQtT0Qk2+xu66a337MyeWSq2OoUYIu7Z99VIRGRNKmLdQIwV9c8hvUu4IahRpjZZWa23szWNzU1ZSgcEZHo1ce6AHTmMRQzKwLOAn411Hh3v8bdV7v76trajPdnIiISmfrwzGN2lZLHUNYCj7j7rgwsS0Qka9TFOikvLqCyZEL1y5eUTCSPCximyEpEZDKLt/Ews6hDSVlak4eZlQKnAjenczkiItmoPkur6UKak4e7d7j7dHdvSedyRESyUbY2EAS1MBcRiURnTz972nuYq+QhIiLJqm8Jalpl432tQMlDRCQSO+NtPLKwmi4oeYiIRCLexkPXPEREJGl1sU7MYFaViq1ERCRJ9bFOZlaUUJifnbvh7IxaRCTLBW08svOsA5Q8REQikc1tPEDJQ0Qk49ydulhn1rbxACUPEZGMa27voadvQGceIiKSvGyvpgtKHiIiGfdy8tAFcxERSVJd2Lpc1zxERCRp9bFOSovyqZpSGHUoY6bkISKSYUEnUFOyshOoOCUPEZEMq491MjtLb0sSp+QhIpJhdbGurL7eAUoeIiIZ1dXbz+627qyupgtKHiIiGdXQEvbjoeQhIiLJyoU2HpDm5GFm1Wb2azN72syeMrNj07k8EZGJri5MHtl+zaMgzfP/OnC7u59vZkVAaZqXJyIyodWHDQSztROouLQlDzOrBN4AXATg7j1AT7qWJyKSDXa2dFJbUUxxQX7UoRyQdBZbLQKagB+Z2aNmdq2ZlQ2eyMwuM7P1Zra+qakpjeGIiESvLmwgmO3SmTwKgFXAd9z9KKAd+OTgidz9Gndf7e6ra2tr0xiOiEj06mOdzM3yi+WQ3uSxA9jh7g+Fr39NkExERCYldw96EKzSmcew3L0B2G5mS8K3TgGeTNfyREQmulhHL529/TlRbJXu2lYfAq4Pa1q9AFyc5uWJiExYdTnQCVRcWpOHuz8GrE7nMkREskV9jrTxALUwFxHJmFxpXQ5KHiIiGVPf0kVxQR7TyoqiDuWAKXmIiGRIXayTuVneCVSckoeISIbU50gDQVDyEBHJmCB5ZP/1DlDyEBHJiJ6+ARpbs78TqDglDxGRDNi1rwt3cqJ1OSh5iIhkRC41EAQlDxGRjMilNh6g5CEikhH1OvMQEZFU1cW6mF5WRElhdncCFafkISKSAbnUxgOUPEREMiKX2niAkoeISNoFnUDpzENERFKwr6uP9p7+nLgVe5ySh4hImuVaTStQ8hARSTslDxERSVmuNRAEJQ8RkbSri3VRlJ9HTVlx1KGMGyUPEZE0q491Mru6hLy87O8EKq4gnTM3s61AK9AP9Ln76nQuT0RkIqqPdebM3XTj0po8Qie7++4MLEdEZEKqj3Vy7CE1UYcxrlRsJSKSRn39AzTs62JuDl0shySSh5mVmVle+PwwMzvLzAqTnL8DfzSzDWZ22TDzv8zM1pvZ+qampuQjFxHJArtauxnw3KqmC8mdedwLlJjZXODPwMXAdUnO/zh3XwWsBf7FzN4weAJ3v8bdV7v76tra2iRnKyKSHXKxjQcklzzM3TuAc4FvuvvbgGXJzNzd68PHRuAW4JixBioiko0mdfIws2OBdwO/D98b9UJ7WNxVEX8OnAZsGmugIiLZqC4HGwhCcrWtPgJ8CrjF3Teb2SLgriQ+NxO4xcziy/m5u98+5khFRLJQfayT6tJCSosyUbk1c0ZdG3e/B7gHILxwvtvd/08Sn3sBWHnAEYqIZLH6WFfOtfGA5Gpb/dzMKsOipyeBZ8zs39IfmohI9su1fjzikrnmsczd9wHnAH8ADgLem9aoRERyRF2sM+faeEByyaMwbNdxDvBbd+8laL8hIiIj2NfVS2tX36Q98/gesBUoA+41s4OBfekMSkQkF+yMdQG5V00Xkrtg/g3gGwlvbTOzk9MXkohIbsjVNh6Q3AXzKjP7SvwWImb2PwRnISIiMoL6liB55FLf5XHJFFv9kOC26u8Ih33Aj9IZlIhILqiPdVKQZ9RW5E4nUHHJtFo5xN3PS3h9pZk9lq6ARERyRX2si1lVJeTnUCdQccmceXSa2fHxF2Z2HNCZvpBERHJDXY628YDkzjz+CfixmVUBBuwBLkpnUCIiuaA+1slrF0yLOoy0SKa21WPASjOrDF+rmq6IyCj6B5yGlq6cuyFi3LDJw8w+Osz7ALj7V9IUk4hI1mtq7aZvwCdlsVVFxqIQEckxdTncxgNGSB7ufmUmAxERySXxBoK52MYDkqttJSIiKYonj9lVuXnNQ8lDRCQN6mOdVJYUUFFSGHUoaaHkISKSBnWxrpy93gHJ9UVeDJwHLEic3t2vSl9YIiLZLVc7gYpL5szjt8DZQB/QnjCIiMgw6ls6c7aNByTXwnyeu5+e9khERHJEe3cfsY7eSX/m8TczO2KsCzCzfDN71MxuG+s8RESyyc4cvhV7XDJnHscDF5nZi0A3wf2t3N1fk+QyPgw8BVSOLUQRkexSl8M9CMYlkzzWjnXmZjYPeAvwn8CQtzsREck1udyDYNyoxVbuvg2oBs4Mh+rwvWR8DfgEMDDcBGZ2WbyXwqampiRn+7L27j4u/+kGfvn37Sl/VkQkHepjneQZzMzBTqDikumG9sPA9cCMcPiZmX0oic+9FWh09w0jTefu17j7andfXVtbm2TYLystymdLUxs3rlfyEJGJoS7WyazKEgryc7cpXTJrdgnwOnf/nLt/DlgDXJrE544DzjKzrcAvgDea2c/GHOkwzIxzV81jw7a9vLhbNYhFJHo7c7yBICSXPAzoT3jdH743Inf/lLvPc/cFwLuAv7j7e8YU5SjedtRczOCWR3akY/YiIikJ2ngoefwIeMjMrjCzK4AHgR+kNaoUzaoq4fhDa7jpkToGBjzqcERkEhsYcJ15wP5Ony4m6H52L3Cxu38tlYW4+93u/taxhZic81bNoy7WycNb96RzMSIiI9rd3k1P/wBzc7h1OYzck2Clu+8zs2nA1nCIj5vm7hNqL33a8pmUFeVz04YdrFk0PepwRGSSqp8EbTxg5DOPn4ePG4D1CUP89YRSWlTAGUfM5g8bd9LZ0z/6B0RE0mAytPGAEZJHvJjJ3Re6+6KEYaG7L8pciMk7d9U82nv6uWNzQ9ShiMgkNemTR5yZ/TmZ9yaC1y2cxtzqKdykWlciEpG6WCflxQVUliRzA4/sNWzyMLOS8HpHjZlNNbNp4bAAmJOpAFORl2ecu2ou9z+/m4aWrqjDEZFJKOjHowSzUVs0ZLWRzjw+QHB9Y2n4GB9+C3w7/aGNzbmr5jHg8JvH6qIORUQmofpJUE0XRr7m8XV3Xwh8POFax0J3X+nu38pgjClZWFPGqoOquWnDDtzV5kNEMivXexCMS6adxzfNbIWZvcPM3hcfMhHcWJ139Dyea2xjU92+qEMRkUmkq7ef5vaenO7HIy6ZC+afB74ZDicDXwLOSnNcB+StR8yhqCBPF85FJKPiNa1mV+V2A0FI7vYk5wOnAA3ufjGwEpjQ9xmuKi3k1MNncuvj9fT0DXs3eBGRcTVZGghCcsmj090HgD4zqwQagQnZziPReUfPZU97D3c/0xh1KCIyScTPPFRsFVhvZtXA9wlqWz0CPJzWqMbBCYtrqSkv4uZHVOtKRDKjLtaJGcyszP1iq1Fbsbj7P4dPv2tmtwOV7v5EesM6cIX5eZx95Fx+8sBW9rb3MLWsKOqQRCTH1cc6mVFRTFFB7nYCFTdSI8FVgwdgGlAQPp/wzl01l95+57Yn6qMORUQmgcnQj0fcSGce/xM+lgCrgccJOoF6DfAQcHx6Qztwy+dUsXRWBb9+pI73Hrsg6nBEJMfVx7pYNqcy6jAyYqRGgie7+8nANmBV2M/40cBRwPOZCvBAnbdqHo9vj/F8Y1vUoYhIDnN36mOdk+JiOSR3wXypu2+Mv3D3TcCR6QtpfJ191BzyDG5Wmw8RSaM97T109w0wZxK08YDkksdTZnatmZ1kZiea2feBp9Id2HiZUVHCGw6r5ZZH1UWtiKTPZGrjAcklj4uBzcCHgY8AT4bvZY3zVs1jZ0sXD7zQHHUoIpKj6iZJPx5xyVTV7QK+Gg5Z6dRlM6koKeCmR3Zw3KE1UYcjIjloMjUQhJGr6v4yfNxoZk8MHkabcdgfyMNm9riZbTazK8cz8FSUFObz1tfM5vZNDbR390UVhojksPpYJ1MK86kuLYw6lIwY6czjw+HjW8c4727gje7eZmaFwH1mts7dHxzj/A7IuavmccPD27l9UwPnHT0vihBEJIcFbTxyvxOouGGTh7vvDB+3jWXGHnSmEa8fWxgOkV2xXn3wVA6aVspNj+xQ8hCRcVc3STqBihup2KrVzPYNMbSaWVIdZZhZvpk9RnAzxTvd/aEhprnMzNab2fqmpqaxr8nosXDuqrk88ELz/gtbIiLjZTK18YCRGwlWuHvlEEOFuyfVhNLd+939SGAecIyZrRhimmvCBoira2trx74mSTj3qHm4w28e1c0SRWT8dPf109TarTOPoZjZDDM7KD6kshB3jwF3A6enGN+4Omh6KccsmMZNj6iLWhEZPw0tk6uNByTXk+BZZvYc8CJwD7AVWJfE52rDW7ljZlOANwFPH1C04+C8o+fyQlM7j22PRR2KiOSIl9t4TI7W5ZDcmccXgDXAs+6+kKBXwfuT+Nxs4K6wWu/fCa553DbmSMfJ2iNmU1yQp34+RGTcxFuX65rHK/W6ezOQZ2Z57n4XSdzbyt2fcPej3P017r7C3a864GjHQWVJIW9ePotbH6+nu68/6nBEJAfEGwjOmiT3tYLkkkfMzMqBe4HrzezrQFa3tDt31VxaOnu562l1USsiB64+1klNeTHFBflRh5IxySSPs4FO4F+B24EtwJnpDCrdjj+0hhkVxfx6g4quROTA1cU6mTuJrnfAyO08vmVmr3f39rDKbZ+7/9jdvxEWY2Wtgvw8zjlqLnc/00hzW3fU4YhIlquPTZ4eBONGOvN4DvgfM9tqZl80s6zpwyMZ562aR9+Ac+vj6qJWRMYu6ARqcrUuh5EbCX7d3Y8FTgT2AD8ys6fM7HNmdljGIkyTJbMqWD6nUrWuROSAxDp66eztV/IYzN23ufsX3f0o4ELgbWRRZ1AjOW/VPDbWtfDsrtaoQxGRLFW3/1bsuubxCmZWaGZnmtn1BI0DnwXOS3tkGXDWkXMoyDNuUhe1IjJG9ZOsE6i4kS6Yn2pmPwR2AJcBfwAOcfd3uvtvMhVgOtWUF3PSklp+82gd/eqiVkTGYOckvDUJjHzm8WngAeBwdz/T3a939/YMxZUx566ax6593dz//O6oQxGRLFQf66SoII/pZUVRh5JRI/XncXImA4nKKYfPYGppId+9ZwsnLK6ZNB25iMj4eOSlvRxaWz7p9h1J31U3VxUX5POvpx7G37Y0s25TQ9ThiEgWadzXxfpte3nz8llRh5Jxkz55AFx4zEEcPruSq297ko6erL7ziohk0B2bG3CHM45Q8piUCvLzuOrs5dS3dPGdu7dEHY6IZIl1mxo4pLaMxTMrog4l45Q8Qq9dMI1zjpzD9+55gW3NOVcvQETGWXNbNw++0MzaFbOjDiUSSh4JPnXG4RTmG1+47cmoQxGRCe7OJ3cx4LB2EhZZgZLHK8ysLOH/nLKYPz3VqNu1i8iI1m1q4KBppSybXRl1KJFQ8hjk4uMWsqi2jCt/t1mdRYnIkFo6ern/+d2sXTFr0lXRjVPyGKSoII8rzlzO1uYOrv3ri1GHIyIT0J+e2kXfgLP2iMl5vQOUPIb0hsNqefPymXzrL8+zs6Uz6nBEZIJZt6mBOVUlrJxXFXUokVHyGMZn37KMAXf+8/c5cQNhERknbd193PtcE2+exEVWkMbkYWbzzeyusA+QzWb24XQtKx3mTyvln046hNue2Mnftui+VyIS+MvTjfT0DXDGJC6ygvSeefQBH3P3w4E1wL+Y2bI0Lm/cXX7iIcybOoUrb32S3v6BqMMRkQng9k07qa0o5uiDpkYdSqTSljzcfae7PxI+byXoQGpuupaXDiWF+fzHW5fxzK5WfvrAtqjDEZGIdfb0c9fTTbx5+Uzy8iZvkRVk6JqHmS0AjgIeGmLcZWa23szWNzU1ZSKclJy2bCZvOKyWr975LE2t3VGHIyIRuufZRjp7+zljkrYqT5T25GFm5cBNwEfcfd/g8e5+jbuvdvfVtddWRJsAABKYSURBVLW16Q4nZWbG589cRldfP1+6/emowxGRCK3b1MDU0kKOWTgt6lAil9bkYWaFBInjene/OZ3LSqdDasv5h+MX8qsNO3j0pb1RhyMiEeju6+fPTzVy2rJZFOSromo6a1sZ8APgKXf/SrqWkykfeuNiZlQU8/lbNzOgLmtFJp37nttNW3ffpL2X1WDpTJ/HAe8F3mhmj4XDGWlcXlqVFxfwmbcczhM7Wvjl+u1RhyMiGbZuUwMVJQW8/pCaqEOZEIbthvZAuft9QE5VRzhr5Ryuf/Alvnj705y+YhbVpZOrz2KRyaq3f4A7n9zFqYfPpKhARVagFuYpMTOuOGs5LZ29fOXOZ6MOR0Qy5IEtzbR09k7qe1kNpuSRomVzKnnvmoP52YPbeLL+VZXHRCQHrdvUQFlRPicsVpFVnJLHGHz01CVUlxZxxa2bcdfFc5Fc1j/g/HFzAycvnUFJYX7U4UwYSh5jUFVayCfevISHt+7h1sfrow5HRNLo4Rf30NzeM+nvZTWYkscYvWP1fF4zr4r//P1TtHX3RR2OiKTJ7Zt2UlKYx0lLJl4j5igpeYxRXp5x5VnLaWzt5pt/eS7qcEQkDQYGnHWbGjjxsFpKi9JWOTUrKXkcgKMOmsrbj57HD+97kecb26IOR0TG2aPb99LY2s1a3cvqVZQ8DtC/r11KSWE+H/jpehr3dUUdjoiMo3UbGyjKz+ONh8+IOpQJR8njANWUF/OD97+WhpYu3nnNg+q2ViRHuAdFVscvrqGypDDqcCYcJY9xcMzCafzkkmNoau3mnd97kB17O6IOSUQO0Ma6FupinZy+QveyGoqSxzg5+uBp/OwfX8fejh7e+b0H2b5HCUQkm63b1EBBnnHasplRhzIhKXmMoyPnV3PDpWto6+7jnd97gK2726MOSUTGwN1Zt3Enxx4yXfewG4aSxzhbMbeKGy5dQ1ffAO+85gG2NKkWlki2ebqhla3NHSqyGoGSRxosm1PJDZeuoX/Aedc1D/LcrtaoQxKRFKzb1ECewWnLlDyGo+SRJktmVfCLy9YA8K5rHuTpBt1EUSRbrNu4k9cumEZtRXHUoUxYSh5pdOiMCm68bA2F+XlccM2DbK5viTokERnF842tPNfYxloVWY1IySPNFtWWc+MH1jClMJ8Lv/8QG3cogYhMZOs2NgBwulqVj0jJIwMOnl7GjR84loqSAi689kEefWlv1CGJyDDWbWpg1UHVzKoqiTqUCU3JI0PmTyvlxg8cy7SyIt77g4dZv3VP1CGJyCDbmtt5cuc+3csqCWlLHmb2QzNrNLNN6VpGtplbPYUbLzuWGRXFvO+HD/PQC81RhyQiCdZtihdZ6XrHaNJ55nEdcHoa55+VZlWV8IvL1jCnegrv/9HD/O353VGHJCKhdZsaOGJuFfOnlUYdyoSXtuTh7vcCKpsZwozKEm64dA0HTyvj4uv+zr3PNkUdksikVxfr5PHtMZ11JEnXPCJSW1HMzy99HYtqy/nHH6/n1xt2MDCg/tBFonJ7WGSlKrrJiTx5mNllZrbezNY3NU2uI/Dp5cXccOnrWDG3ko//6nHe+s37uOfZJtyVREQy7fZNO1k6q4JFteVRh5IVIk8e7n6Nu69299W1tZOvj+Dq0iJ+ffnr+fq7jqS1u5f3//Bh3n3tQzy+PRZ1aCKTRuO+LtZv26siqxREnjwk6A/97CPn8uePnsQVZy7jmYZWzv72/fzL9Y/wgm6sKJJ2d2xuwB3OOEJVdJOVzqq6NwAPAEvMbIeZXZKuZeWKooI8LjpuIfd84mQ+fMpi7nqmkVO/ei+fuWWjurgVSaN1mxpYVFvG4hkqskpWQbpm7O4XpGveua68uIB/PfUw3rPmYL71l+e4/qGXuPmROi45fiGXnbhIXWKKjKPmtm4eenEPl5+4CDOLOpysoWKrCay2opgrz17Bnz92Iqcum8m37nqeE790F9f+9QW6+/qjDk8kJ/zxyV30D7haladIySMLHDy9jG9ccBS3feh4Vsyt4urfP8Ubv3wPN23YQb+q94qMycCAc+1fX+CKWzdz6Ixyls+pjDqkrGITqVro6tWrff369VGHMeHd//xu/nvd02ysa2HprAo+cfoSTl4yQ6fcIkna1tzOv/3qCR7euodTls7gv849ghmV2XkjRDPb4O6rM75cJY/sNDDg/GHTTr58xzNsbe7gsJnlnL5iNmtXzGLprAolEpEhDAw41z+0jf/7h6cpyDM+d+Yyzj96Xlb/X5Q8UPIYi97+AW7asINbHq3j71v3MOCwYHopb14xi7UrZrNyXlVW/zEEuvv6aWrtZte+bhr3dbFrXxe7WrvZta+Lxn3BY0fP6NfABm8Gia+nlxVz5PxqVs6v4sj5U1kwvTTntpsdezv495ue4P7nmzlhcQ1fPO81zKmeEnVYB0zJAyWPA7W7rZs/bt7Fuk07eWBLM30DzuyqEt68fBZrV8xi9YJp5Ofl1g4h23X29PPC7jZ27O0ME0P3/uQQTxR7O3pf9bnCfGNGRQkzKouZWVFCWXHBK5LB4L+144kvBo2D+lgnG+ta9iehqimFrJxfzZHzqlg5v5qV86upKc/OLlndnRv/vp2rf/8UA+585i2Hc+ExB+VMclTyQMljPLV09PKnp3axblMD9z7XRE/fADXlRZy2fBanL5/FsYdMpzBf9SUywd1pbO1mS2MbW5ra2NLUzpamNl5oaqcu1vmKafPzjNryYmZWFjOjsoSZYXKYEX9dEbw3tbSIvHE+EOgfcJ5rbOXx7TEe2x7jse0tPNOwj3idjHlTp7ByfjVHhclkxZwqphTlj2sM462hpYtP3vwEdz/TxJpF0/h/56/MuTvmKnmg5JEubd193PV0I7dvbuCupxvp6Omnakohbzp8JqevmMUJi2soKZzYO4Fs0NXbz7bmjjAxvDJJtHX37Z+urCifRbXlHFJbxiG15Rwyo5z5U0uZWVXM9LLiCXV22NHTx6a6fQkJJbY/4eXnGUtmVrByfjWvXTCVExbXUlsxMc5O3J1bHq3jils309M/wCdPX8r7jl0w7gl3IlDyQMkjE7p6+7n32SZu39TAnU/torWrj7KifFYdPJXDZ1eyZGYFS2ZVcOiMciWUIfT1D1Af6+LF5na27m7nxd3tbG0OHrfv6SCx5vScqhIOmVEeJIjasjBhlDOzsjiri0waW7t4YnsLj+94OaG0dgXJccXcSk46bAYnLqnlqPnVFERwdtvU2s2nb9nInU/u4uiDp/Llt69kYU1ZxuPIFCUPlDwyradvgAdeaOaOzQ08vj3Gc41t9PQNAMFR5YLppSydVcmSWUFCWTqrgvlTS3Py6C1R/4BTH+tk6/4E0bH/+fa9HfT2v/yfKS3KZ8H0MhbGzyLCx0W1ZZQWpe0GDhPKwIDz5M593P1MI/c828QjL8XoH3AqSgo4YXHN/mQyMwNVYX//xE4++5uNtPf08/HTDuOS4xdNqDO5dFDyQMkjan39A2xtbufphlaeaWjd//jSno7905QW5bN4ZgVLZ1awdHY8qVQyrawowsiTNzDgtHT20tzeTVNrD83t3TS39bBjb8f+JPHSno79SRRgSmE+B08vZWFNGQtqylg4vWz/69qK7D6LSIeWzl7uf373/mSya183AEtnVXDSkhmceFgtqxdMHddrbnvae/jcbzdx2xM7WTmvii+/fSWLZ1aM2/wnMiUPlDwmqvbuPp7d9cqE8syuVva09+yfprQon+ophVSVFjG1tJDq0kKqS4uonlLI1NIiqkqDx+rSQqaWFlI1JXg+lh2Iu9M/4PTHHwec1q4+mtt62N3Wze62bprbe9jdGj62dbO7rYfmtm72tPfQN0Sr/OKCPBYkJIUFNWXBGUVNWdYXM0XJ3Xm6oZW7n2ninmcbWb91L30DTnlxAa8/ZHqQTJbUMjesMtvXP0Bnb38w9Lzysau3n86ecHxPX/g4QEdPHzc9UkdLZw8fPmUxl594SCTFZVFR8kDJI5u4O01t3UEiaWhlZ0sXsY5eYh09xDp72dvRQ0tHL7HO3hFvoVJeXEB1aSFF+Xn0u9PX7wy40zfwcmJ4xRAmjGSUFOZRU14cDkXUlBczvbyI6WXF1FQUU1NWRE1FMdPLitJSe0lerbWrl79taQ6SyTON1LcEd4uuKC6gq6//FUWCySrKz+PwOZX819uOYNkkvMWIkgdKHrloYMBp6+kj1t5LrLOHvfEE09FLrCNIMrGOHnoHnII8Iz/PyDejIN/IMwvfyyM/D/Lz8ijIM/Ly7OVpw+nLSwqYHiaDmrIgSZQVT45rDtnK3Xm+sY27n2miLtZJaVE+UwrzmVKUT0nhy8+nFIavi/L3TxN/XVKQN6nOMoYSVfLQv0vSKi/PqCwppLKkkIPIrfr1cmDMjMUzKybNtYlcM7lTtoiIjImSh4iIpEzJQ0REUqbkISIiKVPyEBGRlCl5iIhIypQ8REQkZUoeIiKSsgnVwtzMmoBtY/x4DbB7HMPJBlrn3DfZ1he0zqk62N1rxzOYZEyo5HEgzGx9FE30o6R1zn2TbX1B65wtVGwlIiIpU/IQEZGU5VLyuCbqACKgdc59k219QeucFXLmmoeIiGROLp15iIhIhih5iIhIyrI2eZjZ281ss5kNmNmwVdzM7HQze8bMnjezT2YyxvFmZtPM7E4zey58nDrMdP1m9lg43JrpOA/UaL+ZmRWb2Y3h+IfMbEHmoxxfSazzRWbWlPC7/mMUcY4XM/uhmTWa2aZhxpuZfSP8Pp4ws1WZjnG8JbHOJ5lZS8Jv/LlMx5iKrE0ewCbgXODe4SYws3zg28BaYBlwgZkty0x4afFJ4M/uvhj4c/h6KJ3ufmQ4nJW58A5ckr/ZJcBedz8U+CrwxcxGOb5S2E5vTPhdr81okOPvOuD0EcavBRaHw2XAdzIQU7pdx8jrDPDXhN/4qgzENGZZmzzc/Sl3f2aUyY4Bnnf3F9y9B/gFcHb6o0ubs4Efh89/DJwTYSzpksxvlvg9/Bo4xcwsgzGOt1zbTkfl7vcCe0aY5GzgJx54EKg2s9mZiS49kljnrJK1ySNJc4HtCa93hO9lq5nuvhMgfJwxzHQlZrbezB40s2xLMMn8Zvuncfc+oAWYnpHo0iPZ7fS8sAjn12Y2PzOhRSbX/rvJOtbMHjezdWa2POpgRlIQdQAjMbM/AbOGGPUZd/9tMrMY4r0JXTd5pHVOYTYHuXu9mS0C/mJmG919y/hEmHbJ/GZZ97uOIpn1+R1wg7t3m9nlBGdeb0x7ZNHJtd84GY8Q3KeqzczOAH5DUGw3IU3o5OHubzrAWewAEo/Q5gH1BzjPtBppnc1sl5nNdved4Sl84zDzqA8fXzCzu4GjgGxJHsn8ZvFpdphZAVBFdhcHjLrO7t6c8PL7ZPl1niRk3X/3QLn7voTnfzCz/zWzGnefkDeJzPViq78Di81soZkVAe8Csq72UYJbgfeHz98PvOrsy8ymmllx+LwGOA54MmMRHrhkfrPE7+F84C+e3a1dR13nQeX9ZwFPZTC+KNwKvC+sdbUGaIkX2eYqM5sVv3ZnZscQ7J+bR/5UhNw9KwfgbQRHJ93ALuCO8P05wB8SpjsDeJbgyPszUcd9gOs8naCW1XPh47Tw/dXAteHz1wMbgcfDx0uijnsM6/mq3wy4CjgrfF4C/Ap4HngYWBR1zBlY5/8CNoe/613A0qhjPsD1vQHYCfSG/+NLgMuBy8PxRlADbUu4Ha+OOuYMrPMHE37jB4HXRx3zSINuTyIiIinL9WIrERFJAyUPERFJmZKHiIikTMlDRERSpuQhIiIpU/KQScnM3mZmbmZLw9cLhrvbacJnRp1GZLJQ8pDJ6gLgPoIGeSKSIiUPmXTMrJyg5f0lDJE8wr4zfmtmt4d9bHw+YXS+mX0/7Evmj2Y2JfzMpWb29/CmdjeZWWlm1kYkGkoeMhmdA9zu7s8Ce4bpaOgY4N3AkcDbEzocWwx8292XAzHgvPD9m939te6+kuDWIZekdQ1EIqbkIZPRBQR9ZhA+XjDENHe6e7O7dwI3A8eH77/o7o+FzzcAC8LnK8zsr2a2kSDpTOjbaYscqAl9V12R8WZm0wluZb7CzBzIJ7jV9/8OmnTwfXvir7sT3usHpoTPrwPOcffHzewi4KTxi1pk4tGZh0w25xP0UHewuy9w9/nAiwS3/E50athn/BSCYq77R5lvBbDTzAoJzjxEcpqSh0w2FwC3DHrvJuDTg967D/gp8Bhwk7uvH2W+/wE8BNwJPD0OcYpMaLqrrsggYbHTanf/YNSxiExUOvMQEZGU6cxDRERSpjMPERFJmZKHiIikTMlDRERSpuQhIiIpU/IQEZGU/X9FZT6QA5SjSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(alpha_values, losses)\n",
    "plt.title(\"Compare batch 32 (alpha=0) and batch 256 (alpha=1) minima\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.show()\n",
    "plt.savefig('graphs/batch_size_alpha_trial_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we observe that the large batch size found a sharp minimizer, as predicted by the paper. Compare the minimum for batch size 256 (alpha=1) and that of batch size 32 (alpha=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use smaller learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use a smaller learning rate and see if that resolves the weird behavior we observed previously (large batch size outperforming small one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 25s 43ms/step - loss: 0.6902 - accuracy: 0.5350\n",
      "Saving weights for epoch 0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68862, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 31s 54ms/step - loss: 0.6902 - accuracy: 0.5350 - val_loss: 0.6886 - val_accuracy: 0.5202\n",
      "Epoch 2/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5776\n",
      "Epoch 00002: val_loss improved from 0.68862 to 0.68747, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6794 - accuracy: 0.5775 - val_loss: 0.6875 - val_accuracy: 0.5252\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6710 - accuracy: 0.5881\n",
      "Epoch 00003: val_loss improved from 0.68747 to 0.67682, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6709 - accuracy: 0.5879 - val_loss: 0.6768 - val_accuracy: 0.5699\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.6029\n",
      "Epoch 00004: val_loss improved from 0.67682 to 0.65073, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6619 - accuracy: 0.6026 - val_loss: 0.6507 - val_accuracy: 0.6320\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.6120\n",
      "Epoch 00005: val_loss improved from 0.65073 to 0.64336, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6530 - accuracy: 0.6120 - val_loss: 0.6434 - val_accuracy: 0.6322\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6450 - accuracy: 0.6236\n",
      "Saving weights for epoch 5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.64336 to 0.63746, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6449 - accuracy: 0.6234 - val_loss: 0.6375 - val_accuracy: 0.6393\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6395 - accuracy: 0.6320\n",
      "Epoch 00007: val_loss improved from 0.63746 to 0.63528, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6394 - accuracy: 0.6321 - val_loss: 0.6353 - val_accuracy: 0.6374\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6271 - accuracy: 0.6459\n",
      "Epoch 00008: val_loss improved from 0.63528 to 0.62683, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6270 - accuracy: 0.6460 - val_loss: 0.6268 - val_accuracy: 0.6410\n",
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.6694\n",
      "Epoch 00009: val_loss improved from 0.62683 to 0.58856, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6054 - accuracy: 0.6693 - val_loss: 0.5886 - val_accuracy: 0.6885\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.6922\n",
      "Epoch 00010: val_loss improved from 0.58856 to 0.57121, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5844 - accuracy: 0.6922 - val_loss: 0.5712 - val_accuracy: 0.6999\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7159\n",
      "Saving weights for epoch 10\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.57121 to 0.53828, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5602 - accuracy: 0.7157 - val_loss: 0.5383 - val_accuracy: 0.7343\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7276\n",
      "Epoch 00012: val_loss improved from 0.53828 to 0.51815, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5399 - accuracy: 0.7273 - val_loss: 0.5182 - val_accuracy: 0.7487\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7400\n",
      "Epoch 00013: val_loss improved from 0.51815 to 0.51482, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5261 - accuracy: 0.7397 - val_loss: 0.5148 - val_accuracy: 0.7633\n",
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5184 - accuracy: 0.7453\n",
      "Epoch 00014: val_loss improved from 0.51482 to 0.49813, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5186 - accuracy: 0.7450 - val_loss: 0.4981 - val_accuracy: 0.7584\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7498\n",
      "Epoch 00015: val_loss improved from 0.49813 to 0.48875, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5088 - accuracy: 0.7495 - val_loss: 0.4888 - val_accuracy: 0.7683\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7560\n",
      "Saving weights for epoch 15\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.48875 to 0.48066, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5011 - accuracy: 0.7558 - val_loss: 0.4807 - val_accuracy: 0.7764\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7585\n",
      "Epoch 00017: val_loss improved from 0.48066 to 0.47625, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4959 - accuracy: 0.7584 - val_loss: 0.4762 - val_accuracy: 0.7775\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.7616\n",
      "Epoch 00018: val_loss did not improve from 0.47625\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4891 - accuracy: 0.7614 - val_loss: 0.4801 - val_accuracy: 0.7657\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4829 - accuracy: 0.7633\n",
      "Epoch 00019: val_loss did not improve from 0.47625\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4833 - accuracy: 0.7631 - val_loss: 0.4835 - val_accuracy: 0.7721\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7713\n",
      "Epoch 00020: val_loss improved from 0.47625 to 0.45731, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4775 - accuracy: 0.7710 - val_loss: 0.4573 - val_accuracy: 0.7876\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7731\n",
      "Saving weights for epoch 20\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.45731 to 0.45708, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4719 - accuracy: 0.7730 - val_loss: 0.4571 - val_accuracy: 0.7913\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4688 - accuracy: 0.7710\n",
      "Epoch 00022: val_loss improved from 0.45708 to 0.45270, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4693 - accuracy: 0.7706 - val_loss: 0.4527 - val_accuracy: 0.7876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.7781\n",
      "Epoch 00023: val_loss improved from 0.45270 to 0.45264, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4635 - accuracy: 0.7779 - val_loss: 0.4526 - val_accuracy: 0.7893\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.7824\n",
      "Epoch 00024: val_loss improved from 0.45264 to 0.44685, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4605 - accuracy: 0.7820 - val_loss: 0.4469 - val_accuracy: 0.7951\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4535 - accuracy: 0.7836\n",
      "Epoch 00025: val_loss improved from 0.44685 to 0.44369, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4539 - accuracy: 0.7832 - val_loss: 0.4437 - val_accuracy: 0.7966\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4513 - accuracy: 0.7869\n",
      "Saving weights for epoch 25\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.44369 to 0.44090, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4516 - accuracy: 0.7866 - val_loss: 0.4409 - val_accuracy: 0.7945\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4465 - accuracy: 0.7888\n",
      "Epoch 00027: val_loss improved from 0.44090 to 0.43908, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4466 - accuracy: 0.7887 - val_loss: 0.4391 - val_accuracy: 0.7973\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7922\n",
      "Epoch 00028: val_loss did not improve from 0.43908\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4437 - accuracy: 0.7920 - val_loss: 0.4422 - val_accuracy: 0.7923\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4386 - accuracy: 0.7947\n",
      "Epoch 00029: val_loss improved from 0.43908 to 0.43406, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4389 - accuracy: 0.7944 - val_loss: 0.4341 - val_accuracy: 0.7982\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.7935\n",
      "Epoch 00030: val_loss did not improve from 0.43406\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4339 - accuracy: 0.7933 - val_loss: 0.4410 - val_accuracy: 0.7921\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.7977\n",
      "Saving weights for epoch 30\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.43406 to 0.42159, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4341 - accuracy: 0.7976 - val_loss: 0.4216 - val_accuracy: 0.8022\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4287 - accuracy: 0.7964\n",
      "Epoch 00032: val_loss did not improve from 0.42159\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4288 - accuracy: 0.7962 - val_loss: 0.4263 - val_accuracy: 0.7984\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8026\n",
      "Epoch 00033: val_loss improved from 0.42159 to 0.41989, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4267 - accuracy: 0.8025 - val_loss: 0.4199 - val_accuracy: 0.8035\n",
      "Epoch 34/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8007\n",
      "Epoch 00034: val_loss did not improve from 0.41989\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4250 - accuracy: 0.8008 - val_loss: 0.4262 - val_accuracy: 0.8031\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8029\n",
      "Epoch 00035: val_loss improved from 0.41989 to 0.41478, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4186 - accuracy: 0.8028 - val_loss: 0.4148 - val_accuracy: 0.8080\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.8050\n",
      "Saving weights for epoch 35\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.41478\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4184 - accuracy: 0.8047 - val_loss: 0.4209 - val_accuracy: 0.8085\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8110\n",
      "Epoch 00037: val_loss did not improve from 0.41478\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4140 - accuracy: 0.8110 - val_loss: 0.4243 - val_accuracy: 0.8001\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8069\n",
      "Epoch 00038: val_loss improved from 0.41478 to 0.40853, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4111 - accuracy: 0.8069 - val_loss: 0.4085 - val_accuracy: 0.8104\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8105\n",
      "Epoch 00039: val_loss improved from 0.40853 to 0.40743, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4061 - accuracy: 0.8103 - val_loss: 0.4074 - val_accuracy: 0.8132\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8150\n",
      "Epoch 00040: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4008 - accuracy: 0.8147 - val_loss: 0.4102 - val_accuracy: 0.8072\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8158\n",
      "Saving weights for epoch 40\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3988 - accuracy: 0.8156 - val_loss: 0.4091 - val_accuracy: 0.8085\n",
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8144\n",
      "Epoch 00042: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4040 - accuracy: 0.8140 - val_loss: 0.4264 - val_accuracy: 0.7962\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8169\n",
      "Epoch 00043: val_loss improved from 0.40743 to 0.40309, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3979 - accuracy: 0.8165 - val_loss: 0.4031 - val_accuracy: 0.8110\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8155\n",
      "Epoch 00044: val_loss improved from 0.40309 to 0.39577, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3987 - accuracy: 0.8156 - val_loss: 0.3958 - val_accuracy: 0.8160\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8242\n",
      "Epoch 00045: val_loss improved from 0.39577 to 0.39531, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3887 - accuracy: 0.8242 - val_loss: 0.3953 - val_accuracy: 0.8173\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.8225\n",
      "Saving weights for epoch 45\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.39531 to 0.39132, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3878 - accuracy: 0.8224 - val_loss: 0.3913 - val_accuracy: 0.8216\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8235\n",
      "Epoch 00047: val_loss improved from 0.39132 to 0.39124, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3833 - accuracy: 0.8235 - val_loss: 0.3912 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8253\n",
      "Epoch 00048: val_loss did not improve from 0.39124\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3812 - accuracy: 0.8250 - val_loss: 0.3967 - val_accuracy: 0.8201\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8284\n",
      "Epoch 00049: val_loss improved from 0.39124 to 0.38379, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3787 - accuracy: 0.8283 - val_loss: 0.3838 - val_accuracy: 0.8224\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8310\n",
      "Epoch 00050: val_loss improved from 0.38379 to 0.38313, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3729 - accuracy: 0.8310 - val_loss: 0.3831 - val_accuracy: 0.8212\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8304\n",
      "Saving weights for epoch 50\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.38313\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3743 - accuracy: 0.8306 - val_loss: 0.3875 - val_accuracy: 0.8175\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3707 - accuracy: 0.8326\n",
      "Epoch 00052: val_loss improved from 0.38313 to 0.38275, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3707 - accuracy: 0.8326 - val_loss: 0.3828 - val_accuracy: 0.8267\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8322\n",
      "Epoch 00053: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3683 - accuracy: 0.8321 - val_loss: 0.3872 - val_accuracy: 0.8209\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3630 - accuracy: 0.8374\n",
      "Epoch 00054: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3632 - accuracy: 0.8373 - val_loss: 0.3902 - val_accuracy: 0.8192\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8336\n",
      "Epoch 00055: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3686 - accuracy: 0.8334 - val_loss: 0.3840 - val_accuracy: 0.8203\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8385\n",
      "Saving weights for epoch 55\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3589 - accuracy: 0.8386 - val_loss: 0.3856 - val_accuracy: 0.8246\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8402\n",
      "Epoch 00057: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3595 - accuracy: 0.8402 - val_loss: 0.3851 - val_accuracy: 0.8203\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.8372\n",
      "Epoch 00058: val_loss improved from 0.38275 to 0.38232, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3622 - accuracy: 0.8371 - val_loss: 0.3823 - val_accuracy: 0.8227\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8406\n",
      "Epoch 00059: val_loss improved from 0.38232 to 0.37136, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3548 - accuracy: 0.8407 - val_loss: 0.3714 - val_accuracy: 0.8323\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8418\n",
      "Epoch 00060: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3528 - accuracy: 0.8416 - val_loss: 0.3831 - val_accuracy: 0.8209\n",
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8439\n",
      "Saving weights for epoch 60\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3501 - accuracy: 0.8437 - val_loss: 0.3811 - val_accuracy: 0.8257\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.8432\n",
      "Epoch 00062: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3456 - accuracy: 0.8433 - val_loss: 0.3814 - val_accuracy: 0.8255\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8436\n",
      "Epoch 00063: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3486 - accuracy: 0.8435 - val_loss: 0.3804 - val_accuracy: 0.8257\n",
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8486\n",
      "Epoch 00064: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3425 - accuracy: 0.8485 - val_loss: 0.3727 - val_accuracy: 0.8259\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8483\n",
      "Epoch 00065: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3393 - accuracy: 0.8482 - val_loss: 0.3753 - val_accuracy: 0.8222\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8496\n",
      "Saving weights for epoch 65\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.37136 to 0.36717, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3354 - accuracy: 0.8497 - val_loss: 0.3672 - val_accuracy: 0.8323\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8504\n",
      "Epoch 00067: val_loss improved from 0.36717 to 0.36542, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3343 - accuracy: 0.8503 - val_loss: 0.3654 - val_accuracy: 0.8364\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8485\n",
      "Epoch 00068: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3372 - accuracy: 0.8486 - val_loss: 0.3679 - val_accuracy: 0.8304\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3329 - accuracy: 0.8520\n",
      "Epoch 00069: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3331 - accuracy: 0.8519 - val_loss: 0.3660 - val_accuracy: 0.8336\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8548\n",
      "Epoch 00070: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3264 - accuracy: 0.8550 - val_loss: 0.3777 - val_accuracy: 0.8270\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8548\n",
      "Saving weights for epoch 70\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3240 - accuracy: 0.8546 - val_loss: 0.3799 - val_accuracy: 0.8252\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8596\n",
      "Epoch 00072: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3245 - accuracy: 0.8596 - val_loss: 0.3768 - val_accuracy: 0.8304\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8592\n",
      "Epoch 00073: val_loss improved from 0.36542 to 0.36301, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3194 - accuracy: 0.8591 - val_loss: 0.3630 - val_accuracy: 0.8362\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8599\n",
      "Epoch 00074: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3181 - accuracy: 0.8598 - val_loss: 0.3808 - val_accuracy: 0.8233\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8578\n",
      "Epoch 00075: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3163 - accuracy: 0.8579 - val_loss: 0.3765 - val_accuracy: 0.8285\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8575\n",
      "Saving weights for epoch 75\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3171 - accuracy: 0.8576 - val_loss: 0.3829 - val_accuracy: 0.8259\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8645\n",
      "Epoch 00077: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3071 - accuracy: 0.8646 - val_loss: 0.3750 - val_accuracy: 0.8263\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8655\n",
      "Epoch 00078: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3106 - accuracy: 0.8654 - val_loss: 0.3752 - val_accuracy: 0.8287\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8630\n",
      "Epoch 00079: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3099 - accuracy: 0.8631 - val_loss: 0.3693 - val_accuracy: 0.8338\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8652\n",
      "Epoch 00080: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3059 - accuracy: 0.8653 - val_loss: 0.3849 - val_accuracy: 0.8259\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8688\n",
      "Saving weights for epoch 80\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.36301 to 0.36234, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3020 - accuracy: 0.8688 - val_loss: 0.3623 - val_accuracy: 0.8349\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8659\n",
      "Epoch 00082: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3018 - accuracy: 0.8660 - val_loss: 0.3710 - val_accuracy: 0.8351\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8699\n",
      "Epoch 00083: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2979 - accuracy: 0.8700 - val_loss: 0.3735 - val_accuracy: 0.8248\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8658\n",
      "Epoch 00084: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3030 - accuracy: 0.8659 - val_loss: 0.3690 - val_accuracy: 0.8360\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8730\n",
      "Epoch 00085: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2938 - accuracy: 0.8730 - val_loss: 0.3696 - val_accuracy: 0.8347\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8748\n",
      "Saving weights for epoch 85\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2898 - accuracy: 0.8749 - val_loss: 0.3686 - val_accuracy: 0.8315\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.8728\n",
      "Epoch 00087: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2893 - accuracy: 0.8728 - val_loss: 0.3711 - val_accuracy: 0.8325\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8733\n",
      "Epoch 00088: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2904 - accuracy: 0.8733 - val_loss: 0.3735 - val_accuracy: 0.8317\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8768\n",
      "Epoch 00089: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2832 - accuracy: 0.8768 - val_loss: 0.3741 - val_accuracy: 0.8298\n",
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8745\n",
      "Epoch 00090: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2886 - accuracy: 0.8746 - val_loss: 0.3767 - val_accuracy: 0.8306\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8750\n",
      "Saving weights for epoch 90\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2843 - accuracy: 0.8750 - val_loss: 0.3657 - val_accuracy: 0.8351\n",
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8803\n",
      "Epoch 00092: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2786 - accuracy: 0.8803 - val_loss: 0.3670 - val_accuracy: 0.8362\n",
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.8787\n",
      "Epoch 00093: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2778 - accuracy: 0.8788 - val_loss: 0.3680 - val_accuracy: 0.8358\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.8792\n",
      "Epoch 00094: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2791 - accuracy: 0.8791 - val_loss: 0.3868 - val_accuracy: 0.8233\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.8819\n",
      "Epoch 00095: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2752 - accuracy: 0.8820 - val_loss: 0.3747 - val_accuracy: 0.8310\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2691 - accuracy: 0.8851\n",
      "Saving weights for epoch 95\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2689 - accuracy: 0.8850 - val_loss: 0.3881 - val_accuracy: 0.8295\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8814\n",
      "Epoch 00097: val_loss improved from 0.36234 to 0.35756, saving model to pickled_objects/batch_size_32_best_weights_trial_1.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2732 - accuracy: 0.8813 - val_loss: 0.3576 - val_accuracy: 0.8433\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.8850\n",
      "Epoch 00098: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2651 - accuracy: 0.8852 - val_loss: 0.3922 - val_accuracy: 0.8227\n",
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8892\n",
      "Epoch 00099: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2616 - accuracy: 0.8892 - val_loss: 0.4109 - val_accuracy: 0.8207\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8854\n",
      "Epoch 00100: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2657 - accuracy: 0.8855 - val_loss: 0.3755 - val_accuracy: 0.8375\n",
      "Epoch 101/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.8865\n",
      "Saving weights for epoch 100\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2627 - accuracy: 0.8865 - val_loss: 0.3661 - val_accuracy: 0.8368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.8902\n",
      "Epoch 00102: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2590 - accuracy: 0.8902 - val_loss: 0.3678 - val_accuracy: 0.8383\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.8899\n",
      "Epoch 00103: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2585 - accuracy: 0.8897 - val_loss: 0.3884 - val_accuracy: 0.8263\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8890\n",
      "Epoch 00104: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2602 - accuracy: 0.8890 - val_loss: 0.3767 - val_accuracy: 0.8334\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.8884\n",
      "Epoch 00105: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2575 - accuracy: 0.8884 - val_loss: 0.3714 - val_accuracy: 0.8375\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.8924\n",
      "Saving weights for epoch 105\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2483 - accuracy: 0.8925 - val_loss: 0.3833 - val_accuracy: 0.8332\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.8934\n",
      "Epoch 00107: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2524 - accuracy: 0.8934 - val_loss: 0.3965 - val_accuracy: 0.8257\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.8939\n",
      "Epoch 00108: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2530 - accuracy: 0.8939 - val_loss: 0.3748 - val_accuracy: 0.8340\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.8931\n",
      "Epoch 00109: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2498 - accuracy: 0.8931 - val_loss: 0.3911 - val_accuracy: 0.8306\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.8969\n",
      "Epoch 00110: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2412 - accuracy: 0.8970 - val_loss: 0.3883 - val_accuracy: 0.8287\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8991\n",
      "Saving weights for epoch 110\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2435 - accuracy: 0.8989 - val_loss: 0.3842 - val_accuracy: 0.8345\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.8975\n",
      "Epoch 00112: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2419 - accuracy: 0.8974 - val_loss: 0.4049 - val_accuracy: 0.8164\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.8966\n",
      "Epoch 00113: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2398 - accuracy: 0.8967 - val_loss: 0.3776 - val_accuracy: 0.8340\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.8985\n",
      "Epoch 00114: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2400 - accuracy: 0.8986 - val_loss: 0.3724 - val_accuracy: 0.8323\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.9040\n",
      "Epoch 00115: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2274 - accuracy: 0.9042 - val_loss: 0.3896 - val_accuracy: 0.8321\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9018\n",
      "Saving weights for epoch 115\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2333 - accuracy: 0.9018 - val_loss: 0.4003 - val_accuracy: 0.8175\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9007\n",
      "Epoch 00117: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2322 - accuracy: 0.9008 - val_loss: 0.3821 - val_accuracy: 0.8353\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9026\n",
      "Epoch 00118: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2293 - accuracy: 0.9026 - val_loss: 0.4011 - val_accuracy: 0.8261\n",
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9054\n",
      "Epoch 00119: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2262 - accuracy: 0.9055 - val_loss: 0.3797 - val_accuracy: 0.8328\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9003\n",
      "Epoch 00120: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2345 - accuracy: 0.9003 - val_loss: 0.3694 - val_accuracy: 0.8409\n",
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9058\n",
      "Saving weights for epoch 120\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2231 - accuracy: 0.9057 - val_loss: 0.3897 - val_accuracy: 0.8334\n",
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.9065\n",
      "Epoch 00122: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2241 - accuracy: 0.9064 - val_loss: 0.3729 - val_accuracy: 0.8383\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9064\n",
      "Epoch 00123: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2218 - accuracy: 0.9064 - val_loss: 0.3803 - val_accuracy: 0.8347\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9067\n",
      "Epoch 00124: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2240 - accuracy: 0.9066 - val_loss: 0.3790 - val_accuracy: 0.8347\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9099\n",
      "Epoch 00125: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2166 - accuracy: 0.9099 - val_loss: 0.3938 - val_accuracy: 0.8386\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9071\n",
      "Saving weights for epoch 125\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2204 - accuracy: 0.9069 - val_loss: 0.3719 - val_accuracy: 0.8373\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9087\n",
      "Epoch 00127: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2153 - accuracy: 0.9088 - val_loss: 0.3873 - val_accuracy: 0.8343\n",
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.9137\n",
      "Epoch 00128: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2120 - accuracy: 0.9137 - val_loss: 0.3951 - val_accuracy: 0.8332\n",
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9098\n",
      "Epoch 00129: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2108 - accuracy: 0.9098 - val_loss: 0.3951 - val_accuracy: 0.8317\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9122\n",
      "Epoch 00130: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2085 - accuracy: 0.9123 - val_loss: 0.3952 - val_accuracy: 0.8351\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9114\n",
      "Saving weights for epoch 130\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2115 - accuracy: 0.9113 - val_loss: 0.3889 - val_accuracy: 0.8293\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9146\n",
      "Epoch 00132: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2092 - accuracy: 0.9146 - val_loss: 0.3860 - val_accuracy: 0.8353\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9097\n",
      "Epoch 00133: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2111 - accuracy: 0.9098 - val_loss: 0.3791 - val_accuracy: 0.8373\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9176\n",
      "Epoch 00134: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2000 - accuracy: 0.9176 - val_loss: 0.3874 - val_accuracy: 0.8323\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9137\n",
      "Epoch 00135: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2082 - accuracy: 0.9139 - val_loss: 0.4030 - val_accuracy: 0.8267\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9161\n",
      "Saving weights for epoch 135\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1974 - accuracy: 0.9160 - val_loss: 0.4098 - val_accuracy: 0.8287\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9163\n",
      "Epoch 00137: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2017 - accuracy: 0.9162 - val_loss: 0.3937 - val_accuracy: 0.8351\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9167\n",
      "Epoch 00138: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2025 - accuracy: 0.9169 - val_loss: 0.4301 - val_accuracy: 0.8214\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9197\n",
      "Epoch 00139: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1965 - accuracy: 0.9198 - val_loss: 0.4326 - val_accuracy: 0.8153\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9199\n",
      "Epoch 00140: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1965 - accuracy: 0.9200 - val_loss: 0.3909 - val_accuracy: 0.8325\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9162\n",
      "Saving weights for epoch 140\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1986 - accuracy: 0.9162 - val_loss: 0.4026 - val_accuracy: 0.8274\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9194\n",
      "Epoch 00142: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1884 - accuracy: 0.9195 - val_loss: 0.3946 - val_accuracy: 0.8319\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9230\n",
      "Epoch 00143: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1887 - accuracy: 0.9228 - val_loss: 0.4123 - val_accuracy: 0.8212\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9209\n",
      "Epoch 00144: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1938 - accuracy: 0.9208 - val_loss: 0.4160 - val_accuracy: 0.8255\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9231\n",
      "Epoch 00145: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1894 - accuracy: 0.9232 - val_loss: 0.3889 - val_accuracy: 0.8330\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9239\n",
      "Saving weights for epoch 145\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1884 - accuracy: 0.9240 - val_loss: 0.4052 - val_accuracy: 0.8276\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9261\n",
      "Epoch 00147: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1856 - accuracy: 0.9261 - val_loss: 0.3983 - val_accuracy: 0.8338\n",
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9226\n",
      "Epoch 00148: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1869 - accuracy: 0.9225 - val_loss: 0.4142 - val_accuracy: 0.8239\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9214\n",
      "Epoch 00149: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1881 - accuracy: 0.9216 - val_loss: 0.4163 - val_accuracy: 0.8272\n",
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9277\n",
      "Epoch 00150: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1787 - accuracy: 0.9278 - val_loss: 0.4192 - val_accuracy: 0.8278\n",
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9234\n",
      "Saving weights for epoch 150\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1844 - accuracy: 0.9233 - val_loss: 0.4030 - val_accuracy: 0.8353\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9267\n",
      "Epoch 00152: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1819 - accuracy: 0.9268 - val_loss: 0.4242 - val_accuracy: 0.8205\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9287\n",
      "Epoch 00153: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1770 - accuracy: 0.9289 - val_loss: 0.4111 - val_accuracy: 0.8321\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9288\n",
      "Epoch 00154: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1729 - accuracy: 0.9289 - val_loss: 0.4083 - val_accuracy: 0.8338\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9274\n",
      "Epoch 00155: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1765 - accuracy: 0.9273 - val_loss: 0.4194 - val_accuracy: 0.8244\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9294\n",
      "Saving weights for epoch 155\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1736 - accuracy: 0.9294 - val_loss: 0.4112 - val_accuracy: 0.8351\n",
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9292\n",
      "Epoch 00157: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1711 - accuracy: 0.9293 - val_loss: 0.4261 - val_accuracy: 0.8274\n",
      "Epoch 158/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/582 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9318\n",
      "Epoch 00158: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1726 - accuracy: 0.9319 - val_loss: 0.4273 - val_accuracy: 0.8285\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9314\n",
      "Epoch 00159: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1702 - accuracy: 0.9314 - val_loss: 0.4357 - val_accuracy: 0.8293\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9279\n",
      "Epoch 00160: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1769 - accuracy: 0.9277 - val_loss: 0.4142 - val_accuracy: 0.8291\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9340\n",
      "Saving weights for epoch 160\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1666 - accuracy: 0.9341 - val_loss: 0.4247 - val_accuracy: 0.8289\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9327\n",
      "Epoch 00162: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1646 - accuracy: 0.9327 - val_loss: 0.4152 - val_accuracy: 0.8308\n",
      "Epoch 163/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9294\n",
      "Epoch 00163: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1718 - accuracy: 0.9294 - val_loss: 0.4183 - val_accuracy: 0.8356\n",
      "Epoch 164/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9314\n",
      "Epoch 00164: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1666 - accuracy: 0.9315 - val_loss: 0.4110 - val_accuracy: 0.8321\n",
      "Epoch 165/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9318\n",
      "Epoch 00165: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1675 - accuracy: 0.9319 - val_loss: 0.4143 - val_accuracy: 0.8298\n",
      "Epoch 166/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9323\n",
      "Saving weights for epoch 165\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1660 - accuracy: 0.9322 - val_loss: 0.4151 - val_accuracy: 0.8291\n",
      "Epoch 167/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9342\n",
      "Epoch 00167: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1586 - accuracy: 0.9342 - val_loss: 0.4162 - val_accuracy: 0.8319\n",
      "Epoch 168/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9351\n",
      "Epoch 00168: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1608 - accuracy: 0.9350 - val_loss: 0.4172 - val_accuracy: 0.8356\n",
      "Epoch 169/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9378\n",
      "Epoch 00169: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1555 - accuracy: 0.9379 - val_loss: 0.4244 - val_accuracy: 0.8259\n",
      "Epoch 170/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9379\n",
      "Epoch 00170: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1556 - accuracy: 0.9379 - val_loss: 0.4449 - val_accuracy: 0.8110\n",
      "Epoch 171/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9393\n",
      "Saving weights for epoch 170\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1552 - accuracy: 0.9395 - val_loss: 0.4237 - val_accuracy: 0.8300\n",
      "Epoch 172/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9348\n",
      "Epoch 00172: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1603 - accuracy: 0.9348 - val_loss: 0.4200 - val_accuracy: 0.8334\n",
      "Epoch 173/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9363\n",
      "Epoch 00173: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1579 - accuracy: 0.9362 - val_loss: 0.4336 - val_accuracy: 0.8259\n",
      "Epoch 174/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9370\n",
      "Epoch 00174: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1542 - accuracy: 0.9369 - val_loss: 0.4300 - val_accuracy: 0.8282\n",
      "Epoch 175/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9381\n",
      "Epoch 00175: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1568 - accuracy: 0.9382 - val_loss: 0.4504 - val_accuracy: 0.8169\n",
      "Epoch 176/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9367\n",
      "Saving weights for epoch 175\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1598 - accuracy: 0.9367 - val_loss: 0.4363 - val_accuracy: 0.8246\n",
      "Epoch 177/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9422\n",
      "Epoch 00177: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1474 - accuracy: 0.9423 - val_loss: 0.4490 - val_accuracy: 0.8263\n",
      "Epoch 178/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9385\n",
      "Epoch 00178: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1520 - accuracy: 0.9385 - val_loss: 0.4235 - val_accuracy: 0.8310\n",
      "Epoch 179/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9379\n",
      "Epoch 00179: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1520 - accuracy: 0.9379 - val_loss: 0.4439 - val_accuracy: 0.8293\n",
      "Epoch 180/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9397\n",
      "Epoch 00180: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1491 - accuracy: 0.9398 - val_loss: 0.4361 - val_accuracy: 0.8282\n",
      "Epoch 181/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9399\n",
      "Saving weights for epoch 180\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1529 - accuracy: 0.9399 - val_loss: 0.4436 - val_accuracy: 0.8231\n",
      "Epoch 182/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9405\n",
      "Epoch 00182: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1472 - accuracy: 0.9405 - val_loss: 0.4284 - val_accuracy: 0.8317\n",
      "Epoch 183/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9399\n",
      "Epoch 00183: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1467 - accuracy: 0.9399 - val_loss: 0.4459 - val_accuracy: 0.8293\n",
      "Epoch 184/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9426\n",
      "Epoch 00184: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1471 - accuracy: 0.9425 - val_loss: 0.4510 - val_accuracy: 0.8302\n",
      "Epoch 185/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.9413\n",
      "Epoch 00185: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1508 - accuracy: 0.9415 - val_loss: 0.4381 - val_accuracy: 0.8214\n",
      "Epoch 186/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9430\n",
      "Saving weights for epoch 185\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1446 - accuracy: 0.9430 - val_loss: 0.4447 - val_accuracy: 0.8237\n",
      "Epoch 187/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.9428\n",
      "Epoch 00187: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1439 - accuracy: 0.9429 - val_loss: 0.4511 - val_accuracy: 0.8280\n",
      "Epoch 188/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.9454\n",
      "Epoch 00188: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1401 - accuracy: 0.9455 - val_loss: 0.4648 - val_accuracy: 0.8175\n",
      "Epoch 189/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9434\n",
      "Epoch 00189: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1437 - accuracy: 0.9434 - val_loss: 0.4515 - val_accuracy: 0.8250\n",
      "Epoch 190/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9466\n",
      "Epoch 00190: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1380 - accuracy: 0.9465 - val_loss: 0.4478 - val_accuracy: 0.8274\n",
      "Epoch 191/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9459\n",
      "Saving weights for epoch 190\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1359 - accuracy: 0.9459 - val_loss: 0.4612 - val_accuracy: 0.8239\n",
      "Epoch 192/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9456\n",
      "Epoch 00192: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1388 - accuracy: 0.9456 - val_loss: 0.4481 - val_accuracy: 0.8272\n",
      "Epoch 193/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9459\n",
      "Epoch 00193: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1382 - accuracy: 0.9460 - val_loss: 0.4838 - val_accuracy: 0.8162\n",
      "Epoch 194/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9452\n",
      "Epoch 00194: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1342 - accuracy: 0.9452 - val_loss: 0.4695 - val_accuracy: 0.8196\n",
      "Epoch 195/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9454\n",
      "Epoch 00195: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1387 - accuracy: 0.9455 - val_loss: 0.4888 - val_accuracy: 0.8147\n",
      "Epoch 196/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9450\n",
      "Saving weights for epoch 195\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1338 - accuracy: 0.9450 - val_loss: 0.4619 - val_accuracy: 0.8237\n",
      "Epoch 197/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9473\n",
      "Epoch 00197: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1334 - accuracy: 0.9473 - val_loss: 0.4602 - val_accuracy: 0.8257\n",
      "Epoch 00197: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 23s 313ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "Saving weights for epoch 0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69310, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 29s 393ms/step - loss: 0.6936 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5151\n",
      "Epoch 00002: val_loss improved from 0.69310 to 0.69276, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6928 - val_accuracy: 0.4951\n",
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5193\n",
      "Epoch 00003: val_loss improved from 0.69276 to 0.69275, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6920 - accuracy: 0.5196 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.5432\n",
      "Epoch 00004: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6906 - accuracy: 0.5430 - val_loss: 0.6928 - val_accuracy: 0.4914\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5567\n",
      "Epoch 00005: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6895 - accuracy: 0.5562 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.5695\n",
      "Saving weights for epoch 5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6876 - accuracy: 0.5693 - val_loss: 0.6928 - val_accuracy: 0.4923\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5744\n",
      "Epoch 00007: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6852 - accuracy: 0.5747 - val_loss: 0.6929 - val_accuracy: 0.4931\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5803\n",
      "Epoch 00008: val_loss improved from 0.69275 to 0.69264, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6834 - accuracy: 0.5805 - val_loss: 0.6926 - val_accuracy: 0.4946\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5832\n",
      "Epoch 00009: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6809 - accuracy: 0.5828 - val_loss: 0.6935 - val_accuracy: 0.4953\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5864\n",
      "Epoch 00010: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6780 - accuracy: 0.5865 - val_loss: 0.6939 - val_accuracy: 0.4981\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5869\n",
      "Saving weights for epoch 10\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6761 - accuracy: 0.5870 - val_loss: 0.6958 - val_accuracy: 0.4994\n",
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6729 - accuracy: 0.5921\n",
      "Epoch 00012: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6727 - accuracy: 0.5920 - val_loss: 0.6999 - val_accuracy: 0.4991\n",
      "Epoch 13/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6718 - accuracy: 0.5926\n",
      "Epoch 00013: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6717 - accuracy: 0.5930 - val_loss: 0.6992 - val_accuracy: 0.5034\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6686 - accuracy: 0.5965\n",
      "Epoch 00014: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6685 - accuracy: 0.5967 - val_loss: 0.7036 - val_accuracy: 0.5037\n",
      "Epoch 15/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.6672 - accuracy: 0.5995\n",
      "Epoch 00015: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6670 - accuracy: 0.6000 - val_loss: 0.6963 - val_accuracy: 0.5157\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6642 - accuracy: 0.6045\n",
      "Saving weights for epoch 15\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.69264 to 0.69188, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6639 - accuracy: 0.6049 - val_loss: 0.6919 - val_accuracy: 0.5277\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.6030\n",
      "Epoch 00017: val_loss did not improve from 0.69188\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6615 - accuracy: 0.6031 - val_loss: 0.7072 - val_accuracy: 0.5107\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.5980\n",
      "Epoch 00018: val_loss improved from 0.69188 to 0.68787, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6632 - accuracy: 0.5984 - val_loss: 0.6879 - val_accuracy: 0.5406\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6601 - accuracy: 0.6050\n",
      "Epoch 00019: val_loss did not improve from 0.68787\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6597 - accuracy: 0.6056 - val_loss: 0.6881 - val_accuracy: 0.5432\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6592 - accuracy: 0.6059\n",
      "Epoch 00020: val_loss improved from 0.68787 to 0.68777, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6590 - accuracy: 0.6061 - val_loss: 0.6878 - val_accuracy: 0.5466\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6588 - accuracy: 0.6047\n",
      "Saving weights for epoch 20\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.68777 to 0.68541, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6587 - accuracy: 0.6051 - val_loss: 0.6854 - val_accuracy: 0.5497\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6562 - accuracy: 0.6080\n",
      "Epoch 00022: val_loss improved from 0.68541 to 0.67720, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6560 - accuracy: 0.6084 - val_loss: 0.6772 - val_accuracy: 0.5692\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6558 - accuracy: 0.6082\n",
      "Epoch 00023: val_loss improved from 0.67720 to 0.67496, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6556 - accuracy: 0.6088 - val_loss: 0.6750 - val_accuracy: 0.5739\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6546 - accuracy: 0.6094\n",
      "Epoch 00024: val_loss improved from 0.67496 to 0.67310, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6544 - accuracy: 0.6102 - val_loss: 0.6731 - val_accuracy: 0.5772\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6536 - accuracy: 0.6118\n",
      "Epoch 00025: val_loss did not improve from 0.67310\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6534 - accuracy: 0.6123 - val_loss: 0.6732 - val_accuracy: 0.5757\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6521 - accuracy: 0.6156\n",
      "Saving weights for epoch 25\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.67310 to 0.66970, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6517 - accuracy: 0.6161 - val_loss: 0.6697 - val_accuracy: 0.5808\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6499 - accuracy: 0.6125\n",
      "Epoch 00027: val_loss improved from 0.66970 to 0.66447, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.6496 - accuracy: 0.6133 - val_loss: 0.6645 - val_accuracy: 0.5896\n",
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6135\n",
      "Epoch 00028: val_loss improved from 0.66447 to 0.66290, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6491 - accuracy: 0.6143 - val_loss: 0.6629 - val_accuracy: 0.5937\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6474 - accuracy: 0.6170\n",
      "Epoch 00029: val_loss did not improve from 0.66290\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6472 - accuracy: 0.6176 - val_loss: 0.6639 - val_accuracy: 0.5901\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6473 - accuracy: 0.6167\n",
      "Epoch 00030: val_loss improved from 0.66290 to 0.66075, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6468 - accuracy: 0.6175 - val_loss: 0.6608 - val_accuracy: 0.5976\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.6201\n",
      "Saving weights for epoch 30\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66075\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6461 - accuracy: 0.6204 - val_loss: 0.6648 - val_accuracy: 0.5875\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6444 - accuracy: 0.6205\n",
      "Epoch 00032: val_loss improved from 0.66075 to 0.65843, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6442 - accuracy: 0.6206 - val_loss: 0.6584 - val_accuracy: 0.5982\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.6211\n",
      "Epoch 00033: val_loss improved from 0.65843 to 0.65058, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6443 - accuracy: 0.6216 - val_loss: 0.6506 - val_accuracy: 0.6135\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.6203\n",
      "Epoch 00034: val_loss improved from 0.65058 to 0.64900, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6432 - accuracy: 0.6207 - val_loss: 0.6490 - val_accuracy: 0.6139\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6420 - accuracy: 0.6263\n",
      "Epoch 00035: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6417 - accuracy: 0.6264 - val_loss: 0.6502 - val_accuracy: 0.6120\n",
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6411 - accuracy: 0.6246\n",
      "Saving weights for epoch 35\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6410 - accuracy: 0.6249 - val_loss: 0.6508 - val_accuracy: 0.6124\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.6228\n",
      "Epoch 00037: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6401 - accuracy: 0.6229 - val_loss: 0.6505 - val_accuracy: 0.6098\n",
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.6283\n",
      "Epoch 00038: val_loss improved from 0.64900 to 0.63857, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6383 - accuracy: 0.6284 - val_loss: 0.6386 - val_accuracy: 0.6264\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.6312\n",
      "Epoch 00039: val_loss did not improve from 0.63857\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6371 - accuracy: 0.6314 - val_loss: 0.6449 - val_accuracy: 0.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.6321\n",
      "Epoch 00040: val_loss did not improve from 0.63857\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6357 - accuracy: 0.6322 - val_loss: 0.6422 - val_accuracy: 0.6195\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6350 - accuracy: 0.6316\n",
      "Saving weights for epoch 40\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.63857 to 0.63716, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6349 - accuracy: 0.6318 - val_loss: 0.6372 - val_accuracy: 0.6262\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6329 - accuracy: 0.6382\n",
      "Epoch 00042: val_loss did not improve from 0.63716\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.6327 - accuracy: 0.6384 - val_loss: 0.6390 - val_accuracy: 0.6227\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6324 - accuracy: 0.6319\n",
      "Epoch 00043: val_loss improved from 0.63716 to 0.63604, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6323 - accuracy: 0.6319 - val_loss: 0.6360 - val_accuracy: 0.6268\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6309 - accuracy: 0.6365\n",
      "Epoch 00044: val_loss did not improve from 0.63604\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6311 - accuracy: 0.6363 - val_loss: 0.6413 - val_accuracy: 0.6202\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6282 - accuracy: 0.6397\n",
      "Epoch 00045: val_loss improved from 0.63604 to 0.62894, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6282 - accuracy: 0.6397 - val_loss: 0.6289 - val_accuracy: 0.6391\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.6404\n",
      "Saving weights for epoch 45\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.62894\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6289 - accuracy: 0.6402 - val_loss: 0.6301 - val_accuracy: 0.6354\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6275 - accuracy: 0.6404\n",
      "Epoch 00047: val_loss did not improve from 0.62894\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6275 - accuracy: 0.6403 - val_loss: 0.6313 - val_accuracy: 0.6320\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.6486\n",
      "Epoch 00048: val_loss improved from 0.62894 to 0.62723, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6247 - accuracy: 0.6485 - val_loss: 0.6272 - val_accuracy: 0.6346\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6237 - accuracy: 0.6451\n",
      "Epoch 00049: val_loss did not improve from 0.62723\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6239 - accuracy: 0.6451 - val_loss: 0.6289 - val_accuracy: 0.6346\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.6450\n",
      "Epoch 00050: val_loss improved from 0.62723 to 0.62277, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6229 - accuracy: 0.6451 - val_loss: 0.6228 - val_accuracy: 0.6395\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6211 - accuracy: 0.6460\n",
      "Saving weights for epoch 50\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.62277\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6213 - accuracy: 0.6458 - val_loss: 0.6283 - val_accuracy: 0.6354\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.6513\n",
      "Epoch 00052: val_loss improved from 0.62277 to 0.61855, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6183 - accuracy: 0.6514 - val_loss: 0.6186 - val_accuracy: 0.6438\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6182 - accuracy: 0.6523\n",
      "Epoch 00053: val_loss did not improve from 0.61855\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6183 - accuracy: 0.6522 - val_loss: 0.6206 - val_accuracy: 0.6417\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.6564\n",
      "Epoch 00054: val_loss did not improve from 0.61855\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6140 - accuracy: 0.6560 - val_loss: 0.6257 - val_accuracy: 0.6376\n",
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.6608\n",
      "Epoch 00055: val_loss improved from 0.61855 to 0.61534, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6119 - accuracy: 0.6606 - val_loss: 0.6153 - val_accuracy: 0.6455\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6089 - accuracy: 0.6610\n",
      "Saving weights for epoch 55\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.61534\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6093 - accuracy: 0.6605 - val_loss: 0.6218 - val_accuracy: 0.6408\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.6654\n",
      "Epoch 00057: val_loss improved from 0.61534 to 0.61423, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6063 - accuracy: 0.6649 - val_loss: 0.6142 - val_accuracy: 0.6492\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.6659\n",
      "Epoch 00058: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6046 - accuracy: 0.6654 - val_loss: 0.6165 - val_accuracy: 0.6429\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.6698\n",
      "Epoch 00059: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5999 - accuracy: 0.6694 - val_loss: 0.6201 - val_accuracy: 0.6406\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5961 - accuracy: 0.6744\n",
      "Epoch 00060: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5966 - accuracy: 0.6738 - val_loss: 0.6181 - val_accuracy: 0.6436\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.6757\n",
      "Saving weights for epoch 60\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5950 - accuracy: 0.6752 - val_loss: 0.6188 - val_accuracy: 0.6434\n",
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.6797\n",
      "Epoch 00062: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5924 - accuracy: 0.6791 - val_loss: 0.6256 - val_accuracy: 0.6341\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5869 - accuracy: 0.6858\n",
      "Epoch 00063: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5874 - accuracy: 0.6850 - val_loss: 0.6273 - val_accuracy: 0.6318\n",
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.6852\n",
      "Epoch 00064: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5847 - accuracy: 0.6847 - val_loss: 0.6244 - val_accuracy: 0.6348\n",
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.6873\n",
      "Epoch 00065: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5819 - accuracy: 0.6863 - val_loss: 0.6206 - val_accuracy: 0.6361\n",
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.6898\n",
      "Saving weights for epoch 65\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5803 - accuracy: 0.6895 - val_loss: 0.6283 - val_accuracy: 0.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.6937\n",
      "Epoch 00067: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5778 - accuracy: 0.6931 - val_loss: 0.6296 - val_accuracy: 0.6279\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.6969\n",
      "Epoch 00068: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5741 - accuracy: 0.6964 - val_loss: 0.6387 - val_accuracy: 0.6249\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.6949\n",
      "Epoch 00069: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5745 - accuracy: 0.6941 - val_loss: 0.6308 - val_accuracy: 0.6303\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5716 - accuracy: 0.6934\n",
      "Epoch 00070: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5723 - accuracy: 0.6927 - val_loss: 0.6326 - val_accuracy: 0.6292\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7043\n",
      "Saving weights for epoch 70\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5678 - accuracy: 0.7038 - val_loss: 0.6282 - val_accuracy: 0.6311\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7055\n",
      "Epoch 00072: val_loss improved from 0.61423 to 0.61340, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5634 - accuracy: 0.7048 - val_loss: 0.6134 - val_accuracy: 0.6457\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7050\n",
      "Epoch 00073: val_loss improved from 0.61340 to 0.59906, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5643 - accuracy: 0.7046 - val_loss: 0.5991 - val_accuracy: 0.6552\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7052\n",
      "Epoch 00074: val_loss did not improve from 0.59906\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5612 - accuracy: 0.7047 - val_loss: 0.6282 - val_accuracy: 0.6292\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7078\n",
      "Epoch 00075: val_loss improved from 0.59906 to 0.59271, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5627 - accuracy: 0.7074 - val_loss: 0.5927 - val_accuracy: 0.6608\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7079\n",
      "Saving weights for epoch 75\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5614 - accuracy: 0.7073 - val_loss: 0.6067 - val_accuracy: 0.6528\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5586 - accuracy: 0.7083\n",
      "Epoch 00077: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5592 - accuracy: 0.7080 - val_loss: 0.6116 - val_accuracy: 0.6477\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5580 - accuracy: 0.7063\n",
      "Epoch 00078: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5587 - accuracy: 0.7054 - val_loss: 0.5977 - val_accuracy: 0.6604\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7167\n",
      "Epoch 00079: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5529 - accuracy: 0.7165 - val_loss: 0.6174 - val_accuracy: 0.6507\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7161\n",
      "Epoch 00080: val_loss improved from 0.59271 to 0.57793, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5521 - accuracy: 0.7154 - val_loss: 0.5779 - val_accuracy: 0.6733\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7134\n",
      "Saving weights for epoch 80\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.57793\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5521 - accuracy: 0.7130 - val_loss: 0.5883 - val_accuracy: 0.6625\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7197\n",
      "Epoch 00082: val_loss improved from 0.57793 to 0.56616, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5465 - accuracy: 0.7193 - val_loss: 0.5662 - val_accuracy: 0.6838\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7199\n",
      "Epoch 00083: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5455 - accuracy: 0.7192 - val_loss: 0.5971 - val_accuracy: 0.6634\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7215\n",
      "Epoch 00084: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5469 - accuracy: 0.7211 - val_loss: 0.5812 - val_accuracy: 0.6687\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7223\n",
      "Epoch 00085: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5444 - accuracy: 0.7217 - val_loss: 0.5770 - val_accuracy: 0.6786\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5422 - accuracy: 0.7202\n",
      "Saving weights for epoch 85\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5430 - accuracy: 0.7193 - val_loss: 0.5938 - val_accuracy: 0.6636\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7252\n",
      "Epoch 00087: val_loss improved from 0.56616 to 0.56482, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5396 - accuracy: 0.7245 - val_loss: 0.5648 - val_accuracy: 0.6862\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5370 - accuracy: 0.7288\n",
      "Epoch 00088: val_loss improved from 0.56482 to 0.56343, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5377 - accuracy: 0.7285 - val_loss: 0.5634 - val_accuracy: 0.6885\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7255\n",
      "Epoch 00089: val_loss did not improve from 0.56343\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5391 - accuracy: 0.7248 - val_loss: 0.5691 - val_accuracy: 0.6791\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7273\n",
      "Epoch 00090: val_loss improved from 0.56343 to 0.55753, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5370 - accuracy: 0.7270 - val_loss: 0.5575 - val_accuracy: 0.6999\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7263\n",
      "Saving weights for epoch 90\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5383 - accuracy: 0.7257 - val_loss: 0.5752 - val_accuracy: 0.6847\n",
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7296\n",
      "Epoch 00092: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5352 - accuracy: 0.7291 - val_loss: 0.5666 - val_accuracy: 0.6853\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5317 - accuracy: 0.7310\n",
      "Epoch 00093: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5324 - accuracy: 0.7304 - val_loss: 0.5599 - val_accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7333\n",
      "Epoch 00094: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5311 - accuracy: 0.7329 - val_loss: 0.5588 - val_accuracy: 0.6928\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5314 - accuracy: 0.7296\n",
      "Epoch 00095: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5321 - accuracy: 0.7292 - val_loss: 0.5617 - val_accuracy: 0.6933\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5312 - accuracy: 0.7309\n",
      "Saving weights for epoch 95\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5318 - accuracy: 0.7303 - val_loss: 0.5827 - val_accuracy: 0.6791\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7341\n",
      "Epoch 00097: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5285 - accuracy: 0.7337 - val_loss: 0.5577 - val_accuracy: 0.6978\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7337\n",
      "Epoch 00098: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5268 - accuracy: 0.7336 - val_loss: 0.5578 - val_accuracy: 0.6922\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7348\n",
      "Epoch 00099: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5297 - accuracy: 0.7343 - val_loss: 0.5667 - val_accuracy: 0.6868\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7333\n",
      "Epoch 00100: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5286 - accuracy: 0.7328 - val_loss: 0.5579 - val_accuracy: 0.6935\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5182 - accuracy: 0.7403\n",
      "Saving weights for epoch 100\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.55753 to 0.55664, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5184 - accuracy: 0.7402 - val_loss: 0.5566 - val_accuracy: 0.7016\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5214 - accuracy: 0.7397\n",
      "Epoch 00102: val_loss improved from 0.55664 to 0.54574, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5225 - accuracy: 0.7390 - val_loss: 0.5457 - val_accuracy: 0.7061\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7356\n",
      "Epoch 00103: val_loss did not improve from 0.54574\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5256 - accuracy: 0.7352 - val_loss: 0.5561 - val_accuracy: 0.6930\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5210 - accuracy: 0.7389\n",
      "Epoch 00104: val_loss did not improve from 0.54574\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5216 - accuracy: 0.7384 - val_loss: 0.5506 - val_accuracy: 0.7012\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7391\n",
      "Epoch 00105: val_loss improved from 0.54574 to 0.52690, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5199 - accuracy: 0.7387 - val_loss: 0.5269 - val_accuracy: 0.7268\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7402\n",
      "Saving weights for epoch 105\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5208 - accuracy: 0.7397 - val_loss: 0.5566 - val_accuracy: 0.6999\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.7422\n",
      "Epoch 00107: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5195 - accuracy: 0.7420 - val_loss: 0.5721 - val_accuracy: 0.6924\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7434\n",
      "Epoch 00108: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5179 - accuracy: 0.7429 - val_loss: 0.5318 - val_accuracy: 0.7184\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5178 - accuracy: 0.7413\n",
      "Epoch 00109: val_loss improved from 0.52690 to 0.51703, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5185 - accuracy: 0.7407 - val_loss: 0.5170 - val_accuracy: 0.7339\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5138 - accuracy: 0.7457\n",
      "Epoch 00110: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5145 - accuracy: 0.7450 - val_loss: 0.5182 - val_accuracy: 0.7343\n",
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5170 - accuracy: 0.7409\n",
      "Saving weights for epoch 110\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5178 - accuracy: 0.7403 - val_loss: 0.5421 - val_accuracy: 0.7077\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7461\n",
      "Epoch 00112: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5141 - accuracy: 0.7455 - val_loss: 0.5176 - val_accuracy: 0.7350\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7466\n",
      "Epoch 00113: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5128 - accuracy: 0.7460 - val_loss: 0.5734 - val_accuracy: 0.6885\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5126 - accuracy: 0.7453\n",
      "Epoch 00114: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5135 - accuracy: 0.7447 - val_loss: 0.5436 - val_accuracy: 0.7102\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7472\n",
      "Epoch 00115: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5121 - accuracy: 0.7473 - val_loss: 0.5941 - val_accuracy: 0.6754\n",
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7493\n",
      "Saving weights for epoch 115\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5101 - accuracy: 0.7493 - val_loss: 0.5421 - val_accuracy: 0.7096\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5114 - accuracy: 0.7463\n",
      "Epoch 00117: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5124 - accuracy: 0.7454 - val_loss: 0.5253 - val_accuracy: 0.7225\n",
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5068 - accuracy: 0.7491\n",
      "Epoch 00118: val_loss improved from 0.51703 to 0.51377, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5071 - accuracy: 0.7486 - val_loss: 0.5138 - val_accuracy: 0.7388\n",
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.7471\n",
      "Epoch 00119: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5089 - accuracy: 0.7463 - val_loss: 0.5286 - val_accuracy: 0.7216\n",
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7486\n",
      "Epoch 00120: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5044 - accuracy: 0.7486 - val_loss: 0.5705 - val_accuracy: 0.6917\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7511\n",
      "Saving weights for epoch 120\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5069 - accuracy: 0.7506 - val_loss: 0.5445 - val_accuracy: 0.7100\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.7516\n",
      "Epoch 00122: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5040 - accuracy: 0.7516 - val_loss: 0.5793 - val_accuracy: 0.6847\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7515\n",
      "Epoch 00123: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5050 - accuracy: 0.7509 - val_loss: 0.5152 - val_accuracy: 0.7326\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7507\n",
      "Epoch 00124: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5047 - accuracy: 0.7503 - val_loss: 0.5421 - val_accuracy: 0.7113\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4995 - accuracy: 0.7557\n",
      "Epoch 00125: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5000 - accuracy: 0.7552 - val_loss: 0.5207 - val_accuracy: 0.7272\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5017 - accuracy: 0.7540\n",
      "Saving weights for epoch 125\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5024 - accuracy: 0.7536 - val_loss: 0.5239 - val_accuracy: 0.7261\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5018 - accuracy: 0.7546\n",
      "Epoch 00127: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5022 - accuracy: 0.7542 - val_loss: 0.5356 - val_accuracy: 0.7173\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7563\n",
      "Epoch 00128: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5008 - accuracy: 0.7564 - val_loss: 0.5420 - val_accuracy: 0.7152\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.7564\n",
      "Epoch 00129: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4994 - accuracy: 0.7560 - val_loss: 0.5300 - val_accuracy: 0.7229\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.7510\n",
      "Epoch 00130: val_loss improved from 0.51377 to 0.50360, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5021 - accuracy: 0.7505 - val_loss: 0.5036 - val_accuracy: 0.7423\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4976 - accuracy: 0.7540\n",
      "Saving weights for epoch 130\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4983 - accuracy: 0.7537 - val_loss: 0.5337 - val_accuracy: 0.7178\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7550\n",
      "Epoch 00132: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4979 - accuracy: 0.7545 - val_loss: 0.5401 - val_accuracy: 0.7128\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4990 - accuracy: 0.7535\n",
      "Epoch 00133: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4994 - accuracy: 0.7531 - val_loss: 0.5262 - val_accuracy: 0.7231\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.7569\n",
      "Epoch 00134: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4966 - accuracy: 0.7564 - val_loss: 0.5118 - val_accuracy: 0.7375\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.7562\n",
      "Epoch 00135: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4941 - accuracy: 0.7558 - val_loss: 0.5141 - val_accuracy: 0.7341\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4942 - accuracy: 0.7546\n",
      "Saving weights for epoch 135\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4948 - accuracy: 0.7540 - val_loss: 0.5272 - val_accuracy: 0.7279\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.7545\n",
      "Epoch 00137: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4968 - accuracy: 0.7542 - val_loss: 0.5046 - val_accuracy: 0.7423\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7576\n",
      "Epoch 00138: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4957 - accuracy: 0.7575 - val_loss: 0.5138 - val_accuracy: 0.7337\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4932 - accuracy: 0.7592\n",
      "Epoch 00139: val_loss improved from 0.50360 to 0.50187, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4939 - accuracy: 0.7589 - val_loss: 0.5019 - val_accuracy: 0.7431\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4872 - accuracy: 0.7627\n",
      "Epoch 00140: val_loss did not improve from 0.50187\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4878 - accuracy: 0.7624 - val_loss: 0.5177 - val_accuracy: 0.7339\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7618\n",
      "Saving weights for epoch 140\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.50187 to 0.49735, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4915 - accuracy: 0.7615 - val_loss: 0.4973 - val_accuracy: 0.7498\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7629\n",
      "Epoch 00142: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4896 - accuracy: 0.7628 - val_loss: 0.5117 - val_accuracy: 0.7384\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4934 - accuracy: 0.7593\n",
      "Epoch 00143: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4944 - accuracy: 0.7589 - val_loss: 0.5033 - val_accuracy: 0.7448\n",
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.7626\n",
      "Epoch 00144: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4897 - accuracy: 0.7624 - val_loss: 0.5409 - val_accuracy: 0.7139\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7613\n",
      "Epoch 00145: val_loss improved from 0.49735 to 0.48245, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4900 - accuracy: 0.7615 - val_loss: 0.4825 - val_accuracy: 0.7607\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.7611\n",
      "Saving weights for epoch 145\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4885 - accuracy: 0.7607 - val_loss: 0.5055 - val_accuracy: 0.7418\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7613\n",
      "Epoch 00147: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4881 - accuracy: 0.7607 - val_loss: 0.5089 - val_accuracy: 0.7393\n",
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.7617\n",
      "Epoch 00148: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4888 - accuracy: 0.7616 - val_loss: 0.5405 - val_accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7632\n",
      "Epoch 00149: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4885 - accuracy: 0.7626 - val_loss: 0.5038 - val_accuracy: 0.7446\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4861 - accuracy: 0.7611\n",
      "Epoch 00150: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4868 - accuracy: 0.7608 - val_loss: 0.5185 - val_accuracy: 0.7289\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4830 - accuracy: 0.7660\n",
      "Saving weights for epoch 150\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4834 - accuracy: 0.7659 - val_loss: 0.5031 - val_accuracy: 0.7444\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4838 - accuracy: 0.7622\n",
      "Epoch 00152: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4840 - accuracy: 0.7621 - val_loss: 0.5078 - val_accuracy: 0.7397\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7649\n",
      "Epoch 00153: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4854 - accuracy: 0.7645 - val_loss: 0.5197 - val_accuracy: 0.7326\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4858 - accuracy: 0.7668\n",
      "Epoch 00154: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4859 - accuracy: 0.7665 - val_loss: 0.5060 - val_accuracy: 0.7401\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4837 - accuracy: 0.7664\n",
      "Epoch 00155: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4841 - accuracy: 0.7659 - val_loss: 0.4946 - val_accuracy: 0.7504\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7671\n",
      "Saving weights for epoch 155\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4829 - accuracy: 0.7669 - val_loss: 0.5039 - val_accuracy: 0.7420\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.7647\n",
      "Epoch 00157: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4858 - accuracy: 0.7645 - val_loss: 0.5611 - val_accuracy: 0.7055\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.7675\n",
      "Epoch 00158: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4805 - accuracy: 0.7674 - val_loss: 0.5525 - val_accuracy: 0.7096\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4812 - accuracy: 0.7678\n",
      "Epoch 00159: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4815 - accuracy: 0.7679 - val_loss: 0.5141 - val_accuracy: 0.7365\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.7669\n",
      "Epoch 00160: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4826 - accuracy: 0.7665 - val_loss: 0.5471 - val_accuracy: 0.7126\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.7693\n",
      "Saving weights for epoch 160\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4810 - accuracy: 0.7690 - val_loss: 0.4900 - val_accuracy: 0.7502\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.7648\n",
      "Epoch 00162: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4814 - accuracy: 0.7644 - val_loss: 0.4934 - val_accuracy: 0.7498\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7701\n",
      "Epoch 00163: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4786 - accuracy: 0.7697 - val_loss: 0.5104 - val_accuracy: 0.7390\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.7649\n",
      "Epoch 00164: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4802 - accuracy: 0.7646 - val_loss: 0.4962 - val_accuracy: 0.7494\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4774 - accuracy: 0.7664\n",
      "Epoch 00165: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4781 - accuracy: 0.7665 - val_loss: 0.5666 - val_accuracy: 0.7003\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7706\n",
      "Saving weights for epoch 165\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4761 - accuracy: 0.7704 - val_loss: 0.5225 - val_accuracy: 0.7283\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7673\n",
      "Epoch 00167: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4773 - accuracy: 0.7668 - val_loss: 0.4868 - val_accuracy: 0.7547\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7695\n",
      "Epoch 00168: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4766 - accuracy: 0.7694 - val_loss: 0.4885 - val_accuracy: 0.7515\n",
      "Epoch 169/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.7713\n",
      "Epoch 00169: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4738 - accuracy: 0.7709 - val_loss: 0.4951 - val_accuracy: 0.7481\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7702\n",
      "Epoch 00170: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4761 - accuracy: 0.7699 - val_loss: 0.4834 - val_accuracy: 0.7537\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.7658\n",
      "Saving weights for epoch 170\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.48245 to 0.47492, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4759 - accuracy: 0.7656 - val_loss: 0.4749 - val_accuracy: 0.7588\n",
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7695\n",
      "Epoch 00172: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4751 - accuracy: 0.7694 - val_loss: 0.4995 - val_accuracy: 0.7457\n",
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7712\n",
      "Epoch 00173: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4756 - accuracy: 0.7709 - val_loss: 0.4813 - val_accuracy: 0.7586\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.7719\n",
      "Epoch 00174: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4730 - accuracy: 0.7717 - val_loss: 0.4996 - val_accuracy: 0.7453\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.7729\n",
      "Epoch 00175: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4749 - accuracy: 0.7724 - val_loss: 0.4823 - val_accuracy: 0.7580\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7713\n",
      "Saving weights for epoch 175\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4720 - accuracy: 0.7708 - val_loss: 0.5139 - val_accuracy: 0.7345\n",
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4679 - accuracy: 0.7744\n",
      "Epoch 00177: val_loss improved from 0.47492 to 0.46389, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4688 - accuracy: 0.7737 - val_loss: 0.4639 - val_accuracy: 0.7696\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4713 - accuracy: 0.7706\n",
      "Epoch 00178: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4719 - accuracy: 0.7704 - val_loss: 0.4859 - val_accuracy: 0.7549\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7742\n",
      "Epoch 00179: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4718 - accuracy: 0.7742 - val_loss: 0.5128 - val_accuracy: 0.7354\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7710\n",
      "Epoch 00180: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4736 - accuracy: 0.7709 - val_loss: 0.4658 - val_accuracy: 0.7674\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7741\n",
      "Saving weights for epoch 180\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4668 - accuracy: 0.7736 - val_loss: 0.4849 - val_accuracy: 0.7558\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7738\n",
      "Epoch 00182: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4697 - accuracy: 0.7737 - val_loss: 0.5138 - val_accuracy: 0.7330\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.7747\n",
      "Epoch 00183: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4683 - accuracy: 0.7745 - val_loss: 0.4831 - val_accuracy: 0.7560\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7722\n",
      "Epoch 00184: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4695 - accuracy: 0.7719 - val_loss: 0.4708 - val_accuracy: 0.7642\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4680 - accuracy: 0.7715\n",
      "Epoch 00185: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4686 - accuracy: 0.7713 - val_loss: 0.4923 - val_accuracy: 0.7494\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.7765\n",
      "Saving weights for epoch 185\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4662 - accuracy: 0.7767 - val_loss: 0.4945 - val_accuracy: 0.7479\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7759\n",
      "Epoch 00187: val_loss improved from 0.46389 to 0.45718, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4670 - accuracy: 0.7759 - val_loss: 0.4572 - val_accuracy: 0.7814\n",
      "Epoch 188/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4666 - accuracy: 0.7751\n",
      "Epoch 00188: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4673 - accuracy: 0.7750 - val_loss: 0.4635 - val_accuracy: 0.7687\n",
      "Epoch 189/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7754\n",
      "Epoch 00189: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4650 - accuracy: 0.7754 - val_loss: 0.4823 - val_accuracy: 0.7562\n",
      "Epoch 190/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.7754\n",
      "Epoch 00190: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4694 - accuracy: 0.7752 - val_loss: 0.4729 - val_accuracy: 0.7616\n",
      "Epoch 191/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7794\n",
      "Saving weights for epoch 190\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.45718 to 0.45063, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4651 - accuracy: 0.7790 - val_loss: 0.4506 - val_accuracy: 0.7818\n",
      "Epoch 192/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7767\n",
      "Epoch 00192: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4660 - accuracy: 0.7761 - val_loss: 0.4760 - val_accuracy: 0.7590\n",
      "Epoch 193/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7760\n",
      "Epoch 00193: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4643 - accuracy: 0.7760 - val_loss: 0.4661 - val_accuracy: 0.7670\n",
      "Epoch 194/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7802\n",
      "Epoch 00194: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4607 - accuracy: 0.7802 - val_loss: 0.5064 - val_accuracy: 0.7408\n",
      "Epoch 195/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.7779\n",
      "Epoch 00195: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4637 - accuracy: 0.7775 - val_loss: 0.4702 - val_accuracy: 0.7638\n",
      "Epoch 196/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7807\n",
      "Saving weights for epoch 195\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4569 - accuracy: 0.7805 - val_loss: 0.4861 - val_accuracy: 0.7521\n",
      "Epoch 197/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7782\n",
      "Epoch 00197: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4599 - accuracy: 0.7782 - val_loss: 0.4818 - val_accuracy: 0.7562\n",
      "Epoch 198/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7772\n",
      "Epoch 00198: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4612 - accuracy: 0.7773 - val_loss: 0.4693 - val_accuracy: 0.7646\n",
      "Epoch 199/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4594 - accuracy: 0.7788\n",
      "Epoch 00199: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4604 - accuracy: 0.7785 - val_loss: 0.4684 - val_accuracy: 0.7661\n",
      "Epoch 200/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4603 - accuracy: 0.7790\n",
      "Epoch 00200: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4606 - accuracy: 0.7788 - val_loss: 0.4992 - val_accuracy: 0.7440\n",
      "Epoch 201/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.7824\n",
      "Saving weights for epoch 200\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4582 - accuracy: 0.7822 - val_loss: 0.5122 - val_accuracy: 0.7375\n",
      "Epoch 202/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.7800\n",
      "Epoch 00202: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4593 - accuracy: 0.7796 - val_loss: 0.5008 - val_accuracy: 0.7427\n",
      "Epoch 203/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7808\n",
      "Epoch 00203: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4589 - accuracy: 0.7804 - val_loss: 0.4889 - val_accuracy: 0.7541\n",
      "Epoch 204/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4570 - accuracy: 0.7824\n",
      "Epoch 00204: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4579 - accuracy: 0.7819 - val_loss: 0.4696 - val_accuracy: 0.7644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7798\n",
      "Epoch 00205: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4576 - accuracy: 0.7800 - val_loss: 0.4823 - val_accuracy: 0.7582\n",
      "Epoch 206/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7817\n",
      "Saving weights for epoch 205\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.45063 to 0.45034, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4534 - accuracy: 0.7813 - val_loss: 0.4503 - val_accuracy: 0.7771\n",
      "Epoch 207/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7829\n",
      "Epoch 00207: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4566 - accuracy: 0.7829 - val_loss: 0.5213 - val_accuracy: 0.7311\n",
      "Epoch 208/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4568 - accuracy: 0.7819\n",
      "Epoch 00208: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4568 - accuracy: 0.7821 - val_loss: 0.4979 - val_accuracy: 0.7453\n",
      "Epoch 209/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7843\n",
      "Epoch 00209: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4538 - accuracy: 0.7841 - val_loss: 0.4745 - val_accuracy: 0.7603\n",
      "Epoch 210/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.7829\n",
      "Epoch 00210: val_loss improved from 0.45034 to 0.44169, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4555 - accuracy: 0.7825 - val_loss: 0.4417 - val_accuracy: 0.7880\n",
      "Epoch 211/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.7832\n",
      "Saving weights for epoch 210\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4526 - accuracy: 0.7830 - val_loss: 0.4580 - val_accuracy: 0.7745\n",
      "Epoch 212/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7836\n",
      "Epoch 00212: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4559 - accuracy: 0.7831 - val_loss: 0.4828 - val_accuracy: 0.7552\n",
      "Epoch 213/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4505 - accuracy: 0.7853\n",
      "Epoch 00213: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4509 - accuracy: 0.7852 - val_loss: 0.4651 - val_accuracy: 0.7698\n",
      "Epoch 214/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.7868\n",
      "Epoch 00214: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4514 - accuracy: 0.7869 - val_loss: 0.5067 - val_accuracy: 0.7388\n",
      "Epoch 215/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4505 - accuracy: 0.7860\n",
      "Epoch 00215: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4505 - accuracy: 0.7864 - val_loss: 0.4614 - val_accuracy: 0.7730\n",
      "Epoch 216/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.7846\n",
      "Saving weights for epoch 215\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4522 - accuracy: 0.7844 - val_loss: 0.4769 - val_accuracy: 0.7612\n",
      "Epoch 217/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7855\n",
      "Epoch 00217: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4514 - accuracy: 0.7853 - val_loss: 0.5418 - val_accuracy: 0.7214\n",
      "Epoch 218/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7863\n",
      "Epoch 00218: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4509 - accuracy: 0.7862 - val_loss: 0.4688 - val_accuracy: 0.7681\n",
      "Epoch 219/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.7889\n",
      "Epoch 00219: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4498 - accuracy: 0.7884 - val_loss: 0.4931 - val_accuracy: 0.7479\n",
      "Epoch 220/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4499 - accuracy: 0.7858\n",
      "Epoch 00220: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4503 - accuracy: 0.7856 - val_loss: 0.4485 - val_accuracy: 0.7812\n",
      "Epoch 221/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4479 - accuracy: 0.7898\n",
      "Saving weights for epoch 220\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4488 - accuracy: 0.7894 - val_loss: 0.5292 - val_accuracy: 0.7285\n",
      "Epoch 222/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7791\n",
      "Epoch 00222: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4530 - accuracy: 0.7791 - val_loss: 0.4892 - val_accuracy: 0.7504\n",
      "Epoch 223/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4459 - accuracy: 0.7867\n",
      "Epoch 00223: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4460 - accuracy: 0.7866 - val_loss: 0.4484 - val_accuracy: 0.7767\n",
      "Epoch 224/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.7935\n",
      "Epoch 00224: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4427 - accuracy: 0.7931 - val_loss: 0.4509 - val_accuracy: 0.7777\n",
      "Epoch 225/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.7885\n",
      "Epoch 00225: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4451 - accuracy: 0.7882 - val_loss: 0.4561 - val_accuracy: 0.7754\n",
      "Epoch 226/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7860\n",
      "Saving weights for epoch 225\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4489 - accuracy: 0.7859 - val_loss: 0.4836 - val_accuracy: 0.7552\n",
      "Epoch 227/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7855\n",
      "Epoch 00227: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4482 - accuracy: 0.7854 - val_loss: 0.4438 - val_accuracy: 0.7857\n",
      "Epoch 228/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4468 - accuracy: 0.7880\n",
      "Epoch 00228: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4475 - accuracy: 0.7880 - val_loss: 0.4607 - val_accuracy: 0.7717\n",
      "Epoch 229/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4463 - accuracy: 0.7837\n",
      "Epoch 00229: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4467 - accuracy: 0.7838 - val_loss: 0.4597 - val_accuracy: 0.7717\n",
      "Epoch 230/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.7858\n",
      "Epoch 00230: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4504 - accuracy: 0.7857 - val_loss: 0.4890 - val_accuracy: 0.7511\n",
      "Epoch 231/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.7893\n",
      "Saving weights for epoch 230\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4436 - accuracy: 0.7891 - val_loss: 0.4505 - val_accuracy: 0.7801\n",
      "Epoch 232/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7871\n",
      "Epoch 00232: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4447 - accuracy: 0.7873 - val_loss: 0.4724 - val_accuracy: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7935\n",
      "Epoch 00233: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4400 - accuracy: 0.7936 - val_loss: 0.4647 - val_accuracy: 0.7678\n",
      "Epoch 234/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4454 - accuracy: 0.7869\n",
      "Epoch 00234: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4454 - accuracy: 0.7872 - val_loss: 0.4659 - val_accuracy: 0.7661\n",
      "Epoch 235/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7860\n",
      "Epoch 00235: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4447 - accuracy: 0.7856 - val_loss: 0.4479 - val_accuracy: 0.7820\n",
      "Epoch 236/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.7923\n",
      "Saving weights for epoch 235\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4432 - accuracy: 0.7922 - val_loss: 0.4549 - val_accuracy: 0.7756\n",
      "Epoch 237/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4438 - accuracy: 0.7896\n",
      "Epoch 00237: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4440 - accuracy: 0.7894 - val_loss: 0.4625 - val_accuracy: 0.7676\n",
      "Epoch 238/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.7892\n",
      "Epoch 00238: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4440 - accuracy: 0.7890 - val_loss: 0.4439 - val_accuracy: 0.7822\n",
      "Epoch 239/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.7938\n",
      "Epoch 00239: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4377 - accuracy: 0.7938 - val_loss: 0.4469 - val_accuracy: 0.7810\n",
      "Epoch 240/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.7914\n",
      "Epoch 00240: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4416 - accuracy: 0.7916 - val_loss: 0.4707 - val_accuracy: 0.7638\n",
      "Epoch 241/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.7914\n",
      "Saving weights for epoch 240\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4409 - accuracy: 0.7913 - val_loss: 0.4511 - val_accuracy: 0.7769\n",
      "Epoch 242/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.7918\n",
      "Epoch 00242: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4386 - accuracy: 0.7918 - val_loss: 0.4502 - val_accuracy: 0.7773\n",
      "Epoch 243/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.7919\n",
      "Epoch 00243: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4409 - accuracy: 0.7916 - val_loss: 0.4607 - val_accuracy: 0.7702\n",
      "Epoch 244/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4359 - accuracy: 0.7928\n",
      "Epoch 00244: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4359 - accuracy: 0.7927 - val_loss: 0.4759 - val_accuracy: 0.7648\n",
      "Epoch 245/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7944\n",
      "Epoch 00245: val_loss improved from 0.44169 to 0.43419, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4372 - accuracy: 0.7940 - val_loss: 0.4342 - val_accuracy: 0.7917\n",
      "Epoch 246/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7877\n",
      "Saving weights for epoch 245\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4448 - accuracy: 0.7875 - val_loss: 0.4525 - val_accuracy: 0.7782\n",
      "Epoch 247/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.7911\n",
      "Epoch 00247: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4375 - accuracy: 0.7909 - val_loss: 0.4513 - val_accuracy: 0.7801\n",
      "Epoch 248/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7891\n",
      "Epoch 00248: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4407 - accuracy: 0.7891 - val_loss: 0.4684 - val_accuracy: 0.7650\n",
      "Epoch 249/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.7889\n",
      "Epoch 00249: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4410 - accuracy: 0.7890 - val_loss: 0.4358 - val_accuracy: 0.7896\n",
      "Epoch 250/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7932\n",
      "Epoch 00250: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4367 - accuracy: 0.7933 - val_loss: 0.4736 - val_accuracy: 0.7631\n",
      "Epoch 251/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4353 - accuracy: 0.7954\n",
      "Saving weights for epoch 250\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4358 - accuracy: 0.7951 - val_loss: 0.4606 - val_accuracy: 0.7734\n",
      "Epoch 252/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7911\n",
      "Epoch 00252: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4397 - accuracy: 0.7908 - val_loss: 0.4502 - val_accuracy: 0.7805\n",
      "Epoch 253/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7938\n",
      "Epoch 00253: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4338 - accuracy: 0.7935 - val_loss: 0.4839 - val_accuracy: 0.7547\n",
      "Epoch 254/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7955\n",
      "Epoch 00254: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4346 - accuracy: 0.7956 - val_loss: 0.4523 - val_accuracy: 0.7767\n",
      "Epoch 255/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.7900\n",
      "Epoch 00255: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4388 - accuracy: 0.7902 - val_loss: 0.4541 - val_accuracy: 0.7728\n",
      "Epoch 256/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.7979\n",
      "Saving weights for epoch 255\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4319 - accuracy: 0.7977 - val_loss: 0.4397 - val_accuracy: 0.7865\n",
      "Epoch 257/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.7966\n",
      "Epoch 00257: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4361 - accuracy: 0.7965 - val_loss: 0.4473 - val_accuracy: 0.7788\n",
      "Epoch 258/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.7951\n",
      "Epoch 00258: val_loss improved from 0.43419 to 0.42990, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4313 - accuracy: 0.7947 - val_loss: 0.4299 - val_accuracy: 0.7945\n",
      "Epoch 259/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.7968\n",
      "Epoch 00259: val_loss did not improve from 0.42990\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4317 - accuracy: 0.7962 - val_loss: 0.4343 - val_accuracy: 0.7919\n",
      "Epoch 260/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7955\n",
      "Epoch 00260: val_loss did not improve from 0.42990\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4333 - accuracy: 0.7951 - val_loss: 0.4464 - val_accuracy: 0.7840\n",
      "Epoch 261/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy: 0.7962\n",
      "Saving weights for epoch 260\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.42990 to 0.42889, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4331 - accuracy: 0.7959 - val_loss: 0.4289 - val_accuracy: 0.7921\n",
      "Epoch 262/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7973\n",
      "Epoch 00262: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4294 - accuracy: 0.7975 - val_loss: 0.4555 - val_accuracy: 0.7734\n",
      "Epoch 263/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.7986\n",
      "Epoch 00263: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4289 - accuracy: 0.7988 - val_loss: 0.4521 - val_accuracy: 0.7758\n",
      "Epoch 264/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.7949\n",
      "Epoch 00264: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4328 - accuracy: 0.7949 - val_loss: 0.4650 - val_accuracy: 0.7661\n",
      "Epoch 265/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4316 - accuracy: 0.7961\n",
      "Epoch 00265: val_loss improved from 0.42889 to 0.42342, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4319 - accuracy: 0.7960 - val_loss: 0.4234 - val_accuracy: 0.7977\n",
      "Epoch 266/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.7952\n",
      "Saving weights for epoch 265\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4326 - accuracy: 0.7952 - val_loss: 0.4485 - val_accuracy: 0.7779\n",
      "Epoch 267/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.7977\n",
      "Epoch 00267: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4288 - accuracy: 0.7979 - val_loss: 0.4477 - val_accuracy: 0.7805\n",
      "Epoch 268/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.7956\n",
      "Epoch 00268: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4312 - accuracy: 0.7958 - val_loss: 0.4366 - val_accuracy: 0.7891\n",
      "Epoch 269/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4297 - accuracy: 0.7960\n",
      "Epoch 00269: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4303 - accuracy: 0.7957 - val_loss: 0.4395 - val_accuracy: 0.7868\n",
      "Epoch 270/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8030\n",
      "Epoch 00270: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4237 - accuracy: 0.8026 - val_loss: 0.4324 - val_accuracy: 0.7900\n",
      "Epoch 271/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4300 - accuracy: 0.8009\n",
      "Saving weights for epoch 270\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4309 - accuracy: 0.8005 - val_loss: 0.4269 - val_accuracy: 0.7958\n",
      "Epoch 272/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.7995\n",
      "Epoch 00272: val_loss improved from 0.42342 to 0.42275, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4265 - accuracy: 0.7998 - val_loss: 0.4227 - val_accuracy: 0.7988\n",
      "Epoch 273/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.7951\n",
      "Epoch 00273: val_loss did not improve from 0.42275\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4317 - accuracy: 0.7952 - val_loss: 0.4513 - val_accuracy: 0.7767\n",
      "Epoch 274/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.8014\n",
      "Epoch 00274: val_loss improved from 0.42275 to 0.41889, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4267 - accuracy: 0.8008 - val_loss: 0.4189 - val_accuracy: 0.8044\n",
      "Epoch 275/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8014\n",
      "Epoch 00275: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4254 - accuracy: 0.8016 - val_loss: 0.4289 - val_accuracy: 0.7956\n",
      "Epoch 276/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.8004\n",
      "Saving weights for epoch 275\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4239 - accuracy: 0.8003 - val_loss: 0.4225 - val_accuracy: 0.7994\n",
      "Epoch 277/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.7985\n",
      "Epoch 00277: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4285 - accuracy: 0.7982 - val_loss: 0.4246 - val_accuracy: 0.8016\n",
      "Epoch 278/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.7990\n",
      "Epoch 00278: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4257 - accuracy: 0.7991 - val_loss: 0.4853 - val_accuracy: 0.7539\n",
      "Epoch 279/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.7991\n",
      "Epoch 00279: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4266 - accuracy: 0.7988 - val_loss: 0.4202 - val_accuracy: 0.8087\n",
      "Epoch 280/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.7983\n",
      "Epoch 00280: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4279 - accuracy: 0.7981 - val_loss: 0.4431 - val_accuracy: 0.7833\n",
      "Epoch 281/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4251 - accuracy: 0.8002\n",
      "Saving weights for epoch 280\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.41889 to 0.41883, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4260 - accuracy: 0.8001 - val_loss: 0.4188 - val_accuracy: 0.8061\n",
      "Epoch 282/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.7987\n",
      "Epoch 00282: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4231 - accuracy: 0.7983 - val_loss: 0.4297 - val_accuracy: 0.7945\n",
      "Epoch 283/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8011\n",
      "Epoch 00283: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4225 - accuracy: 0.8009 - val_loss: 0.4499 - val_accuracy: 0.7764\n",
      "Epoch 284/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.7991\n",
      "Epoch 00284: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4237 - accuracy: 0.7987 - val_loss: 0.4329 - val_accuracy: 0.7902\n",
      "Epoch 285/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8030\n",
      "Epoch 00285: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4202 - accuracy: 0.8028 - val_loss: 0.4380 - val_accuracy: 0.7861\n",
      "Epoch 286/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8011\n",
      "Saving weights for epoch 285\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4230 - accuracy: 0.8011 - val_loss: 0.4358 - val_accuracy: 0.7900\n",
      "Epoch 287/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8009\n",
      "Epoch 00287: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4237 - accuracy: 0.8005 - val_loss: 0.4263 - val_accuracy: 0.7960\n",
      "Epoch 288/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8059\n",
      "Epoch 00288: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4148 - accuracy: 0.8056 - val_loss: 0.4962 - val_accuracy: 0.7491\n",
      "Epoch 289/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8028\n",
      "Epoch 00289: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4189 - accuracy: 0.8030 - val_loss: 0.4533 - val_accuracy: 0.7767\n",
      "Epoch 290/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8043\n",
      "Epoch 00290: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4220 - accuracy: 0.8040 - val_loss: 0.4352 - val_accuracy: 0.7908\n",
      "Epoch 291/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8018\n",
      "Saving weights for epoch 290\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4195 - accuracy: 0.8017 - val_loss: 0.4744 - val_accuracy: 0.7623\n",
      "Epoch 292/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8042\n",
      "Epoch 00292: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4191 - accuracy: 0.8044 - val_loss: 0.4248 - val_accuracy: 0.7975\n",
      "Epoch 293/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8059\n",
      "Epoch 00293: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4187 - accuracy: 0.8061 - val_loss: 0.4220 - val_accuracy: 0.7982\n",
      "Epoch 294/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8039\n",
      "Epoch 00294: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4180 - accuracy: 0.8041 - val_loss: 0.4295 - val_accuracy: 0.7919\n",
      "Epoch 295/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8035\n",
      "Epoch 00295: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4191 - accuracy: 0.8037 - val_loss: 0.4357 - val_accuracy: 0.7885\n",
      "Epoch 296/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.7990\n",
      "Saving weights for epoch 295\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4204 - accuracy: 0.7988 - val_loss: 0.4192 - val_accuracy: 0.8025\n",
      "Epoch 297/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8062\n",
      "Epoch 00297: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4194 - accuracy: 0.8061 - val_loss: 0.4322 - val_accuracy: 0.7904\n",
      "Epoch 298/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8062\n",
      "Epoch 00298: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4167 - accuracy: 0.8062 - val_loss: 0.4376 - val_accuracy: 0.7900\n",
      "Epoch 299/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8040\n",
      "Epoch 00299: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4155 - accuracy: 0.8040 - val_loss: 0.4275 - val_accuracy: 0.7939\n",
      "Epoch 300/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4148 - accuracy: 0.8045\n",
      "Epoch 00300: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4148 - accuracy: 0.8045 - val_loss: 0.4394 - val_accuracy: 0.7870\n",
      "Epoch 301/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8037\n",
      "Saving weights for epoch 300\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4170 - accuracy: 0.8035 - val_loss: 0.4388 - val_accuracy: 0.7870\n",
      "Epoch 302/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4154 - accuracy: 0.8051\n",
      "Epoch 00302: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4153 - accuracy: 0.8053 - val_loss: 0.4420 - val_accuracy: 0.7874\n",
      "Epoch 303/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8040\n",
      "Epoch 00303: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4161 - accuracy: 0.8041 - val_loss: 0.4397 - val_accuracy: 0.7870\n",
      "Epoch 304/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8079\n",
      "Epoch 00304: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4129 - accuracy: 0.8080 - val_loss: 0.4345 - val_accuracy: 0.7908\n",
      "Epoch 305/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8085\n",
      "Epoch 00305: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4137 - accuracy: 0.8084 - val_loss: 0.4199 - val_accuracy: 0.8009\n",
      "Epoch 306/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8060\n",
      "Saving weights for epoch 305\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.41883 to 0.41646, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4107 - accuracy: 0.8057 - val_loss: 0.4165 - val_accuracy: 0.8022\n",
      "Epoch 307/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4164 - accuracy: 0.8059\n",
      "Epoch 00307: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4166 - accuracy: 0.8057 - val_loss: 0.4361 - val_accuracy: 0.7874\n",
      "Epoch 308/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8142\n",
      "Epoch 00308: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4091 - accuracy: 0.8140 - val_loss: 0.4309 - val_accuracy: 0.7919\n",
      "Epoch 309/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8132\n",
      "Epoch 00309: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4052 - accuracy: 0.8132 - val_loss: 0.4389 - val_accuracy: 0.7868\n",
      "Epoch 310/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.8073\n",
      "Epoch 00310: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4123 - accuracy: 0.8071 - val_loss: 0.4232 - val_accuracy: 0.7999\n",
      "Epoch 311/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8110\n",
      "Saving weights for epoch 310\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.41646 to 0.41469, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4108 - accuracy: 0.8107 - val_loss: 0.4147 - val_accuracy: 0.8080\n",
      "Epoch 312/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8082\n",
      "Epoch 00312: val_loss did not improve from 0.41469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4112 - accuracy: 0.8078 - val_loss: 0.4162 - val_accuracy: 0.8044\n",
      "Epoch 313/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8059\n",
      "Epoch 00313: val_loss improved from 0.41469 to 0.40701, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4110 - accuracy: 0.8061 - val_loss: 0.4070 - val_accuracy: 0.8158\n",
      "Epoch 314/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8110\n",
      "Epoch 00314: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4103 - accuracy: 0.8110 - val_loss: 0.4284 - val_accuracy: 0.7947\n",
      "Epoch 315/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8111\n",
      "Epoch 00315: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4083 - accuracy: 0.8111 - val_loss: 0.4296 - val_accuracy: 0.7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8099\n",
      "Saving weights for epoch 315\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4077 - accuracy: 0.8098 - val_loss: 0.4232 - val_accuracy: 0.7969\n",
      "Epoch 317/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8070\n",
      "Epoch 00317: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4131 - accuracy: 0.8069 - val_loss: 0.4230 - val_accuracy: 0.7960\n",
      "Epoch 318/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8103\n",
      "Epoch 00318: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4067 - accuracy: 0.8103 - val_loss: 0.4310 - val_accuracy: 0.7902\n",
      "Epoch 319/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8083\n",
      "Epoch 00319: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4083 - accuracy: 0.8082 - val_loss: 0.4470 - val_accuracy: 0.7816\n",
      "Epoch 320/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8096\n",
      "Epoch 00320: val_loss improved from 0.40701 to 0.40501, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4066 - accuracy: 0.8093 - val_loss: 0.4050 - val_accuracy: 0.8106\n",
      "Epoch 321/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8123\n",
      "Saving weights for epoch 320\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4075 - accuracy: 0.8122 - val_loss: 0.4575 - val_accuracy: 0.7728\n",
      "Epoch 322/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8112\n",
      "Epoch 00322: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4071 - accuracy: 0.8111 - val_loss: 0.4228 - val_accuracy: 0.7984\n",
      "Epoch 323/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8116\n",
      "Epoch 00323: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4092 - accuracy: 0.8118 - val_loss: 0.4090 - val_accuracy: 0.8106\n",
      "Epoch 324/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8106\n",
      "Epoch 00324: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4080 - accuracy: 0.8105 - val_loss: 0.4357 - val_accuracy: 0.7868\n",
      "Epoch 325/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4062 - accuracy: 0.8101\n",
      "Epoch 00325: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4058 - accuracy: 0.8102 - val_loss: 0.4349 - val_accuracy: 0.7898\n",
      "Epoch 326/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8132\n",
      "Saving weights for epoch 325\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4026 - accuracy: 0.8133 - val_loss: 0.4248 - val_accuracy: 0.7943\n",
      "Epoch 327/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8114\n",
      "Epoch 00327: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4054 - accuracy: 0.8117 - val_loss: 0.4355 - val_accuracy: 0.7896\n",
      "Epoch 328/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8117\n",
      "Epoch 00328: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4055 - accuracy: 0.8116 - val_loss: 0.4145 - val_accuracy: 0.8050\n",
      "Epoch 329/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8141\n",
      "Epoch 00329: val_loss improved from 0.40501 to 0.40432, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4029 - accuracy: 0.8142 - val_loss: 0.4043 - val_accuracy: 0.8106\n",
      "Epoch 330/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8117\n",
      "Epoch 00330: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4055 - accuracy: 0.8118 - val_loss: 0.4397 - val_accuracy: 0.7859\n",
      "Epoch 331/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8135\n",
      "Saving weights for epoch 330\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4030 - accuracy: 0.8134 - val_loss: 0.4099 - val_accuracy: 0.8078\n",
      "Epoch 332/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8164\n",
      "Epoch 00332: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3991 - accuracy: 0.8163 - val_loss: 0.4124 - val_accuracy: 0.8044\n",
      "Epoch 333/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4007 - accuracy: 0.8127\n",
      "Epoch 00333: val_loss improved from 0.40432 to 0.40157, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4013 - accuracy: 0.8121 - val_loss: 0.4016 - val_accuracy: 0.8138\n",
      "Epoch 334/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.8147\n",
      "Epoch 00334: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3993 - accuracy: 0.8149 - val_loss: 0.4233 - val_accuracy: 0.7943\n",
      "Epoch 335/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8117\n",
      "Epoch 00335: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4034 - accuracy: 0.8114 - val_loss: 0.4019 - val_accuracy: 0.8158\n",
      "Epoch 336/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8161\n",
      "Saving weights for epoch 335\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3989 - accuracy: 0.8160 - val_loss: 0.4186 - val_accuracy: 0.7984\n",
      "Epoch 337/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3997 - accuracy: 0.8142\n",
      "Epoch 00337: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4002 - accuracy: 0.8141 - val_loss: 0.4026 - val_accuracy: 0.8160\n",
      "Epoch 338/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8110\n",
      "Epoch 00338: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4021 - accuracy: 0.8107 - val_loss: 0.4077 - val_accuracy: 0.8095\n",
      "Epoch 339/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4015 - accuracy: 0.8104\n",
      "Epoch 00339: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4015 - accuracy: 0.8106 - val_loss: 0.4178 - val_accuracy: 0.8020\n",
      "Epoch 340/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8113\n",
      "Epoch 00340: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4035 - accuracy: 0.8110 - val_loss: 0.4077 - val_accuracy: 0.8104\n",
      "Epoch 341/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8145\n",
      "Saving weights for epoch 340\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3974 - accuracy: 0.8145 - val_loss: 0.4258 - val_accuracy: 0.7973\n",
      "Epoch 342/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.8137\n",
      "Epoch 00342: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4009 - accuracy: 0.8137 - val_loss: 0.4286 - val_accuracy: 0.7919\n",
      "Epoch 343/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8173\n",
      "Epoch 00343: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3959 - accuracy: 0.8173 - val_loss: 0.4151 - val_accuracy: 0.8031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8123\n",
      "Epoch 00344: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4030 - accuracy: 0.8119 - val_loss: 0.4064 - val_accuracy: 0.8132\n",
      "Epoch 345/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8155\n",
      "Epoch 00345: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3931 - accuracy: 0.8153 - val_loss: 0.4068 - val_accuracy: 0.8091\n",
      "Epoch 346/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3930 - accuracy: 0.8193\n",
      "Saving weights for epoch 345\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3938 - accuracy: 0.8189 - val_loss: 0.4023 - val_accuracy: 0.8147\n",
      "Epoch 347/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8134\n",
      "Epoch 00347: val_loss improved from 0.40157 to 0.39875, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3969 - accuracy: 0.8132 - val_loss: 0.3988 - val_accuracy: 0.8194\n",
      "Epoch 348/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8161\n",
      "Epoch 00348: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3956 - accuracy: 0.8159 - val_loss: 0.4121 - val_accuracy: 0.8035\n",
      "Epoch 349/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8173\n",
      "Epoch 00349: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3947 - accuracy: 0.8173 - val_loss: 0.3996 - val_accuracy: 0.8179\n",
      "Epoch 350/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.8192\n",
      "Epoch 00350: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3929 - accuracy: 0.8193 - val_loss: 0.4141 - val_accuracy: 0.8050\n",
      "Epoch 351/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8181\n",
      "Saving weights for epoch 350\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3936 - accuracy: 0.8180 - val_loss: 0.4141 - val_accuracy: 0.8035\n",
      "Epoch 352/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3927 - accuracy: 0.8167\n",
      "Epoch 00352: val_loss improved from 0.39875 to 0.39657, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3929 - accuracy: 0.8166 - val_loss: 0.3966 - val_accuracy: 0.8220\n",
      "Epoch 353/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8167\n",
      "Epoch 00353: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3946 - accuracy: 0.8166 - val_loss: 0.4142 - val_accuracy: 0.8025\n",
      "Epoch 354/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8150\n",
      "Epoch 00354: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3940 - accuracy: 0.8148 - val_loss: 0.4277 - val_accuracy: 0.7923\n",
      "Epoch 355/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8143\n",
      "Epoch 00355: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3958 - accuracy: 0.8143 - val_loss: 0.4124 - val_accuracy: 0.8042\n",
      "Epoch 356/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8193\n",
      "Saving weights for epoch 355\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3901 - accuracy: 0.8193 - val_loss: 0.4122 - val_accuracy: 0.8063\n",
      "Epoch 357/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8183\n",
      "Epoch 00357: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3956 - accuracy: 0.8180 - val_loss: 0.4142 - val_accuracy: 0.8050\n",
      "Epoch 358/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8184\n",
      "Epoch 00358: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3911 - accuracy: 0.8183 - val_loss: 0.4127 - val_accuracy: 0.8031\n",
      "Epoch 359/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8184\n",
      "Epoch 00359: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3916 - accuracy: 0.8181 - val_loss: 0.4034 - val_accuracy: 0.8141\n",
      "Epoch 360/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8150\n",
      "Epoch 00360: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3947 - accuracy: 0.8149 - val_loss: 0.4085 - val_accuracy: 0.8102\n",
      "Epoch 361/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8154\n",
      "Saving weights for epoch 360\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3925 - accuracy: 0.8156 - val_loss: 0.4206 - val_accuracy: 0.7982\n",
      "Epoch 362/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8223\n",
      "Epoch 00362: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3890 - accuracy: 0.8226 - val_loss: 0.4090 - val_accuracy: 0.8080\n",
      "Epoch 363/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8217\n",
      "Epoch 00363: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3896 - accuracy: 0.8214 - val_loss: 0.4080 - val_accuracy: 0.8115\n",
      "Epoch 364/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8217\n",
      "Epoch 00364: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3847 - accuracy: 0.8218 - val_loss: 0.4154 - val_accuracy: 0.8048\n",
      "Epoch 365/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8210\n",
      "Epoch 00365: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3890 - accuracy: 0.8213 - val_loss: 0.4011 - val_accuracy: 0.8141\n",
      "Epoch 366/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8217\n",
      "Saving weights for epoch 365\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3885 - accuracy: 0.8215 - val_loss: 0.4183 - val_accuracy: 0.8018\n",
      "Epoch 367/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8210\n",
      "Epoch 00367: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3868 - accuracy: 0.8206 - val_loss: 0.4199 - val_accuracy: 0.8020\n",
      "Epoch 368/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8181\n",
      "Epoch 00368: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3902 - accuracy: 0.8179 - val_loss: 0.4108 - val_accuracy: 0.8072\n",
      "Epoch 369/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8191\n",
      "Epoch 00369: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3896 - accuracy: 0.8190 - val_loss: 0.4056 - val_accuracy: 0.8113\n",
      "Epoch 370/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8202\n",
      "Epoch 00370: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3896 - accuracy: 0.8205 - val_loss: 0.3989 - val_accuracy: 0.8184\n",
      "Epoch 371/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8216\n",
      "Saving weights for epoch 370\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3871 - accuracy: 0.8211 - val_loss: 0.4037 - val_accuracy: 0.8093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 372/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8204\n",
      "Epoch 00372: val_loss improved from 0.39657 to 0.39076, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3887 - accuracy: 0.8202 - val_loss: 0.3908 - val_accuracy: 0.8188\n",
      "Epoch 373/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8208\n",
      "Epoch 00373: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3844 - accuracy: 0.8207 - val_loss: 0.4138 - val_accuracy: 0.8085\n",
      "Epoch 374/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8206\n",
      "Epoch 00374: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3873 - accuracy: 0.8205 - val_loss: 0.3972 - val_accuracy: 0.8186\n",
      "Epoch 375/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8257\n",
      "Epoch 00375: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3823 - accuracy: 0.8256 - val_loss: 0.3979 - val_accuracy: 0.8151\n",
      "Epoch 376/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8200\n",
      "Saving weights for epoch 375\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3880 - accuracy: 0.8197 - val_loss: 0.4130 - val_accuracy: 0.8050\n",
      "Epoch 377/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.8225\n",
      "Epoch 00377: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3841 - accuracy: 0.8220 - val_loss: 0.3919 - val_accuracy: 0.8214\n",
      "Epoch 378/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8238\n",
      "Epoch 00378: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3836 - accuracy: 0.8237 - val_loss: 0.3974 - val_accuracy: 0.8171\n",
      "Epoch 379/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8220\n",
      "Epoch 00379: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3873 - accuracy: 0.8220 - val_loss: 0.3974 - val_accuracy: 0.8194\n",
      "Epoch 380/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8242\n",
      "Epoch 00380: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3817 - accuracy: 0.8238 - val_loss: 0.4124 - val_accuracy: 0.8031\n",
      "Epoch 381/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.8225\n",
      "Saving weights for epoch 380\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3856 - accuracy: 0.8223 - val_loss: 0.4186 - val_accuracy: 0.7977\n",
      "Epoch 382/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8246\n",
      "Epoch 00382: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3845 - accuracy: 0.8240 - val_loss: 0.4095 - val_accuracy: 0.8095\n",
      "Epoch 383/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3827 - accuracy: 0.8252\n",
      "Epoch 00383: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3836 - accuracy: 0.8248 - val_loss: 0.4014 - val_accuracy: 0.8153\n",
      "Epoch 384/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8234\n",
      "Epoch 00384: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3821 - accuracy: 0.8230 - val_loss: 0.4097 - val_accuracy: 0.8074\n",
      "Epoch 385/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8272\n",
      "Epoch 00385: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3755 - accuracy: 0.8270 - val_loss: 0.4005 - val_accuracy: 0.8141\n",
      "Epoch 386/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3860 - accuracy: 0.8208\n",
      "Saving weights for epoch 385\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3864 - accuracy: 0.8209 - val_loss: 0.4091 - val_accuracy: 0.8104\n",
      "Epoch 387/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.8275\n",
      "Epoch 00387: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3785 - accuracy: 0.8274 - val_loss: 0.4126 - val_accuracy: 0.8065\n",
      "Epoch 388/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8228\n",
      "Epoch 00388: val_loss improved from 0.39076 to 0.39055, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3839 - accuracy: 0.8229 - val_loss: 0.3905 - val_accuracy: 0.8233\n",
      "Epoch 389/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8213\n",
      "Epoch 00389: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3813 - accuracy: 0.8212 - val_loss: 0.4185 - val_accuracy: 0.8033\n",
      "Epoch 390/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8289\n",
      "Epoch 00390: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3764 - accuracy: 0.8289 - val_loss: 0.4059 - val_accuracy: 0.8089\n",
      "Epoch 391/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.8254\n",
      "Saving weights for epoch 390\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3803 - accuracy: 0.8251 - val_loss: 0.4021 - val_accuracy: 0.8136\n",
      "Epoch 392/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.8269\n",
      "Epoch 00392: val_loss improved from 0.39055 to 0.38848, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3758 - accuracy: 0.8264 - val_loss: 0.3885 - val_accuracy: 0.8263\n",
      "Epoch 393/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8269\n",
      "Epoch 00393: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3757 - accuracy: 0.8266 - val_loss: 0.4074 - val_accuracy: 0.8110\n",
      "Epoch 394/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8279\n",
      "Epoch 00394: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3758 - accuracy: 0.8276 - val_loss: 0.4007 - val_accuracy: 0.8151\n",
      "Epoch 395/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8277\n",
      "Epoch 00395: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3745 - accuracy: 0.8271 - val_loss: 0.3962 - val_accuracy: 0.8196\n",
      "Epoch 396/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.8280\n",
      "Saving weights for epoch 395\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3770 - accuracy: 0.8276 - val_loss: 0.3899 - val_accuracy: 0.8237\n",
      "Epoch 397/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8258\n",
      "Epoch 00397: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3801 - accuracy: 0.8258 - val_loss: 0.4262 - val_accuracy: 0.7936\n",
      "Epoch 398/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8265\n",
      "Epoch 00398: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3770 - accuracy: 0.8265 - val_loss: 0.3904 - val_accuracy: 0.8220\n",
      "Epoch 399/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8318\n",
      "Epoch 00399: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3707 - accuracy: 0.8318 - val_loss: 0.4044 - val_accuracy: 0.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8293\n",
      "Epoch 00400: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3754 - accuracy: 0.8293 - val_loss: 0.4006 - val_accuracy: 0.8151\n",
      "Epoch 401/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8324\n",
      "Saving weights for epoch 400\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3729 - accuracy: 0.8320 - val_loss: 0.3890 - val_accuracy: 0.8239\n",
      "Epoch 402/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8290\n",
      "Epoch 00402: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3729 - accuracy: 0.8289 - val_loss: 0.4020 - val_accuracy: 0.8149\n",
      "Epoch 403/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3730 - accuracy: 0.8281\n",
      "Epoch 00403: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3731 - accuracy: 0.8279 - val_loss: 0.4045 - val_accuracy: 0.8110\n",
      "Epoch 404/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8308\n",
      "Epoch 00404: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3697 - accuracy: 0.8305 - val_loss: 0.3928 - val_accuracy: 0.8190\n",
      "Epoch 405/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8268\n",
      "Epoch 00405: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3730 - accuracy: 0.8271 - val_loss: 0.4076 - val_accuracy: 0.8080\n",
      "Epoch 406/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8308\n",
      "Saving weights for epoch 405\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3735 - accuracy: 0.8305 - val_loss: 0.4030 - val_accuracy: 0.8134\n",
      "Epoch 407/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8294\n",
      "Epoch 00407: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3700 - accuracy: 0.8294 - val_loss: 0.4008 - val_accuracy: 0.8156\n",
      "Epoch 408/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8286\n",
      "Epoch 00408: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3729 - accuracy: 0.8288 - val_loss: 0.3966 - val_accuracy: 0.8177\n",
      "Epoch 409/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8315\n",
      "Epoch 00409: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3715 - accuracy: 0.8310 - val_loss: 0.3903 - val_accuracy: 0.8203\n",
      "Epoch 410/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8289\n",
      "Epoch 00410: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3754 - accuracy: 0.8290 - val_loss: 0.3928 - val_accuracy: 0.8203\n",
      "Epoch 411/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8328\n",
      "Saving weights for epoch 410\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3682 - accuracy: 0.8329 - val_loss: 0.4126 - val_accuracy: 0.8080\n",
      "Epoch 412/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8324\n",
      "Epoch 00412: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3675 - accuracy: 0.8321 - val_loss: 0.4010 - val_accuracy: 0.8147\n",
      "Epoch 413/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8337\n",
      "Epoch 00413: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3655 - accuracy: 0.8333 - val_loss: 0.4016 - val_accuracy: 0.8132\n",
      "Epoch 414/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8357\n",
      "Epoch 00414: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3654 - accuracy: 0.8356 - val_loss: 0.4097 - val_accuracy: 0.8070\n",
      "Epoch 415/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8290\n",
      "Epoch 00415: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3708 - accuracy: 0.8292 - val_loss: 0.4194 - val_accuracy: 0.7990\n",
      "Epoch 416/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8352\n",
      "Saving weights for epoch 415\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.38848 to 0.38803, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3656 - accuracy: 0.8351 - val_loss: 0.3880 - val_accuracy: 0.8229\n",
      "Epoch 417/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8283\n",
      "Epoch 00417: val_loss did not improve from 0.38803\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3700 - accuracy: 0.8285 - val_loss: 0.3926 - val_accuracy: 0.8205\n",
      "Epoch 418/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8269\n",
      "Epoch 00418: val_loss improved from 0.38803 to 0.38572, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3742 - accuracy: 0.8267 - val_loss: 0.3857 - val_accuracy: 0.8237\n",
      "Epoch 419/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8325\n",
      "Epoch 00419: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3664 - accuracy: 0.8323 - val_loss: 0.3892 - val_accuracy: 0.8231\n",
      "Epoch 420/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8339\n",
      "Epoch 00420: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3654 - accuracy: 0.8336 - val_loss: 0.3893 - val_accuracy: 0.8205\n",
      "Epoch 421/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8318\n",
      "Saving weights for epoch 420\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3669 - accuracy: 0.8319 - val_loss: 0.3944 - val_accuracy: 0.8207\n",
      "Epoch 422/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8298\n",
      "Epoch 00422: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3668 - accuracy: 0.8297 - val_loss: 0.4106 - val_accuracy: 0.8070\n",
      "Epoch 423/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8337\n",
      "Epoch 00423: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3695 - accuracy: 0.8337 - val_loss: 0.4000 - val_accuracy: 0.8153\n",
      "Epoch 424/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8319\n",
      "Epoch 00424: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3709 - accuracy: 0.8316 - val_loss: 0.3923 - val_accuracy: 0.8216\n",
      "Epoch 425/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8355\n",
      "Epoch 00425: val_loss improved from 0.38572 to 0.38433, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3603 - accuracy: 0.8356 - val_loss: 0.3843 - val_accuracy: 0.8257\n",
      "Epoch 426/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8314\n",
      "Saving weights for epoch 425\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3647 - accuracy: 0.8315 - val_loss: 0.4079 - val_accuracy: 0.8070\n",
      "Epoch 427/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8339\n",
      "Epoch 00427: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3621 - accuracy: 0.8341 - val_loss: 0.4044 - val_accuracy: 0.8098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8355\n",
      "Epoch 00428: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3603 - accuracy: 0.8355 - val_loss: 0.3869 - val_accuracy: 0.8237\n",
      "Epoch 429/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8354\n",
      "Epoch 00429: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3612 - accuracy: 0.8356 - val_loss: 0.3982 - val_accuracy: 0.8175\n",
      "Epoch 430/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8364\n",
      "Epoch 00430: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3609 - accuracy: 0.8363 - val_loss: 0.3991 - val_accuracy: 0.8164\n",
      "Epoch 431/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8314\n",
      "Saving weights for epoch 430\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3637 - accuracy: 0.8314 - val_loss: 0.4030 - val_accuracy: 0.8119\n",
      "Epoch 432/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8375\n",
      "Epoch 00432: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3581 - accuracy: 0.8375 - val_loss: 0.4157 - val_accuracy: 0.8072\n",
      "Epoch 433/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8322\n",
      "Epoch 00433: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3686 - accuracy: 0.8320 - val_loss: 0.3874 - val_accuracy: 0.8233\n",
      "Epoch 434/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8345\n",
      "Epoch 00434: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3629 - accuracy: 0.8344 - val_loss: 0.3850 - val_accuracy: 0.8237\n",
      "Epoch 435/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8362\n",
      "Epoch 00435: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3585 - accuracy: 0.8358 - val_loss: 0.3953 - val_accuracy: 0.8179\n",
      "Epoch 436/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8382\n",
      "Saving weights for epoch 435\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3558 - accuracy: 0.8382 - val_loss: 0.4037 - val_accuracy: 0.8106\n",
      "Epoch 437/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8359\n",
      "Epoch 00437: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3601 - accuracy: 0.8355 - val_loss: 0.3892 - val_accuracy: 0.8203\n",
      "Epoch 438/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8341\n",
      "Epoch 00438: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3581 - accuracy: 0.8340 - val_loss: 0.3887 - val_accuracy: 0.8250\n",
      "Epoch 439/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8416\n",
      "Epoch 00439: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3567 - accuracy: 0.8412 - val_loss: 0.3996 - val_accuracy: 0.8156\n",
      "Epoch 440/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8389\n",
      "Epoch 00440: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3580 - accuracy: 0.8389 - val_loss: 0.3901 - val_accuracy: 0.8224\n",
      "Epoch 441/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8404\n",
      "Saving weights for epoch 440\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3558 - accuracy: 0.8402 - val_loss: 0.4001 - val_accuracy: 0.8115\n",
      "Epoch 442/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8423\n",
      "Epoch 00442: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3553 - accuracy: 0.8420 - val_loss: 0.3897 - val_accuracy: 0.8201\n",
      "Epoch 443/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8392\n",
      "Epoch 00443: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3545 - accuracy: 0.8391 - val_loss: 0.3915 - val_accuracy: 0.8190\n",
      "Epoch 444/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8351\n",
      "Epoch 00444: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3598 - accuracy: 0.8350 - val_loss: 0.3915 - val_accuracy: 0.8173\n",
      "Epoch 445/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8346\n",
      "Epoch 00445: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3623 - accuracy: 0.8344 - val_loss: 0.4104 - val_accuracy: 0.8052\n",
      "Epoch 446/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8378\n",
      "Saving weights for epoch 445\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3589 - accuracy: 0.8374 - val_loss: 0.3945 - val_accuracy: 0.8177\n",
      "Epoch 447/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 00447: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3549 - accuracy: 0.8394 - val_loss: 0.4227 - val_accuracy: 0.8009\n",
      "Epoch 448/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8392\n",
      "Epoch 00448: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3534 - accuracy: 0.8393 - val_loss: 0.4197 - val_accuracy: 0.8022\n",
      "Epoch 449/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8383\n",
      "Epoch 00449: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3568 - accuracy: 0.8382 - val_loss: 0.4032 - val_accuracy: 0.8117\n",
      "Epoch 450/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8395\n",
      "Epoch 00450: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3516 - accuracy: 0.8395 - val_loss: 0.4118 - val_accuracy: 0.8065\n",
      "Epoch 451/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.8397\n",
      "Saving weights for epoch 450\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3522 - accuracy: 0.8401 - val_loss: 0.4103 - val_accuracy: 0.8085\n",
      "Epoch 452/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8387\n",
      "Epoch 00452: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3543 - accuracy: 0.8387 - val_loss: 0.3924 - val_accuracy: 0.8166\n",
      "Epoch 453/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.8417\n",
      "Epoch 00453: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3514 - accuracy: 0.8416 - val_loss: 0.4057 - val_accuracy: 0.8065\n",
      "Epoch 454/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8385\n",
      "Epoch 00454: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3558 - accuracy: 0.8387 - val_loss: 0.3931 - val_accuracy: 0.8201\n",
      "Epoch 455/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8401\n",
      "Epoch 00455: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3519 - accuracy: 0.8402 - val_loss: 0.4004 - val_accuracy: 0.8110\n",
      "Epoch 456/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8394\n",
      "Saving weights for epoch 455\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3518 - accuracy: 0.8392 - val_loss: 0.3915 - val_accuracy: 0.8207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8395\n",
      "Epoch 00457: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3513 - accuracy: 0.8394 - val_loss: 0.3858 - val_accuracy: 0.8244\n",
      "Epoch 458/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8430\n",
      "Epoch 00458: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3494 - accuracy: 0.8427 - val_loss: 0.3944 - val_accuracy: 0.8184\n",
      "Epoch 459/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8427\n",
      "Epoch 00459: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3517 - accuracy: 0.8428 - val_loss: 0.4100 - val_accuracy: 0.8091\n",
      "Epoch 460/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8419\n",
      "Epoch 00460: val_loss improved from 0.38433 to 0.38427, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3489 - accuracy: 0.8419 - val_loss: 0.3843 - val_accuracy: 0.8242\n",
      "Epoch 461/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8404\n",
      "Saving weights for epoch 460\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3513 - accuracy: 0.8400 - val_loss: 0.3935 - val_accuracy: 0.8175\n",
      "Epoch 462/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8424\n",
      "Epoch 00462: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3493 - accuracy: 0.8428 - val_loss: 0.4050 - val_accuracy: 0.8130\n",
      "Epoch 463/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8399\n",
      "Epoch 00463: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3530 - accuracy: 0.8402 - val_loss: 0.4070 - val_accuracy: 0.8100\n",
      "Epoch 464/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8452\n",
      "Epoch 00464: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3432 - accuracy: 0.8453 - val_loss: 0.4009 - val_accuracy: 0.8102\n",
      "Epoch 465/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8409\n",
      "Epoch 00465: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3457 - accuracy: 0.8407 - val_loss: 0.3865 - val_accuracy: 0.8239\n",
      "Epoch 466/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8454\n",
      "Saving weights for epoch 465\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3439 - accuracy: 0.8453 - val_loss: 0.4082 - val_accuracy: 0.8078\n",
      "Epoch 467/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8439\n",
      "Epoch 00467: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3448 - accuracy: 0.8436 - val_loss: 0.4009 - val_accuracy: 0.8113\n",
      "Epoch 468/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8479\n",
      "Epoch 00468: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3414 - accuracy: 0.8477 - val_loss: 0.4033 - val_accuracy: 0.8121\n",
      "Epoch 469/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8464\n",
      "Epoch 00469: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3401 - accuracy: 0.8462 - val_loss: 0.4070 - val_accuracy: 0.8074\n",
      "Epoch 470/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8425\n",
      "Epoch 00470: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3487 - accuracy: 0.8422 - val_loss: 0.4021 - val_accuracy: 0.8117\n",
      "Epoch 471/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8473\n",
      "Saving weights for epoch 470\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3409 - accuracy: 0.8475 - val_loss: 0.3911 - val_accuracy: 0.8201\n",
      "Epoch 472/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8475\n",
      "Epoch 00472: val_loss improved from 0.38427 to 0.38286, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3441 - accuracy: 0.8472 - val_loss: 0.3829 - val_accuracy: 0.8261\n",
      "Epoch 473/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8449\n",
      "Epoch 00473: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3440 - accuracy: 0.8447 - val_loss: 0.3881 - val_accuracy: 0.8239\n",
      "Epoch 474/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8453\n",
      "Epoch 00474: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3464 - accuracy: 0.8452 - val_loss: 0.3940 - val_accuracy: 0.8119\n",
      "Epoch 475/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.8465\n",
      "Epoch 00475: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3403 - accuracy: 0.8464 - val_loss: 0.4006 - val_accuracy: 0.8098\n",
      "Epoch 476/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8424\n",
      "Saving weights for epoch 475\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3478 - accuracy: 0.8426 - val_loss: 0.4283 - val_accuracy: 0.7954\n",
      "Epoch 477/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8413\n",
      "Epoch 00477: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3502 - accuracy: 0.8415 - val_loss: 0.4001 - val_accuracy: 0.8098\n",
      "Epoch 478/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3380 - accuracy: 0.8472\n",
      "Epoch 00478: val_loss improved from 0.38286 to 0.38008, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3386 - accuracy: 0.8469 - val_loss: 0.3801 - val_accuracy: 0.8276\n",
      "Epoch 479/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8487\n",
      "Epoch 00479: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3399 - accuracy: 0.8488 - val_loss: 0.4013 - val_accuracy: 0.8132\n",
      "Epoch 480/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8500\n",
      "Epoch 00480: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3430 - accuracy: 0.8498 - val_loss: 0.4000 - val_accuracy: 0.8093\n",
      "Epoch 481/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8445\n",
      "Saving weights for epoch 480\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3443 - accuracy: 0.8447 - val_loss: 0.3904 - val_accuracy: 0.8190\n",
      "Epoch 482/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8488\n",
      "Epoch 00482: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3392 - accuracy: 0.8490 - val_loss: 0.3947 - val_accuracy: 0.8171\n",
      "Epoch 483/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8477\n",
      "Epoch 00483: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3409 - accuracy: 0.8474 - val_loss: 0.3974 - val_accuracy: 0.8138\n",
      "Epoch 484/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8435\n",
      "Epoch 00484: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3455 - accuracy: 0.8436 - val_loss: 0.3851 - val_accuracy: 0.8194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8447\n",
      "Epoch 00485: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.3416 - accuracy: 0.8450 - val_loss: 0.3914 - val_accuracy: 0.8214\n",
      "Epoch 486/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8441\n",
      "Saving weights for epoch 485\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3412 - accuracy: 0.8442 - val_loss: 0.4228 - val_accuracy: 0.8003\n",
      "Epoch 487/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8428\n",
      "Epoch 00487: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3439 - accuracy: 0.8428 - val_loss: 0.3819 - val_accuracy: 0.8250\n",
      "Epoch 488/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8477\n",
      "Epoch 00488: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3394 - accuracy: 0.8479 - val_loss: 0.4003 - val_accuracy: 0.8110\n",
      "Epoch 489/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8490\n",
      "Epoch 00489: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3374 - accuracy: 0.8490 - val_loss: 0.3849 - val_accuracy: 0.8218\n",
      "Epoch 490/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8499\n",
      "Epoch 00490: val_loss improved from 0.38008 to 0.37859, saving model to pickled_objects/batch_size_256_best_weights_trial_1.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3366 - accuracy: 0.8499 - val_loss: 0.3786 - val_accuracy: 0.8274\n",
      "Epoch 491/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8471\n",
      "Saving weights for epoch 490\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3361 - accuracy: 0.8471 - val_loss: 0.3986 - val_accuracy: 0.8132\n",
      "Epoch 492/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8497\n",
      "Epoch 00492: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3352 - accuracy: 0.8493 - val_loss: 0.3992 - val_accuracy: 0.8106\n",
      "Epoch 493/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8467\n",
      "Epoch 00493: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3348 - accuracy: 0.8466 - val_loss: 0.3872 - val_accuracy: 0.8220\n",
      "Epoch 494/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8500\n",
      "Epoch 00494: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3364 - accuracy: 0.8496 - val_loss: 0.3823 - val_accuracy: 0.8222\n",
      "Epoch 495/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8493\n",
      "Epoch 00495: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3376 - accuracy: 0.8487 - val_loss: 0.3844 - val_accuracy: 0.8218\n",
      "Epoch 496/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8496\n",
      "Saving weights for epoch 495\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3352 - accuracy: 0.8499 - val_loss: 0.3810 - val_accuracy: 0.8239\n",
      "Epoch 497/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8519\n",
      "Epoch 00497: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3324 - accuracy: 0.8519 - val_loss: 0.4126 - val_accuracy: 0.8009\n",
      "Epoch 498/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8526\n",
      "Epoch 00498: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3295 - accuracy: 0.8526 - val_loss: 0.3826 - val_accuracy: 0.8252\n",
      "Epoch 499/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8498\n",
      "Epoch 00499: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.3311 - accuracy: 0.8500 - val_loss: 0.3890 - val_accuracy: 0.8186\n",
      "Epoch 500/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8497\n",
      "Epoch 00500: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.3344 - accuracy: 0.8495 - val_loss: 0.3786 - val_accuracy: 0.8265\n",
      "Epoch 501/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8503\n",
      "Saving weights for epoch 500\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3340 - accuracy: 0.8503 - val_loss: 0.4061 - val_accuracy: 0.8100\n",
      "Epoch 502/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8497\n",
      "Epoch 00502: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3338 - accuracy: 0.8493 - val_loss: 0.3906 - val_accuracy: 0.8186\n",
      "Epoch 503/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8506\n",
      "Epoch 00503: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3313 - accuracy: 0.8507 - val_loss: 0.3988 - val_accuracy: 0.8121\n",
      "Epoch 504/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8529\n",
      "Epoch 00504: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3313 - accuracy: 0.8528 - val_loss: 0.3907 - val_accuracy: 0.8203\n",
      "Epoch 505/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8536\n",
      "Epoch 00505: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3294 - accuracy: 0.8535 - val_loss: 0.3896 - val_accuracy: 0.8199\n",
      "Epoch 506/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8529\n",
      "Saving weights for epoch 505\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3290 - accuracy: 0.8531 - val_loss: 0.4089 - val_accuracy: 0.8050\n",
      "Epoch 507/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3263 - accuracy: 0.8511\n",
      "Epoch 00507: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3263 - accuracy: 0.8513 - val_loss: 0.3841 - val_accuracy: 0.8186\n",
      "Epoch 508/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8519\n",
      "Epoch 00508: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3299 - accuracy: 0.8523 - val_loss: 0.4239 - val_accuracy: 0.7966\n",
      "Epoch 509/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8522\n",
      "Epoch 00509: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3317 - accuracy: 0.8526 - val_loss: 0.3947 - val_accuracy: 0.8181\n",
      "Epoch 510/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8529\n",
      "Epoch 00510: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3306 - accuracy: 0.8522 - val_loss: 0.4080 - val_accuracy: 0.8072\n",
      "Epoch 511/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8520\n",
      "Saving weights for epoch 510\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3314 - accuracy: 0.8523 - val_loss: 0.3837 - val_accuracy: 0.8239\n",
      "Epoch 512/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8507\n",
      "Epoch 00512: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3313 - accuracy: 0.8508 - val_loss: 0.4017 - val_accuracy: 0.8102\n",
      "Epoch 513/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8580\n",
      "Epoch 00513: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3228 - accuracy: 0.8580 - val_loss: 0.3932 - val_accuracy: 0.8160\n",
      "Epoch 514/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8552\n",
      "Epoch 00514: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3241 - accuracy: 0.8555 - val_loss: 0.3932 - val_accuracy: 0.8149\n",
      "Epoch 515/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8535\n",
      "Epoch 00515: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3275 - accuracy: 0.8534 - val_loss: 0.3878 - val_accuracy: 0.8177\n",
      "Epoch 516/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8547\n",
      "Saving weights for epoch 515\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3243 - accuracy: 0.8546 - val_loss: 0.3888 - val_accuracy: 0.8192\n",
      "Epoch 517/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8563\n",
      "Epoch 00517: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3214 - accuracy: 0.8562 - val_loss: 0.3871 - val_accuracy: 0.8205\n",
      "Epoch 518/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8552\n",
      "Epoch 00518: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3271 - accuracy: 0.8551 - val_loss: 0.4172 - val_accuracy: 0.8033\n",
      "Epoch 519/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8555\n",
      "Epoch 00519: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3217 - accuracy: 0.8556 - val_loss: 0.3925 - val_accuracy: 0.8188\n",
      "Epoch 520/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8560\n",
      "Epoch 00520: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3264 - accuracy: 0.8559 - val_loss: 0.3959 - val_accuracy: 0.8173\n",
      "Epoch 521/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8536\n",
      "Saving weights for epoch 520\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3245 - accuracy: 0.8539 - val_loss: 0.4093 - val_accuracy: 0.8063\n",
      "Epoch 522/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8548\n",
      "Epoch 00522: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3264 - accuracy: 0.8549 - val_loss: 0.3994 - val_accuracy: 0.8126\n",
      "Epoch 523/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8580\n",
      "Epoch 00523: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3185 - accuracy: 0.8582 - val_loss: 0.3888 - val_accuracy: 0.8181\n",
      "Epoch 524/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8585\n",
      "Epoch 00524: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3201 - accuracy: 0.8581 - val_loss: 0.3891 - val_accuracy: 0.8190\n",
      "Epoch 525/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8538\n",
      "Epoch 00525: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3234 - accuracy: 0.8539 - val_loss: 0.3907 - val_accuracy: 0.8179\n",
      "Epoch 526/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8553\n",
      "Saving weights for epoch 525\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3240 - accuracy: 0.8553 - val_loss: 0.3952 - val_accuracy: 0.8141\n",
      "Epoch 527/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8569\n",
      "Epoch 00527: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3208 - accuracy: 0.8567 - val_loss: 0.4507 - val_accuracy: 0.7915\n",
      "Epoch 528/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8560\n",
      "Epoch 00528: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3245 - accuracy: 0.8560 - val_loss: 0.4056 - val_accuracy: 0.8083\n",
      "Epoch 529/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8553\n",
      "Epoch 00529: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3271 - accuracy: 0.8550 - val_loss: 0.4253 - val_accuracy: 0.8007\n",
      "Epoch 530/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8569\n",
      "Epoch 00530: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3206 - accuracy: 0.8566 - val_loss: 0.3917 - val_accuracy: 0.8192\n",
      "Epoch 531/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8564\n",
      "Saving weights for epoch 530\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3222 - accuracy: 0.8562 - val_loss: 0.3883 - val_accuracy: 0.8201\n",
      "Epoch 532/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3166 - accuracy: 0.8593\n",
      "Epoch 00532: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3169 - accuracy: 0.8593 - val_loss: 0.3879 - val_accuracy: 0.8196\n",
      "Epoch 533/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8568\n",
      "Epoch 00533: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3225 - accuracy: 0.8566 - val_loss: 0.3945 - val_accuracy: 0.8166\n",
      "Epoch 534/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8578\n",
      "Epoch 00534: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3171 - accuracy: 0.8580 - val_loss: 0.3998 - val_accuracy: 0.8138\n",
      "Epoch 535/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8592\n",
      "Epoch 00535: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3153 - accuracy: 0.8590 - val_loss: 0.3927 - val_accuracy: 0.8160\n",
      "Epoch 536/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8589\n",
      "Saving weights for epoch 535\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3179 - accuracy: 0.8589 - val_loss: 0.4349 - val_accuracy: 0.7969\n",
      "Epoch 537/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8572\n",
      "Epoch 00537: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3192 - accuracy: 0.8572 - val_loss: 0.4090 - val_accuracy: 0.8083\n",
      "Epoch 538/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8551\n",
      "Epoch 00538: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3201 - accuracy: 0.8554 - val_loss: 0.4168 - val_accuracy: 0.8037\n",
      "Epoch 539/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8580\n",
      "Epoch 00539: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3181 - accuracy: 0.8578 - val_loss: 0.4158 - val_accuracy: 0.8020\n",
      "Epoch 540/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8568\n",
      "Epoch 00540: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3178 - accuracy: 0.8567 - val_loss: 0.3868 - val_accuracy: 0.8218\n",
      "Epoch 541/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8592\n",
      "Saving weights for epoch 540\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3137 - accuracy: 0.8595 - val_loss: 0.4259 - val_accuracy: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 542/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8575\n",
      "Epoch 00542: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3208 - accuracy: 0.8574 - val_loss: 0.3888 - val_accuracy: 0.8186\n",
      "Epoch 543/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8611\n",
      "Epoch 00543: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3159 - accuracy: 0.8610 - val_loss: 0.3911 - val_accuracy: 0.8164\n",
      "Epoch 544/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8614\n",
      "Epoch 00544: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3142 - accuracy: 0.8612 - val_loss: 0.3861 - val_accuracy: 0.8181\n",
      "Epoch 545/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8614\n",
      "Epoch 00545: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3134 - accuracy: 0.8614 - val_loss: 0.4110 - val_accuracy: 0.8093\n",
      "Epoch 546/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8611\n",
      "Saving weights for epoch 545\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3109 - accuracy: 0.8615 - val_loss: 0.4254 - val_accuracy: 0.8025\n",
      "Epoch 547/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8607\n",
      "Epoch 00547: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3100 - accuracy: 0.8607 - val_loss: 0.4269 - val_accuracy: 0.8007\n",
      "Epoch 548/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8594\n",
      "Epoch 00548: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3142 - accuracy: 0.8597 - val_loss: 0.3957 - val_accuracy: 0.8184\n",
      "Epoch 549/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8592\n",
      "Epoch 00549: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3155 - accuracy: 0.8593 - val_loss: 0.3832 - val_accuracy: 0.8188\n",
      "Epoch 550/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8592\n",
      "Epoch 00550: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3130 - accuracy: 0.8591 - val_loss: 0.3832 - val_accuracy: 0.8224\n",
      "Epoch 551/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8624\n",
      "Saving weights for epoch 550\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3118 - accuracy: 0.8625 - val_loss: 0.4106 - val_accuracy: 0.8091\n",
      "Epoch 552/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8612\n",
      "Epoch 00552: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3138 - accuracy: 0.8611 - val_loss: 0.4357 - val_accuracy: 0.7960\n",
      "Epoch 553/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8626\n",
      "Epoch 00553: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3112 - accuracy: 0.8625 - val_loss: 0.3907 - val_accuracy: 0.8186\n",
      "Epoch 554/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8617\n",
      "Epoch 00554: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3103 - accuracy: 0.8615 - val_loss: 0.3871 - val_accuracy: 0.8216\n",
      "Epoch 555/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8633\n",
      "Epoch 00555: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3086 - accuracy: 0.8632 - val_loss: 0.4406 - val_accuracy: 0.7949\n",
      "Epoch 556/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8632\n",
      "Saving weights for epoch 555\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3095 - accuracy: 0.8632 - val_loss: 0.4002 - val_accuracy: 0.8126\n",
      "Epoch 557/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8623\n",
      "Epoch 00557: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3089 - accuracy: 0.8624 - val_loss: 0.3954 - val_accuracy: 0.8153\n",
      "Epoch 558/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8630\n",
      "Epoch 00558: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3055 - accuracy: 0.8632 - val_loss: 0.4084 - val_accuracy: 0.8121\n",
      "Epoch 559/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8597\n",
      "Epoch 00559: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3095 - accuracy: 0.8601 - val_loss: 0.3962 - val_accuracy: 0.8156\n",
      "Epoch 560/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8613\n",
      "Epoch 00560: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3089 - accuracy: 0.8614 - val_loss: 0.3954 - val_accuracy: 0.8149\n",
      "Epoch 561/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8634\n",
      "Saving weights for epoch 560\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3083 - accuracy: 0.8634 - val_loss: 0.3804 - val_accuracy: 0.8270\n",
      "Epoch 562/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8634\n",
      "Epoch 00562: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3087 - accuracy: 0.8631 - val_loss: 0.3985 - val_accuracy: 0.8132\n",
      "Epoch 563/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8630\n",
      "Epoch 00563: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3036 - accuracy: 0.8628 - val_loss: 0.4022 - val_accuracy: 0.8134\n",
      "Epoch 564/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8677\n",
      "Epoch 00564: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3053 - accuracy: 0.8678 - val_loss: 0.3886 - val_accuracy: 0.8233\n",
      "Epoch 565/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8678\n",
      "Epoch 00565: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.3005 - accuracy: 0.8679 - val_loss: 0.4001 - val_accuracy: 0.8181\n",
      "Epoch 566/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8656\n",
      "Saving weights for epoch 565\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3097 - accuracy: 0.8657 - val_loss: 0.4063 - val_accuracy: 0.8138\n",
      "Epoch 567/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8623\n",
      "Epoch 00567: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3056 - accuracy: 0.8627 - val_loss: 0.3850 - val_accuracy: 0.8227\n",
      "Epoch 568/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8667\n",
      "Epoch 00568: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3043 - accuracy: 0.8671 - val_loss: 0.4621 - val_accuracy: 0.7835\n",
      "Epoch 569/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8655\n",
      "Epoch 00569: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3038 - accuracy: 0.8656 - val_loss: 0.3916 - val_accuracy: 0.8171\n",
      "Epoch 570/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8632\n",
      "Epoch 00570: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3049 - accuracy: 0.8634 - val_loss: 0.3859 - val_accuracy: 0.8207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8660\n",
      "Saving weights for epoch 570\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3056 - accuracy: 0.8660 - val_loss: 0.4049 - val_accuracy: 0.8134\n",
      "Epoch 572/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.8644\n",
      "Epoch 00572: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.3009 - accuracy: 0.8644 - val_loss: 0.3947 - val_accuracy: 0.8162\n",
      "Epoch 573/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8649\n",
      "Epoch 00573: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3050 - accuracy: 0.8651 - val_loss: 0.4198 - val_accuracy: 0.8063\n",
      "Epoch 574/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8664\n",
      "Epoch 00574: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3031 - accuracy: 0.8665 - val_loss: 0.4054 - val_accuracy: 0.8128\n",
      "Epoch 575/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8656\n",
      "Epoch 00575: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3030 - accuracy: 0.8658 - val_loss: 0.4543 - val_accuracy: 0.7865\n",
      "Epoch 576/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8602\n",
      "Saving weights for epoch 575\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3101 - accuracy: 0.8604 - val_loss: 0.4062 - val_accuracy: 0.8102\n",
      "Epoch 577/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8668\n",
      "Epoch 00577: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3024 - accuracy: 0.8671 - val_loss: 0.4177 - val_accuracy: 0.8059\n",
      "Epoch 578/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8678\n",
      "Epoch 00578: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.2978 - accuracy: 0.8675 - val_loss: 0.3875 - val_accuracy: 0.8203\n",
      "Epoch 579/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8670\n",
      "Epoch 00579: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3023 - accuracy: 0.8674 - val_loss: 0.4052 - val_accuracy: 0.8123\n",
      "Epoch 580/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8705\n",
      "Epoch 00580: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2978 - accuracy: 0.8704 - val_loss: 0.4127 - val_accuracy: 0.8089\n",
      "Epoch 581/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8697\n",
      "Saving weights for epoch 580\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.2955 - accuracy: 0.8696 - val_loss: 0.3832 - val_accuracy: 0.8212\n",
      "Epoch 582/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8686\n",
      "Epoch 00582: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2961 - accuracy: 0.8689 - val_loss: 0.4076 - val_accuracy: 0.8095\n",
      "Epoch 583/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8663\n",
      "Epoch 00583: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3021 - accuracy: 0.8665 - val_loss: 0.4125 - val_accuracy: 0.8072\n",
      "Epoch 584/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8662\n",
      "Epoch 00584: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3020 - accuracy: 0.8663 - val_loss: 0.4348 - val_accuracy: 0.7962\n",
      "Epoch 585/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8675\n",
      "Epoch 00585: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2938 - accuracy: 0.8678 - val_loss: 0.3935 - val_accuracy: 0.8162\n",
      "Epoch 586/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8673\n",
      "Saving weights for epoch 585\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2936 - accuracy: 0.8674 - val_loss: 0.3940 - val_accuracy: 0.8212\n",
      "Epoch 587/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8677\n",
      "Epoch 00587: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2972 - accuracy: 0.8680 - val_loss: 0.3959 - val_accuracy: 0.8166\n",
      "Epoch 588/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8716\n",
      "Epoch 00588: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2952 - accuracy: 0.8716 - val_loss: 0.3987 - val_accuracy: 0.8177\n",
      "Epoch 589/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8713\n",
      "Epoch 00589: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2913 - accuracy: 0.8716 - val_loss: 0.4261 - val_accuracy: 0.8033\n",
      "Epoch 590/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8714\n",
      "Epoch 00590: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2917 - accuracy: 0.8717 - val_loss: 0.3987 - val_accuracy: 0.8179\n",
      "Epoch 00590: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 256]\n",
    "model_state_by_batch_size_trial_2 = {}\n",
    "        \n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model(optimizer=keras.optimizers.SGD(0.02))\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    mc = keras.callbacks.ModelCheckpoint(\n",
    "        'pickled_objects/batch_size_{}_best_weights_trial_2.h5'.format(batch_size),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    model_state_by_batch_size_trial_2[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=10000,\n",
    "        extra_callbacks=[es, mc],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml_utils.plot_loss_by_param(model_state_by_batch_size_trial_2, 'batch size', 'batch_size_loss_trial_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now observe that the small batch size outperforms the larger one. (Though, one could object by saying that the large batch size run didn't fully converge yet, seeing as it hasn't achieved the same training loss as the small one.)\n",
    "\n",
    "Is it also true here that the large batch size finds weights close to initialization, and sharp minimizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_state(model_state_by_batch_size_trial_2, 'model_state_by_batch_size_trial_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml_utils.build_model()\n",
    "initial_weights = model.get_weights()\n",
    "model.load_weights('pickled_objects/batch_size_32_best_weights_trial_2.h5')\n",
    "batch_32_weights = model.get_weights()\n",
    "model.load_weights('pickled_objects/batch_size_256_best_weights_trial_2.h5')\n",
    "batch_256_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: 0\n",
      "\tDistance from batch 32 to initial:  5.4449635\n",
      "\tDistance from batch 256 to initial:  3.971358\n",
      "Parameter set: 1\n",
      "\tDistance from batch 32 to initial:  0.4515521\n",
      "\tDistance from batch 256 to initial:  0.6010699\n",
      "Parameter set: 2\n",
      "\tDistance from batch 32 to initial:  3.6385295\n",
      "\tDistance from batch 256 to initial:  3.0198355\n",
      "Parameter set: 3\n",
      "\tDistance from batch 32 to initial:  0.7692873\n",
      "\tDistance from batch 256 to initial:  0.37877738\n",
      "Parameter set: 4\n",
      "\tDistance from batch 32 to initial:  3.9281173\n",
      "\tDistance from batch 256 to initial:  3.204115\n",
      "Parameter set: 5\n",
      "\tDistance from batch 32 to initial:  0.8728317\n",
      "\tDistance from batch 256 to initial:  0.3926258\n",
      "Parameter set: 6\n",
      "\tDistance from batch 32 to initial:  5.157456\n",
      "\tDistance from batch 256 to initial:  3.3155155\n",
      "Parameter set: 7\n",
      "\tDistance from batch 32 to initial:  1.6288005\n",
      "\tDistance from batch 256 to initial:  0.80456364\n",
      "Parameter set: 8\n",
      "\tDistance from batch 32 to initial:  7.22961\n",
      "\tDistance from batch 256 to initial:  4.5245585\n",
      "Parameter set: 9\n",
      "\tDistance from batch 32 to initial:  1.3064474\n",
      "\tDistance from batch 256 to initial:  1.0120746\n",
      "Parameter set: 10\n",
      "\tDistance from batch 32 to initial:  8.768858\n",
      "\tDistance from batch 256 to initial:  4.955517\n",
      "Parameter set: 11\n",
      "\tDistance from batch 32 to initial:  0.86218196\n",
      "\tDistance from batch 256 to initial:  0.6435349\n",
      "Parameter set: 12\n",
      "\tDistance from batch 32 to initial:  2.481973\n",
      "\tDistance from batch 256 to initial:  1.8302994\n",
      "Parameter set: 13\n",
      "\tDistance from batch 32 to initial:  1.130119\n",
      "\tDistance from batch 256 to initial:  0.84699386\n",
      "Parameter set: 14\n",
      "\tDistance from batch 32 to initial:  1.65546\n",
      "\tDistance from batch 256 to initial:  1.7074164\n",
      "Parameter set: 15\n",
      "\tDistance from batch 32 to initial:  0.30166999\n",
      "\tDistance from batch 256 to initial:  0.07132306\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(initial_weights)):\n",
    "    print(\"Parameter set:\", i)\n",
    "    print(\"\\tDistance from batch 32 to initial: \", np.linalg.norm(batch_32_weights[i] - initial_weights[i]))\n",
    "    print(\"\\tDistance from batch 256 to initial: \", np.linalg.norm(batch_256_weights[i] - initial_weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_initial_weights = np.concatenate([x.flatten() for x in initial_weights])\n",
    "flattened_32_weights = np.concatenate([x.flatten() for x in batch_32_weights])\n",
    "flattened_256_weights = np.concatenate([x.flatten() for x in batch_256_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from flattened batch 32 weights to initial weights:  15.1988735\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Distance from flattened batch 32 weights to initial weights: \",\n",
    "    np.linalg.norm(flattened_32_weights - flattened_initial_weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from flattened batch 256 weights to initial weights:  10.045458\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Distance from flattened batch 256 weights to initial weights: \",\n",
    "    np.linalg.norm(flattened_256_weights - flattened_initial_weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing alpha:  -1.0\n",
      "     19/Unknown - 1s 40ms/step - loss: 4.2534 - accuracy: 0.5365\n",
      "Testing alpha:  -0.8421052631578947\n",
      "     19/Unknown - 1s 36ms/step - loss: 2.4581 - accuracy: 0.5815\n",
      "Testing alpha:  -0.6842105263157895\n",
      "     19/Unknown - 1s 36ms/step - loss: 1.3630 - accuracy: 0.6423\n",
      "Testing alpha:  -0.5263157894736843\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.7839 - accuracy: 0.7259\n",
      "Testing alpha:  -0.368421052631579\n",
      "     19/Unknown - 1s 35ms/step - loss: 0.5164 - accuracy: 0.8012\n",
      "Testing alpha:  -0.21052631578947367\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.4049 - accuracy: 0.8298\n",
      "Testing alpha:  -0.052631578947368474\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.3587 - accuracy: 0.8435\n",
      "Testing alpha:  0.10526315789473673\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.3571 - accuracy: 0.8375\n",
      "Testing alpha:  0.26315789473684204\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.3916 - accuracy: 0.8175\n",
      "Testing alpha:  0.42105263157894735\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.4421 - accuracy: 0.7939\n",
      "Testing alpha:  0.5789473684210527\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.4794 - accuracy: 0.7597\n",
      "Testing alpha:  0.7368421052631577\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.4799 - accuracy: 0.7562\n",
      "Testing alpha:  0.894736842105263\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.4158 - accuracy: 0.8001\n",
      "Testing alpha:  1.0526315789473681\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.3791 - accuracy: 0.8280\n",
      "Testing alpha:  1.2105263157894735\n",
      "     19/Unknown - 1s 37ms/step - loss: 0.4768 - accuracy: 0.78 - 1s 37ms/step - loss: 0.4768 - accuracy: 0.7898\n",
      "Testing alpha:  1.3684210526315788\n",
      "     19/Unknown - 1s 36ms/step - loss: 0.8632 - accuracy: 0.6892\n",
      "Testing alpha:  1.526315789473684\n",
      "     19/Unknown - 1s 37ms/step - loss: 2.3112 - accuracy: 0.6000\n",
      "Testing alpha:  1.6842105263157894\n",
      "     19/Unknown - 1s 36ms/step - loss: 7.1011 - accuracy: 0.5503\n",
      "Testing alpha:  1.8421052631578947\n",
      "     19/Unknown - 1s 36ms/step - loss: 19.2208 - accuracy: 0.5254\n",
      "Testing alpha:  2.0\n",
      "     19/Unknown - 1s 35ms/step - loss: 45.1792 - accuracy: 0.5163"
     ]
    }
   ],
   "source": [
    "alpha_values = np.linspace(-1, 2, 20)\n",
    "losses = []\n",
    "for alpha in alpha_values:\n",
    "    print(\"\\nTesting alpha: \", alpha)\n",
    "    target_weights = [batch_256_weights[i]*alpha + batch_32_weights[i]*(1-alpha) for i in range(len(batch_32_weights))]\n",
    "    model.set_weights(target_weights)\n",
    "    loss, accuracy = model.evaluate(validation)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(alpha_values, losses)\n",
    "plt.title(\"Compare batch 32 (alpha=0) and batch 256 (alpha=1) minima\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.show()\n",
    "plt.savefig('graphs/batch_size_alpha_trial_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the large batch size still finds a solution closer to the initial weights, and that its minimizer is sharper than that of the small batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion so far**: Performance on batch size is sensitive to the learning rate. Lower learning rates are better for smaller batch sizes. Also, large batch sizes a) find solutions closer to the initial weights b) find sharp minimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do large batch sizes converge more slowly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, given the same learning rate, the larger batch size converges more slowly. Why is this the case? Maybe larger batch sizes make smaller steps. This would be in line with the observation that they find solutions closer to the initial weights.\n",
    "\n",
    "To evaluate this hypothesis, let's measure the distance traversed per epoch and per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightHistory(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.weights_per_epoch = []\n",
    "        self.weights_per_batch = []\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.weights_per_epoch.append(self.model.get_weights())\n",
    "        self.weights_per_batch.append(self.model.get_weights())\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.weights_per_batch.append(self.model.get_weights())\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.weights_per_epoch.append(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10\n",
      "582/582 [==============================] - 37s 64ms/step - loss: 0.6902 - accuracy: 0.5350 - val_loss: 0.6886 - val_accuracy: 0.5202\n",
      "Epoch 2/10\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 0.6794 - accuracy: 0.5775 - val_loss: 0.6875 - val_accuracy: 0.5252\n",
      "Epoch 3/10\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 0.6709 - accuracy: 0.5879 - val_loss: 0.6768 - val_accuracy: 0.5699\n",
      "Epoch 4/10\n",
      "582/582 [==============================] - 17s 29ms/step - loss: 0.6619 - accuracy: 0.6026 - val_loss: 0.6507 - val_accuracy: 0.6320\n",
      "Epoch 5/10\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 0.6530 - accuracy: 0.6120 - val_loss: 0.6434 - val_accuracy: 0.6322\n",
      "Epoch 6/10\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 0.6449 - accuracy: 0.6234 - val_loss: 0.6375 - val_accuracy: 0.6393\n",
      "Epoch 7/10\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 0.6394 - accuracy: 0.6321 - val_loss: 0.6353 - val_accuracy: 0.6374\n",
      "Epoch 8/10\n",
      "582/582 [==============================] - 15s 26ms/step - loss: 0.6270 - accuracy: 0.6460 - val_loss: 0.6268 - val_accuracy: 0.6410\n",
      "Epoch 9/10\n",
      "582/582 [==============================] - 15s 26ms/step - loss: 0.6054 - accuracy: 0.6693 - val_loss: 0.5886 - val_accuracy: 0.6885\n",
      "Epoch 10/10\n",
      "582/582 [==============================] - 15s 26ms/step - loss: 0.5844 - accuracy: 0.6922 - val_loss: 0.5712 - val_accuracy: 0.6999\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 30s 413ms/step - loss: 0.6936 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6928 - val_accuracy: 0.4951\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.6920 - accuracy: 0.5196 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.6906 - accuracy: 0.5430 - val_loss: 0.6928 - val_accuracy: 0.4914\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.6895 - accuracy: 0.5562 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.6876 - accuracy: 0.5693 - val_loss: 0.6928 - val_accuracy: 0.4923\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.6852 - accuracy: 0.5747 - val_loss: 0.6929 - val_accuracy: 0.4931\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.6834 - accuracy: 0.5805 - val_loss: 0.6926 - val_accuracy: 0.4946\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.6809 - accuracy: 0.5828 - val_loss: 0.6935 - val_accuracy: 0.4953\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.6780 - accuracy: 0.5865 - val_loss: 0.6939 - val_accuracy: 0.4981\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 256]\n",
    "model_state_by_batch_size_trial_3 = {}\n",
    "weight_history_by_batch_size_trial_3 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model(optimizer=keras.optimizers.SGD(0.02))\n",
    "    wh = WeightHistory()\n",
    "    model_state_by_batch_size_trial_3[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=10,\n",
    "        extra_callbacks=[wh],\n",
    "    )\n",
    "    weight_history_by_batch_size_trial_3[batch_size] = wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the last layer, compute the distance between between epoch weights and epoch+1 weights\n",
    "epoch_update_magnitudes_by_batch_size = {}\n",
    "for batch_size in batch_sizes:\n",
    "    epoch_update_magnitudes_by_batch_size[batch_size] = []\n",
    "    weights_per_epoch = weight_history_by_batch_size_trial_3[batch_size].weights_per_epoch\n",
    "    for i in range(len(weights_per_epoch)-1):\n",
    "        curr_epoch_weights = weights_per_epoch[i][-2].flatten()\n",
    "        next_epoch_weights = weights_per_epoch[i+1][-2].flatten()\n",
    "        epoch_update_magnitudes_by_batch_size[batch_size].append(np.linalg.norm(next_epoch_weights - curr_epoch_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(epoch_update_magnitudes_by_batch_size[32], label='batch_32')\n",
    "plt.plot(epoch_update_magnitudes_by_batch_size[256], label='batch_256')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "plt.title('Distance between initial and final weights in each epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that the larger batch sizes do indeed traverse less distance per epoch. In fact, this ratio looks very similar to the inverse of the batch size ratios! (256/32 = 8).\n",
    "\n",
    "But why does it traverse less distance per epoch? Is it because we have fewer updates with a large batch size? Or is it because each batch update traverses less distance? Or both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the last layer, compute the distance between between epoch weights and epoch+1 weights\n",
    "batch_update_magnitudes_by_batch_size = {}\n",
    "for batch_size in batch_sizes:\n",
    "    batch_update_magnitudes_by_batch_size[batch_size] = []\n",
    "    weights_per_batch = weight_history_by_batch_size_trial_3[batch_size].weights_per_batch\n",
    "    for i in range(len(weights_per_batch)-1):\n",
    "        curr_batch_weights = weights_per_batch[i][-2].flatten()\n",
    "        next_batch_weights = weights_per_batch[i+1][-2].flatten()\n",
    "        batch_update_magnitudes_by_batch_size[batch_size].append(np.linalg.norm(next_batch_weights - curr_batch_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(batch_update_magnitudes_by_batch_size[32], label='batch_32')\n",
    "plt.plot(batch_update_magnitudes_by_batch_size[256], label='batch_256')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "plt.title('Distance between initial and final weights in each batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average batch update norm for batch size 32:  0.0032435427\n",
      "Average batch update norm for batch size 256:  0.00043920582\n"
     ]
    }
   ],
   "source": [
    "print(\"Average batch update norm for batch size 32: \", np.mean(batch_update_magnitudes_by_batch_size[32]))\n",
    "print(\"Average batch update norm for batch size 256: \", np.mean(batch_update_magnitudes_by_batch_size[256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each batch update traverses less distance, In fact, the ratio of the batch update sizes is almost exactly the inverse of the batch size ratio!\n",
    "\n",
    "Now we know why batch size 256 converges so much more slowly: not only is it making fewer updates, but each update is also smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does learning rate interact with batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven't fully resolved the question of what batch size to use, and how important of a decision it is when training a model. When we train a model, does it really matter what batch size I use? To answer that question, let's try out more batch sizes. Since we know batch size is sensitive to learning rate, let's also try different learning rates, so we can find the optimal learning rate for each batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 27s 46ms/step - loss: 0.6926 - accuracy: 0.5135- 27s 46ms/step - loss: 0.6926 - accuracy: 0.513\n",
      "Epoch 00001: val_loss improved from inf to 0.69189, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 33s 57ms/step - loss: 0.6926 - accuracy: 0.5135 - val_loss: 0.6919 - val_accuracy: 0.4963\n",
      "Epoch 2/10000\n",
      "581/582 [============================>.] - ETA: 0s - loss: 0.6869 - accuracy: 0.5650\n",
      "Epoch 00002: val_loss improved from 0.69189 to 0.69019, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6869 - accuracy: 0.5651 - val_loss: 0.6902 - val_accuracy: 0.5045\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6786 - accuracy: 0.5803\n",
      "Epoch 00003: val_loss did not improve from 0.69019\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6785 - accuracy: 0.5801 - val_loss: 0.6920 - val_accuracy: 0.5103\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6717 - accuracy: 0.5862\n",
      "Epoch 00004: val_loss improved from 0.69019 to 0.67934, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6716 - accuracy: 0.5861 - val_loss: 0.6793 - val_accuracy: 0.5638\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6652 - accuracy: 0.5975\n",
      "Epoch 00005: val_loss improved from 0.67934 to 0.67834, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6651 - accuracy: 0.5974 - val_loss: 0.6783 - val_accuracy: 0.5686\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6597 - accuracy: 0.6048\n",
      "Epoch 00006: val_loss improved from 0.67834 to 0.66950, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6596 - accuracy: 0.6048 - val_loss: 0.6695 - val_accuracy: 0.5862\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6559 - accuracy: 0.6101\n",
      "Epoch 00007: val_loss improved from 0.66950 to 0.65920, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6558 - accuracy: 0.6100 - val_loss: 0.6592 - val_accuracy: 0.5982\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6509 - accuracy: 0.6130\n",
      "Epoch 00008: val_loss improved from 0.65920 to 0.65471, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6508 - accuracy: 0.6131 - val_loss: 0.6547 - val_accuracy: 0.6058\n",
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.6183\n",
      "Epoch 00009: val_loss improved from 0.65471 to 0.64351, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6456 - accuracy: 0.6183 - val_loss: 0.6435 - val_accuracy: 0.6255\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.6227\n",
      "Epoch 00010: val_loss improved from 0.64351 to 0.64226, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6436 - accuracy: 0.6227 - val_loss: 0.6423 - val_accuracy: 0.6249\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.6256\n",
      "Epoch 00011: val_loss improved from 0.64226 to 0.63751, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6402 - accuracy: 0.6255 - val_loss: 0.6375 - val_accuracy: 0.6320\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.6298\n",
      "Epoch 00012: val_loss improved from 0.63751 to 0.63272, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6367 - accuracy: 0.6298 - val_loss: 0.6327 - val_accuracy: 0.6419\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6332 - accuracy: 0.6388\n",
      "Epoch 00013: val_loss improved from 0.63272 to 0.63019, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6331 - accuracy: 0.6386 - val_loss: 0.6302 - val_accuracy: 0.6462\n",
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6301 - accuracy: 0.6390\n",
      "Epoch 00014: val_loss improved from 0.63019 to 0.62629, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6301 - accuracy: 0.6389 - val_loss: 0.6263 - val_accuracy: 0.6498\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.6433\n",
      "Epoch 00015: val_loss improved from 0.62629 to 0.62124, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6263 - accuracy: 0.6431 - val_loss: 0.6212 - val_accuracy: 0.6569\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6234 - accuracy: 0.6482\n",
      "Epoch 00016: val_loss improved from 0.62124 to 0.61610, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6232 - accuracy: 0.6484 - val_loss: 0.6161 - val_accuracy: 0.6627\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6168 - accuracy: 0.6523\n",
      "Epoch 00017: val_loss improved from 0.61610 to 0.61308, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6167 - accuracy: 0.6522 - val_loss: 0.6131 - val_accuracy: 0.6668\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6083 - accuracy: 0.6672\n",
      "Epoch 00018: val_loss improved from 0.61308 to 0.60535, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6082 - accuracy: 0.6670 - val_loss: 0.6054 - val_accuracy: 0.6763\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.6720\n",
      "Epoch 00019: val_loss improved from 0.60535 to 0.59641, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5997 - accuracy: 0.6718 - val_loss: 0.5964 - val_accuracy: 0.6769\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5819 - accuracy: 0.6910\n",
      "Epoch 00020: val_loss improved from 0.59641 to 0.57687, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5820 - accuracy: 0.6908 - val_loss: 0.5769 - val_accuracy: 0.6926\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7044\n",
      "Epoch 00021: val_loss improved from 0.57687 to 0.56515, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5666 - accuracy: 0.7043 - val_loss: 0.5651 - val_accuracy: 0.7034\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.7137\n",
      "Epoch 00022: val_loss improved from 0.56515 to 0.53693, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5575 - accuracy: 0.7135 - val_loss: 0.5369 - val_accuracy: 0.7347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7242\n",
      "Epoch 00023: val_loss improved from 0.53693 to 0.52775, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5463 - accuracy: 0.7242 - val_loss: 0.5277 - val_accuracy: 0.7386\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5371 - accuracy: 0.7334\n",
      "Epoch 00024: val_loss improved from 0.52775 to 0.51654, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5374 - accuracy: 0.7331 - val_loss: 0.5165 - val_accuracy: 0.7494\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7385\n",
      "Epoch 00025: val_loss improved from 0.51654 to 0.50366, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5254 - accuracy: 0.7382 - val_loss: 0.5037 - val_accuracy: 0.7599\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5184 - accuracy: 0.7454\n",
      "Epoch 00026: val_loss improved from 0.50366 to 0.50106, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5187 - accuracy: 0.7452 - val_loss: 0.5011 - val_accuracy: 0.7564\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7443\n",
      "Epoch 00027: val_loss improved from 0.50106 to 0.49703, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5133 - accuracy: 0.7440 - val_loss: 0.4970 - val_accuracy: 0.7592\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7516\n",
      "Epoch 00028: val_loss improved from 0.49703 to 0.49678, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5089 - accuracy: 0.7514 - val_loss: 0.4968 - val_accuracy: 0.7552\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5002 - accuracy: 0.7552\n",
      "Epoch 00029: val_loss improved from 0.49678 to 0.48222, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5005 - accuracy: 0.7550 - val_loss: 0.4822 - val_accuracy: 0.7687\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.7557\n",
      "Epoch 00030: val_loss improved from 0.48222 to 0.47967, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4967 - accuracy: 0.7553 - val_loss: 0.4797 - val_accuracy: 0.7681\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4927 - accuracy: 0.7619\n",
      "Epoch 00031: val_loss improved from 0.47967 to 0.47592, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4930 - accuracy: 0.7617 - val_loss: 0.4759 - val_accuracy: 0.7764\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7658\n",
      "Epoch 00032: val_loss did not improve from 0.47592\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4907 - accuracy: 0.7655 - val_loss: 0.4770 - val_accuracy: 0.7728\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4878 - accuracy: 0.7663\n",
      "Epoch 00033: val_loss improved from 0.47592 to 0.47513, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4881 - accuracy: 0.7659 - val_loss: 0.4751 - val_accuracy: 0.7734\n",
      "Epoch 34/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4831 - accuracy: 0.7649\n",
      "Epoch 00034: val_loss improved from 0.47513 to 0.46243, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4834 - accuracy: 0.7648 - val_loss: 0.4624 - val_accuracy: 0.7857\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.7702\n",
      "Epoch 00035: val_loss did not improve from 0.46243\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4774 - accuracy: 0.7702 - val_loss: 0.4686 - val_accuracy: 0.7801\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7741\n",
      "Epoch 00036: val_loss improved from 0.46243 to 0.45973, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4720 - accuracy: 0.7738 - val_loss: 0.4597 - val_accuracy: 0.7861\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4699 - accuracy: 0.7745\n",
      "Epoch 00037: val_loss did not improve from 0.45973\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4701 - accuracy: 0.7742 - val_loss: 0.4628 - val_accuracy: 0.7831\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.7765\n",
      "Epoch 00038: val_loss improved from 0.45973 to 0.45030, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4675 - accuracy: 0.7762 - val_loss: 0.4503 - val_accuracy: 0.7919\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.7823\n",
      "Epoch 00039: val_loss improved from 0.45030 to 0.44689, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4634 - accuracy: 0.7821 - val_loss: 0.4469 - val_accuracy: 0.7947\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4620 - accuracy: 0.7796\n",
      "Epoch 00040: val_loss did not improve from 0.44689\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4623 - accuracy: 0.7793 - val_loss: 0.4690 - val_accuracy: 0.7788\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.7847\n",
      "Epoch 00041: val_loss did not improve from 0.44689\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4564 - accuracy: 0.7844 - val_loss: 0.4495 - val_accuracy: 0.7900\n",
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4593 - accuracy: 0.7778\n",
      "Epoch 00042: val_loss did not improve from 0.44689\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4593 - accuracy: 0.7778 - val_loss: 0.4628 - val_accuracy: 0.7816\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7841\n",
      "Epoch 00043: val_loss improved from 0.44689 to 0.44450, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4565 - accuracy: 0.7838 - val_loss: 0.4445 - val_accuracy: 0.7915\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7836\n",
      "Epoch 00044: val_loss did not improve from 0.44450\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4531 - accuracy: 0.7833 - val_loss: 0.4474 - val_accuracy: 0.7891\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4460 - accuracy: 0.7905\n",
      "Epoch 00045: val_loss did not improve from 0.44450\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4461 - accuracy: 0.7905 - val_loss: 0.4450 - val_accuracy: 0.7917\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7898\n",
      "Epoch 00046: val_loss did not improve from 0.44450\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4481 - accuracy: 0.7896 - val_loss: 0.4678 - val_accuracy: 0.7730\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7895\n",
      "Epoch 00047: val_loss did not improve from 0.44450\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4471 - accuracy: 0.7895 - val_loss: 0.4553 - val_accuracy: 0.7807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.7945\n",
      "Epoch 00048: val_loss did not improve from 0.44450\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4433 - accuracy: 0.7942 - val_loss: 0.4607 - val_accuracy: 0.7827\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.7952\n",
      "Epoch 00049: val_loss improved from 0.44450 to 0.44247, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4391 - accuracy: 0.7949 - val_loss: 0.4425 - val_accuracy: 0.7917\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.7957\n",
      "Epoch 00050: val_loss improved from 0.44247 to 0.44167, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4357 - accuracy: 0.7956 - val_loss: 0.4417 - val_accuracy: 0.7911\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.7961\n",
      "Epoch 00051: val_loss improved from 0.44167 to 0.43201, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4334 - accuracy: 0.7959 - val_loss: 0.4320 - val_accuracy: 0.8009\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7960\n",
      "Epoch 00052: val_loss did not improve from 0.43201\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4330 - accuracy: 0.7960 - val_loss: 0.4354 - val_accuracy: 0.7958\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4279 - accuracy: 0.7991\n",
      "Epoch 00053: val_loss did not improve from 0.43201\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4280 - accuracy: 0.7991 - val_loss: 0.4390 - val_accuracy: 0.7919\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8001\n",
      "Epoch 00054: val_loss improved from 0.43201 to 0.42292, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4273 - accuracy: 0.8000 - val_loss: 0.4229 - val_accuracy: 0.8046\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4308 - accuracy: 0.7984\n",
      "Epoch 00055: val_loss did not improve from 0.42292\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4311 - accuracy: 0.7982 - val_loss: 0.4316 - val_accuracy: 0.7932\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4240 - accuracy: 0.8013\n",
      "Epoch 00056: val_loss improved from 0.42292 to 0.41984, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4243 - accuracy: 0.8012 - val_loss: 0.4198 - val_accuracy: 0.8037\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4230 - accuracy: 0.8039\n",
      "Epoch 00057: val_loss improved from 0.41984 to 0.41330, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4231 - accuracy: 0.8038 - val_loss: 0.4133 - val_accuracy: 0.8072\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8018\n",
      "Epoch 00058: val_loss did not improve from 0.41330\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4211 - accuracy: 0.8013 - val_loss: 0.4264 - val_accuracy: 0.7939\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4194 - accuracy: 0.8055\n",
      "Epoch 00059: val_loss improved from 0.41330 to 0.40753, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4196 - accuracy: 0.8055 - val_loss: 0.4075 - val_accuracy: 0.8156\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8027\n",
      "Epoch 00060: val_loss did not improve from 0.40753\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4178 - accuracy: 0.8026 - val_loss: 0.4129 - val_accuracy: 0.8059\n",
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8090\n",
      "Epoch 00061: val_loss did not improve from 0.40753\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4141 - accuracy: 0.8089 - val_loss: 0.4089 - val_accuracy: 0.8121\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8095\n",
      "Epoch 00062: val_loss improved from 0.40753 to 0.40297, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4121 - accuracy: 0.8095 - val_loss: 0.4030 - val_accuracy: 0.8141\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4114 - accuracy: 0.8103\n",
      "Epoch 00063: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4116 - accuracy: 0.8099 - val_loss: 0.4287 - val_accuracy: 0.7919\n",
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8100\n",
      "Epoch 00064: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4084 - accuracy: 0.8099 - val_loss: 0.4232 - val_accuracy: 0.7954\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4072 - accuracy: 0.8067\n",
      "Epoch 00065: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4072 - accuracy: 0.8064 - val_loss: 0.4059 - val_accuracy: 0.8074\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4043 - accuracy: 0.8136\n",
      "Epoch 00066: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4043 - accuracy: 0.8137 - val_loss: 0.4079 - val_accuracy: 0.8055\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8101\n",
      "Epoch 00067: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4052 - accuracy: 0.8100 - val_loss: 0.4161 - val_accuracy: 0.8007\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8145\n",
      "Epoch 00068: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4045 - accuracy: 0.8143 - val_loss: 0.4100 - val_accuracy: 0.8050\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8159\n",
      "Epoch 00069: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4000 - accuracy: 0.8158 - val_loss: 0.4044 - val_accuracy: 0.8085\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8155\n",
      "Epoch 00070: val_loss did not improve from 0.40297\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3983 - accuracy: 0.8154 - val_loss: 0.4237 - val_accuracy: 0.7966\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8205\n",
      "Epoch 00071: val_loss improved from 0.40297 to 0.39174, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3942 - accuracy: 0.8203 - val_loss: 0.3917 - val_accuracy: 0.8175\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8189\n",
      "Epoch 00072: val_loss did not improve from 0.39174\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3945 - accuracy: 0.8188 - val_loss: 0.4025 - val_accuracy: 0.8136\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8209\n",
      "Epoch 00073: val_loss did not improve from 0.39174\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3907 - accuracy: 0.8207 - val_loss: 0.4039 - val_accuracy: 0.8076\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3924 - accuracy: 0.8191\n",
      "Epoch 00074: val_loss did not improve from 0.39174\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3926 - accuracy: 0.8189 - val_loss: 0.3978 - val_accuracy: 0.8132\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3853 - accuracy: 0.8233\n",
      "Epoch 00075: val_loss did not improve from 0.39174\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3853 - accuracy: 0.8234 - val_loss: 0.4028 - val_accuracy: 0.8095\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8217\n",
      "Epoch 00076: val_loss did not improve from 0.39174\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3916 - accuracy: 0.8215 - val_loss: 0.4040 - val_accuracy: 0.8070\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8253\n",
      "Epoch 00077: val_loss improved from 0.39174 to 0.38795, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3819 - accuracy: 0.8251 - val_loss: 0.3880 - val_accuracy: 0.8192\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.8228\n",
      "Epoch 00078: val_loss did not improve from 0.38795\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3853 - accuracy: 0.8228 - val_loss: 0.3971 - val_accuracy: 0.8156\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8218\n",
      "Epoch 00079: val_loss improved from 0.38795 to 0.38633, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3854 - accuracy: 0.8219 - val_loss: 0.3863 - val_accuracy: 0.8214\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8266\n",
      "Epoch 00080: val_loss did not improve from 0.38633\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3830 - accuracy: 0.8267 - val_loss: 0.4075 - val_accuracy: 0.8076\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8270\n",
      "Epoch 00081: val_loss did not improve from 0.38633\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3802 - accuracy: 0.8271 - val_loss: 0.3890 - val_accuracy: 0.8175\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8247\n",
      "Epoch 00082: val_loss did not improve from 0.38633\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3805 - accuracy: 0.8246 - val_loss: 0.3897 - val_accuracy: 0.8173\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8321\n",
      "Epoch 00083: val_loss improved from 0.38633 to 0.38538, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3744 - accuracy: 0.8319 - val_loss: 0.3854 - val_accuracy: 0.8216\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8279\n",
      "Epoch 00084: val_loss did not improve from 0.38538\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3769 - accuracy: 0.8279 - val_loss: 0.3910 - val_accuracy: 0.8169\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8288\n",
      "Epoch 00085: val_loss improved from 0.38538 to 0.38396, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3746 - accuracy: 0.8288 - val_loss: 0.3840 - val_accuracy: 0.8233\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8321\n",
      "Epoch 00086: val_loss did not improve from 0.38396\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3702 - accuracy: 0.8320 - val_loss: 0.4007 - val_accuracy: 0.8110\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.8287\n",
      "Epoch 00087: val_loss did not improve from 0.38396\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3737 - accuracy: 0.8286 - val_loss: 0.3871 - val_accuracy: 0.8224\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8298\n",
      "Epoch 00088: val_loss did not improve from 0.38396\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3733 - accuracy: 0.8298 - val_loss: 0.3882 - val_accuracy: 0.8188\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8342\n",
      "Epoch 00089: val_loss did not improve from 0.38396\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3611 - accuracy: 0.8342 - val_loss: 0.4028 - val_accuracy: 0.8138\n",
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3711 - accuracy: 0.8296\n",
      "Epoch 00090: val_loss improved from 0.38396 to 0.36981, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3713 - accuracy: 0.8294 - val_loss: 0.3698 - val_accuracy: 0.8353\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8321\n",
      "Epoch 00091: val_loss improved from 0.36981 to 0.36953, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3675 - accuracy: 0.8320 - val_loss: 0.3695 - val_accuracy: 0.8371\n",
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8351\n",
      "Epoch 00092: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3625 - accuracy: 0.8350 - val_loss: 0.3927 - val_accuracy: 0.8153\n",
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3626 - accuracy: 0.8358\n",
      "Epoch 00093: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3626 - accuracy: 0.8357 - val_loss: 0.3746 - val_accuracy: 0.8300\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8328\n",
      "Epoch 00094: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3657 - accuracy: 0.8327 - val_loss: 0.3780 - val_accuracy: 0.8278\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8371\n",
      "Epoch 00095: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3594 - accuracy: 0.8370 - val_loss: 0.3732 - val_accuracy: 0.8313\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8361\n",
      "Epoch 00096: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3608 - accuracy: 0.8361 - val_loss: 0.3868 - val_accuracy: 0.8207\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3589 - accuracy: 0.8393\n",
      "Epoch 00097: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3592 - accuracy: 0.8391 - val_loss: 0.3869 - val_accuracy: 0.8194\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8405\n",
      "Epoch 00098: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3565 - accuracy: 0.8404 - val_loss: 0.3840 - val_accuracy: 0.8199\n",
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8379\n",
      "Epoch 00099: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3552 - accuracy: 0.8378 - val_loss: 0.3729 - val_accuracy: 0.8306\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8422\n",
      "Epoch 00100: val_loss did not improve from 0.36953\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3549 - accuracy: 0.8421 - val_loss: 0.3862 - val_accuracy: 0.8237\n",
      "Epoch 101/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/582 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8449\n",
      "Epoch 00101: val_loss improved from 0.36953 to 0.35900, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3509 - accuracy: 0.8448 - val_loss: 0.3590 - val_accuracy: 0.8373\n",
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3519 - accuracy: 0.8421\n",
      "Epoch 00102: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3521 - accuracy: 0.8420 - val_loss: 0.3653 - val_accuracy: 0.8336\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8425\n",
      "Epoch 00103: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3506 - accuracy: 0.8423 - val_loss: 0.3676 - val_accuracy: 0.8338\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8420\n",
      "Epoch 00104: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3509 - accuracy: 0.8419 - val_loss: 0.3642 - val_accuracy: 0.8336\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8454\n",
      "Epoch 00105: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3467 - accuracy: 0.8454 - val_loss: 0.3715 - val_accuracy: 0.8338\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8463\n",
      "Epoch 00106: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3443 - accuracy: 0.8463 - val_loss: 0.3681 - val_accuracy: 0.8330\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8437\n",
      "Epoch 00107: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3447 - accuracy: 0.8435 - val_loss: 0.3658 - val_accuracy: 0.8362\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3467 - accuracy: 0.8470\n",
      "Epoch 00108: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3468 - accuracy: 0.8470 - val_loss: 0.3608 - val_accuracy: 0.8377\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8477\n",
      "Epoch 00109: val_loss did not improve from 0.35900\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3391 - accuracy: 0.8478 - val_loss: 0.3618 - val_accuracy: 0.8353\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8473\n",
      "Epoch 00110: val_loss improved from 0.35900 to 0.35344, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.3394 - accuracy: 0.8472 - val_loss: 0.3534 - val_accuracy: 0.8388\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8482\n",
      "Epoch 00111: val_loss did not improve from 0.35344\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3366 - accuracy: 0.8481 - val_loss: 0.3655 - val_accuracy: 0.8360\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8482\n",
      "Epoch 00112: val_loss did not improve from 0.35344\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3396 - accuracy: 0.8481 - val_loss: 0.3584 - val_accuracy: 0.8399\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8509\n",
      "Epoch 00113: val_loss improved from 0.35344 to 0.35081, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3358 - accuracy: 0.8507 - val_loss: 0.3508 - val_accuracy: 0.8426\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8522\n",
      "Epoch 00114: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3357 - accuracy: 0.8521 - val_loss: 0.3599 - val_accuracy: 0.8394\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8518\n",
      "Epoch 00115: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3333 - accuracy: 0.8517 - val_loss: 0.3750 - val_accuracy: 0.8265\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8539\n",
      "Epoch 00116: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3308 - accuracy: 0.8538 - val_loss: 0.3659 - val_accuracy: 0.8336\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8504\n",
      "Epoch 00117: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3335 - accuracy: 0.8503 - val_loss: 0.3543 - val_accuracy: 0.8420\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8557\n",
      "Epoch 00118: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3243 - accuracy: 0.8555 - val_loss: 0.3719 - val_accuracy: 0.8291\n",
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8554\n",
      "Epoch 00119: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3279 - accuracy: 0.8553 - val_loss: 0.3569 - val_accuracy: 0.8381\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.8547\n",
      "Epoch 00120: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3286 - accuracy: 0.8549 - val_loss: 0.3741 - val_accuracy: 0.8274\n",
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8550\n",
      "Epoch 00121: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3251 - accuracy: 0.8549 - val_loss: 0.3609 - val_accuracy: 0.8343\n",
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8606\n",
      "Epoch 00122: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3222 - accuracy: 0.8605 - val_loss: 0.3609 - val_accuracy: 0.8353\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8596\n",
      "Epoch 00123: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3192 - accuracy: 0.8597 - val_loss: 0.3692 - val_accuracy: 0.8334\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8577\n",
      "Epoch 00124: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3249 - accuracy: 0.8579 - val_loss: 0.3594 - val_accuracy: 0.8424\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8585\n",
      "Epoch 00125: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3192 - accuracy: 0.8584 - val_loss: 0.3722 - val_accuracy: 0.8298\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8608\n",
      "Epoch 00126: val_loss did not improve from 0.35081\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3177 - accuracy: 0.8607 - val_loss: 0.3799 - val_accuracy: 0.8272\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8589\n",
      "Epoch 00127: val_loss improved from 0.35081 to 0.34834, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3196 - accuracy: 0.8587 - val_loss: 0.3483 - val_accuracy: 0.8452\n",
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8607\n",
      "Epoch 00128: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3145 - accuracy: 0.8607 - val_loss: 0.3513 - val_accuracy: 0.8446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8618\n",
      "Epoch 00129: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3132 - accuracy: 0.8617 - val_loss: 0.3535 - val_accuracy: 0.8433\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8595\n",
      "Epoch 00130: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3138 - accuracy: 0.8595 - val_loss: 0.3778 - val_accuracy: 0.8298\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8595\n",
      "Epoch 00131: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3153 - accuracy: 0.8594 - val_loss: 0.3581 - val_accuracy: 0.8379\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8614\n",
      "Epoch 00132: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3151 - accuracy: 0.8616 - val_loss: 0.3536 - val_accuracy: 0.8414\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8628\n",
      "Epoch 00133: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3087 - accuracy: 0.8627 - val_loss: 0.3514 - val_accuracy: 0.8418\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8673\n",
      "Epoch 00134: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3065 - accuracy: 0.8674 - val_loss: 0.3530 - val_accuracy: 0.8379\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8647\n",
      "Epoch 00135: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3075 - accuracy: 0.8645 - val_loss: 0.3526 - val_accuracy: 0.8411\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8635\n",
      "Epoch 00136: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3099 - accuracy: 0.8636 - val_loss: 0.3575 - val_accuracy: 0.8356\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8668\n",
      "Epoch 00137: val_loss did not improve from 0.34834\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2998 - accuracy: 0.8666 - val_loss: 0.3554 - val_accuracy: 0.8371\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8656\n",
      "Epoch 00138: val_loss improved from 0.34834 to 0.34509, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3051 - accuracy: 0.8655 - val_loss: 0.3451 - val_accuracy: 0.8435\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8672\n",
      "Epoch 00139: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3014 - accuracy: 0.8672 - val_loss: 0.3507 - val_accuracy: 0.8403\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8683\n",
      "Epoch 00140: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3004 - accuracy: 0.8684 - val_loss: 0.3504 - val_accuracy: 0.8431\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8671\n",
      "Epoch 00141: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2997 - accuracy: 0.8672 - val_loss: 0.3512 - val_accuracy: 0.8375\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8690\n",
      "Epoch 00142: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2974 - accuracy: 0.8690 - val_loss: 0.3531 - val_accuracy: 0.8383\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8670\n",
      "Epoch 00143: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3007 - accuracy: 0.8671 - val_loss: 0.3576 - val_accuracy: 0.8416\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8681\n",
      "Epoch 00144: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3023 - accuracy: 0.8681 - val_loss: 0.3562 - val_accuracy: 0.8386\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8719\n",
      "Epoch 00145: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2946 - accuracy: 0.8720 - val_loss: 0.3511 - val_accuracy: 0.8407\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8718\n",
      "Epoch 00146: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2929 - accuracy: 0.8721 - val_loss: 0.3479 - val_accuracy: 0.8444\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8721\n",
      "Epoch 00147: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2892 - accuracy: 0.8720 - val_loss: 0.3637 - val_accuracy: 0.8304\n",
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8719\n",
      "Epoch 00148: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2919 - accuracy: 0.8721 - val_loss: 0.3545 - val_accuracy: 0.8414\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.8765\n",
      "Epoch 00149: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2901 - accuracy: 0.8764 - val_loss: 0.3545 - val_accuracy: 0.8416\n",
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8768\n",
      "Epoch 00150: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2851 - accuracy: 0.8768 - val_loss: 0.3506 - val_accuracy: 0.8414\n",
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8744\n",
      "Epoch 00151: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2897 - accuracy: 0.8743 - val_loss: 0.3503 - val_accuracy: 0.8463\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8798\n",
      "Epoch 00152: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2813 - accuracy: 0.8796 - val_loss: 0.3573 - val_accuracy: 0.8366\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8752\n",
      "Epoch 00153: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2850 - accuracy: 0.8753 - val_loss: 0.3614 - val_accuracy: 0.8390\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8785\n",
      "Epoch 00154: val_loss did not improve from 0.34509\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2838 - accuracy: 0.8783 - val_loss: 0.3474 - val_accuracy: 0.8446\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8742\n",
      "Epoch 00155: val_loss improved from 0.34509 to 0.34388, saving model to pickled_objects/batch_size_32_lr_0.01_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2825 - accuracy: 0.8742 - val_loss: 0.3439 - val_accuracy: 0.8446\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8802\n",
      "Epoch 00156: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2795 - accuracy: 0.8802 - val_loss: 0.3503 - val_accuracy: 0.8429\n",
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8810\n",
      "Epoch 00157: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2756 - accuracy: 0.8809 - val_loss: 0.3526 - val_accuracy: 0.8405\n",
      "Epoch 158/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.8830\n",
      "Epoch 00158: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2751 - accuracy: 0.8831 - val_loss: 0.3537 - val_accuracy: 0.8388\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.8804\n",
      "Epoch 00159: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2775 - accuracy: 0.8807 - val_loss: 0.3592 - val_accuracy: 0.8368\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8803\n",
      "Epoch 00160: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2769 - accuracy: 0.8801 - val_loss: 0.3522 - val_accuracy: 0.8394\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8799\n",
      "Epoch 00161: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2778 - accuracy: 0.8797 - val_loss: 0.3535 - val_accuracy: 0.8411\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.8780\n",
      "Epoch 00162: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2775 - accuracy: 0.8779 - val_loss: 0.3583 - val_accuracy: 0.8390\n",
      "Epoch 163/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.8820\n",
      "Epoch 00163: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2748 - accuracy: 0.8821 - val_loss: 0.3499 - val_accuracy: 0.8407\n",
      "Epoch 164/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8834\n",
      "Epoch 00164: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2687 - accuracy: 0.8836 - val_loss: 0.3517 - val_accuracy: 0.8437\n",
      "Epoch 165/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.8808\n",
      "Epoch 00165: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2724 - accuracy: 0.8809 - val_loss: 0.3593 - val_accuracy: 0.8396\n",
      "Epoch 166/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.8840\n",
      "Epoch 00166: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2706 - accuracy: 0.8839 - val_loss: 0.3536 - val_accuracy: 0.8381\n",
      "Epoch 167/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.8839\n",
      "Epoch 00167: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2704 - accuracy: 0.8839 - val_loss: 0.3573 - val_accuracy: 0.8418\n",
      "Epoch 168/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.8851\n",
      "Epoch 00168: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2653 - accuracy: 0.8852 - val_loss: 0.3496 - val_accuracy: 0.8457\n",
      "Epoch 169/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.8868\n",
      "Epoch 00169: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2660 - accuracy: 0.8869 - val_loss: 0.3527 - val_accuracy: 0.8431\n",
      "Epoch 170/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.8852\n",
      "Epoch 00170: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2662 - accuracy: 0.8851 - val_loss: 0.3462 - val_accuracy: 0.8461\n",
      "Epoch 171/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.8870\n",
      "Epoch 00171: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2651 - accuracy: 0.8871 - val_loss: 0.3529 - val_accuracy: 0.8399\n",
      "Epoch 172/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.8886\n",
      "Epoch 00172: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2619 - accuracy: 0.8886 - val_loss: 0.3459 - val_accuracy: 0.8487\n",
      "Epoch 173/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.8855\n",
      "Epoch 00173: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2648 - accuracy: 0.8854 - val_loss: 0.3482 - val_accuracy: 0.8442\n",
      "Epoch 174/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8895\n",
      "Epoch 00174: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2590 - accuracy: 0.8896 - val_loss: 0.3490 - val_accuracy: 0.8444\n",
      "Epoch 175/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.8900\n",
      "Epoch 00175: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2591 - accuracy: 0.8902 - val_loss: 0.3580 - val_accuracy: 0.8409\n",
      "Epoch 176/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.8896\n",
      "Epoch 00176: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2553 - accuracy: 0.8896 - val_loss: 0.3551 - val_accuracy: 0.8390\n",
      "Epoch 177/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.8880\n",
      "Epoch 00177: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2596 - accuracy: 0.8882 - val_loss: 0.3572 - val_accuracy: 0.8399\n",
      "Epoch 178/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.8920\n",
      "Epoch 00178: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2519 - accuracy: 0.8921 - val_loss: 0.3508 - val_accuracy: 0.8407\n",
      "Epoch 179/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.8914\n",
      "Epoch 00179: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2537 - accuracy: 0.8913 - val_loss: 0.3594 - val_accuracy: 0.8416\n",
      "Epoch 180/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.8953\n",
      "Epoch 00180: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2503 - accuracy: 0.8953 - val_loss: 0.3500 - val_accuracy: 0.8431\n",
      "Epoch 181/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.8947\n",
      "Epoch 00181: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2486 - accuracy: 0.8948 - val_loss: 0.3580 - val_accuracy: 0.8375\n",
      "Epoch 182/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.8921\n",
      "Epoch 00182: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2501 - accuracy: 0.8922 - val_loss: 0.3527 - val_accuracy: 0.8474\n",
      "Epoch 183/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8923\n",
      "Epoch 00183: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2496 - accuracy: 0.8924 - val_loss: 0.3592 - val_accuracy: 0.8435\n",
      "Epoch 184/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.8927\n",
      "Epoch 00184: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2504 - accuracy: 0.8927 - val_loss: 0.3483 - val_accuracy: 0.8403\n",
      "Epoch 185/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.8948\n",
      "Epoch 00185: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2461 - accuracy: 0.8948 - val_loss: 0.3636 - val_accuracy: 0.8334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.8911\n",
      "Epoch 00186: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2512 - accuracy: 0.8911 - val_loss: 0.3650 - val_accuracy: 0.8360\n",
      "Epoch 187/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.8969\n",
      "Epoch 00187: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2421 - accuracy: 0.8969 - val_loss: 0.3765 - val_accuracy: 0.8338\n",
      "Epoch 188/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.8952\n",
      "Epoch 00188: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2437 - accuracy: 0.8953 - val_loss: 0.3605 - val_accuracy: 0.8386\n",
      "Epoch 189/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9004\n",
      "Epoch 00189: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2403 - accuracy: 0.9004 - val_loss: 0.3567 - val_accuracy: 0.8356\n",
      "Epoch 190/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.8978\n",
      "Epoch 00190: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2390 - accuracy: 0.8979 - val_loss: 0.3583 - val_accuracy: 0.8452\n",
      "Epoch 191/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.8947\n",
      "Epoch 00191: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2392 - accuracy: 0.8948 - val_loss: 0.3537 - val_accuracy: 0.8433\n",
      "Epoch 192/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.8947\n",
      "Epoch 00192: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2430 - accuracy: 0.8948 - val_loss: 0.3566 - val_accuracy: 0.8392\n",
      "Epoch 193/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.8994\n",
      "Epoch 00193: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2366 - accuracy: 0.8994 - val_loss: 0.3685 - val_accuracy: 0.8401\n",
      "Epoch 194/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.8976\n",
      "Epoch 00194: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2371 - accuracy: 0.8976 - val_loss: 0.3546 - val_accuracy: 0.8394\n",
      "Epoch 195/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9007\n",
      "Epoch 00195: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2325 - accuracy: 0.9007 - val_loss: 0.3536 - val_accuracy: 0.8433\n",
      "Epoch 196/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9016\n",
      "Epoch 00196: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2345 - accuracy: 0.9015 - val_loss: 0.3555 - val_accuracy: 0.8405\n",
      "Epoch 197/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.8999\n",
      "Epoch 00197: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2336 - accuracy: 0.8999 - val_loss: 0.3698 - val_accuracy: 0.8353\n",
      "Epoch 198/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9027\n",
      "Epoch 00198: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2304 - accuracy: 0.9028 - val_loss: 0.3665 - val_accuracy: 0.8362\n",
      "Epoch 199/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9040\n",
      "Epoch 00199: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2262 - accuracy: 0.9040 - val_loss: 0.3570 - val_accuracy: 0.8454\n",
      "Epoch 200/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2333 - accuracy: 0.9023\n",
      "Epoch 00200: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2330 - accuracy: 0.9024 - val_loss: 0.3672 - val_accuracy: 0.8379\n",
      "Epoch 201/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9009\n",
      "Epoch 00201: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2283 - accuracy: 0.9010 - val_loss: 0.3735 - val_accuracy: 0.8330\n",
      "Epoch 202/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.9039\n",
      "Epoch 00202: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2266 - accuracy: 0.9039 - val_loss: 0.3599 - val_accuracy: 0.8446\n",
      "Epoch 203/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9036\n",
      "Epoch 00203: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2266 - accuracy: 0.9036 - val_loss: 0.3604 - val_accuracy: 0.8386\n",
      "Epoch 204/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9045\n",
      "Epoch 00204: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2252 - accuracy: 0.9046 - val_loss: 0.3636 - val_accuracy: 0.8414\n",
      "Epoch 205/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.9040\n",
      "Epoch 00205: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2287 - accuracy: 0.9040 - val_loss: 0.3643 - val_accuracy: 0.8424\n",
      "Epoch 206/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9054\n",
      "Epoch 00206: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2241 - accuracy: 0.9054 - val_loss: 0.3659 - val_accuracy: 0.8368\n",
      "Epoch 207/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9080\n",
      "Epoch 00207: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2201 - accuracy: 0.9078 - val_loss: 0.3691 - val_accuracy: 0.8343\n",
      "Epoch 208/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2200 - accuracy: 0.9070\n",
      "Epoch 00208: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2201 - accuracy: 0.9069 - val_loss: 0.3830 - val_accuracy: 0.8287\n",
      "Epoch 209/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2194 - accuracy: 0.9067\n",
      "Epoch 00209: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2192 - accuracy: 0.9069 - val_loss: 0.3702 - val_accuracy: 0.8383\n",
      "Epoch 210/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9100\n",
      "Epoch 00210: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2203 - accuracy: 0.9099 - val_loss: 0.3756 - val_accuracy: 0.8358\n",
      "Epoch 211/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9102\n",
      "Epoch 00211: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2123 - accuracy: 0.9104 - val_loss: 0.3744 - val_accuracy: 0.8360\n",
      "Epoch 212/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.9136\n",
      "Epoch 00212: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2101 - accuracy: 0.9136 - val_loss: 0.3663 - val_accuracy: 0.8416\n",
      "Epoch 213/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9117\n",
      "Epoch 00213: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2124 - accuracy: 0.9118 - val_loss: 0.3761 - val_accuracy: 0.8373\n",
      "Epoch 214/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9116\n",
      "Epoch 00214: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2153 - accuracy: 0.9117 - val_loss: 0.3699 - val_accuracy: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9130\n",
      "Epoch 00215: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2070 - accuracy: 0.9130 - val_loss: 0.3749 - val_accuracy: 0.8388\n",
      "Epoch 216/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9117\n",
      "Epoch 00216: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2116 - accuracy: 0.9119 - val_loss: 0.3797 - val_accuracy: 0.8362\n",
      "Epoch 217/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9135\n",
      "Epoch 00217: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2081 - accuracy: 0.9135 - val_loss: 0.3816 - val_accuracy: 0.8362\n",
      "Epoch 218/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9128\n",
      "Epoch 00218: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2110 - accuracy: 0.9128 - val_loss: 0.3872 - val_accuracy: 0.8345\n",
      "Epoch 219/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2092 - accuracy: 0.9127\n",
      "Epoch 00219: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2093 - accuracy: 0.9125 - val_loss: 0.3662 - val_accuracy: 0.8405\n",
      "Epoch 220/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9130\n",
      "Epoch 00220: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2072 - accuracy: 0.9132 - val_loss: 0.3772 - val_accuracy: 0.8377\n",
      "Epoch 221/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9169\n",
      "Epoch 00221: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2048 - accuracy: 0.9170 - val_loss: 0.3891 - val_accuracy: 0.8362\n",
      "Epoch 222/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9114\n",
      "Epoch 00222: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2140 - accuracy: 0.9115 - val_loss: 0.3696 - val_accuracy: 0.8386\n",
      "Epoch 223/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9144\n",
      "Epoch 00223: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2079 - accuracy: 0.9142 - val_loss: 0.3798 - val_accuracy: 0.8345\n",
      "Epoch 224/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9149\n",
      "Epoch 00224: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2022 - accuracy: 0.9149 - val_loss: 0.3695 - val_accuracy: 0.8364\n",
      "Epoch 225/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9158\n",
      "Epoch 00225: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2035 - accuracy: 0.9159 - val_loss: 0.3807 - val_accuracy: 0.8356\n",
      "Epoch 226/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1991 - accuracy: 0.9177\n",
      "Epoch 00226: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1989 - accuracy: 0.9177 - val_loss: 0.3785 - val_accuracy: 0.8401\n",
      "Epoch 227/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9113\n",
      "Epoch 00227: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2027 - accuracy: 0.9114 - val_loss: 0.3783 - val_accuracy: 0.8362\n",
      "Epoch 228/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 00228: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1962 - accuracy: 0.9191 - val_loss: 0.3600 - val_accuracy: 0.8433\n",
      "Epoch 229/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1990 - accuracy: 0.9173\n",
      "Epoch 00229: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1989 - accuracy: 0.9173 - val_loss: 0.3760 - val_accuracy: 0.8435\n",
      "Epoch 230/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9179\n",
      "Epoch 00230: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2004 - accuracy: 0.9178 - val_loss: 0.3859 - val_accuracy: 0.8345\n",
      "Epoch 231/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9163\n",
      "Epoch 00231: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2010 - accuracy: 0.9163 - val_loss: 0.3757 - val_accuracy: 0.8390\n",
      "Epoch 232/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9178\n",
      "Epoch 00232: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2008 - accuracy: 0.9177 - val_loss: 0.4010 - val_accuracy: 0.8293\n",
      "Epoch 233/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9191\n",
      "Epoch 00233: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1969 - accuracy: 0.9191 - val_loss: 0.3792 - val_accuracy: 0.8351\n",
      "Epoch 234/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9196\n",
      "Epoch 00234: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1929 - accuracy: 0.9196 - val_loss: 0.3893 - val_accuracy: 0.8373\n",
      "Epoch 235/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1879 - accuracy: 0.9212\n",
      "Epoch 00235: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1875 - accuracy: 0.9213 - val_loss: 0.3731 - val_accuracy: 0.8407\n",
      "Epoch 236/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9212\n",
      "Epoch 00236: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1899 - accuracy: 0.9211 - val_loss: 0.3770 - val_accuracy: 0.8416\n",
      "Epoch 237/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9213\n",
      "Epoch 00237: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1922 - accuracy: 0.9214 - val_loss: 0.3851 - val_accuracy: 0.8383\n",
      "Epoch 238/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9181\n",
      "Epoch 00238: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1972 - accuracy: 0.9183 - val_loss: 0.3862 - val_accuracy: 0.8338\n",
      "Epoch 239/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9238\n",
      "Epoch 00239: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1853 - accuracy: 0.9239 - val_loss: 0.3793 - val_accuracy: 0.8388\n",
      "Epoch 240/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9226\n",
      "Epoch 00240: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1912 - accuracy: 0.9226 - val_loss: 0.3896 - val_accuracy: 0.8328\n",
      "Epoch 241/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9225\n",
      "Epoch 00241: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1915 - accuracy: 0.9226 - val_loss: 0.3864 - val_accuracy: 0.8332\n",
      "Epoch 242/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1853 - accuracy: 0.9254\n",
      "Epoch 00242: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1852 - accuracy: 0.9254 - val_loss: 0.3829 - val_accuracy: 0.8364\n",
      "Epoch 243/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1889 - accuracy: 0.9215\n",
      "Epoch 00243: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1888 - accuracy: 0.9216 - val_loss: 0.3857 - val_accuracy: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9251\n",
      "Epoch 00244: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1828 - accuracy: 0.9252 - val_loss: 0.3901 - val_accuracy: 0.8381\n",
      "Epoch 245/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9260\n",
      "Epoch 00245: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1815 - accuracy: 0.9261 - val_loss: 0.3839 - val_accuracy: 0.8403\n",
      "Epoch 246/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1779 - accuracy: 0.9259\n",
      "Epoch 00246: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1776 - accuracy: 0.9260 - val_loss: 0.3784 - val_accuracy: 0.8394\n",
      "Epoch 247/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9268\n",
      "Epoch 00247: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1789 - accuracy: 0.9269 - val_loss: 0.3956 - val_accuracy: 0.8347\n",
      "Epoch 248/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9238\n",
      "Epoch 00248: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1837 - accuracy: 0.9236 - val_loss: 0.3869 - val_accuracy: 0.8409\n",
      "Epoch 249/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9274\n",
      "Epoch 00249: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1789 - accuracy: 0.9275 - val_loss: 0.3978 - val_accuracy: 0.8332\n",
      "Epoch 250/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9252\n",
      "Epoch 00250: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1820 - accuracy: 0.9253 - val_loss: 0.3975 - val_accuracy: 0.8368\n",
      "Epoch 251/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9293\n",
      "Epoch 00251: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1751 - accuracy: 0.9293 - val_loss: 0.3801 - val_accuracy: 0.8394\n",
      "Epoch 252/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1760 - accuracy: 0.9268\n",
      "Epoch 00252: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1759 - accuracy: 0.9268 - val_loss: 0.3816 - val_accuracy: 0.8403\n",
      "Epoch 253/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9267\n",
      "Epoch 00253: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1794 - accuracy: 0.9267 - val_loss: 0.4136 - val_accuracy: 0.8306\n",
      "Epoch 254/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9280\n",
      "Epoch 00254: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1794 - accuracy: 0.9281 - val_loss: 0.3940 - val_accuracy: 0.8368\n",
      "Epoch 255/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9291\n",
      "Epoch 00255: val_loss did not improve from 0.34388\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1741 - accuracy: 0.9293 - val_loss: 0.4003 - val_accuracy: 0.8349\n",
      "Epoch 00255: early stopping\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 13s 22ms/step - loss: 0.6902 - accuracy: 0.5350- 12s 22ms/step - l - 12s 22ms/step - loss: 0.6902 - accuracy: 0.5\n",
      "Epoch 00001: val_loss improved from inf to 0.68862, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 14s 24ms/step - loss: 0.6902 - accuracy: 0.5350 - val_loss: 0.6886 - val_accuracy: 0.5202\n",
      "Epoch 2/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5776\n",
      "Epoch 00002: val_loss improved from 0.68862 to 0.68747, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6794 - accuracy: 0.5775 - val_loss: 0.6875 - val_accuracy: 0.5252\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6710 - accuracy: 0.5881\n",
      "Epoch 00003: val_loss improved from 0.68747 to 0.67682, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6709 - accuracy: 0.5879 - val_loss: 0.6768 - val_accuracy: 0.5699\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.6029\n",
      "Epoch 00004: val_loss improved from 0.67682 to 0.65073, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6619 - accuracy: 0.6026 - val_loss: 0.6507 - val_accuracy: 0.6320\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.6120\n",
      "Epoch 00005: val_loss improved from 0.65073 to 0.64336, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6530 - accuracy: 0.6120 - val_loss: 0.6434 - val_accuracy: 0.6322\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6450 - accuracy: 0.6236\n",
      "Epoch 00006: val_loss improved from 0.64336 to 0.63746, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.6449 - accuracy: 0.6234 - val_loss: 0.6375 - val_accuracy: 0.6393\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6395 - accuracy: 0.6320\n",
      "Epoch 00007: val_loss improved from 0.63746 to 0.63528, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6394 - accuracy: 0.6321 - val_loss: 0.6353 - val_accuracy: 0.6374\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6271 - accuracy: 0.6459\n",
      "Epoch 00008: val_loss improved from 0.63528 to 0.62683, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6270 - accuracy: 0.6460 - val_loss: 0.6268 - val_accuracy: 0.6410\n",
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.6694\n",
      "Epoch 00009: val_loss improved from 0.62683 to 0.58856, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6054 - accuracy: 0.6693 - val_loss: 0.5886 - val_accuracy: 0.6885\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.6922\n",
      "Epoch 00010: val_loss improved from 0.58856 to 0.57121, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5844 - accuracy: 0.6922 - val_loss: 0.5712 - val_accuracy: 0.6999\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7159\n",
      "Epoch 00011: val_loss improved from 0.57121 to 0.53828, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5602 - accuracy: 0.7157 - val_loss: 0.5383 - val_accuracy: 0.7343\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7276\n",
      "Epoch 00012: val_loss improved from 0.53828 to 0.51815, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5399 - accuracy: 0.7273 - val_loss: 0.5182 - val_accuracy: 0.7487\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7400\n",
      "Epoch 00013: val_loss improved from 0.51815 to 0.51482, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5261 - accuracy: 0.7397 - val_loss: 0.5148 - val_accuracy: 0.7633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5184 - accuracy: 0.7453\n",
      "Epoch 00014: val_loss improved from 0.51482 to 0.49813, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5186 - accuracy: 0.7450 - val_loss: 0.4981 - val_accuracy: 0.7584\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7498\n",
      "Epoch 00015: val_loss improved from 0.49813 to 0.48875, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5088 - accuracy: 0.7495 - val_loss: 0.4888 - val_accuracy: 0.7683\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7560\n",
      "Epoch 00016: val_loss improved from 0.48875 to 0.48066, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5011 - accuracy: 0.7558 - val_loss: 0.4807 - val_accuracy: 0.7764\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7585\n",
      "Epoch 00017: val_loss improved from 0.48066 to 0.47625, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.4959 - accuracy: 0.7584 - val_loss: 0.4762 - val_accuracy: 0.7775\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.7616\n",
      "Epoch 00018: val_loss did not improve from 0.47625\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4891 - accuracy: 0.7614 - val_loss: 0.4801 - val_accuracy: 0.7657\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4829 - accuracy: 0.7633\n",
      "Epoch 00019: val_loss did not improve from 0.47625\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4833 - accuracy: 0.7631 - val_loss: 0.4835 - val_accuracy: 0.7721\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7713\n",
      "Epoch 00020: val_loss improved from 0.47625 to 0.45731, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4775 - accuracy: 0.7710 - val_loss: 0.4573 - val_accuracy: 0.7876\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7731\n",
      "Epoch 00021: val_loss improved from 0.45731 to 0.45708, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4719 - accuracy: 0.7730 - val_loss: 0.4571 - val_accuracy: 0.7913\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4688 - accuracy: 0.7710\n",
      "Epoch 00022: val_loss improved from 0.45708 to 0.45270, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4693 - accuracy: 0.7706 - val_loss: 0.4527 - val_accuracy: 0.7876\n",
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.7781\n",
      "Epoch 00023: val_loss improved from 0.45270 to 0.45264, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4635 - accuracy: 0.7779 - val_loss: 0.4526 - val_accuracy: 0.7893\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.7824\n",
      "Epoch 00024: val_loss improved from 0.45264 to 0.44685, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4605 - accuracy: 0.7820 - val_loss: 0.4469 - val_accuracy: 0.7951\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4535 - accuracy: 0.7836\n",
      "Epoch 00025: val_loss improved from 0.44685 to 0.44369, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4539 - accuracy: 0.7832 - val_loss: 0.4437 - val_accuracy: 0.7966\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4513 - accuracy: 0.7869\n",
      "Epoch 00026: val_loss improved from 0.44369 to 0.44090, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4516 - accuracy: 0.7866 - val_loss: 0.4409 - val_accuracy: 0.7945\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4465 - accuracy: 0.7888\n",
      "Epoch 00027: val_loss improved from 0.44090 to 0.43908, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4466 - accuracy: 0.7887 - val_loss: 0.4391 - val_accuracy: 0.7973\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7922\n",
      "Epoch 00028: val_loss did not improve from 0.43908\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4437 - accuracy: 0.7920 - val_loss: 0.4422 - val_accuracy: 0.7923\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4386 - accuracy: 0.7947\n",
      "Epoch 00029: val_loss improved from 0.43908 to 0.43406, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4389 - accuracy: 0.7944 - val_loss: 0.4341 - val_accuracy: 0.7982\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.7935\n",
      "Epoch 00030: val_loss did not improve from 0.43406\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4339 - accuracy: 0.7933 - val_loss: 0.4410 - val_accuracy: 0.7921\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.7977\n",
      "Epoch 00031: val_loss improved from 0.43406 to 0.42159, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4341 - accuracy: 0.7976 - val_loss: 0.4216 - val_accuracy: 0.8022\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4287 - accuracy: 0.7964\n",
      "Epoch 00032: val_loss did not improve from 0.42159\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4288 - accuracy: 0.7962 - val_loss: 0.4263 - val_accuracy: 0.7984\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8026\n",
      "Epoch 00033: val_loss improved from 0.42159 to 0.41989, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4267 - accuracy: 0.8025 - val_loss: 0.4199 - val_accuracy: 0.8035\n",
      "Epoch 34/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8007\n",
      "Epoch 00034: val_loss did not improve from 0.41989\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4250 - accuracy: 0.8008 - val_loss: 0.4262 - val_accuracy: 0.8031\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8029\n",
      "Epoch 00035: val_loss improved from 0.41989 to 0.41478, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4186 - accuracy: 0.8028 - val_loss: 0.4148 - val_accuracy: 0.8080\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.8050\n",
      "Epoch 00036: val_loss did not improve from 0.41478\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4184 - accuracy: 0.8047 - val_loss: 0.4209 - val_accuracy: 0.8085\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8110\n",
      "Epoch 00037: val_loss did not improve from 0.41478\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4140 - accuracy: 0.8110 - val_loss: 0.4243 - val_accuracy: 0.8001\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8069\n",
      "Epoch 00038: val_loss improved from 0.41478 to 0.40853, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4111 - accuracy: 0.8069 - val_loss: 0.4085 - val_accuracy: 0.8104\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8105\n",
      "Epoch 00039: val_loss improved from 0.40853 to 0.40743, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4061 - accuracy: 0.8103 - val_loss: 0.4074 - val_accuracy: 0.8132\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8150\n",
      "Epoch 00040: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4008 - accuracy: 0.8147 - val_loss: 0.4102 - val_accuracy: 0.8072\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8158\n",
      "Epoch 00041: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3988 - accuracy: 0.8156 - val_loss: 0.4091 - val_accuracy: 0.8085\n",
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8144\n",
      "Epoch 00042: val_loss did not improve from 0.40743\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4040 - accuracy: 0.8140 - val_loss: 0.4264 - val_accuracy: 0.7962\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8169\n",
      "Epoch 00043: val_loss improved from 0.40743 to 0.40309, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3979 - accuracy: 0.8165 - val_loss: 0.4031 - val_accuracy: 0.8110\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8155\n",
      "Epoch 00044: val_loss improved from 0.40309 to 0.39577, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3987 - accuracy: 0.8156 - val_loss: 0.3958 - val_accuracy: 0.8160\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8242\n",
      "Epoch 00045: val_loss improved from 0.39577 to 0.39531, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3887 - accuracy: 0.8242 - val_loss: 0.3953 - val_accuracy: 0.8173\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.8225\n",
      "Epoch 00046: val_loss improved from 0.39531 to 0.39132, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3878 - accuracy: 0.8224 - val_loss: 0.3913 - val_accuracy: 0.8216\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8235\n",
      "Epoch 00047: val_loss improved from 0.39132 to 0.39124, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3833 - accuracy: 0.8235 - val_loss: 0.3912 - val_accuracy: 0.8160\n",
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8253\n",
      "Epoch 00048: val_loss did not improve from 0.39124\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3812 - accuracy: 0.8250 - val_loss: 0.3967 - val_accuracy: 0.8201\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8284\n",
      "Epoch 00049: val_loss improved from 0.39124 to 0.38379, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3787 - accuracy: 0.8283 - val_loss: 0.3838 - val_accuracy: 0.8224\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8310\n",
      "Epoch 00050: val_loss improved from 0.38379 to 0.38313, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3729 - accuracy: 0.8310 - val_loss: 0.3831 - val_accuracy: 0.8212\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8304\n",
      "Epoch 00051: val_loss did not improve from 0.38313\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3743 - accuracy: 0.8306 - val_loss: 0.3875 - val_accuracy: 0.8175\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3707 - accuracy: 0.8326\n",
      "Epoch 00052: val_loss improved from 0.38313 to 0.38275, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3707 - accuracy: 0.8326 - val_loss: 0.3828 - val_accuracy: 0.8267\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8322\n",
      "Epoch 00053: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3683 - accuracy: 0.8321 - val_loss: 0.3872 - val_accuracy: 0.8209\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3630 - accuracy: 0.8374\n",
      "Epoch 00054: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3632 - accuracy: 0.8373 - val_loss: 0.3902 - val_accuracy: 0.8192\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8336\n",
      "Epoch 00055: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3686 - accuracy: 0.8334 - val_loss: 0.3840 - val_accuracy: 0.8203\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8385\n",
      "Epoch 00056: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3589 - accuracy: 0.8386 - val_loss: 0.3856 - val_accuracy: 0.8246\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8402\n",
      "Epoch 00057: val_loss did not improve from 0.38275\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3595 - accuracy: 0.8402 - val_loss: 0.3851 - val_accuracy: 0.8203\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.8372\n",
      "Epoch 00058: val_loss improved from 0.38275 to 0.38232, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3622 - accuracy: 0.8371 - val_loss: 0.3823 - val_accuracy: 0.8227\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8406\n",
      "Epoch 00059: val_loss improved from 0.38232 to 0.37136, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3548 - accuracy: 0.8407 - val_loss: 0.3714 - val_accuracy: 0.8323\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8418\n",
      "Epoch 00060: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3528 - accuracy: 0.8416 - val_loss: 0.3831 - val_accuracy: 0.8209\n",
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8439\n",
      "Epoch 00061: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3501 - accuracy: 0.8437 - val_loss: 0.3811 - val_accuracy: 0.8257\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.8432\n",
      "Epoch 00062: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3456 - accuracy: 0.8433 - val_loss: 0.3814 - val_accuracy: 0.8255\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8436\n",
      "Epoch 00063: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3486 - accuracy: 0.8435 - val_loss: 0.3804 - val_accuracy: 0.8257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8486\n",
      "Epoch 00064: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3425 - accuracy: 0.8485 - val_loss: 0.3727 - val_accuracy: 0.8259\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8483\n",
      "Epoch 00065: val_loss did not improve from 0.37136\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3393 - accuracy: 0.8482 - val_loss: 0.3753 - val_accuracy: 0.8222\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8496\n",
      "Epoch 00066: val_loss improved from 0.37136 to 0.36717, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3354 - accuracy: 0.8497 - val_loss: 0.3672 - val_accuracy: 0.8323\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8504\n",
      "Epoch 00067: val_loss improved from 0.36717 to 0.36542, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3343 - accuracy: 0.8503 - val_loss: 0.3654 - val_accuracy: 0.8364\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8485\n",
      "Epoch 00068: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3372 - accuracy: 0.8486 - val_loss: 0.3679 - val_accuracy: 0.8304\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3329 - accuracy: 0.8520\n",
      "Epoch 00069: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3331 - accuracy: 0.8519 - val_loss: 0.3660 - val_accuracy: 0.8336\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8548\n",
      "Epoch 00070: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3264 - accuracy: 0.8550 - val_loss: 0.3777 - val_accuracy: 0.8270\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8548\n",
      "Epoch 00071: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3240 - accuracy: 0.8546 - val_loss: 0.3799 - val_accuracy: 0.8252\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8596\n",
      "Epoch 00072: val_loss did not improve from 0.36542\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3245 - accuracy: 0.8596 - val_loss: 0.3768 - val_accuracy: 0.8304\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8592\n",
      "Epoch 00073: val_loss improved from 0.36542 to 0.36301, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3194 - accuracy: 0.8591 - val_loss: 0.3630 - val_accuracy: 0.8362\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8599\n",
      "Epoch 00074: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3181 - accuracy: 0.8598 - val_loss: 0.3808 - val_accuracy: 0.8233\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8578\n",
      "Epoch 00075: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3163 - accuracy: 0.8579 - val_loss: 0.3765 - val_accuracy: 0.8285\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8575\n",
      "Epoch 00076: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3171 - accuracy: 0.8576 - val_loss: 0.3829 - val_accuracy: 0.8259\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8645\n",
      "Epoch 00077: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3071 - accuracy: 0.8646 - val_loss: 0.3750 - val_accuracy: 0.8263\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8655\n",
      "Epoch 00078: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3106 - accuracy: 0.8654 - val_loss: 0.3752 - val_accuracy: 0.8287\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8630\n",
      "Epoch 00079: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3099 - accuracy: 0.8631 - val_loss: 0.3693 - val_accuracy: 0.8338\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8652\n",
      "Epoch 00080: val_loss did not improve from 0.36301\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3059 - accuracy: 0.8653 - val_loss: 0.3849 - val_accuracy: 0.8259\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8688\n",
      "Epoch 00081: val_loss improved from 0.36301 to 0.36234, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3020 - accuracy: 0.8688 - val_loss: 0.3623 - val_accuracy: 0.8349\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8659\n",
      "Epoch 00082: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3018 - accuracy: 0.8660 - val_loss: 0.3710 - val_accuracy: 0.8351\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8699\n",
      "Epoch 00083: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2979 - accuracy: 0.8700 - val_loss: 0.3735 - val_accuracy: 0.8248\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8658\n",
      "Epoch 00084: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3030 - accuracy: 0.8659 - val_loss: 0.3690 - val_accuracy: 0.8360\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8730\n",
      "Epoch 00085: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2938 - accuracy: 0.8730 - val_loss: 0.3696 - val_accuracy: 0.8347\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8748\n",
      "Epoch 00086: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2898 - accuracy: 0.8749 - val_loss: 0.3686 - val_accuracy: 0.8315\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.8728\n",
      "Epoch 00087: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2893 - accuracy: 0.8728 - val_loss: 0.3711 - val_accuracy: 0.8325\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8733\n",
      "Epoch 00088: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2904 - accuracy: 0.8733 - val_loss: 0.3735 - val_accuracy: 0.8317\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8768\n",
      "Epoch 00089: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2832 - accuracy: 0.8768 - val_loss: 0.3741 - val_accuracy: 0.8298\n",
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8745\n",
      "Epoch 00090: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2886 - accuracy: 0.8746 - val_loss: 0.3767 - val_accuracy: 0.8306\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8750\n",
      "Epoch 00091: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2843 - accuracy: 0.8750 - val_loss: 0.3657 - val_accuracy: 0.8351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8803\n",
      "Epoch 00092: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2786 - accuracy: 0.8803 - val_loss: 0.3670 - val_accuracy: 0.8362\n",
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.8787\n",
      "Epoch 00093: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2778 - accuracy: 0.8788 - val_loss: 0.3680 - val_accuracy: 0.8358\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.8792\n",
      "Epoch 00094: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2791 - accuracy: 0.8791 - val_loss: 0.3868 - val_accuracy: 0.8233\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.8819\n",
      "Epoch 00095: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2752 - accuracy: 0.8820 - val_loss: 0.3747 - val_accuracy: 0.8310\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2691 - accuracy: 0.8851\n",
      "Epoch 00096: val_loss did not improve from 0.36234\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2689 - accuracy: 0.8850 - val_loss: 0.3881 - val_accuracy: 0.8295\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8814\n",
      "Epoch 00097: val_loss improved from 0.36234 to 0.35756, saving model to pickled_objects/batch_size_32_lr_0.02_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2732 - accuracy: 0.8813 - val_loss: 0.3576 - val_accuracy: 0.8433\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.8850\n",
      "Epoch 00098: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2651 - accuracy: 0.8852 - val_loss: 0.3922 - val_accuracy: 0.8227\n",
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8892\n",
      "Epoch 00099: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2616 - accuracy: 0.8892 - val_loss: 0.4109 - val_accuracy: 0.8207\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8854\n",
      "Epoch 00100: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2657 - accuracy: 0.8855 - val_loss: 0.3755 - val_accuracy: 0.8375\n",
      "Epoch 101/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.8865\n",
      "Epoch 00101: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2627 - accuracy: 0.8865 - val_loss: 0.3661 - val_accuracy: 0.8368\n",
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.8902\n",
      "Epoch 00102: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2590 - accuracy: 0.8902 - val_loss: 0.3678 - val_accuracy: 0.8383\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.8899\n",
      "Epoch 00103: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2585 - accuracy: 0.8897 - val_loss: 0.3884 - val_accuracy: 0.8263\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8890\n",
      "Epoch 00104: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2602 - accuracy: 0.8890 - val_loss: 0.3767 - val_accuracy: 0.8334\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.8884\n",
      "Epoch 00105: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2575 - accuracy: 0.8884 - val_loss: 0.3714 - val_accuracy: 0.8375\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.8924\n",
      "Epoch 00106: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2483 - accuracy: 0.8925 - val_loss: 0.3833 - val_accuracy: 0.8332\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.8934\n",
      "Epoch 00107: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2524 - accuracy: 0.8934 - val_loss: 0.3965 - val_accuracy: 0.8257\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.8939\n",
      "Epoch 00108: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2530 - accuracy: 0.8939 - val_loss: 0.3748 - val_accuracy: 0.8340\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.8931\n",
      "Epoch 00109: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2498 - accuracy: 0.8931 - val_loss: 0.3911 - val_accuracy: 0.8306\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.8969\n",
      "Epoch 00110: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2412 - accuracy: 0.8970 - val_loss: 0.3883 - val_accuracy: 0.8287\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8991\n",
      "Epoch 00111: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2435 - accuracy: 0.8989 - val_loss: 0.3842 - val_accuracy: 0.8345\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.8975\n",
      "Epoch 00112: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2419 - accuracy: 0.8974 - val_loss: 0.4049 - val_accuracy: 0.8164\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.8966\n",
      "Epoch 00113: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2398 - accuracy: 0.8967 - val_loss: 0.3776 - val_accuracy: 0.8340\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.8985\n",
      "Epoch 00114: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2400 - accuracy: 0.8986 - val_loss: 0.3724 - val_accuracy: 0.8323\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.9040\n",
      "Epoch 00115: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2274 - accuracy: 0.9042 - val_loss: 0.3896 - val_accuracy: 0.8321\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9018\n",
      "Epoch 00116: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2333 - accuracy: 0.9018 - val_loss: 0.4003 - val_accuracy: 0.8175\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9007\n",
      "Epoch 00117: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2322 - accuracy: 0.9008 - val_loss: 0.3821 - val_accuracy: 0.8353\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9026\n",
      "Epoch 00118: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2293 - accuracy: 0.9026 - val_loss: 0.4011 - val_accuracy: 0.8261\n",
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9054\n",
      "Epoch 00119: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2262 - accuracy: 0.9055 - val_loss: 0.3797 - val_accuracy: 0.8328\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9003\n",
      "Epoch 00120: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2345 - accuracy: 0.9003 - val_loss: 0.3694 - val_accuracy: 0.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9058\n",
      "Epoch 00121: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2231 - accuracy: 0.9057 - val_loss: 0.3897 - val_accuracy: 0.8334\n",
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.9065\n",
      "Epoch 00122: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2241 - accuracy: 0.9064 - val_loss: 0.3729 - val_accuracy: 0.8383\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9064\n",
      "Epoch 00123: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2218 - accuracy: 0.9064 - val_loss: 0.3803 - val_accuracy: 0.8347\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9067\n",
      "Epoch 00124: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2240 - accuracy: 0.9066 - val_loss: 0.3790 - val_accuracy: 0.8347\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9099\n",
      "Epoch 00125: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2166 - accuracy: 0.9099 - val_loss: 0.3938 - val_accuracy: 0.8386\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9071\n",
      "Epoch 00126: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2204 - accuracy: 0.9069 - val_loss: 0.3719 - val_accuracy: 0.8373\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9087\n",
      "Epoch 00127: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2153 - accuracy: 0.9088 - val_loss: 0.3873 - val_accuracy: 0.8343\n",
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.9137\n",
      "Epoch 00128: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2120 - accuracy: 0.9137 - val_loss: 0.3951 - val_accuracy: 0.8332\n",
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9098\n",
      "Epoch 00129: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2108 - accuracy: 0.9098 - val_loss: 0.3951 - val_accuracy: 0.8317\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9122\n",
      "Epoch 00130: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2085 - accuracy: 0.9123 - val_loss: 0.3952 - val_accuracy: 0.8351\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9114\n",
      "Epoch 00131: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2115 - accuracy: 0.9113 - val_loss: 0.3889 - val_accuracy: 0.8293\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9146\n",
      "Epoch 00132: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2092 - accuracy: 0.9146 - val_loss: 0.3860 - val_accuracy: 0.8353\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9097\n",
      "Epoch 00133: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2111 - accuracy: 0.9098 - val_loss: 0.3791 - val_accuracy: 0.8373\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9176\n",
      "Epoch 00134: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2000 - accuracy: 0.9176 - val_loss: 0.3874 - val_accuracy: 0.8323\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9137\n",
      "Epoch 00135: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2082 - accuracy: 0.9139 - val_loss: 0.4030 - val_accuracy: 0.8267\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9161\n",
      "Epoch 00136: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1974 - accuracy: 0.9160 - val_loss: 0.4098 - val_accuracy: 0.8287\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9163\n",
      "Epoch 00137: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2017 - accuracy: 0.9162 - val_loss: 0.3937 - val_accuracy: 0.8351\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9167\n",
      "Epoch 00138: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2025 - accuracy: 0.9169 - val_loss: 0.4301 - val_accuracy: 0.8214\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9197\n",
      "Epoch 00139: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1965 - accuracy: 0.9198 - val_loss: 0.4326 - val_accuracy: 0.8153\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9199\n",
      "Epoch 00140: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1965 - accuracy: 0.9200 - val_loss: 0.3909 - val_accuracy: 0.8325\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9162\n",
      "Epoch 00141: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1986 - accuracy: 0.9162 - val_loss: 0.4026 - val_accuracy: 0.8274\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9194\n",
      "Epoch 00142: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1884 - accuracy: 0.9195 - val_loss: 0.3946 - val_accuracy: 0.8319\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9230\n",
      "Epoch 00143: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1887 - accuracy: 0.9228 - val_loss: 0.4123 - val_accuracy: 0.8212\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9209\n",
      "Epoch 00144: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1938 - accuracy: 0.9208 - val_loss: 0.4160 - val_accuracy: 0.8255\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9231\n",
      "Epoch 00145: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1894 - accuracy: 0.9232 - val_loss: 0.3889 - val_accuracy: 0.8330\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9239\n",
      "Epoch 00146: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1884 - accuracy: 0.9240 - val_loss: 0.4052 - val_accuracy: 0.8276\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9261\n",
      "Epoch 00147: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1856 - accuracy: 0.9261 - val_loss: 0.3983 - val_accuracy: 0.8338\n",
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9226\n",
      "Epoch 00148: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1869 - accuracy: 0.9225 - val_loss: 0.4142 - val_accuracy: 0.8239\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9214\n",
      "Epoch 00149: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1881 - accuracy: 0.9216 - val_loss: 0.4163 - val_accuracy: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9277\n",
      "Epoch 00150: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.1787 - accuracy: 0.9278 - val_loss: 0.4192 - val_accuracy: 0.8278\n",
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9234\n",
      "Epoch 00151: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1844 - accuracy: 0.9233 - val_loss: 0.4030 - val_accuracy: 0.8353\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9267\n",
      "Epoch 00152: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1819 - accuracy: 0.9268 - val_loss: 0.4242 - val_accuracy: 0.8205\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9287\n",
      "Epoch 00153: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1770 - accuracy: 0.9289 - val_loss: 0.4111 - val_accuracy: 0.8321\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9288\n",
      "Epoch 00154: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1729 - accuracy: 0.9289 - val_loss: 0.4083 - val_accuracy: 0.8338\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9274\n",
      "Epoch 00155: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1765 - accuracy: 0.9273 - val_loss: 0.4194 - val_accuracy: 0.8244\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9294\n",
      "Epoch 00156: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1736 - accuracy: 0.9294 - val_loss: 0.4112 - val_accuracy: 0.8351\n",
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9292\n",
      "Epoch 00157: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1711 - accuracy: 0.9293 - val_loss: 0.4261 - val_accuracy: 0.8274\n",
      "Epoch 158/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9318\n",
      "Epoch 00158: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1726 - accuracy: 0.9319 - val_loss: 0.4273 - val_accuracy: 0.8285\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9314\n",
      "Epoch 00159: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1702 - accuracy: 0.9314 - val_loss: 0.4357 - val_accuracy: 0.8293\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9279\n",
      "Epoch 00160: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1769 - accuracy: 0.9277 - val_loss: 0.4142 - val_accuracy: 0.8291\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9340\n",
      "Epoch 00161: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1666 - accuracy: 0.9341 - val_loss: 0.4247 - val_accuracy: 0.8289\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9327\n",
      "Epoch 00162: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1646 - accuracy: 0.9327 - val_loss: 0.4152 - val_accuracy: 0.8308\n",
      "Epoch 163/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9294\n",
      "Epoch 00163: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1718 - accuracy: 0.9294 - val_loss: 0.4183 - val_accuracy: 0.8356\n",
      "Epoch 164/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9314\n",
      "Epoch 00164: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1666 - accuracy: 0.9315 - val_loss: 0.4110 - val_accuracy: 0.8321\n",
      "Epoch 165/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9318\n",
      "Epoch 00165: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1675 - accuracy: 0.9319 - val_loss: 0.4143 - val_accuracy: 0.8298\n",
      "Epoch 166/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9323\n",
      "Epoch 00166: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1660 - accuracy: 0.9322 - val_loss: 0.4151 - val_accuracy: 0.8291\n",
      "Epoch 167/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9342\n",
      "Epoch 00167: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1586 - accuracy: 0.9342 - val_loss: 0.4162 - val_accuracy: 0.8319\n",
      "Epoch 168/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9351\n",
      "Epoch 00168: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1608 - accuracy: 0.9350 - val_loss: 0.4172 - val_accuracy: 0.8356\n",
      "Epoch 169/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9378\n",
      "Epoch 00169: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1555 - accuracy: 0.9379 - val_loss: 0.4244 - val_accuracy: 0.8259\n",
      "Epoch 170/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9379\n",
      "Epoch 00170: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1556 - accuracy: 0.9379 - val_loss: 0.4449 - val_accuracy: 0.8110\n",
      "Epoch 171/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9393\n",
      "Epoch 00171: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1552 - accuracy: 0.9395 - val_loss: 0.4237 - val_accuracy: 0.8300\n",
      "Epoch 172/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9348\n",
      "Epoch 00172: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1603 - accuracy: 0.9348 - val_loss: 0.4200 - val_accuracy: 0.8334\n",
      "Epoch 173/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9363\n",
      "Epoch 00173: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1579 - accuracy: 0.9362 - val_loss: 0.4336 - val_accuracy: 0.8259\n",
      "Epoch 174/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9370\n",
      "Epoch 00174: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1542 - accuracy: 0.9369 - val_loss: 0.4300 - val_accuracy: 0.8282\n",
      "Epoch 175/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9381\n",
      "Epoch 00175: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1568 - accuracy: 0.9382 - val_loss: 0.4504 - val_accuracy: 0.8169\n",
      "Epoch 176/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9367\n",
      "Epoch 00176: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1598 - accuracy: 0.9367 - val_loss: 0.4363 - val_accuracy: 0.8246\n",
      "Epoch 177/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9422\n",
      "Epoch 00177: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1474 - accuracy: 0.9423 - val_loss: 0.4490 - val_accuracy: 0.8263\n",
      "Epoch 178/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9385\n",
      "Epoch 00178: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1520 - accuracy: 0.9385 - val_loss: 0.4235 - val_accuracy: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9379\n",
      "Epoch 00179: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1520 - accuracy: 0.9379 - val_loss: 0.4439 - val_accuracy: 0.8293\n",
      "Epoch 180/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9397\n",
      "Epoch 00180: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1491 - accuracy: 0.9398 - val_loss: 0.4361 - val_accuracy: 0.8282\n",
      "Epoch 181/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9399\n",
      "Epoch 00181: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1529 - accuracy: 0.9399 - val_loss: 0.4436 - val_accuracy: 0.8231\n",
      "Epoch 182/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9405\n",
      "Epoch 00182: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1472 - accuracy: 0.9405 - val_loss: 0.4284 - val_accuracy: 0.8317\n",
      "Epoch 183/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9399\n",
      "Epoch 00183: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1467 - accuracy: 0.9399 - val_loss: 0.4459 - val_accuracy: 0.8293\n",
      "Epoch 184/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9426\n",
      "Epoch 00184: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1471 - accuracy: 0.9425 - val_loss: 0.4510 - val_accuracy: 0.8302\n",
      "Epoch 185/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.9413\n",
      "Epoch 00185: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1508 - accuracy: 0.9415 - val_loss: 0.4381 - val_accuracy: 0.8214\n",
      "Epoch 186/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9430\n",
      "Epoch 00186: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1446 - accuracy: 0.9430 - val_loss: 0.4447 - val_accuracy: 0.8237\n",
      "Epoch 187/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.9428\n",
      "Epoch 00187: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1439 - accuracy: 0.9429 - val_loss: 0.4511 - val_accuracy: 0.8280\n",
      "Epoch 188/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.9454\n",
      "Epoch 00188: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1401 - accuracy: 0.9455 - val_loss: 0.4648 - val_accuracy: 0.8175\n",
      "Epoch 189/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9434\n",
      "Epoch 00189: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1437 - accuracy: 0.9434 - val_loss: 0.4515 - val_accuracy: 0.8250\n",
      "Epoch 190/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9466\n",
      "Epoch 00190: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1380 - accuracy: 0.9465 - val_loss: 0.4478 - val_accuracy: 0.8274\n",
      "Epoch 191/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9459\n",
      "Epoch 00191: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1359 - accuracy: 0.9459 - val_loss: 0.4612 - val_accuracy: 0.8239\n",
      "Epoch 192/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9456\n",
      "Epoch 00192: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1388 - accuracy: 0.9456 - val_loss: 0.4481 - val_accuracy: 0.8272\n",
      "Epoch 193/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9459\n",
      "Epoch 00193: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1382 - accuracy: 0.9460 - val_loss: 0.4838 - val_accuracy: 0.8162\n",
      "Epoch 194/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9452\n",
      "Epoch 00194: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1342 - accuracy: 0.9452 - val_loss: 0.4695 - val_accuracy: 0.8196\n",
      "Epoch 195/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9454\n",
      "Epoch 00195: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1387 - accuracy: 0.9455 - val_loss: 0.4888 - val_accuracy: 0.8147\n",
      "Epoch 196/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9450\n",
      "Epoch 00196: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1338 - accuracy: 0.9450 - val_loss: 0.4619 - val_accuracy: 0.8237\n",
      "Epoch 197/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9473\n",
      "Epoch 00197: val_loss did not improve from 0.35756\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1334 - accuracy: 0.9473 - val_loss: 0.4602 - val_accuracy: 0.8257\n",
      "Epoch 00197: early stopping\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 12s 21ms/step - loss: 0.6880 - accuracy: 0.5421\n",
      "Epoch 00001: val_loss improved from inf to 0.67542, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 23ms/step - loss: 0.6880 - accuracy: 0.5421 - val_loss: 0.6754 - val_accuracy: 0.6068\n",
      "Epoch 2/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.5764\n",
      "Epoch 00002: val_loss improved from 0.67542 to 0.66237, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6770 - accuracy: 0.5763 - val_loss: 0.6624 - val_accuracy: 0.6124\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6570 - accuracy: 0.6102\n",
      "Epoch 00003: val_loss improved from 0.66237 to 0.64491, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6568 - accuracy: 0.6103 - val_loss: 0.6449 - val_accuracy: 0.6275\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.6439\n",
      "Epoch 00004: val_loss improved from 0.64491 to 0.62793, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6334 - accuracy: 0.6440 - val_loss: 0.6279 - val_accuracy: 0.6574\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.6641\n",
      "Epoch 00005: val_loss improved from 0.62793 to 0.60374, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6156 - accuracy: 0.6643 - val_loss: 0.6037 - val_accuracy: 0.6814\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5976 - accuracy: 0.6786\n",
      "Epoch 00006: val_loss improved from 0.60374 to 0.58232, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5976 - accuracy: 0.6785 - val_loss: 0.5823 - val_accuracy: 0.7010\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5787 - accuracy: 0.6984\n",
      "Epoch 00007: val_loss improved from 0.58232 to 0.55334, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5788 - accuracy: 0.6983 - val_loss: 0.5533 - val_accuracy: 0.7216\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7101\n",
      "Epoch 00008: val_loss improved from 0.55334 to 0.54363, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5629 - accuracy: 0.7098 - val_loss: 0.5436 - val_accuracy: 0.7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5483 - accuracy: 0.7185\n",
      "Epoch 00009: val_loss improved from 0.54363 to 0.53008, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5485 - accuracy: 0.7183 - val_loss: 0.5301 - val_accuracy: 0.7287\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5388 - accuracy: 0.7245\n",
      "Epoch 00010: val_loss improved from 0.53008 to 0.51803, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5391 - accuracy: 0.7242 - val_loss: 0.5180 - val_accuracy: 0.7418\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7340\n",
      "Epoch 00011: val_loss improved from 0.51803 to 0.50134, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5296 - accuracy: 0.7336 - val_loss: 0.5013 - val_accuracy: 0.7577\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5196 - accuracy: 0.7372\n",
      "Epoch 00012: val_loss did not improve from 0.50134\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5198 - accuracy: 0.7369 - val_loss: 0.5155 - val_accuracy: 0.7511\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5080 - accuracy: 0.7434\n",
      "Epoch 00013: val_loss improved from 0.50134 to 0.47988, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5084 - accuracy: 0.7433 - val_loss: 0.4799 - val_accuracy: 0.7730\n",
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5001 - accuracy: 0.7523\n",
      "Epoch 00014: val_loss did not improve from 0.47988\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5005 - accuracy: 0.7521 - val_loss: 0.4837 - val_accuracy: 0.7728\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.7560\n",
      "Epoch 00015: val_loss did not improve from 0.47988\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4939 - accuracy: 0.7557 - val_loss: 0.4834 - val_accuracy: 0.7700\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.7621\n",
      "Epoch 00016: val_loss improved from 0.47988 to 0.47658, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4896 - accuracy: 0.7617 - val_loss: 0.4766 - val_accuracy: 0.7773\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4816 - accuracy: 0.7643\n",
      "Epoch 00017: val_loss did not improve from 0.47658\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4822 - accuracy: 0.7641 - val_loss: 0.5023 - val_accuracy: 0.7481\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7699\n",
      "Epoch 00018: val_loss did not improve from 0.47658\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4736 - accuracy: 0.7696 - val_loss: 0.4887 - val_accuracy: 0.7582\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7680\n",
      "Epoch 00019: val_loss improved from 0.47658 to 0.46379, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4754 - accuracy: 0.7679 - val_loss: 0.4638 - val_accuracy: 0.7857\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4677 - accuracy: 0.7723\n",
      "Epoch 00020: val_loss did not improve from 0.46379\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4679 - accuracy: 0.7720 - val_loss: 0.4880 - val_accuracy: 0.7640\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4654 - accuracy: 0.7745\n",
      "Epoch 00021: val_loss did not improve from 0.46379\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4660 - accuracy: 0.7743 - val_loss: 0.4725 - val_accuracy: 0.7711\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.7803\n",
      "Epoch 00022: val_loss did not improve from 0.46379\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4565 - accuracy: 0.7802 - val_loss: 0.4858 - val_accuracy: 0.7629\n",
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4498 - accuracy: 0.7842\n",
      "Epoch 00023: val_loss improved from 0.46379 to 0.44294, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4504 - accuracy: 0.7837 - val_loss: 0.4429 - val_accuracy: 0.7926\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7888\n",
      "Epoch 00024: val_loss did not improve from 0.44294\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4448 - accuracy: 0.7888 - val_loss: 0.4847 - val_accuracy: 0.7610\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.7948\n",
      "Epoch 00025: val_loss improved from 0.44294 to 0.44225, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4393 - accuracy: 0.7943 - val_loss: 0.4422 - val_accuracy: 0.7902\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.7913\n",
      "Epoch 00026: val_loss did not improve from 0.44225\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4363 - accuracy: 0.7909 - val_loss: 0.4509 - val_accuracy: 0.7908\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.7984\n",
      "Epoch 00027: val_loss improved from 0.44225 to 0.43844, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4303 - accuracy: 0.7981 - val_loss: 0.4384 - val_accuracy: 0.7919\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8004\n",
      "Epoch 00028: val_loss improved from 0.43844 to 0.42172, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4256 - accuracy: 0.8003 - val_loss: 0.4217 - val_accuracy: 0.8037\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4167 - accuracy: 0.8026\n",
      "Epoch 00029: val_loss did not improve from 0.42172\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4174 - accuracy: 0.8024 - val_loss: 0.4221 - val_accuracy: 0.8050\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8033\n",
      "Epoch 00030: val_loss improved from 0.42172 to 0.41543, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4154 - accuracy: 0.8030 - val_loss: 0.4154 - val_accuracy: 0.8115\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8062\n",
      "Epoch 00031: val_loss improved from 0.41543 to 0.39948, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4115 - accuracy: 0.8060 - val_loss: 0.3995 - val_accuracy: 0.8207\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.8121\n",
      "Epoch 00032: val_loss improved from 0.39948 to 0.39879, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4057 - accuracy: 0.8120 - val_loss: 0.3988 - val_accuracy: 0.8158\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8138\n",
      "Epoch 00033: val_loss did not improve from 0.39879\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3997 - accuracy: 0.8133 - val_loss: 0.4165 - val_accuracy: 0.8055\n",
      "Epoch 34/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/582 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8139\n",
      "Epoch 00034: val_loss did not improve from 0.39879\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3994 - accuracy: 0.8138 - val_loss: 0.4157 - val_accuracy: 0.8074\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8225\n",
      "Epoch 00035: val_loss improved from 0.39879 to 0.39776, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3913 - accuracy: 0.8226 - val_loss: 0.3978 - val_accuracy: 0.8166\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3922 - accuracy: 0.8169\n",
      "Epoch 00036: val_loss improved from 0.39776 to 0.39436, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3925 - accuracy: 0.8167 - val_loss: 0.3944 - val_accuracy: 0.8203\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8220\n",
      "Epoch 00037: val_loss did not improve from 0.39436\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3870 - accuracy: 0.8218 - val_loss: 0.3946 - val_accuracy: 0.8227\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8252\n",
      "Epoch 00038: val_loss improved from 0.39436 to 0.38364, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3825 - accuracy: 0.8250 - val_loss: 0.3836 - val_accuracy: 0.8278\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3758 - accuracy: 0.8257\n",
      "Epoch 00039: val_loss did not improve from 0.38364\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3760 - accuracy: 0.8257 - val_loss: 0.4013 - val_accuracy: 0.8143\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8299\n",
      "Epoch 00040: val_loss improved from 0.38364 to 0.37974, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3723 - accuracy: 0.8299 - val_loss: 0.3797 - val_accuracy: 0.8259\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8311\n",
      "Epoch 00041: val_loss did not improve from 0.37974\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3656 - accuracy: 0.8310 - val_loss: 0.3803 - val_accuracy: 0.8310\n",
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8332\n",
      "Epoch 00042: val_loss did not improve from 0.37974\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3646 - accuracy: 0.8330 - val_loss: 0.4007 - val_accuracy: 0.8171\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8360\n",
      "Epoch 00043: val_loss did not improve from 0.37974\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3612 - accuracy: 0.8357 - val_loss: 0.3870 - val_accuracy: 0.8298\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8363\n",
      "Epoch 00044: val_loss improved from 0.37974 to 0.37599, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3608 - accuracy: 0.8363 - val_loss: 0.3760 - val_accuracy: 0.8315\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.8377\n",
      "Epoch 00045: val_loss did not improve from 0.37599\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3568 - accuracy: 0.8378 - val_loss: 0.3882 - val_accuracy: 0.8244\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8388\n",
      "Epoch 00046: val_loss improved from 0.37599 to 0.37472, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3547 - accuracy: 0.8386 - val_loss: 0.3747 - val_accuracy: 0.8308\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8465\n",
      "Epoch 00047: val_loss did not improve from 0.37472\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3475 - accuracy: 0.8465 - val_loss: 0.3904 - val_accuracy: 0.8252\n",
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8442\n",
      "Epoch 00048: val_loss did not improve from 0.37472\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3423 - accuracy: 0.8441 - val_loss: 0.3780 - val_accuracy: 0.8334\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.8442\n",
      "Epoch 00049: val_loss improved from 0.37472 to 0.36836, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3441 - accuracy: 0.8443 - val_loss: 0.3684 - val_accuracy: 0.8356\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8487\n",
      "Epoch 00050: val_loss improved from 0.36836 to 0.36219, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3398 - accuracy: 0.8485 - val_loss: 0.3622 - val_accuracy: 0.8407\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8503\n",
      "Epoch 00051: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3307 - accuracy: 0.8505 - val_loss: 0.3784 - val_accuracy: 0.8330\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8515\n",
      "Epoch 00052: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3325 - accuracy: 0.8513 - val_loss: 0.3666 - val_accuracy: 0.8368\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8542\n",
      "Epoch 00053: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3282 - accuracy: 0.8544 - val_loss: 0.3939 - val_accuracy: 0.8229\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8531\n",
      "Epoch 00054: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3277 - accuracy: 0.8531 - val_loss: 0.3632 - val_accuracy: 0.8426\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8524\n",
      "Epoch 00055: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3279 - accuracy: 0.8526 - val_loss: 0.3667 - val_accuracy: 0.8381\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8584\n",
      "Epoch 00056: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3155 - accuracy: 0.8585 - val_loss: 0.3679 - val_accuracy: 0.8338\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8632\n",
      "Epoch 00057: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3159 - accuracy: 0.8632 - val_loss: 0.3783 - val_accuracy: 0.8364\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8556\n",
      "Epoch 00058: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3207 - accuracy: 0.8557 - val_loss: 0.3658 - val_accuracy: 0.8388\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8642\n",
      "Epoch 00059: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3128 - accuracy: 0.8639 - val_loss: 0.3966 - val_accuracy: 0.8162\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8679\n",
      "Epoch 00060: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3038 - accuracy: 0.8678 - val_loss: 0.3796 - val_accuracy: 0.8298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8652\n",
      "Epoch 00061: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3067 - accuracy: 0.8651 - val_loss: 0.3718 - val_accuracy: 0.8347\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8700\n",
      "Epoch 00062: val_loss did not improve from 0.36219\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3007 - accuracy: 0.8701 - val_loss: 0.3654 - val_accuracy: 0.8383\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8699\n",
      "Epoch 00063: val_loss improved from 0.36219 to 0.35423, saving model to pickled_objects/batch_size_32_lr_0.04_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2965 - accuracy: 0.8699 - val_loss: 0.3542 - val_accuracy: 0.8450\n",
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8673\n",
      "Epoch 00064: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3018 - accuracy: 0.8674 - val_loss: 0.3957 - val_accuracy: 0.8218\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.8715\n",
      "Epoch 00065: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2898 - accuracy: 0.8712 - val_loss: 0.3920 - val_accuracy: 0.8295\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8718\n",
      "Epoch 00066: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2950 - accuracy: 0.8716 - val_loss: 0.3572 - val_accuracy: 0.8433\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8760\n",
      "Epoch 00067: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2862 - accuracy: 0.8761 - val_loss: 0.3857 - val_accuracy: 0.8304\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8762\n",
      "Epoch 00068: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2858 - accuracy: 0.8762 - val_loss: 0.3857 - val_accuracy: 0.8274\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8768\n",
      "Epoch 00069: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2854 - accuracy: 0.8768 - val_loss: 0.3861 - val_accuracy: 0.8255\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8781\n",
      "Epoch 00070: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2801 - accuracy: 0.8782 - val_loss: 0.3708 - val_accuracy: 0.8332\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8809\n",
      "Epoch 00071: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2784 - accuracy: 0.8809 - val_loss: 0.3840 - val_accuracy: 0.8323\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2760 - accuracy: 0.8800\n",
      "Epoch 00072: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2758 - accuracy: 0.8801 - val_loss: 0.3665 - val_accuracy: 0.8409\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.8813\n",
      "Epoch 00073: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2742 - accuracy: 0.8814 - val_loss: 0.3677 - val_accuracy: 0.8381\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.8818\n",
      "Epoch 00074: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2724 - accuracy: 0.8817 - val_loss: 0.3707 - val_accuracy: 0.8353\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.8841\n",
      "Epoch 00075: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2673 - accuracy: 0.8843 - val_loss: 0.3780 - val_accuracy: 0.8360\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.8839\n",
      "Epoch 00076: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2690 - accuracy: 0.8838 - val_loss: 0.3874 - val_accuracy: 0.8246\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.8843\n",
      "Epoch 00077: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2646 - accuracy: 0.8843 - val_loss: 0.3601 - val_accuracy: 0.8418\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.8870\n",
      "Epoch 00078: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2576 - accuracy: 0.8870 - val_loss: 0.3759 - val_accuracy: 0.8338\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2616 - accuracy: 0.8859\n",
      "Epoch 00079: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2616 - accuracy: 0.8859 - val_loss: 0.3610 - val_accuracy: 0.8435\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2630 - accuracy: 0.8877\n",
      "Epoch 00080: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2629 - accuracy: 0.8877 - val_loss: 0.3701 - val_accuracy: 0.8371\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.8921\n",
      "Epoch 00081: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2520 - accuracy: 0.8922 - val_loss: 0.3637 - val_accuracy: 0.8386\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2563 - accuracy: 0.8932\n",
      "Epoch 00082: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2560 - accuracy: 0.8933 - val_loss: 0.3715 - val_accuracy: 0.8368\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2475 - accuracy: 0.8964\n",
      "Epoch 00083: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2472 - accuracy: 0.8965 - val_loss: 0.3657 - val_accuracy: 0.8414\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.8944\n",
      "Epoch 00084: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2463 - accuracy: 0.8944 - val_loss: 0.3761 - val_accuracy: 0.8317\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.8943\n",
      "Epoch 00085: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2445 - accuracy: 0.8941 - val_loss: 0.3732 - val_accuracy: 0.8349\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.8978\n",
      "Epoch 00086: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2429 - accuracy: 0.8979 - val_loss: 0.3764 - val_accuracy: 0.8289\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.8988\n",
      "Epoch 00087: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2402 - accuracy: 0.8988 - val_loss: 0.3805 - val_accuracy: 0.8366\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.8980\n",
      "Epoch 00088: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2402 - accuracy: 0.8979 - val_loss: 0.3778 - val_accuracy: 0.8334\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9041\n",
      "Epoch 00089: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2286 - accuracy: 0.9041 - val_loss: 0.4063 - val_accuracy: 0.8285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9019\n",
      "Epoch 00090: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2309 - accuracy: 0.9019 - val_loss: 0.3799 - val_accuracy: 0.8347\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9041\n",
      "Epoch 00091: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2307 - accuracy: 0.9040 - val_loss: 0.3891 - val_accuracy: 0.8315\n",
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2274 - accuracy: 0.9053\n",
      "Epoch 00092: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2275 - accuracy: 0.9053 - val_loss: 0.3745 - val_accuracy: 0.8407\n",
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9043\n",
      "Epoch 00093: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2265 - accuracy: 0.9043 - val_loss: 0.3751 - val_accuracy: 0.8403\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9047\n",
      "Epoch 00094: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2276 - accuracy: 0.9048 - val_loss: 0.3735 - val_accuracy: 0.8377\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9086\n",
      "Epoch 00095: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2203 - accuracy: 0.9085 - val_loss: 0.3789 - val_accuracy: 0.8383\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9074\n",
      "Epoch 00096: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2193 - accuracy: 0.9075 - val_loss: 0.3973 - val_accuracy: 0.8274\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9093\n",
      "Epoch 00097: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2188 - accuracy: 0.9092 - val_loss: 0.3673 - val_accuracy: 0.8422\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.9105\n",
      "Epoch 00098: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2157 - accuracy: 0.9105 - val_loss: 0.3749 - val_accuracy: 0.8439\n",
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9131\n",
      "Epoch 00099: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2089 - accuracy: 0.9130 - val_loss: 0.3787 - val_accuracy: 0.8390\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2169 - accuracy: 0.9111\n",
      "Epoch 00100: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2167 - accuracy: 0.9112 - val_loss: 0.3829 - val_accuracy: 0.8390\n",
      "Epoch 101/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9120\n",
      "Epoch 00101: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2145 - accuracy: 0.9120 - val_loss: 0.3915 - val_accuracy: 0.8416\n",
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2130 - accuracy: 0.9125\n",
      "Epoch 00102: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2130 - accuracy: 0.9126 - val_loss: 0.3839 - val_accuracy: 0.8364\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2057 - accuracy: 0.9123\n",
      "Epoch 00103: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2061 - accuracy: 0.9124 - val_loss: 0.3878 - val_accuracy: 0.8371\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9142\n",
      "Epoch 00104: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2067 - accuracy: 0.9141 - val_loss: 0.3724 - val_accuracy: 0.8457\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9095\n",
      "Epoch 00105: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2121 - accuracy: 0.9096 - val_loss: 0.3758 - val_accuracy: 0.8368\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9173\n",
      "Epoch 00106: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2031 - accuracy: 0.9174 - val_loss: 0.3827 - val_accuracy: 0.8343\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9170\n",
      "Epoch 00107: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1977 - accuracy: 0.9171 - val_loss: 0.3908 - val_accuracy: 0.8317\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.9173\n",
      "Epoch 00108: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2014 - accuracy: 0.9174 - val_loss: 0.3887 - val_accuracy: 0.8433\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1983 - accuracy: 0.9176\n",
      "Epoch 00109: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1983 - accuracy: 0.9176 - val_loss: 0.3955 - val_accuracy: 0.8368\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9177\n",
      "Epoch 00110: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2017 - accuracy: 0.9176 - val_loss: 0.3910 - val_accuracy: 0.8280\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9200\n",
      "Epoch 00111: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1972 - accuracy: 0.9200 - val_loss: 0.3825 - val_accuracy: 0.8383\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9213\n",
      "Epoch 00112: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1930 - accuracy: 0.9214 - val_loss: 0.3888 - val_accuracy: 0.8371\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9206\n",
      "Epoch 00113: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1925 - accuracy: 0.9205 - val_loss: 0.4277 - val_accuracy: 0.8190\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9213\n",
      "Epoch 00114: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1939 - accuracy: 0.9214 - val_loss: 0.3891 - val_accuracy: 0.8407\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9240\n",
      "Epoch 00115: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1897 - accuracy: 0.9241 - val_loss: 0.4043 - val_accuracy: 0.8340\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9228\n",
      "Epoch 00116: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1902 - accuracy: 0.9229 - val_loss: 0.3953 - val_accuracy: 0.8444\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9254\n",
      "Epoch 00117: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1868 - accuracy: 0.9254 - val_loss: 0.4204 - val_accuracy: 0.8302\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9242\n",
      "Epoch 00118: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1861 - accuracy: 0.9243 - val_loss: 0.3912 - val_accuracy: 0.8403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9240\n",
      "Epoch 00119: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1840 - accuracy: 0.9238 - val_loss: 0.4064 - val_accuracy: 0.8330\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9282\n",
      "Epoch 00120: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1786 - accuracy: 0.9281 - val_loss: 0.4005 - val_accuracy: 0.8426\n",
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9302\n",
      "Epoch 00121: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1768 - accuracy: 0.9302 - val_loss: 0.4004 - val_accuracy: 0.8332\n",
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9284\n",
      "Epoch 00122: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1757 - accuracy: 0.9285 - val_loss: 0.4033 - val_accuracy: 0.8366\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1760 - accuracy: 0.9300\n",
      "Epoch 00123: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1761 - accuracy: 0.9300 - val_loss: 0.4067 - val_accuracy: 0.8366\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9297\n",
      "Epoch 00124: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1753 - accuracy: 0.9297 - val_loss: 0.4027 - val_accuracy: 0.8261\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9284\n",
      "Epoch 00125: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1777 - accuracy: 0.9284 - val_loss: 0.4019 - val_accuracy: 0.8306\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9266\n",
      "Epoch 00126: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1763 - accuracy: 0.9266 - val_loss: 0.4089 - val_accuracy: 0.8353\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.9306\n",
      "Epoch 00127: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1752 - accuracy: 0.9307 - val_loss: 0.4028 - val_accuracy: 0.8343\n",
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1749 - accuracy: 0.9292\n",
      "Epoch 00128: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1749 - accuracy: 0.9292 - val_loss: 0.4115 - val_accuracy: 0.8315\n",
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9317\n",
      "Epoch 00129: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1693 - accuracy: 0.9316 - val_loss: 0.4579 - val_accuracy: 0.8231\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9319\n",
      "Epoch 00130: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1710 - accuracy: 0.9320 - val_loss: 0.4014 - val_accuracy: 0.8353\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9310\n",
      "Epoch 00131: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1709 - accuracy: 0.9309 - val_loss: 0.4230 - val_accuracy: 0.8291\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.9300\n",
      "Epoch 00132: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1721 - accuracy: 0.9300 - val_loss: 0.4043 - val_accuracy: 0.8368\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.9345\n",
      "Epoch 00133: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1608 - accuracy: 0.9346 - val_loss: 0.4248 - val_accuracy: 0.8289\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9340\n",
      "Epoch 00134: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1614 - accuracy: 0.9341 - val_loss: 0.4263 - val_accuracy: 0.8252\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.9328\n",
      "Epoch 00135: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1652 - accuracy: 0.9328 - val_loss: 0.4217 - val_accuracy: 0.8332\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9331\n",
      "Epoch 00136: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1662 - accuracy: 0.9330 - val_loss: 0.4177 - val_accuracy: 0.8276\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9354\n",
      "Epoch 00137: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1599 - accuracy: 0.9352 - val_loss: 0.4341 - val_accuracy: 0.8227\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9341\n",
      "Epoch 00138: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1630 - accuracy: 0.9341 - val_loss: 0.4030 - val_accuracy: 0.8411\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1610 - accuracy: 0.9346\n",
      "Epoch 00139: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1607 - accuracy: 0.9347 - val_loss: 0.4029 - val_accuracy: 0.8405\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1550 - accuracy: 0.9380\n",
      "Epoch 00140: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1551 - accuracy: 0.9379 - val_loss: 0.4203 - val_accuracy: 0.8308\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9366\n",
      "Epoch 00141: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1598 - accuracy: 0.9366 - val_loss: 0.4039 - val_accuracy: 0.8435\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9373\n",
      "Epoch 00142: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1562 - accuracy: 0.9373 - val_loss: 0.4166 - val_accuracy: 0.8343\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9380\n",
      "Epoch 00143: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1567 - accuracy: 0.9380 - val_loss: 0.4037 - val_accuracy: 0.8330\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9388\n",
      "Epoch 00144: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1520 - accuracy: 0.9388 - val_loss: 0.4047 - val_accuracy: 0.8366\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9416\n",
      "Epoch 00145: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1501 - accuracy: 0.9416 - val_loss: 0.4144 - val_accuracy: 0.8381\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9400\n",
      "Epoch 00146: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1527 - accuracy: 0.9401 - val_loss: 0.4022 - val_accuracy: 0.8375\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9441\n",
      "Epoch 00147: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1433 - accuracy: 0.9440 - val_loss: 0.4280 - val_accuracy: 0.8328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9387\n",
      "Epoch 00148: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1533 - accuracy: 0.9389 - val_loss: 0.4054 - val_accuracy: 0.8358\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9418\n",
      "Epoch 00149: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1464 - accuracy: 0.9417 - val_loss: 0.4142 - val_accuracy: 0.8392\n",
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.9413\n",
      "Epoch 00150: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1433 - accuracy: 0.9413 - val_loss: 0.4084 - val_accuracy: 0.8366\n",
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9404\n",
      "Epoch 00151: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1506 - accuracy: 0.9405 - val_loss: 0.4172 - val_accuracy: 0.8401\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9423\n",
      "Epoch 00152: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1480 - accuracy: 0.9422 - val_loss: 0.4283 - val_accuracy: 0.8338\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9421\n",
      "Epoch 00153: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1487 - accuracy: 0.9422 - val_loss: 0.4033 - val_accuracy: 0.8362\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.9435\n",
      "Epoch 00154: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1421 - accuracy: 0.9435 - val_loss: 0.4180 - val_accuracy: 0.8356\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9434\n",
      "Epoch 00155: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1437 - accuracy: 0.9434 - val_loss: 0.4156 - val_accuracy: 0.8334\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9420\n",
      "Epoch 00156: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1465 - accuracy: 0.9420 - val_loss: 0.4178 - val_accuracy: 0.8362\n",
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9443\n",
      "Epoch 00157: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1416 - accuracy: 0.9442 - val_loss: 0.4066 - val_accuracy: 0.8366\n",
      "Epoch 158/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9447\n",
      "Epoch 00158: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1388 - accuracy: 0.9447 - val_loss: 0.4078 - val_accuracy: 0.8383\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9448\n",
      "Epoch 00159: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1408 - accuracy: 0.9449 - val_loss: 0.4212 - val_accuracy: 0.8328\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9473\n",
      "Epoch 00160: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1362 - accuracy: 0.9473 - val_loss: 0.4478 - val_accuracy: 0.8229\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1442 - accuracy: 0.9440\n",
      "Epoch 00161: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1440 - accuracy: 0.9441 - val_loss: 0.4182 - val_accuracy: 0.8349\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9453\n",
      "Epoch 00162: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1401 - accuracy: 0.9451 - val_loss: 0.4192 - val_accuracy: 0.8325\n",
      "Epoch 163/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9453\n",
      "Epoch 00163: val_loss did not improve from 0.35423\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1362 - accuracy: 0.9452 - val_loss: 0.4259 - val_accuracy: 0.8304\n",
      "Epoch 00163: early stopping\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 13s 22ms/step - loss: 0.6876 - accuracy: 0.5408\n",
      "Epoch 00001: val_loss improved from inf to 0.66987, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 14s 24ms/step - loss: 0.6876 - accuracy: 0.5408 - val_loss: 0.6699 - val_accuracy: 0.6199\n",
      "Epoch 2/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5443\n",
      "Epoch 00002: val_loss improved from 0.66987 to 0.66647, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6880 - accuracy: 0.5443 - val_loss: 0.6665 - val_accuracy: 0.6331\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6637 - accuracy: 0.5983\n",
      "Epoch 00003: val_loss improved from 0.66647 to 0.66332, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6636 - accuracy: 0.5982 - val_loss: 0.6633 - val_accuracy: 0.5907\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6377 - accuracy: 0.6416\n",
      "Epoch 00004: val_loss improved from 0.66332 to 0.63913, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6376 - accuracy: 0.6418 - val_loss: 0.6391 - val_accuracy: 0.6367\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.6634\n",
      "Epoch 00005: val_loss improved from 0.63913 to 0.62557, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6172 - accuracy: 0.6634 - val_loss: 0.6256 - val_accuracy: 0.6496\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.6816\n",
      "Epoch 00006: val_loss improved from 0.62557 to 0.56956, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5978 - accuracy: 0.6814 - val_loss: 0.5696 - val_accuracy: 0.7120\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5701 - accuracy: 0.7087\n",
      "Epoch 00007: val_loss improved from 0.56956 to 0.53659, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5703 - accuracy: 0.7083 - val_loss: 0.5366 - val_accuracy: 0.7358\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7252\n",
      "Epoch 00008: val_loss improved from 0.53659 to 0.51412, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5412 - accuracy: 0.7248 - val_loss: 0.5141 - val_accuracy: 0.7513\n",
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5240 - accuracy: 0.7392\n",
      "Epoch 00009: val_loss improved from 0.51412 to 0.50223, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5242 - accuracy: 0.7389 - val_loss: 0.5022 - val_accuracy: 0.7534\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7565\n",
      "Epoch 00010: val_loss improved from 0.50223 to 0.48551, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5046 - accuracy: 0.7562 - val_loss: 0.4855 - val_accuracy: 0.7762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.7586\n",
      "Epoch 00011: val_loss improved from 0.48551 to 0.47884, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4928 - accuracy: 0.7584 - val_loss: 0.4788 - val_accuracy: 0.7715\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7658\n",
      "Epoch 00012: val_loss did not improve from 0.47884\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4823 - accuracy: 0.7657 - val_loss: 0.4900 - val_accuracy: 0.7584\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7696\n",
      "Epoch 00013: val_loss improved from 0.47884 to 0.46636, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4735 - accuracy: 0.7693 - val_loss: 0.4664 - val_accuracy: 0.7760\n",
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4657 - accuracy: 0.7757\n",
      "Epoch 00014: val_loss improved from 0.46636 to 0.45115, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4657 - accuracy: 0.7757 - val_loss: 0.4511 - val_accuracy: 0.7863\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4558 - accuracy: 0.7835\n",
      "Epoch 00015: val_loss did not improve from 0.45115\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4562 - accuracy: 0.7830 - val_loss: 0.4807 - val_accuracy: 0.7663\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4502 - accuracy: 0.7872\n",
      "Epoch 00016: val_loss did not improve from 0.45115\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4502 - accuracy: 0.7872 - val_loss: 0.4563 - val_accuracy: 0.7779\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7901\n",
      "Epoch 00017: val_loss did not improve from 0.45115\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4447 - accuracy: 0.7898 - val_loss: 0.4546 - val_accuracy: 0.7855\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.7909\n",
      "Epoch 00018: val_loss improved from 0.45115 to 0.44375, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4400 - accuracy: 0.7906 - val_loss: 0.4437 - val_accuracy: 0.7923\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.7975\n",
      "Epoch 00019: val_loss improved from 0.44375 to 0.43624, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4342 - accuracy: 0.7975 - val_loss: 0.4362 - val_accuracy: 0.7896\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8019\n",
      "Epoch 00020: val_loss improved from 0.43624 to 0.42504, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4263 - accuracy: 0.8018 - val_loss: 0.4250 - val_accuracy: 0.7997\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8038\n",
      "Epoch 00021: val_loss did not improve from 0.42504\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4234 - accuracy: 0.8036 - val_loss: 0.4288 - val_accuracy: 0.7979\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8046\n",
      "Epoch 00022: val_loss improved from 0.42504 to 0.41719, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4164 - accuracy: 0.8045 - val_loss: 0.4172 - val_accuracy: 0.8052\n",
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8103\n",
      "Epoch 00023: val_loss improved from 0.41719 to 0.41565, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4091 - accuracy: 0.8101 - val_loss: 0.4157 - val_accuracy: 0.8029\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8128\n",
      "Epoch 00024: val_loss improved from 0.41565 to 0.41149, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4047 - accuracy: 0.8127 - val_loss: 0.4115 - val_accuracy: 0.8085\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8141\n",
      "Epoch 00025: val_loss did not improve from 0.41149\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4016 - accuracy: 0.8139 - val_loss: 0.4127 - val_accuracy: 0.8078\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3996 - accuracy: 0.8148\n",
      "Epoch 00026: val_loss improved from 0.41149 to 0.40322, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3997 - accuracy: 0.8147 - val_loss: 0.4032 - val_accuracy: 0.8102\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8177\n",
      "Epoch 00027: val_loss improved from 0.40322 to 0.40095, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3989 - accuracy: 0.8175 - val_loss: 0.4009 - val_accuracy: 0.8162\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8208\n",
      "Epoch 00028: val_loss did not improve from 0.40095\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3912 - accuracy: 0.8204 - val_loss: 0.4161 - val_accuracy: 0.8046\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8222\n",
      "Epoch 00029: val_loss improved from 0.40095 to 0.39748, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3862 - accuracy: 0.8219 - val_loss: 0.3975 - val_accuracy: 0.8166\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8259\n",
      "Epoch 00030: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3827 - accuracy: 0.8257 - val_loss: 0.4009 - val_accuracy: 0.8147\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.8292\n",
      "Epoch 00031: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3752 - accuracy: 0.8290 - val_loss: 0.4032 - val_accuracy: 0.8117\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8305\n",
      "Epoch 00032: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3766 - accuracy: 0.8304 - val_loss: 0.4064 - val_accuracy: 0.8033\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3682 - accuracy: 0.8354\n",
      "Epoch 00033: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3688 - accuracy: 0.8351 - val_loss: 0.4103 - val_accuracy: 0.8093\n",
      "Epoch 34/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8313\n",
      "Epoch 00034: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3683 - accuracy: 0.8311 - val_loss: 0.4539 - val_accuracy: 0.7945\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8383\n",
      "Epoch 00035: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3579 - accuracy: 0.8382 - val_loss: 0.4266 - val_accuracy: 0.7962\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3634 - accuracy: 0.8364\n",
      "Epoch 00036: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3634 - accuracy: 0.8362 - val_loss: 0.4079 - val_accuracy: 0.8063\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8406\n",
      "Epoch 00037: val_loss did not improve from 0.39748\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3577 - accuracy: 0.8406 - val_loss: 0.4150 - val_accuracy: 0.8048\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8409\n",
      "Epoch 00038: val_loss improved from 0.39748 to 0.38619, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3551 - accuracy: 0.8407 - val_loss: 0.3862 - val_accuracy: 0.8280\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8422\n",
      "Epoch 00039: val_loss did not improve from 0.38619\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3527 - accuracy: 0.8422 - val_loss: 0.3994 - val_accuracy: 0.8162\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8434\n",
      "Epoch 00040: val_loss did not improve from 0.38619\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3448 - accuracy: 0.8432 - val_loss: 0.3937 - val_accuracy: 0.8132\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.8461\n",
      "Epoch 00041: val_loss did not improve from 0.38619\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3442 - accuracy: 0.8459 - val_loss: 0.3978 - val_accuracy: 0.8192\n",
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8474\n",
      "Epoch 00042: val_loss improved from 0.38619 to 0.38316, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3417 - accuracy: 0.8471 - val_loss: 0.3832 - val_accuracy: 0.8255\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8487\n",
      "Epoch 00043: val_loss did not improve from 0.38316\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3349 - accuracy: 0.8486 - val_loss: 0.4019 - val_accuracy: 0.8209\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.8504\n",
      "Epoch 00044: val_loss improved from 0.38316 to 0.37560, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3341 - accuracy: 0.8503 - val_loss: 0.3756 - val_accuracy: 0.8248\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8520\n",
      "Epoch 00045: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3343 - accuracy: 0.8521 - val_loss: 0.3910 - val_accuracy: 0.8239\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.8532\n",
      "Epoch 00046: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3266 - accuracy: 0.8529 - val_loss: 0.4109 - val_accuracy: 0.8083\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8560\n",
      "Epoch 00047: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3245 - accuracy: 0.8559 - val_loss: 0.3910 - val_accuracy: 0.8160\n",
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8596\n",
      "Epoch 00048: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3225 - accuracy: 0.8594 - val_loss: 0.3891 - val_accuracy: 0.8248\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8548\n",
      "Epoch 00049: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3213 - accuracy: 0.8549 - val_loss: 0.3936 - val_accuracy: 0.8212\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8620\n",
      "Epoch 00050: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3171 - accuracy: 0.8621 - val_loss: 0.3812 - val_accuracy: 0.8246\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8648\n",
      "Epoch 00051: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3102 - accuracy: 0.8643 - val_loss: 0.3864 - val_accuracy: 0.8231\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8670\n",
      "Epoch 00052: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3071 - accuracy: 0.8670 - val_loss: 0.4034 - val_accuracy: 0.8173\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8678\n",
      "Epoch 00053: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3029 - accuracy: 0.8678 - val_loss: 0.4165 - val_accuracy: 0.8087\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8678\n",
      "Epoch 00054: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3029 - accuracy: 0.8678 - val_loss: 0.3843 - val_accuracy: 0.8257\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8720\n",
      "Epoch 00055: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2973 - accuracy: 0.8719 - val_loss: 0.3825 - val_accuracy: 0.8246\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8704\n",
      "Epoch 00056: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3006 - accuracy: 0.8704 - val_loss: 0.3906 - val_accuracy: 0.8205\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8710\n",
      "Epoch 00057: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2950 - accuracy: 0.8708 - val_loss: 0.3899 - val_accuracy: 0.8192\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8748\n",
      "Epoch 00058: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2902 - accuracy: 0.8748 - val_loss: 0.3943 - val_accuracy: 0.8173\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8743\n",
      "Epoch 00059: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2876 - accuracy: 0.8743 - val_loss: 0.3821 - val_accuracy: 0.8255\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.8765\n",
      "Epoch 00060: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2858 - accuracy: 0.8764 - val_loss: 0.3903 - val_accuracy: 0.8224\n",
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8745\n",
      "Epoch 00061: val_loss did not improve from 0.37560\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2864 - accuracy: 0.8744 - val_loss: 0.3906 - val_accuracy: 0.8220\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8773\n",
      "Epoch 00062: val_loss improved from 0.37560 to 0.36728, saving model to pickled_objects/batch_size_32_lr_0.08_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2843 - accuracy: 0.8773 - val_loss: 0.3673 - val_accuracy: 0.8308\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.8813\n",
      "Epoch 00063: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2832 - accuracy: 0.8811 - val_loss: 0.3759 - val_accuracy: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8849\n",
      "Epoch 00064: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2698 - accuracy: 0.8848 - val_loss: 0.3941 - val_accuracy: 0.8156\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2743 - accuracy: 0.8833\n",
      "Epoch 00065: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2744 - accuracy: 0.8833 - val_loss: 0.3806 - val_accuracy: 0.8252\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8813\n",
      "Epoch 00066: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2737 - accuracy: 0.8813 - val_loss: 0.3751 - val_accuracy: 0.8298\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.8853\n",
      "Epoch 00067: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2678 - accuracy: 0.8854 - val_loss: 0.3922 - val_accuracy: 0.8229\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.8884\n",
      "Epoch 00068: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2655 - accuracy: 0.8882 - val_loss: 0.3855 - val_accuracy: 0.8278\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8890\n",
      "Epoch 00069: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2644 - accuracy: 0.8888 - val_loss: 0.3769 - val_accuracy: 0.8302\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.8886\n",
      "Epoch 00070: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2609 - accuracy: 0.8886 - val_loss: 0.3738 - val_accuracy: 0.8356\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8886\n",
      "Epoch 00071: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2637 - accuracy: 0.8886 - val_loss: 0.4085 - val_accuracy: 0.8192\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.8918\n",
      "Epoch 00072: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2568 - accuracy: 0.8915 - val_loss: 0.3946 - val_accuracy: 0.8209\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.8895\n",
      "Epoch 00073: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2594 - accuracy: 0.8895 - val_loss: 0.3714 - val_accuracy: 0.8308\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.8915\n",
      "Epoch 00074: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2540 - accuracy: 0.8915 - val_loss: 0.3810 - val_accuracy: 0.8289\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.8918\n",
      "Epoch 00075: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2514 - accuracy: 0.8919 - val_loss: 0.3946 - val_accuracy: 0.8257\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.8958\n",
      "Epoch 00076: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2471 - accuracy: 0.8959 - val_loss: 0.3835 - val_accuracy: 0.8224\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8979\n",
      "Epoch 00077: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2435 - accuracy: 0.8980 - val_loss: 0.3742 - val_accuracy: 0.8282\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.8976\n",
      "Epoch 00078: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2438 - accuracy: 0.8976 - val_loss: 0.3922 - val_accuracy: 0.8280\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.8968\n",
      "Epoch 00079: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2450 - accuracy: 0.8968 - val_loss: 0.3826 - val_accuracy: 0.8336\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9036\n",
      "Epoch 00080: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2383 - accuracy: 0.9033 - val_loss: 0.3854 - val_accuracy: 0.8242\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.8999\n",
      "Epoch 00081: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2403 - accuracy: 0.8998 - val_loss: 0.3907 - val_accuracy: 0.8207\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.8940\n",
      "Epoch 00082: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2459 - accuracy: 0.8939 - val_loss: 0.3846 - val_accuracy: 0.8287\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9010\n",
      "Epoch 00083: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2337 - accuracy: 0.9011 - val_loss: 0.3697 - val_accuracy: 0.8356\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9073\n",
      "Epoch 00084: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2265 - accuracy: 0.9073 - val_loss: 0.3955 - val_accuracy: 0.8261\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9050\n",
      "Epoch 00085: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2303 - accuracy: 0.9048 - val_loss: 0.3883 - val_accuracy: 0.8308\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9037\n",
      "Epoch 00086: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2360 - accuracy: 0.9037 - val_loss: 0.4082 - val_accuracy: 0.8164\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9039\n",
      "Epoch 00087: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 21ms/step - loss: 0.2276 - accuracy: 0.9039 - val_loss: 0.3942 - val_accuracy: 0.8239\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2262 - accuracy: 0.9059\n",
      "Epoch 00088: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2262 - accuracy: 0.9059 - val_loss: 0.4107 - val_accuracy: 0.8175\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9096\n",
      "Epoch 00089: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2213 - accuracy: 0.9094 - val_loss: 0.3969 - val_accuracy: 0.8248\n",
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9075\n",
      "Epoch 00090: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2264 - accuracy: 0.9074 - val_loss: 0.4073 - val_accuracy: 0.8263\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2198 - accuracy: 0.9102\n",
      "Epoch 00091: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2199 - accuracy: 0.9102 - val_loss: 0.3979 - val_accuracy: 0.8190\n",
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9088\n",
      "Epoch 00092: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2162 - accuracy: 0.9089 - val_loss: 0.3800 - val_accuracy: 0.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9097\n",
      "Epoch 00093: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2165 - accuracy: 0.9098 - val_loss: 0.3954 - val_accuracy: 0.8272\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9116\n",
      "Epoch 00094: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2185 - accuracy: 0.9117 - val_loss: 0.3908 - val_accuracy: 0.8239\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9099\n",
      "Epoch 00095: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2167 - accuracy: 0.9098 - val_loss: 0.3968 - val_accuracy: 0.8261\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9101\n",
      "Epoch 00096: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2152 - accuracy: 0.9102 - val_loss: 0.3979 - val_accuracy: 0.8267\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9094\n",
      "Epoch 00097: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2163 - accuracy: 0.9092 - val_loss: 0.3866 - val_accuracy: 0.8237\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9159\n",
      "Epoch 00098: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2094 - accuracy: 0.9159 - val_loss: 0.3978 - val_accuracy: 0.8259\n",
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9154\n",
      "Epoch 00099: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2113 - accuracy: 0.9153 - val_loss: 0.3965 - val_accuracy: 0.8248\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9184\n",
      "Epoch 00100: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2031 - accuracy: 0.9183 - val_loss: 0.4120 - val_accuracy: 0.8199\n",
      "Epoch 101/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9161\n",
      "Epoch 00101: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2080 - accuracy: 0.9161 - val_loss: 0.4091 - val_accuracy: 0.8188\n",
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9126\n",
      "Epoch 00102: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2096 - accuracy: 0.9125 - val_loss: 0.3972 - val_accuracy: 0.8285\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9152\n",
      "Epoch 00103: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2058 - accuracy: 0.9152 - val_loss: 0.4025 - val_accuracy: 0.8257\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9185\n",
      "Epoch 00104: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2022 - accuracy: 0.9183 - val_loss: 0.3959 - val_accuracy: 0.8175\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9184\n",
      "Epoch 00105: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2028 - accuracy: 0.9184 - val_loss: 0.3864 - val_accuracy: 0.8295\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9173\n",
      "Epoch 00106: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2025 - accuracy: 0.9174 - val_loss: 0.4091 - val_accuracy: 0.8162\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9159\n",
      "Epoch 00107: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2051 - accuracy: 0.9160 - val_loss: 0.4134 - val_accuracy: 0.8207\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9168\n",
      "Epoch 00108: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2021 - accuracy: 0.9168 - val_loss: 0.3879 - val_accuracy: 0.8332\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9197\n",
      "Epoch 00109: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1962 - accuracy: 0.9197 - val_loss: 0.4118 - val_accuracy: 0.8212\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9235\n",
      "Epoch 00110: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1944 - accuracy: 0.9235 - val_loss: 0.4014 - val_accuracy: 0.8304\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9183\n",
      "Epoch 00111: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2008 - accuracy: 0.9181 - val_loss: 0.4106 - val_accuracy: 0.8229\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9241\n",
      "Epoch 00112: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1910 - accuracy: 0.9239 - val_loss: 0.5055 - val_accuracy: 0.7685\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1922 - accuracy: 0.9213\n",
      "Epoch 00113: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1920 - accuracy: 0.9213 - val_loss: 0.3961 - val_accuracy: 0.8263\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9235\n",
      "Epoch 00114: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1900 - accuracy: 0.9234 - val_loss: 0.4241 - val_accuracy: 0.8276\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9267\n",
      "Epoch 00115: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1841 - accuracy: 0.9267 - val_loss: 0.4106 - val_accuracy: 0.8250\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9249\n",
      "Epoch 00116: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1884 - accuracy: 0.9249 - val_loss: 0.4064 - val_accuracy: 0.8212\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9233\n",
      "Epoch 00117: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1905 - accuracy: 0.9232 - val_loss: 0.4375 - val_accuracy: 0.8110\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9221\n",
      "Epoch 00118: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1878 - accuracy: 0.9220 - val_loss: 0.4232 - val_accuracy: 0.8175\n",
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9262\n",
      "Epoch 00119: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1836 - accuracy: 0.9262 - val_loss: 0.3998 - val_accuracy: 0.8330\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9231\n",
      "Epoch 00120: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1890 - accuracy: 0.9232 - val_loss: 0.4036 - val_accuracy: 0.8278\n",
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9286\n",
      "Epoch 00121: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1811 - accuracy: 0.9287 - val_loss: 0.4276 - val_accuracy: 0.8199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9259\n",
      "Epoch 00122: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1841 - accuracy: 0.9260 - val_loss: 0.4421 - val_accuracy: 0.8175\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9247\n",
      "Epoch 00123: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1872 - accuracy: 0.9247 - val_loss: 0.4306 - val_accuracy: 0.8123\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9286\n",
      "Epoch 00124: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1811 - accuracy: 0.9285 - val_loss: 0.4631 - val_accuracy: 0.8044\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9266\n",
      "Epoch 00125: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1848 - accuracy: 0.9266 - val_loss: 0.4295 - val_accuracy: 0.8201\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9272\n",
      "Epoch 00126: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1813 - accuracy: 0.9269 - val_loss: 0.4315 - val_accuracy: 0.8160\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9270\n",
      "Epoch 00127: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1826 - accuracy: 0.9272 - val_loss: 0.4173 - val_accuracy: 0.8244\n",
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9292\n",
      "Epoch 00128: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1785 - accuracy: 0.9292 - val_loss: 0.4145 - val_accuracy: 0.8173\n",
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9270\n",
      "Epoch 00129: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1813 - accuracy: 0.9271 - val_loss: 0.4610 - val_accuracy: 0.8029\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9263\n",
      "Epoch 00130: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1790 - accuracy: 0.9263 - val_loss: 0.4451 - val_accuracy: 0.8057\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9298\n",
      "Epoch 00131: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1759 - accuracy: 0.9299 - val_loss: 0.4481 - val_accuracy: 0.8196\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9302\n",
      "Epoch 00132: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1758 - accuracy: 0.9302 - val_loss: 0.4189 - val_accuracy: 0.8177\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1781 - accuracy: 0.9291\n",
      "Epoch 00133: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1783 - accuracy: 0.9291 - val_loss: 0.4110 - val_accuracy: 0.8153\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9323\n",
      "Epoch 00134: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1747 - accuracy: 0.9324 - val_loss: 0.4053 - val_accuracy: 0.8196\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9356\n",
      "Epoch 00135: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1671 - accuracy: 0.9354 - val_loss: 0.4336 - val_accuracy: 0.8188\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1799 - accuracy: 0.9290\n",
      "Epoch 00136: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1798 - accuracy: 0.9291 - val_loss: 0.4336 - val_accuracy: 0.8091\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9342\n",
      "Epoch 00137: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1659 - accuracy: 0.9341 - val_loss: 0.4219 - val_accuracy: 0.8203\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9309\n",
      "Epoch 00138: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1724 - accuracy: 0.9306 - val_loss: 0.4251 - val_accuracy: 0.8194\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9335\n",
      "Epoch 00139: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1689 - accuracy: 0.9336 - val_loss: 0.4471 - val_accuracy: 0.8227\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9297\n",
      "Epoch 00140: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1727 - accuracy: 0.9298 - val_loss: 0.4191 - val_accuracy: 0.8285\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9283\n",
      "Epoch 00141: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1769 - accuracy: 0.9283 - val_loss: 0.4352 - val_accuracy: 0.8175\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9331\n",
      "Epoch 00142: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1674 - accuracy: 0.9332 - val_loss: 0.4247 - val_accuracy: 0.8291\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9316\n",
      "Epoch 00143: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1716 - accuracy: 0.9317 - val_loss: 0.4419 - val_accuracy: 0.8085\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9355\n",
      "Epoch 00144: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1660 - accuracy: 0.9355 - val_loss: 0.4510 - val_accuracy: 0.8201\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9363\n",
      "Epoch 00145: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1614 - accuracy: 0.9363 - val_loss: 0.4948 - val_accuracy: 0.8179\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9328\n",
      "Epoch 00146: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1656 - accuracy: 0.9327 - val_loss: 0.4501 - val_accuracy: 0.8171\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9336\n",
      "Epoch 00147: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1648 - accuracy: 0.9338 - val_loss: 0.4892 - val_accuracy: 0.8138\n",
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9357\n",
      "Epoch 00148: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.1674 - accuracy: 0.9356 - val_loss: 0.4417 - val_accuracy: 0.8145\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1641 - accuracy: 0.9339\n",
      "Epoch 00149: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1638 - accuracy: 0.9341 - val_loss: 0.4230 - val_accuracy: 0.8218\n",
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9367\n",
      "Epoch 00150: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1648 - accuracy: 0.9369 - val_loss: 0.4275 - val_accuracy: 0.8257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9384\n",
      "Epoch 00151: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1613 - accuracy: 0.9385 - val_loss: 0.4580 - val_accuracy: 0.8186\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9382\n",
      "Epoch 00152: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1588 - accuracy: 0.9382 - val_loss: 0.4222 - val_accuracy: 0.8169\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9384\n",
      "Epoch 00153: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1618 - accuracy: 0.9384 - val_loss: 0.4067 - val_accuracy: 0.8188\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9386\n",
      "Epoch 00154: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1568 - accuracy: 0.9385 - val_loss: 0.4743 - val_accuracy: 0.8203\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.9381\n",
      "Epoch 00155: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1611 - accuracy: 0.9383 - val_loss: 0.4619 - val_accuracy: 0.8078\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9371\n",
      "Epoch 00156: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1623 - accuracy: 0.9371 - val_loss: 0.4220 - val_accuracy: 0.8252\n",
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9373\n",
      "Epoch 00157: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1604 - accuracy: 0.9373 - val_loss: 0.4164 - val_accuracy: 0.8224\n",
      "Epoch 158/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9388\n",
      "Epoch 00158: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1545 - accuracy: 0.9387 - val_loss: 0.4409 - val_accuracy: 0.8212\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9377\n",
      "Epoch 00159: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1597 - accuracy: 0.9376 - val_loss: 0.4325 - val_accuracy: 0.8246\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9430\n",
      "Epoch 00160: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1508 - accuracy: 0.9431 - val_loss: 0.4799 - val_accuracy: 0.8153\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9393\n",
      "Epoch 00161: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1558 - accuracy: 0.9393 - val_loss: 0.4228 - val_accuracy: 0.8295\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9370\n",
      "Epoch 00162: val_loss did not improve from 0.36728\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.1601 - accuracy: 0.9370 - val_loss: 0.4256 - val_accuracy: 0.8192\n",
      "Epoch 00162: early stopping\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 12s 21ms/step - loss: 0.6916 - accuracy: 0.5210\n",
      "Epoch 00001: val_loss improved from inf to 0.69310, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 23ms/step - loss: 0.6916 - accuracy: 0.5210 - val_loss: 0.6931 - val_accuracy: 0.5099\n",
      "Epoch 2/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5580\n",
      "Epoch 00002: val_loss improved from 0.69310 to 0.64738, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6788 - accuracy: 0.5582 - val_loss: 0.6474 - val_accuracy: 0.6324\n",
      "Epoch 3/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6376 - accuracy: 0.6426\n",
      "Epoch 00003: val_loss improved from 0.64738 to 0.61988, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6373 - accuracy: 0.6427 - val_loss: 0.6199 - val_accuracy: 0.6614\n",
      "Epoch 4/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6176 - accuracy: 0.6649\n",
      "Epoch 00004: val_loss improved from 0.61988 to 0.59830, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6177 - accuracy: 0.6649 - val_loss: 0.5983 - val_accuracy: 0.6855\n",
      "Epoch 5/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5861 - accuracy: 0.6909\n",
      "Epoch 00005: val_loss improved from 0.59830 to 0.57396, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5861 - accuracy: 0.6910 - val_loss: 0.5740 - val_accuracy: 0.6984\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7086\n",
      "Epoch 00006: val_loss improved from 0.57396 to 0.53114, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5625 - accuracy: 0.7082 - val_loss: 0.5311 - val_accuracy: 0.7384\n",
      "Epoch 7/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5449 - accuracy: 0.7266\n",
      "Epoch 00007: val_loss did not improve from 0.53114\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5451 - accuracy: 0.7262 - val_loss: 0.5622 - val_accuracy: 0.7055\n",
      "Epoch 8/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7344\n",
      "Epoch 00008: val_loss did not improve from 0.53114\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5278 - accuracy: 0.7341 - val_loss: 0.5383 - val_accuracy: 0.7264\n",
      "Epoch 9/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5155 - accuracy: 0.7409\n",
      "Epoch 00009: val_loss improved from 0.53114 to 0.50408, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5158 - accuracy: 0.7407 - val_loss: 0.5041 - val_accuracy: 0.7509\n",
      "Epoch 10/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5038 - accuracy: 0.7520\n",
      "Epoch 00010: val_loss did not improve from 0.50408\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5041 - accuracy: 0.7519 - val_loss: 0.5068 - val_accuracy: 0.7597\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4976 - accuracy: 0.7527\n",
      "Epoch 00011: val_loss did not improve from 0.50408\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4979 - accuracy: 0.7526 - val_loss: 0.5150 - val_accuracy: 0.7517\n",
      "Epoch 12/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4889 - accuracy: 0.7608\n",
      "Epoch 00012: val_loss improved from 0.50408 to 0.48062, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4892 - accuracy: 0.7605 - val_loss: 0.4806 - val_accuracy: 0.7760\n",
      "Epoch 13/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4800 - accuracy: 0.7689\n",
      "Epoch 00013: val_loss improved from 0.48062 to 0.46244, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4803 - accuracy: 0.7686 - val_loss: 0.4624 - val_accuracy: 0.7850\n",
      "Epoch 14/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.7693\n",
      "Epoch 00014: val_loss did not improve from 0.46244\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4715 - accuracy: 0.7693 - val_loss: 0.4692 - val_accuracy: 0.7810\n",
      "Epoch 15/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.7751\n",
      "Epoch 00015: val_loss did not improve from 0.46244\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4689 - accuracy: 0.7749 - val_loss: 0.4792 - val_accuracy: 0.7719\n",
      "Epoch 16/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7818\n",
      "Epoch 00016: val_loss did not improve from 0.46244\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4591 - accuracy: 0.7815 - val_loss: 0.4683 - val_accuracy: 0.7810\n",
      "Epoch 17/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4513 - accuracy: 0.7831\n",
      "Epoch 00017: val_loss did not improve from 0.46244\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4520 - accuracy: 0.7828 - val_loss: 0.5138 - val_accuracy: 0.7311\n",
      "Epoch 18/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7881\n",
      "Epoch 00018: val_loss improved from 0.46244 to 0.44980, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4506 - accuracy: 0.7880 - val_loss: 0.4498 - val_accuracy: 0.7913\n",
      "Epoch 19/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4411 - accuracy: 0.7886\n",
      "Epoch 00019: val_loss improved from 0.44980 to 0.43762, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4412 - accuracy: 0.7885 - val_loss: 0.4376 - val_accuracy: 0.7926\n",
      "Epoch 20/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4334 - accuracy: 0.7936\n",
      "Epoch 00020: val_loss did not improve from 0.43762\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4336 - accuracy: 0.7936 - val_loss: 0.4539 - val_accuracy: 0.7874\n",
      "Epoch 21/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.7984\n",
      "Epoch 00021: val_loss did not improve from 0.43762\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4306 - accuracy: 0.7982 - val_loss: 0.4479 - val_accuracy: 0.7876\n",
      "Epoch 22/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8027\n",
      "Epoch 00022: val_loss improved from 0.43762 to 0.42599, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4263 - accuracy: 0.8026 - val_loss: 0.4260 - val_accuracy: 0.8016\n",
      "Epoch 23/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4179 - accuracy: 0.8024\n",
      "Epoch 00023: val_loss improved from 0.42599 to 0.41952, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4182 - accuracy: 0.8023 - val_loss: 0.4195 - val_accuracy: 0.8035\n",
      "Epoch 24/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8040\n",
      "Epoch 00024: val_loss did not improve from 0.41952\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.4206 - accuracy: 0.8035 - val_loss: 0.4493 - val_accuracy: 0.7814\n",
      "Epoch 25/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8067\n",
      "Epoch 00025: val_loss did not improve from 0.41952\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4111 - accuracy: 0.8065 - val_loss: 0.4981 - val_accuracy: 0.7586\n",
      "Epoch 26/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4091 - accuracy: 0.8107\n",
      "Epoch 00026: val_loss did not improve from 0.41952\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4091 - accuracy: 0.8105 - val_loss: 0.4289 - val_accuracy: 0.7975\n",
      "Epoch 27/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8132\n",
      "Epoch 00027: val_loss did not improve from 0.41952\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4057 - accuracy: 0.8129 - val_loss: 0.4627 - val_accuracy: 0.7816\n",
      "Epoch 28/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8126\n",
      "Epoch 00028: val_loss improved from 0.41952 to 0.40367, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4080 - accuracy: 0.8125 - val_loss: 0.4037 - val_accuracy: 0.8126\n",
      "Epoch 29/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8205\n",
      "Epoch 00029: val_loss did not improve from 0.40367\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3977 - accuracy: 0.8203 - val_loss: 0.4151 - val_accuracy: 0.8110\n",
      "Epoch 30/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4027 - accuracy: 0.8135\n",
      "Epoch 00030: val_loss did not improve from 0.40367\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4031 - accuracy: 0.8134 - val_loss: 0.4311 - val_accuracy: 0.7917\n",
      "Epoch 31/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8195\n",
      "Epoch 00031: val_loss did not improve from 0.40367\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3944 - accuracy: 0.8193 - val_loss: 0.4189 - val_accuracy: 0.8057\n",
      "Epoch 32/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8163\n",
      "Epoch 00032: val_loss improved from 0.40367 to 0.40163, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3959 - accuracy: 0.8160 - val_loss: 0.4016 - val_accuracy: 0.8134\n",
      "Epoch 33/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.8208\n",
      "Epoch 00033: val_loss improved from 0.40163 to 0.38685, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3950 - accuracy: 0.8205 - val_loss: 0.3869 - val_accuracy: 0.8201\n",
      "Epoch 34/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3856 - accuracy: 0.8238\n",
      "Epoch 00034: val_loss improved from 0.38685 to 0.38000, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3859 - accuracy: 0.8235 - val_loss: 0.3800 - val_accuracy: 0.8289\n",
      "Epoch 35/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8268\n",
      "Epoch 00035: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3844 - accuracy: 0.8265 - val_loss: 0.4024 - val_accuracy: 0.8138\n",
      "Epoch 36/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8259\n",
      "Epoch 00036: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3842 - accuracy: 0.8257 - val_loss: 0.4015 - val_accuracy: 0.8098\n",
      "Epoch 37/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8198\n",
      "Epoch 00037: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3869 - accuracy: 0.8195 - val_loss: 0.4097 - val_accuracy: 0.8031\n",
      "Epoch 38/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8268\n",
      "Epoch 00038: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3801 - accuracy: 0.8266 - val_loss: 0.3874 - val_accuracy: 0.8242\n",
      "Epoch 39/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8307\n",
      "Epoch 00039: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3767 - accuracy: 0.8307 - val_loss: 0.4018 - val_accuracy: 0.8162\n",
      "Epoch 40/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8294\n",
      "Epoch 00040: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3773 - accuracy: 0.8293 - val_loss: 0.3974 - val_accuracy: 0.8201\n",
      "Epoch 41/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.8259\n",
      "Epoch 00041: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3786 - accuracy: 0.8256 - val_loss: 0.4500 - val_accuracy: 0.7926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8277\n",
      "Epoch 00042: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3766 - accuracy: 0.8276 - val_loss: 0.3829 - val_accuracy: 0.8222\n",
      "Epoch 43/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8320\n",
      "Epoch 00043: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3725 - accuracy: 0.8319 - val_loss: 0.5566 - val_accuracy: 0.7188\n",
      "Epoch 44/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8297\n",
      "Epoch 00044: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3731 - accuracy: 0.8298 - val_loss: 0.3980 - val_accuracy: 0.8098\n",
      "Epoch 45/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3760 - accuracy: 0.8329\n",
      "Epoch 00045: val_loss did not improve from 0.38000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3764 - accuracy: 0.8327 - val_loss: 0.3987 - val_accuracy: 0.8156\n",
      "Epoch 46/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8315\n",
      "Epoch 00046: val_loss improved from 0.38000 to 0.37899, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3767 - accuracy: 0.8314 - val_loss: 0.3790 - val_accuracy: 0.8265\n",
      "Epoch 47/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.8348\n",
      "Epoch 00047: val_loss did not improve from 0.37899\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3661 - accuracy: 0.8349 - val_loss: 0.3849 - val_accuracy: 0.8222\n",
      "Epoch 48/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8305\n",
      "Epoch 00048: val_loss did not improve from 0.37899\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3733 - accuracy: 0.8305 - val_loss: 0.4029 - val_accuracy: 0.8048\n",
      "Epoch 49/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8334\n",
      "Epoch 00049: val_loss improved from 0.37899 to 0.36655, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3655 - accuracy: 0.8332 - val_loss: 0.3665 - val_accuracy: 0.8304\n",
      "Epoch 50/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3618 - accuracy: 0.8394\n",
      "Epoch 00050: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3624 - accuracy: 0.8391 - val_loss: 0.4218 - val_accuracy: 0.8188\n",
      "Epoch 51/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8318\n",
      "Epoch 00051: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3674 - accuracy: 0.8317 - val_loss: 0.3980 - val_accuracy: 0.8175\n",
      "Epoch 52/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8346\n",
      "Epoch 00052: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3671 - accuracy: 0.8344 - val_loss: 0.4263 - val_accuracy: 0.7984\n",
      "Epoch 53/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.8349\n",
      "Epoch 00053: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3693 - accuracy: 0.8351 - val_loss: 0.3809 - val_accuracy: 0.8265\n",
      "Epoch 54/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8380\n",
      "Epoch 00054: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3650 - accuracy: 0.8378 - val_loss: 0.4321 - val_accuracy: 0.8134\n",
      "Epoch 55/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8400\n",
      "Epoch 00055: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3629 - accuracy: 0.8398 - val_loss: 0.4016 - val_accuracy: 0.8134\n",
      "Epoch 56/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8372\n",
      "Epoch 00056: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3694 - accuracy: 0.8371 - val_loss: 0.3899 - val_accuracy: 0.8278\n",
      "Epoch 57/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8348\n",
      "Epoch 00057: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3634 - accuracy: 0.8347 - val_loss: 0.3928 - val_accuracy: 0.8143\n",
      "Epoch 58/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8347\n",
      "Epoch 00058: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3696 - accuracy: 0.8346 - val_loss: 0.4265 - val_accuracy: 0.8012\n",
      "Epoch 59/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8407\n",
      "Epoch 00059: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3586 - accuracy: 0.8404 - val_loss: 0.4203 - val_accuracy: 0.8123\n",
      "Epoch 60/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8386\n",
      "Epoch 00060: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3651 - accuracy: 0.8384 - val_loss: 0.4056 - val_accuracy: 0.8113\n",
      "Epoch 61/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8375\n",
      "Epoch 00061: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3663 - accuracy: 0.8376 - val_loss: 0.4080 - val_accuracy: 0.8121\n",
      "Epoch 62/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8374\n",
      "Epoch 00062: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3633 - accuracy: 0.8372 - val_loss: 0.3978 - val_accuracy: 0.8087\n",
      "Epoch 63/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8371\n",
      "Epoch 00063: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3627 - accuracy: 0.8369 - val_loss: 0.4098 - val_accuracy: 0.8070\n",
      "Epoch 64/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3636 - accuracy: 0.8356\n",
      "Epoch 00064: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3640 - accuracy: 0.8354 - val_loss: 0.3988 - val_accuracy: 0.8233\n",
      "Epoch 65/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8375\n",
      "Epoch 00065: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3620 - accuracy: 0.8374 - val_loss: 0.3765 - val_accuracy: 0.8291\n",
      "Epoch 66/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8414\n",
      "Epoch 00066: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3595 - accuracy: 0.8414 - val_loss: 0.4143 - val_accuracy: 0.8078\n",
      "Epoch 67/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3603 - accuracy: 0.8423\n",
      "Epoch 00067: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3603 - accuracy: 0.8424 - val_loss: 0.3786 - val_accuracy: 0.8235\n",
      "Epoch 68/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8392\n",
      "Epoch 00068: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3594 - accuracy: 0.8390 - val_loss: 0.3822 - val_accuracy: 0.8291\n",
      "Epoch 69/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.8432\n",
      "Epoch 00069: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3551 - accuracy: 0.8430 - val_loss: 0.4521 - val_accuracy: 0.7754\n",
      "Epoch 70/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8412\n",
      "Epoch 00070: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3578 - accuracy: 0.8413 - val_loss: 0.3925 - val_accuracy: 0.8224\n",
      "Epoch 71/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8427\n",
      "Epoch 00071: val_loss did not improve from 0.36655\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3625 - accuracy: 0.8425 - val_loss: 0.4225 - val_accuracy: 0.8087\n",
      "Epoch 72/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8359\n",
      "Epoch 00072: val_loss improved from 0.36655 to 0.36385, saving model to pickled_objects/batch_size_32_lr_0.16_best_weights_trial_4.h5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3698 - accuracy: 0.8361 - val_loss: 0.3639 - val_accuracy: 0.8313\n",
      "Epoch 73/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8404\n",
      "Epoch 00073: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3592 - accuracy: 0.8403 - val_loss: 0.4001 - val_accuracy: 0.8184\n",
      "Epoch 74/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8359\n",
      "Epoch 00074: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3688 - accuracy: 0.8358 - val_loss: 0.4278 - val_accuracy: 0.8037\n",
      "Epoch 75/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8387\n",
      "Epoch 00075: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3636 - accuracy: 0.8387 - val_loss: 0.4341 - val_accuracy: 0.8083\n",
      "Epoch 76/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3642 - accuracy: 0.8376\n",
      "Epoch 00076: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3644 - accuracy: 0.8375 - val_loss: 0.3992 - val_accuracy: 0.8231\n",
      "Epoch 77/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.8328\n",
      "Epoch 00077: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3701 - accuracy: 0.8327 - val_loss: 0.4143 - val_accuracy: 0.8175\n",
      "Epoch 78/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8398\n",
      "Epoch 00078: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3621 - accuracy: 0.8394 - val_loss: 0.4338 - val_accuracy: 0.8031\n",
      "Epoch 79/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8425\n",
      "Epoch 00079: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3589 - accuracy: 0.8426 - val_loss: 0.4209 - val_accuracy: 0.8113\n",
      "Epoch 80/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8401\n",
      "Epoch 00080: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3558 - accuracy: 0.8400 - val_loss: 0.3939 - val_accuracy: 0.8227\n",
      "Epoch 81/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8439\n",
      "Epoch 00081: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3552 - accuracy: 0.8440 - val_loss: 0.3838 - val_accuracy: 0.8308\n",
      "Epoch 82/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8333\n",
      "Epoch 00082: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3774 - accuracy: 0.8332 - val_loss: 0.3970 - val_accuracy: 0.8177\n",
      "Epoch 83/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8348\n",
      "Epoch 00083: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3683 - accuracy: 0.8347 - val_loss: 0.4359 - val_accuracy: 0.8003\n",
      "Epoch 84/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8377\n",
      "Epoch 00084: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3676 - accuracy: 0.8377 - val_loss: 0.4010 - val_accuracy: 0.8229\n",
      "Epoch 85/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8357\n",
      "Epoch 00085: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3694 - accuracy: 0.8359 - val_loss: 0.4483 - val_accuracy: 0.7932\n",
      "Epoch 86/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8315\n",
      "Epoch 00086: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3796 - accuracy: 0.8310 - val_loss: 0.4500 - val_accuracy: 0.7840\n",
      "Epoch 87/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8364\n",
      "Epoch 00087: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3724 - accuracy: 0.8361 - val_loss: 0.4409 - val_accuracy: 0.7930\n",
      "Epoch 88/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8426\n",
      "Epoch 00088: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3547 - accuracy: 0.8425 - val_loss: 0.4391 - val_accuracy: 0.7874\n",
      "Epoch 89/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8296\n",
      "Epoch 00089: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3803 - accuracy: 0.8296 - val_loss: 0.3872 - val_accuracy: 0.8248\n",
      "Epoch 90/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8395\n",
      "Epoch 00090: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3731 - accuracy: 0.8392 - val_loss: 0.4732 - val_accuracy: 0.7827\n",
      "Epoch 91/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8379\n",
      "Epoch 00091: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3640 - accuracy: 0.8378 - val_loss: 0.3999 - val_accuracy: 0.8257\n",
      "Epoch 92/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8404\n",
      "Epoch 00092: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3657 - accuracy: 0.8403 - val_loss: 0.3928 - val_accuracy: 0.8205\n",
      "Epoch 93/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8302\n",
      "Epoch 00093: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3792 - accuracy: 0.8303 - val_loss: 0.4201 - val_accuracy: 0.8104\n",
      "Epoch 94/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8378\n",
      "Epoch 00094: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3727 - accuracy: 0.8379 - val_loss: 0.4089 - val_accuracy: 0.8242\n",
      "Epoch 95/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8370\n",
      "Epoch 00095: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3751 - accuracy: 0.8370 - val_loss: 0.4134 - val_accuracy: 0.8143\n",
      "Epoch 96/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8370\n",
      "Epoch 00096: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3641 - accuracy: 0.8372 - val_loss: 0.3814 - val_accuracy: 0.8308\n",
      "Epoch 97/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3707 - accuracy: 0.8350\n",
      "Epoch 00097: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3707 - accuracy: 0.8349 - val_loss: 0.4102 - val_accuracy: 0.8138\n",
      "Epoch 98/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3687 - accuracy: 0.8366\n",
      "Epoch 00098: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3685 - accuracy: 0.8367 - val_loss: 0.4194 - val_accuracy: 0.8222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8280\n",
      "Epoch 00099: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3808 - accuracy: 0.8278 - val_loss: 0.4548 - val_accuracy: 0.7816\n",
      "Epoch 100/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3804 - accuracy: 0.8304\n",
      "Epoch 00100: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3810 - accuracy: 0.8303 - val_loss: 0.4926 - val_accuracy: 0.7835\n",
      "Epoch 101/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.8341\n",
      "Epoch 00101: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3743 - accuracy: 0.8337 - val_loss: 0.4159 - val_accuracy: 0.8130\n",
      "Epoch 102/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8313\n",
      "Epoch 00102: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3766 - accuracy: 0.8312 - val_loss: 0.3840 - val_accuracy: 0.8358\n",
      "Epoch 103/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8304\n",
      "Epoch 00103: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3813 - accuracy: 0.8301 - val_loss: 0.4301 - val_accuracy: 0.7979\n",
      "Epoch 104/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3910 - accuracy: 0.8273\n",
      "Epoch 00104: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3913 - accuracy: 0.8274 - val_loss: 0.4464 - val_accuracy: 0.8044\n",
      "Epoch 105/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8264\n",
      "Epoch 00105: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3850 - accuracy: 0.8264 - val_loss: 0.4185 - val_accuracy: 0.8065\n",
      "Epoch 106/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8279\n",
      "Epoch 00106: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3868 - accuracy: 0.8279 - val_loss: 0.4111 - val_accuracy: 0.8194\n",
      "Epoch 107/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8273\n",
      "Epoch 00107: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3815 - accuracy: 0.8273 - val_loss: 0.4076 - val_accuracy: 0.8207\n",
      "Epoch 108/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8271\n",
      "Epoch 00108: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3859 - accuracy: 0.8272 - val_loss: 0.4131 - val_accuracy: 0.8229\n",
      "Epoch 109/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8298\n",
      "Epoch 00109: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3831 - accuracy: 0.8294 - val_loss: 0.4204 - val_accuracy: 0.8175\n",
      "Epoch 110/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8268\n",
      "Epoch 00110: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3858 - accuracy: 0.8269 - val_loss: 0.4016 - val_accuracy: 0.8186\n",
      "Epoch 111/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8197\n",
      "Epoch 00111: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3971 - accuracy: 0.8195 - val_loss: 0.4150 - val_accuracy: 0.8224\n",
      "Epoch 112/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3889 - accuracy: 0.8266\n",
      "Epoch 00112: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3894 - accuracy: 0.8263 - val_loss: 0.4117 - val_accuracy: 0.8160\n",
      "Epoch 113/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8227\n",
      "Epoch 00113: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3917 - accuracy: 0.8226 - val_loss: 0.4510 - val_accuracy: 0.7803\n",
      "Epoch 114/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.8237\n",
      "Epoch 00114: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3923 - accuracy: 0.8239 - val_loss: 0.4123 - val_accuracy: 0.8136\n",
      "Epoch 115/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8216\n",
      "Epoch 00115: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4053 - accuracy: 0.8217 - val_loss: 0.4161 - val_accuracy: 0.8108\n",
      "Epoch 116/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8233\n",
      "Epoch 00116: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3937 - accuracy: 0.8233 - val_loss: 0.4048 - val_accuracy: 0.8239\n",
      "Epoch 117/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8152\n",
      "Epoch 00117: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4004 - accuracy: 0.8153 - val_loss: 0.4124 - val_accuracy: 0.8106\n",
      "Epoch 118/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8124\n",
      "Epoch 00118: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4099 - accuracy: 0.8122 - val_loss: 0.5454 - val_accuracy: 0.7352\n",
      "Epoch 119/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8103\n",
      "Epoch 00119: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4141 - accuracy: 0.8104 - val_loss: 0.4546 - val_accuracy: 0.7917\n",
      "Epoch 120/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8173\n",
      "Epoch 00120: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4066 - accuracy: 0.8172 - val_loss: 0.4231 - val_accuracy: 0.8091\n",
      "Epoch 121/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8223\n",
      "Epoch 00121: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3975 - accuracy: 0.8224 - val_loss: 0.4041 - val_accuracy: 0.8184\n",
      "Epoch 122/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8224\n",
      "Epoch 00122: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4070 - accuracy: 0.8224 - val_loss: 0.4262 - val_accuracy: 0.8070\n",
      "Epoch 123/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8277\n",
      "Epoch 00123: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3911 - accuracy: 0.8276 - val_loss: 0.4116 - val_accuracy: 0.8212\n",
      "Epoch 124/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8188\n",
      "Epoch 00124: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4006 - accuracy: 0.8185 - val_loss: 0.4493 - val_accuracy: 0.7891\n",
      "Epoch 125/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8143\n",
      "Epoch 00125: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4086 - accuracy: 0.8143 - val_loss: 0.4090 - val_accuracy: 0.8160\n",
      "Epoch 126/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8168\n",
      "Epoch 00126: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4065 - accuracy: 0.8166 - val_loss: 0.4408 - val_accuracy: 0.7921\n",
      "Epoch 127/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6770 - accuracy: 0.5204\n",
      "Epoch 00127: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6771 - accuracy: 0.5206 - val_loss: 0.6928 - val_accuracy: 0.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4984\n",
      "Epoch 00128: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6934 - accuracy: 0.4987 - val_loss: 0.6931 - val_accuracy: 0.5125\n",
      "Epoch 129/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5339 - accuracy: 0.6739\n",
      "Epoch 00129: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5335 - accuracy: 0.6742 - val_loss: 0.4503 - val_accuracy: 0.7960\n",
      "Epoch 130/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4055 - accuracy: 0.8183\n",
      "Epoch 00130: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4056 - accuracy: 0.8181 - val_loss: 0.4043 - val_accuracy: 0.8186\n",
      "Epoch 131/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4142 - accuracy: 0.8144\n",
      "Epoch 00131: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4146 - accuracy: 0.8141 - val_loss: 0.5770 - val_accuracy: 0.6978\n",
      "Epoch 132/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8184\n",
      "Epoch 00132: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4010 - accuracy: 0.8181 - val_loss: 0.5380 - val_accuracy: 0.7283\n",
      "Epoch 133/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8241\n",
      "Epoch 00133: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3963 - accuracy: 0.8241 - val_loss: 0.4369 - val_accuracy: 0.8018\n",
      "Epoch 134/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8114\n",
      "Epoch 00134: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4014 - accuracy: 0.8116 - val_loss: 0.4271 - val_accuracy: 0.8089\n",
      "Epoch 135/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8143\n",
      "Epoch 00135: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4083 - accuracy: 0.8143 - val_loss: 0.4185 - val_accuracy: 0.8164\n",
      "Epoch 136/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8133\n",
      "Epoch 00136: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4128 - accuracy: 0.8132 - val_loss: 0.4095 - val_accuracy: 0.8229\n",
      "Epoch 137/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3948 - accuracy: 0.8200\n",
      "Epoch 00137: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.3948 - accuracy: 0.8199 - val_loss: 0.4347 - val_accuracy: 0.7969\n",
      "Epoch 138/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4072 - accuracy: 0.8140\n",
      "Epoch 00138: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4074 - accuracy: 0.8138 - val_loss: 0.4999 - val_accuracy: 0.7629\n",
      "Epoch 139/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8086\n",
      "Epoch 00139: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4202 - accuracy: 0.8087 - val_loss: 0.4290 - val_accuracy: 0.8108\n",
      "Epoch 140/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4095 - accuracy: 0.8150\n",
      "Epoch 00140: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4092 - accuracy: 0.8152 - val_loss: 0.4258 - val_accuracy: 0.8037\n",
      "Epoch 141/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8105\n",
      "Epoch 00141: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4179 - accuracy: 0.8103 - val_loss: 0.4570 - val_accuracy: 0.7870\n",
      "Epoch 142/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8093\n",
      "Epoch 00142: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4196 - accuracy: 0.8093 - val_loss: 0.4541 - val_accuracy: 0.7889\n",
      "Epoch 143/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8146\n",
      "Epoch 00143: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4102 - accuracy: 0.8147 - val_loss: 0.4832 - val_accuracy: 0.7887\n",
      "Epoch 144/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4484 - accuracy: 0.7943\n",
      "Epoch 00144: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4483 - accuracy: 0.7945 - val_loss: 0.4411 - val_accuracy: 0.7956\n",
      "Epoch 145/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.8031\n",
      "Epoch 00145: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4303 - accuracy: 0.8028 - val_loss: 0.4853 - val_accuracy: 0.7754\n",
      "Epoch 146/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.8018\n",
      "Epoch 00146: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4317 - accuracy: 0.8017 - val_loss: 0.4373 - val_accuracy: 0.7958\n",
      "Epoch 147/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4335 - accuracy: 0.8004\n",
      "Epoch 00147: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4340 - accuracy: 0.8003 - val_loss: 0.5633 - val_accuracy: 0.6913\n",
      "Epoch 148/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.7934\n",
      "Epoch 00148: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4432 - accuracy: 0.7933 - val_loss: 0.5078 - val_accuracy: 0.7769\n",
      "Epoch 149/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.7989\n",
      "Epoch 00149: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4323 - accuracy: 0.7990 - val_loss: 0.4303 - val_accuracy: 0.7979\n",
      "Epoch 150/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.8044\n",
      "Epoch 00150: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4319 - accuracy: 0.8044 - val_loss: 0.4581 - val_accuracy: 0.7747\n",
      "Epoch 151/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.7974\n",
      "Epoch 00151: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4379 - accuracy: 0.7973 - val_loss: 0.4282 - val_accuracy: 0.8065\n",
      "Epoch 152/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7984\n",
      "Epoch 00152: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4393 - accuracy: 0.7982 - val_loss: 0.4394 - val_accuracy: 0.8059\n",
      "Epoch 153/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8086\n",
      "Epoch 00153: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4221 - accuracy: 0.8083 - val_loss: 0.4516 - val_accuracy: 0.7900\n",
      "Epoch 154/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8057\n",
      "Epoch 00154: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4250 - accuracy: 0.8052 - val_loss: 0.5108 - val_accuracy: 0.7274\n",
      "Epoch 155/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8015\n",
      "Epoch 00155: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4226 - accuracy: 0.8015 - val_loss: 0.4219 - val_accuracy: 0.8095\n",
      "Epoch 156/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8081\n",
      "Epoch 00156: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4254 - accuracy: 0.8077 - val_loss: 0.4702 - val_accuracy: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8039\n",
      "Epoch 00157: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4307 - accuracy: 0.8039 - val_loss: 0.4785 - val_accuracy: 0.7842\n",
      "Epoch 158/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4288 - accuracy: 0.8030\n",
      "Epoch 00158: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4297 - accuracy: 0.8026 - val_loss: 0.5654 - val_accuracy: 0.7255\n",
      "Epoch 159/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8039\n",
      "Epoch 00159: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4230 - accuracy: 0.8039 - val_loss: 0.4188 - val_accuracy: 0.8040\n",
      "Epoch 160/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.7986\n",
      "Epoch 00160: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4369 - accuracy: 0.7984 - val_loss: 0.4289 - val_accuracy: 0.8001\n",
      "Epoch 161/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.7869\n",
      "Epoch 00161: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4610 - accuracy: 0.7867 - val_loss: 0.4498 - val_accuracy: 0.7820\n",
      "Epoch 162/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.7947\n",
      "Epoch 00162: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4419 - accuracy: 0.7947 - val_loss: 0.4198 - val_accuracy: 0.7982\n",
      "Epoch 163/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8044\n",
      "Epoch 00163: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4292 - accuracy: 0.8040 - val_loss: 0.4252 - val_accuracy: 0.8061\n",
      "Epoch 164/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8042\n",
      "Epoch 00164: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4223 - accuracy: 0.8040 - val_loss: 0.4185 - val_accuracy: 0.8188\n",
      "Epoch 165/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8026\n",
      "Epoch 00165: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4299 - accuracy: 0.8027 - val_loss: 0.4390 - val_accuracy: 0.7926\n",
      "Epoch 166/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4327 - accuracy: 0.8042\n",
      "Epoch 00166: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4331 - accuracy: 0.8041 - val_loss: 0.4340 - val_accuracy: 0.7977\n",
      "Epoch 167/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.7911\n",
      "Epoch 00167: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4459 - accuracy: 0.7910 - val_loss: 0.4553 - val_accuracy: 0.7928\n",
      "Epoch 168/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.7917\n",
      "Epoch 00168: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4490 - accuracy: 0.7918 - val_loss: 0.4251 - val_accuracy: 0.8067\n",
      "Epoch 169/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7895\n",
      "Epoch 00169: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4571 - accuracy: 0.7894 - val_loss: 0.4354 - val_accuracy: 0.7982\n",
      "Epoch 170/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.8062\n",
      "Epoch 00170: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4406 - accuracy: 0.8059 - val_loss: 0.5035 - val_accuracy: 0.7564\n",
      "Epoch 171/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.8018\n",
      "Epoch 00171: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4394 - accuracy: 0.8019 - val_loss: 0.4400 - val_accuracy: 0.7975\n",
      "Epoch 172/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4681 - accuracy: 0.7812\n",
      "Epoch 00172: val_loss did not improve from 0.36385\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4684 - accuracy: 0.7814 - val_loss: 0.4542 - val_accuracy: 0.8044\n",
      "Epoch 00172: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "    291/Unknown - 25s 86ms/step - loss: 0.6930 - accuracy: 0.5106\n",
      "Epoch 00001: val_loss improved from inf to 0.69256, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 31s 107ms/step - loss: 0.6930 - accuracy: 0.5106 - val_loss: 0.6926 - val_accuracy: 0.4976\n",
      "Epoch 2/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5338\n",
      "Epoch 00002: val_loss improved from 0.69256 to 0.69243, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6912 - accuracy: 0.5340 - val_loss: 0.6924 - val_accuracy: 0.4916\n",
      "Epoch 3/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5644\n",
      "Epoch 00003: val_loss did not improve from 0.69243\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6878 - accuracy: 0.5637 - val_loss: 0.6924 - val_accuracy: 0.4927\n",
      "Epoch 4/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5713\n",
      "Epoch 00004: val_loss did not improve from 0.69243\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6840 - accuracy: 0.5714 - val_loss: 0.6927 - val_accuracy: 0.4955\n",
      "Epoch 5/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5816\n",
      "Epoch 00005: val_loss did not improve from 0.69243\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6792 - accuracy: 0.5819 - val_loss: 0.6929 - val_accuracy: 0.5026\n",
      "Epoch 6/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.5897\n",
      "Epoch 00006: val_loss did not improve from 0.69243\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6749 - accuracy: 0.5898 - val_loss: 0.6963 - val_accuracy: 0.5043\n",
      "Epoch 7/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6707 - accuracy: 0.5922\n",
      "Epoch 00007: val_loss improved from 0.69243 to 0.68912, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6705 - accuracy: 0.5923 - val_loss: 0.6891 - val_accuracy: 0.5267\n",
      "Epoch 8/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6668 - accuracy: 0.5950\n",
      "Epoch 00008: val_loss did not improve from 0.68912\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6666 - accuracy: 0.5953 - val_loss: 0.6893 - val_accuracy: 0.5374\n",
      "Epoch 9/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6633 - accuracy: 0.6020\n",
      "Epoch 00009: val_loss improved from 0.68912 to 0.68565, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6632 - accuracy: 0.6022 - val_loss: 0.6856 - val_accuracy: 0.5499\n",
      "Epoch 10/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6607 - accuracy: 0.6057\n",
      "Epoch 00010: val_loss improved from 0.68565 to 0.67699, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6604 - accuracy: 0.6062 - val_loss: 0.6770 - val_accuracy: 0.5727\n",
      "Epoch 11/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.6055\n",
      "Epoch 00011: val_loss improved from 0.67699 to 0.67623, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6582 - accuracy: 0.6057 - val_loss: 0.6762 - val_accuracy: 0.5742\n",
      "Epoch 12/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6561 - accuracy: 0.6052\n",
      "Epoch 00012: val_loss improved from 0.67623 to 0.67203, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6558 - accuracy: 0.6056 - val_loss: 0.6720 - val_accuracy: 0.5808\n",
      "Epoch 13/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6523 - accuracy: 0.6095\n",
      "Epoch 00013: val_loss did not improve from 0.67203\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6521 - accuracy: 0.6097 - val_loss: 0.6724 - val_accuracy: 0.5787\n",
      "Epoch 14/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.6128\n",
      "Epoch 00014: val_loss improved from 0.67203 to 0.66193, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6511 - accuracy: 0.6129 - val_loss: 0.6619 - val_accuracy: 0.5965\n",
      "Epoch 15/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6499 - accuracy: 0.6139\n",
      "Epoch 00015: val_loss improved from 0.66193 to 0.65915, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6496 - accuracy: 0.6141 - val_loss: 0.6592 - val_accuracy: 0.6036\n",
      "Epoch 16/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6473 - accuracy: 0.6188\n",
      "Epoch 00016: val_loss improved from 0.65915 to 0.64907, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6470 - accuracy: 0.6191 - val_loss: 0.6491 - val_accuracy: 0.6146\n",
      "Epoch 17/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6455 - accuracy: 0.6201\n",
      "Epoch 00017: val_loss did not improve from 0.64907\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6453 - accuracy: 0.6205 - val_loss: 0.6499 - val_accuracy: 0.6144\n",
      "Epoch 18/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.6214\n",
      "Epoch 00018: val_loss improved from 0.64907 to 0.64693, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6427 - accuracy: 0.6216 - val_loss: 0.6469 - val_accuracy: 0.6199\n",
      "Epoch 19/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6399 - accuracy: 0.6245\n",
      "Epoch 00019: val_loss improved from 0.64693 to 0.63544, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6396 - accuracy: 0.6246 - val_loss: 0.6354 - val_accuracy: 0.6384\n",
      "Epoch 20/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6399 - accuracy: 0.6229\n",
      "Epoch 00020: val_loss did not improve from 0.63544\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6397 - accuracy: 0.6231 - val_loss: 0.6394 - val_accuracy: 0.6262\n",
      "Epoch 21/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6365 - accuracy: 0.6319\n",
      "Epoch 00021: val_loss improved from 0.63544 to 0.63321, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6362 - accuracy: 0.6323 - val_loss: 0.6332 - val_accuracy: 0.6328\n",
      "Epoch 22/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6344 - accuracy: 0.6315\n",
      "Epoch 00022: val_loss did not improve from 0.63321\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6341 - accuracy: 0.6319 - val_loss: 0.6350 - val_accuracy: 0.6331\n",
      "Epoch 23/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.6333\n",
      "Epoch 00023: val_loss improved from 0.63321 to 0.62888, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6319 - accuracy: 0.6332 - val_loss: 0.6289 - val_accuracy: 0.6419\n",
      "Epoch 24/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.6358\n",
      "Epoch 00024: val_loss improved from 0.62888 to 0.62395, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6296 - accuracy: 0.6358 - val_loss: 0.6240 - val_accuracy: 0.6490\n",
      "Epoch 25/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.6394\n",
      "Epoch 00025: val_loss improved from 0.62395 to 0.62265, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6274 - accuracy: 0.6394 - val_loss: 0.6226 - val_accuracy: 0.6483\n",
      "Epoch 26/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.6424\n",
      "Epoch 00026: val_loss improved from 0.62265 to 0.61981, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6250 - accuracy: 0.6426 - val_loss: 0.6198 - val_accuracy: 0.6481\n",
      "Epoch 27/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6187 - accuracy: 0.6473\n",
      "Epoch 00027: val_loss improved from 0.61981 to 0.61450, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6185 - accuracy: 0.6473 - val_loss: 0.6145 - val_accuracy: 0.6511\n",
      "Epoch 28/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.6530\n",
      "Epoch 00028: val_loss did not improve from 0.61450\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6168 - accuracy: 0.6528 - val_loss: 0.6149 - val_accuracy: 0.6528\n",
      "Epoch 29/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.6614\n",
      "Epoch 00029: val_loss improved from 0.61450 to 0.60944, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6098 - accuracy: 0.6611 - val_loss: 0.6094 - val_accuracy: 0.6578\n",
      "Epoch 30/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6044 - accuracy: 0.6646\n",
      "Epoch 00030: val_loss improved from 0.60944 to 0.60904, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6045 - accuracy: 0.6644 - val_loss: 0.6090 - val_accuracy: 0.6565\n",
      "Epoch 31/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.6705\n",
      "Epoch 00031: val_loss did not improve from 0.60904\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5995 - accuracy: 0.6705 - val_loss: 0.6136 - val_accuracy: 0.6505\n",
      "Epoch 32/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.6755\n",
      "Epoch 00032: val_loss did not improve from 0.60904\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5926 - accuracy: 0.6751 - val_loss: 0.6196 - val_accuracy: 0.6436\n",
      "Epoch 33/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5897 - accuracy: 0.6779\n",
      "Epoch 00033: val_loss did not improve from 0.60904\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5900 - accuracy: 0.6774 - val_loss: 0.6160 - val_accuracy: 0.6464\n",
      "Epoch 34/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5869 - accuracy: 0.6831\n",
      "Epoch 00034: val_loss did not improve from 0.60904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5872 - accuracy: 0.6828 - val_loss: 0.6151 - val_accuracy: 0.6483\n",
      "Epoch 35/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.6884\n",
      "Epoch 00035: val_loss improved from 0.60904 to 0.60692, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5800 - accuracy: 0.6877 - val_loss: 0.6069 - val_accuracy: 0.6539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.6985\n",
      "Epoch 00036: val_loss improved from 0.60692 to 0.60118, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5727 - accuracy: 0.6984 - val_loss: 0.6012 - val_accuracy: 0.6599\n",
      "Epoch 37/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5713 - accuracy: 0.6976\n",
      "Epoch 00037: val_loss did not improve from 0.60118\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5716 - accuracy: 0.6970 - val_loss: 0.6211 - val_accuracy: 0.6468\n",
      "Epoch 38/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7059\n",
      "Epoch 00038: val_loss improved from 0.60118 to 0.58301, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5648 - accuracy: 0.7052 - val_loss: 0.5830 - val_accuracy: 0.6705\n",
      "Epoch 39/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7058\n",
      "Epoch 00039: val_loss did not improve from 0.58301\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5620 - accuracy: 0.7053 - val_loss: 0.5929 - val_accuracy: 0.6687\n",
      "Epoch 40/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5596 - accuracy: 0.7062\n",
      "Epoch 00040: val_loss improved from 0.58301 to 0.56515, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5600 - accuracy: 0.7055 - val_loss: 0.5651 - val_accuracy: 0.6986\n",
      "Epoch 41/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7117\n",
      "Epoch 00041: val_loss improved from 0.56515 to 0.55371, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5547 - accuracy: 0.7112 - val_loss: 0.5537 - val_accuracy: 0.7182\n",
      "Epoch 42/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7149\n",
      "Epoch 00042: val_loss improved from 0.55371 to 0.54878, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5518 - accuracy: 0.7145 - val_loss: 0.5488 - val_accuracy: 0.7152\n",
      "Epoch 43/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.7165\n",
      "Epoch 00043: val_loss did not improve from 0.54878\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5493 - accuracy: 0.7159 - val_loss: 0.5526 - val_accuracy: 0.7046\n",
      "Epoch 44/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5452 - accuracy: 0.7176\n",
      "Epoch 00044: val_loss did not improve from 0.54878\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5453 - accuracy: 0.7173 - val_loss: 0.5680 - val_accuracy: 0.6857\n",
      "Epoch 45/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7177\n",
      "Epoch 00045: val_loss did not improve from 0.54878\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5409 - accuracy: 0.7176 - val_loss: 0.5710 - val_accuracy: 0.6900\n",
      "Epoch 46/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7257\n",
      "Epoch 00046: val_loss improved from 0.54878 to 0.53656, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5395 - accuracy: 0.7255 - val_loss: 0.5366 - val_accuracy: 0.7317\n",
      "Epoch 47/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7256\n",
      "Epoch 00047: val_loss did not improve from 0.53656\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5385 - accuracy: 0.7252 - val_loss: 0.5705 - val_accuracy: 0.6838\n",
      "Epoch 48/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5369 - accuracy: 0.7275\n",
      "Epoch 00048: val_loss did not improve from 0.53656\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5372 - accuracy: 0.7271 - val_loss: 0.5566 - val_accuracy: 0.6997\n",
      "Epoch 49/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5315 - accuracy: 0.7308\n",
      "Epoch 00049: val_loss did not improve from 0.53656\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5320 - accuracy: 0.7304 - val_loss: 0.5446 - val_accuracy: 0.7126\n",
      "Epoch 50/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5311 - accuracy: 0.7278\n",
      "Epoch 00050: val_loss did not improve from 0.53656\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5315 - accuracy: 0.7273 - val_loss: 0.5468 - val_accuracy: 0.7081\n",
      "Epoch 51/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7335\n",
      "Epoch 00051: val_loss improved from 0.53656 to 0.53526, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5270 - accuracy: 0.7331 - val_loss: 0.5353 - val_accuracy: 0.7268\n",
      "Epoch 52/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7358\n",
      "Epoch 00052: val_loss did not improve from 0.53526\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5279 - accuracy: 0.7352 - val_loss: 0.5561 - val_accuracy: 0.6982\n",
      "Epoch 53/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.7317\n",
      "Epoch 00053: val_loss did not improve from 0.53526\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5275 - accuracy: 0.7316 - val_loss: 0.5355 - val_accuracy: 0.7221\n",
      "Epoch 54/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5194 - accuracy: 0.7380\n",
      "Epoch 00054: val_loss improved from 0.53526 to 0.52074, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5198 - accuracy: 0.7376 - val_loss: 0.5207 - val_accuracy: 0.7367\n",
      "Epoch 55/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5192 - accuracy: 0.7385\n",
      "Epoch 00055: val_loss improved from 0.52074 to 0.51804, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5197 - accuracy: 0.7380 - val_loss: 0.5180 - val_accuracy: 0.7403\n",
      "Epoch 56/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5199 - accuracy: 0.7373\n",
      "Epoch 00056: val_loss did not improve from 0.51804\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5206 - accuracy: 0.7368 - val_loss: 0.5384 - val_accuracy: 0.7201\n",
      "Epoch 57/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5164 - accuracy: 0.7373\n",
      "Epoch 00057: val_loss did not improve from 0.51804\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5167 - accuracy: 0.7371 - val_loss: 0.5241 - val_accuracy: 0.7337\n",
      "Epoch 58/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5136 - accuracy: 0.7425\n",
      "Epoch 00058: val_loss improved from 0.51804 to 0.51034, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5140 - accuracy: 0.7420 - val_loss: 0.5103 - val_accuracy: 0.7457\n",
      "Epoch 59/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7445\n",
      "Epoch 00059: val_loss improved from 0.51034 to 0.51000, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5138 - accuracy: 0.7442 - val_loss: 0.5100 - val_accuracy: 0.7506\n",
      "Epoch 60/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5117 - accuracy: 0.7436\n",
      "Epoch 00060: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5122 - accuracy: 0.7429 - val_loss: 0.5390 - val_accuracy: 0.7154\n",
      "Epoch 61/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5086 - accuracy: 0.7452\n",
      "Epoch 00061: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5091 - accuracy: 0.7450 - val_loss: 0.5139 - val_accuracy: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7490\n",
      "Epoch 00062: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5117 - accuracy: 0.7486 - val_loss: 0.5242 - val_accuracy: 0.7315\n",
      "Epoch 63/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.7476\n",
      "Epoch 00063: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5062 - accuracy: 0.7471 - val_loss: 0.5375 - val_accuracy: 0.7182\n",
      "Epoch 64/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7511\n",
      "Epoch 00064: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5046 - accuracy: 0.7509 - val_loss: 0.5520 - val_accuracy: 0.7094\n",
      "Epoch 65/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5015 - accuracy: 0.7491\n",
      "Epoch 00065: val_loss did not improve from 0.51000\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5021 - accuracy: 0.7490 - val_loss: 0.5348 - val_accuracy: 0.7190\n",
      "Epoch 66/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5038 - accuracy: 0.7491\n",
      "Epoch 00066: val_loss improved from 0.51000 to 0.50391, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5040 - accuracy: 0.7489 - val_loss: 0.5039 - val_accuracy: 0.7528\n",
      "Epoch 67/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7517\n",
      "Epoch 00067: val_loss did not improve from 0.50391\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5024 - accuracy: 0.7516 - val_loss: 0.5123 - val_accuracy: 0.7399\n",
      "Epoch 68/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7507\n",
      "Epoch 00068: val_loss did not improve from 0.50391\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5012 - accuracy: 0.7505 - val_loss: 0.5215 - val_accuracy: 0.7285\n",
      "Epoch 69/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4987 - accuracy: 0.7530\n",
      "Epoch 00069: val_loss improved from 0.50391 to 0.50228, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4994 - accuracy: 0.7527 - val_loss: 0.5023 - val_accuracy: 0.7504\n",
      "Epoch 70/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7551\n",
      "Epoch 00070: val_loss did not improve from 0.50228\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4949 - accuracy: 0.7551 - val_loss: 0.5091 - val_accuracy: 0.7451\n",
      "Epoch 71/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4973 - accuracy: 0.7548\n",
      "Epoch 00071: val_loss improved from 0.50228 to 0.49675, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4980 - accuracy: 0.7544 - val_loss: 0.4968 - val_accuracy: 0.7564\n",
      "Epoch 72/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.7556\n",
      "Epoch 00072: val_loss did not improve from 0.49675\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4931 - accuracy: 0.7551 - val_loss: 0.5002 - val_accuracy: 0.7496\n",
      "Epoch 73/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4952 - accuracy: 0.7559\n",
      "Epoch 00073: val_loss did not improve from 0.49675\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4959 - accuracy: 0.7555 - val_loss: 0.5169 - val_accuracy: 0.7369\n",
      "Epoch 74/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.7569\n",
      "Epoch 00074: val_loss did not improve from 0.49675\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4917 - accuracy: 0.7565 - val_loss: 0.5378 - val_accuracy: 0.7216\n",
      "Epoch 75/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7599\n",
      "Epoch 00075: val_loss improved from 0.49675 to 0.49250, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4915 - accuracy: 0.7592 - val_loss: 0.4925 - val_accuracy: 0.7573\n",
      "Epoch 76/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7595\n",
      "Epoch 00076: val_loss improved from 0.49250 to 0.47133, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4903 - accuracy: 0.7589 - val_loss: 0.4713 - val_accuracy: 0.7779\n",
      "Epoch 77/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.7609\n",
      "Epoch 00077: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4870 - accuracy: 0.7605 - val_loss: 0.4900 - val_accuracy: 0.7567\n",
      "Epoch 78/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4856 - accuracy: 0.7641\n",
      "Epoch 00078: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4859 - accuracy: 0.7638 - val_loss: 0.5034 - val_accuracy: 0.7436\n",
      "Epoch 79/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7617\n",
      "Epoch 00079: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4847 - accuracy: 0.7613 - val_loss: 0.5235 - val_accuracy: 0.7294\n",
      "Epoch 80/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7622\n",
      "Epoch 00080: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4873 - accuracy: 0.7622 - val_loss: 0.4787 - val_accuracy: 0.7666\n",
      "Epoch 81/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7619\n",
      "Epoch 00081: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4848 - accuracy: 0.7616 - val_loss: 0.5203 - val_accuracy: 0.7296\n",
      "Epoch 82/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7631\n",
      "Epoch 00082: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4861 - accuracy: 0.7625 - val_loss: 0.4838 - val_accuracy: 0.7661\n",
      "Epoch 83/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4811 - accuracy: 0.7651\n",
      "Epoch 00083: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4817 - accuracy: 0.7647 - val_loss: 0.4907 - val_accuracy: 0.7567\n",
      "Epoch 84/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.7658\n",
      "Epoch 00084: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4793 - accuracy: 0.7655 - val_loss: 0.4891 - val_accuracy: 0.7584\n",
      "Epoch 85/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.7656\n",
      "Epoch 00085: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4790 - accuracy: 0.7650 - val_loss: 0.4894 - val_accuracy: 0.7560\n",
      "Epoch 86/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4780 - accuracy: 0.7670\n",
      "Epoch 00086: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4786 - accuracy: 0.7664 - val_loss: 0.4963 - val_accuracy: 0.7468\n",
      "Epoch 87/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.7684\n",
      "Epoch 00087: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4776 - accuracy: 0.7682 - val_loss: 0.5046 - val_accuracy: 0.7440\n",
      "Epoch 88/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.7641\n",
      "Epoch 00088: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4808 - accuracy: 0.7639 - val_loss: 0.4983 - val_accuracy: 0.7463\n",
      "Epoch 89/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7659\n",
      "Epoch 00089: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4754 - accuracy: 0.7655 - val_loss: 0.5026 - val_accuracy: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.7684\n",
      "Epoch 00090: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4742 - accuracy: 0.7680 - val_loss: 0.4738 - val_accuracy: 0.7698\n",
      "Epoch 91/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4774 - accuracy: 0.7666\n",
      "Epoch 00091: val_loss did not improve from 0.47133\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4782 - accuracy: 0.7664 - val_loss: 0.4733 - val_accuracy: 0.7689\n",
      "Epoch 92/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.7674\n",
      "Epoch 00092: val_loss improved from 0.47133 to 0.46766, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4732 - accuracy: 0.7674 - val_loss: 0.4677 - val_accuracy: 0.7747\n",
      "Epoch 93/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.7675\n",
      "Epoch 00093: val_loss did not improve from 0.46766\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4742 - accuracy: 0.7671 - val_loss: 0.4929 - val_accuracy: 0.7498\n",
      "Epoch 94/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7720\n",
      "Epoch 00094: val_loss improved from 0.46766 to 0.46215, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4716 - accuracy: 0.7718 - val_loss: 0.4622 - val_accuracy: 0.7773\n",
      "Epoch 95/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7734\n",
      "Epoch 00095: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4667 - accuracy: 0.7731 - val_loss: 0.4759 - val_accuracy: 0.7668\n",
      "Epoch 96/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4685 - accuracy: 0.7739\n",
      "Epoch 00096: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4691 - accuracy: 0.7735 - val_loss: 0.4976 - val_accuracy: 0.7509\n",
      "Epoch 97/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4661 - accuracy: 0.7749\n",
      "Epoch 00097: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4669 - accuracy: 0.7747 - val_loss: 0.4932 - val_accuracy: 0.7524\n",
      "Epoch 98/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4651 - accuracy: 0.7771\n",
      "Epoch 00098: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4655 - accuracy: 0.7771 - val_loss: 0.5009 - val_accuracy: 0.7442\n",
      "Epoch 99/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7760\n",
      "Epoch 00099: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4635 - accuracy: 0.7758 - val_loss: 0.4669 - val_accuracy: 0.7713\n",
      "Epoch 100/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4649 - accuracy: 0.7761\n",
      "Epoch 00100: val_loss did not improve from 0.46215\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4655 - accuracy: 0.7756 - val_loss: 0.4934 - val_accuracy: 0.7496\n",
      "Epoch 101/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7784\n",
      "Epoch 00101: val_loss improved from 0.46215 to 0.45693, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4599 - accuracy: 0.7780 - val_loss: 0.4569 - val_accuracy: 0.7794\n",
      "Epoch 102/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7775\n",
      "Epoch 00102: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4620 - accuracy: 0.7772 - val_loss: 0.5036 - val_accuracy: 0.7448\n",
      "Epoch 103/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7777\n",
      "Epoch 00103: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4620 - accuracy: 0.7778 - val_loss: 0.4796 - val_accuracy: 0.7614\n",
      "Epoch 104/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4579 - accuracy: 0.7812\n",
      "Epoch 00104: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4586 - accuracy: 0.7808 - val_loss: 0.4829 - val_accuracy: 0.7605\n",
      "Epoch 105/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.7768\n",
      "Epoch 00105: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4636 - accuracy: 0.7765 - val_loss: 0.4701 - val_accuracy: 0.7672\n",
      "Epoch 106/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4573 - accuracy: 0.7792\n",
      "Epoch 00106: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4581 - accuracy: 0.7790 - val_loss: 0.4610 - val_accuracy: 0.7760\n",
      "Epoch 107/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.7815\n",
      "Epoch 00107: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4557 - accuracy: 0.7812 - val_loss: 0.4618 - val_accuracy: 0.7739\n",
      "Epoch 108/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.7789\n",
      "Epoch 00108: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4587 - accuracy: 0.7788 - val_loss: 0.4718 - val_accuracy: 0.7646\n",
      "Epoch 109/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4553 - accuracy: 0.7814\n",
      "Epoch 00109: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4558 - accuracy: 0.7810 - val_loss: 0.4790 - val_accuracy: 0.7599\n",
      "Epoch 110/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.7848\n",
      "Epoch 00110: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4510 - accuracy: 0.7847 - val_loss: 0.5042 - val_accuracy: 0.7451\n",
      "Epoch 111/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.7830\n",
      "Epoch 00111: val_loss did not improve from 0.45693\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4547 - accuracy: 0.7828 - val_loss: 0.4850 - val_accuracy: 0.7590\n",
      "Epoch 112/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4537 - accuracy: 0.7805\n",
      "Epoch 00112: val_loss improved from 0.45693 to 0.44543, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4540 - accuracy: 0.7804 - val_loss: 0.4454 - val_accuracy: 0.7865\n",
      "Epoch 113/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4519 - accuracy: 0.7826\n",
      "Epoch 00113: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4524 - accuracy: 0.7823 - val_loss: 0.4624 - val_accuracy: 0.7704\n",
      "Epoch 114/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4483 - accuracy: 0.7850\n",
      "Epoch 00114: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4487 - accuracy: 0.7852 - val_loss: 0.4828 - val_accuracy: 0.7597\n",
      "Epoch 115/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7840\n",
      "Epoch 00115: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4471 - accuracy: 0.7836 - val_loss: 0.4834 - val_accuracy: 0.7597\n",
      "Epoch 116/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4502 - accuracy: 0.7848\n",
      "Epoch 00116: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4506 - accuracy: 0.7846 - val_loss: 0.4666 - val_accuracy: 0.7678\n",
      "Epoch 117/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7862\n",
      "Epoch 00117: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4485 - accuracy: 0.7859 - val_loss: 0.4517 - val_accuracy: 0.7814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7890\n",
      "Epoch 00118: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4482 - accuracy: 0.7889 - val_loss: 0.4754 - val_accuracy: 0.7635\n",
      "Epoch 119/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.7843\n",
      "Epoch 00119: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4455 - accuracy: 0.7840 - val_loss: 0.4840 - val_accuracy: 0.7588\n",
      "Epoch 120/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.7879\n",
      "Epoch 00120: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4457 - accuracy: 0.7876 - val_loss: 0.4545 - val_accuracy: 0.7773\n",
      "Epoch 121/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4449 - accuracy: 0.7861\n",
      "Epoch 00121: val_loss did not improve from 0.44543\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4455 - accuracy: 0.7859 - val_loss: 0.4711 - val_accuracy: 0.7631\n",
      "Epoch 122/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4377 - accuracy: 0.7907\n",
      "Epoch 00122: val_loss improved from 0.44543 to 0.43849, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4384 - accuracy: 0.7904 - val_loss: 0.4385 - val_accuracy: 0.7928\n",
      "Epoch 123/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.7894\n",
      "Epoch 00123: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4434 - accuracy: 0.7894 - val_loss: 0.4575 - val_accuracy: 0.7786\n",
      "Epoch 124/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7900\n",
      "Epoch 00124: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4409 - accuracy: 0.7899 - val_loss: 0.4697 - val_accuracy: 0.7678\n",
      "Epoch 125/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.7900\n",
      "Epoch 00125: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4406 - accuracy: 0.7899 - val_loss: 0.4815 - val_accuracy: 0.7605\n",
      "Epoch 126/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4376 - accuracy: 0.7936\n",
      "Epoch 00126: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4382 - accuracy: 0.7936 - val_loss: 0.4477 - val_accuracy: 0.7848\n",
      "Epoch 127/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.7930\n",
      "Epoch 00127: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4368 - accuracy: 0.7925 - val_loss: 0.4392 - val_accuracy: 0.7878\n",
      "Epoch 128/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.7921\n",
      "Epoch 00128: val_loss did not improve from 0.43849\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4352 - accuracy: 0.7917 - val_loss: 0.4880 - val_accuracy: 0.7545\n",
      "Epoch 129/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.7951\n",
      "Epoch 00129: val_loss improved from 0.43849 to 0.43403, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4394 - accuracy: 0.7952 - val_loss: 0.4340 - val_accuracy: 0.7926\n",
      "Epoch 130/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4365 - accuracy: 0.7907\n",
      "Epoch 00130: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4370 - accuracy: 0.7905 - val_loss: 0.4606 - val_accuracy: 0.7741\n",
      "Epoch 131/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4365 - accuracy: 0.7941\n",
      "Epoch 00131: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4374 - accuracy: 0.7937 - val_loss: 0.4346 - val_accuracy: 0.7956\n",
      "Epoch 132/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4351 - accuracy: 0.7943\n",
      "Epoch 00132: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4356 - accuracy: 0.7942 - val_loss: 0.4475 - val_accuracy: 0.7840\n",
      "Epoch 133/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4364 - accuracy: 0.7946\n",
      "Epoch 00133: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4369 - accuracy: 0.7941 - val_loss: 0.4394 - val_accuracy: 0.7876\n",
      "Epoch 134/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.7928\n",
      "Epoch 00134: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4353 - accuracy: 0.7926 - val_loss: 0.4440 - val_accuracy: 0.7850\n",
      "Epoch 135/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.7960\n",
      "Epoch 00135: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4314 - accuracy: 0.7958 - val_loss: 0.4655 - val_accuracy: 0.7689\n",
      "Epoch 136/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7983\n",
      "Epoch 00136: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4302 - accuracy: 0.7979 - val_loss: 0.4538 - val_accuracy: 0.7762\n",
      "Epoch 137/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4297 - accuracy: 0.7939\n",
      "Epoch 00137: val_loss did not improve from 0.43403\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4302 - accuracy: 0.7937 - val_loss: 0.4695 - val_accuracy: 0.7663\n",
      "Epoch 138/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4274 - accuracy: 0.7976\n",
      "Epoch 00138: val_loss improved from 0.43403 to 0.43174, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4280 - accuracy: 0.7970 - val_loss: 0.4317 - val_accuracy: 0.7949\n",
      "Epoch 139/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.7998\n",
      "Epoch 00139: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4263 - accuracy: 0.7996 - val_loss: 0.4319 - val_accuracy: 0.7919\n",
      "Epoch 140/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4273 - accuracy: 0.7975\n",
      "Epoch 00140: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4277 - accuracy: 0.7972 - val_loss: 0.4379 - val_accuracy: 0.7896\n",
      "Epoch 141/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8007\n",
      "Epoch 00141: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4271 - accuracy: 0.8003 - val_loss: 0.4414 - val_accuracy: 0.7865\n",
      "Epoch 142/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.7977\n",
      "Epoch 00142: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.4286 - accuracy: 0.7975 - val_loss: 0.4392 - val_accuracy: 0.7887\n",
      "Epoch 143/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.7958\n",
      "Epoch 00143: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4294 - accuracy: 0.7956 - val_loss: 0.4457 - val_accuracy: 0.7829\n",
      "Epoch 144/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8015\n",
      "Epoch 00144: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4262 - accuracy: 0.8013 - val_loss: 0.4565 - val_accuracy: 0.7769\n",
      "Epoch 145/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.7993\n",
      "Epoch 00145: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4270 - accuracy: 0.7990 - val_loss: 0.4674 - val_accuracy: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8018\n",
      "Epoch 00146: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4233 - accuracy: 0.8013 - val_loss: 0.4326 - val_accuracy: 0.7939\n",
      "Epoch 147/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8006\n",
      "Epoch 00147: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4231 - accuracy: 0.8004 - val_loss: 0.4340 - val_accuracy: 0.7911\n",
      "Epoch 148/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8018\n",
      "Epoch 00148: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4237 - accuracy: 0.8016 - val_loss: 0.4405 - val_accuracy: 0.7874\n",
      "Epoch 149/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8029\n",
      "Epoch 00149: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4223 - accuracy: 0.8031 - val_loss: 0.4733 - val_accuracy: 0.7646\n",
      "Epoch 150/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8015\n",
      "Epoch 00150: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4239 - accuracy: 0.8012 - val_loss: 0.4588 - val_accuracy: 0.7747\n",
      "Epoch 151/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8009\n",
      "Epoch 00151: val_loss did not improve from 0.43174\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4249 - accuracy: 0.8008 - val_loss: 0.4597 - val_accuracy: 0.7764\n",
      "Epoch 152/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8009\n",
      "Epoch 00152: val_loss improved from 0.43174 to 0.42857, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4204 - accuracy: 0.8006 - val_loss: 0.4286 - val_accuracy: 0.7962\n",
      "Epoch 153/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8018\n",
      "Epoch 00153: val_loss improved from 0.42857 to 0.42651, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4199 - accuracy: 0.8016 - val_loss: 0.4265 - val_accuracy: 0.7979\n",
      "Epoch 154/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.8050\n",
      "Epoch 00154: val_loss improved from 0.42651 to 0.41888, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4187 - accuracy: 0.8046 - val_loss: 0.4189 - val_accuracy: 0.8100\n",
      "Epoch 155/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8011\n",
      "Epoch 00155: val_loss did not improve from 0.41888\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4180 - accuracy: 0.8010 - val_loss: 0.4606 - val_accuracy: 0.7721\n",
      "Epoch 156/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8035\n",
      "Epoch 00156: val_loss did not improve from 0.41888\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4175 - accuracy: 0.8032 - val_loss: 0.4472 - val_accuracy: 0.7822\n",
      "Epoch 157/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4173 - accuracy: 0.8016\n",
      "Epoch 00157: val_loss did not improve from 0.41888\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4175 - accuracy: 0.8017 - val_loss: 0.4490 - val_accuracy: 0.7816\n",
      "Epoch 158/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8066\n",
      "Epoch 00158: val_loss improved from 0.41888 to 0.41854, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4144 - accuracy: 0.8064 - val_loss: 0.4185 - val_accuracy: 0.8027\n",
      "Epoch 159/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8074\n",
      "Epoch 00159: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4134 - accuracy: 0.8070 - val_loss: 0.4766 - val_accuracy: 0.7614\n",
      "Epoch 160/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8045\n",
      "Epoch 00160: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4166 - accuracy: 0.8046 - val_loss: 0.4212 - val_accuracy: 0.8018\n",
      "Epoch 161/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8109\n",
      "Epoch 00161: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4102 - accuracy: 0.8109 - val_loss: 0.4304 - val_accuracy: 0.7973\n",
      "Epoch 162/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8073\n",
      "Epoch 00162: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4112 - accuracy: 0.8073 - val_loss: 0.4323 - val_accuracy: 0.7932\n",
      "Epoch 163/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8079\n",
      "Epoch 00163: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4108 - accuracy: 0.8078 - val_loss: 0.4497 - val_accuracy: 0.7820\n",
      "Epoch 164/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4098 - accuracy: 0.8074\n",
      "Epoch 00164: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4097 - accuracy: 0.8076 - val_loss: 0.4283 - val_accuracy: 0.7958\n",
      "Epoch 165/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8100\n",
      "Epoch 00165: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4097 - accuracy: 0.8098 - val_loss: 0.4503 - val_accuracy: 0.7810\n",
      "Epoch 166/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4115 - accuracy: 0.8050\n",
      "Epoch 00166: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4117 - accuracy: 0.8048 - val_loss: 0.4322 - val_accuracy: 0.7941\n",
      "Epoch 167/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4073 - accuracy: 0.8114\n",
      "Epoch 00167: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4076 - accuracy: 0.8112 - val_loss: 0.4344 - val_accuracy: 0.7939\n",
      "Epoch 168/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.8067\n",
      "Epoch 00168: val_loss did not improve from 0.41854\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4109 - accuracy: 0.8065 - val_loss: 0.4195 - val_accuracy: 0.8025\n",
      "Epoch 169/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4060 - accuracy: 0.8067\n",
      "Epoch 00169: val_loss improved from 0.41854 to 0.41637, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4065 - accuracy: 0.8064 - val_loss: 0.4164 - val_accuracy: 0.8033\n",
      "Epoch 170/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8102\n",
      "Epoch 00170: val_loss did not improve from 0.41637\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4087 - accuracy: 0.8099 - val_loss: 0.4391 - val_accuracy: 0.7863\n",
      "Epoch 171/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8091\n",
      "Epoch 00171: val_loss did not improve from 0.41637\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4067 - accuracy: 0.8090 - val_loss: 0.4291 - val_accuracy: 0.7969\n",
      "Epoch 172/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8114\n",
      "Epoch 00172: val_loss did not improve from 0.41637\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4034 - accuracy: 0.8112 - val_loss: 0.4235 - val_accuracy: 0.7990\n",
      "Epoch 173/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8127\n",
      "Epoch 00173: val_loss improved from 0.41637 to 0.41314, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4042 - accuracy: 0.8126 - val_loss: 0.4131 - val_accuracy: 0.8104\n",
      "Epoch 174/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4023 - accuracy: 0.8117\n",
      "Epoch 00174: val_loss did not improve from 0.41314\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4028 - accuracy: 0.8116 - val_loss: 0.4354 - val_accuracy: 0.7902\n",
      "Epoch 175/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4001 - accuracy: 0.8135\n",
      "Epoch 00175: val_loss did not improve from 0.41314\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4006 - accuracy: 0.8133 - val_loss: 0.4184 - val_accuracy: 0.8050\n",
      "Epoch 176/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8122\n",
      "Epoch 00176: val_loss did not improve from 0.41314\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4008 - accuracy: 0.8117 - val_loss: 0.4290 - val_accuracy: 0.7945\n",
      "Epoch 177/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8150\n",
      "Epoch 00177: val_loss did not improve from 0.41314\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3973 - accuracy: 0.8148 - val_loss: 0.4197 - val_accuracy: 0.8018\n",
      "Epoch 178/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8134\n",
      "Epoch 00178: val_loss did not improve from 0.41314\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4013 - accuracy: 0.8135 - val_loss: 0.4233 - val_accuracy: 0.7971\n",
      "Epoch 179/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3975 - accuracy: 0.8175\n",
      "Epoch 00179: val_loss improved from 0.41314 to 0.41145, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3979 - accuracy: 0.8173 - val_loss: 0.4115 - val_accuracy: 0.8115\n",
      "Epoch 180/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8135\n",
      "Epoch 00180: val_loss did not improve from 0.41145\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3989 - accuracy: 0.8133 - val_loss: 0.4161 - val_accuracy: 0.8059\n",
      "Epoch 181/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8163\n",
      "Epoch 00181: val_loss did not improve from 0.41145\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3975 - accuracy: 0.8162 - val_loss: 0.4232 - val_accuracy: 0.8016\n",
      "Epoch 182/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8178\n",
      "Epoch 00182: val_loss did not improve from 0.41145\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3955 - accuracy: 0.8179 - val_loss: 0.4150 - val_accuracy: 0.8076\n",
      "Epoch 183/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8169\n",
      "Epoch 00183: val_loss improved from 0.41145 to 0.41003, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3983 - accuracy: 0.8167 - val_loss: 0.4100 - val_accuracy: 0.8110\n",
      "Epoch 184/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8130\n",
      "Epoch 00184: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3988 - accuracy: 0.8125 - val_loss: 0.4392 - val_accuracy: 0.7868\n",
      "Epoch 185/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8173\n",
      "Epoch 00185: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3924 - accuracy: 0.8169 - val_loss: 0.4129 - val_accuracy: 0.8070\n",
      "Epoch 186/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8197\n",
      "Epoch 00186: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3921 - accuracy: 0.8198 - val_loss: 0.4432 - val_accuracy: 0.7863\n",
      "Epoch 187/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3936 - accuracy: 0.8157\n",
      "Epoch 00187: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3939 - accuracy: 0.8156 - val_loss: 0.4453 - val_accuracy: 0.7865\n",
      "Epoch 188/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8186\n",
      "Epoch 00188: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3917 - accuracy: 0.8184 - val_loss: 0.4332 - val_accuracy: 0.7945\n",
      "Epoch 189/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8203\n",
      "Epoch 00189: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3899 - accuracy: 0.8201 - val_loss: 0.4309 - val_accuracy: 0.7969\n",
      "Epoch 190/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3889 - accuracy: 0.8208\n",
      "Epoch 00190: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3895 - accuracy: 0.8206 - val_loss: 0.4271 - val_accuracy: 0.7960\n",
      "Epoch 191/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.8257\n",
      "Epoch 00191: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3841 - accuracy: 0.8256 - val_loss: 0.4302 - val_accuracy: 0.7984\n",
      "Epoch 192/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8214\n",
      "Epoch 00192: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3851 - accuracy: 0.8214 - val_loss: 0.4156 - val_accuracy: 0.8076\n",
      "Epoch 193/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.8176\n",
      "Epoch 00193: val_loss did not improve from 0.41003\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3904 - accuracy: 0.8173 - val_loss: 0.4315 - val_accuracy: 0.7969\n",
      "Epoch 194/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8202\n",
      "Epoch 00194: val_loss improved from 0.41003 to 0.40690, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3899 - accuracy: 0.8200 - val_loss: 0.4069 - val_accuracy: 0.8110\n",
      "Epoch 195/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8212\n",
      "Epoch 00195: val_loss did not improve from 0.40690\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3898 - accuracy: 0.8211 - val_loss: 0.4163 - val_accuracy: 0.8076\n",
      "Epoch 196/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8215\n",
      "Epoch 00196: val_loss did not improve from 0.40690\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3869 - accuracy: 0.8215 - val_loss: 0.4090 - val_accuracy: 0.8119\n",
      "Epoch 197/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8243\n",
      "Epoch 00197: val_loss did not improve from 0.40690\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3847 - accuracy: 0.8240 - val_loss: 0.4113 - val_accuracy: 0.8065\n",
      "Epoch 198/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8241\n",
      "Epoch 00198: val_loss did not improve from 0.40690\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3811 - accuracy: 0.8243 - val_loss: 0.4098 - val_accuracy: 0.8121\n",
      "Epoch 199/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.8237\n",
      "Epoch 00199: val_loss did not improve from 0.40690\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3841 - accuracy: 0.8234 - val_loss: 0.4244 - val_accuracy: 0.8003\n",
      "Epoch 200/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8235\n",
      "Epoch 00200: val_loss improved from 0.40690 to 0.40282, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3842 - accuracy: 0.8235 - val_loss: 0.4028 - val_accuracy: 0.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8205\n",
      "Epoch 00201: val_loss improved from 0.40282 to 0.40271, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3860 - accuracy: 0.8202 - val_loss: 0.4027 - val_accuracy: 0.8147\n",
      "Epoch 202/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.8249\n",
      "Epoch 00202: val_loss did not improve from 0.40271\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3786 - accuracy: 0.8247 - val_loss: 0.4450 - val_accuracy: 0.7865\n",
      "Epoch 203/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8262\n",
      "Epoch 00203: val_loss did not improve from 0.40271\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3770 - accuracy: 0.8261 - val_loss: 0.4085 - val_accuracy: 0.8100\n",
      "Epoch 204/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8249\n",
      "Epoch 00204: val_loss did not improve from 0.40271\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3833 - accuracy: 0.8248 - val_loss: 0.4082 - val_accuracy: 0.8110\n",
      "Epoch 205/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8273\n",
      "Epoch 00205: val_loss improved from 0.40271 to 0.40067, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.3802 - accuracy: 0.8271 - val_loss: 0.4007 - val_accuracy: 0.8149\n",
      "Epoch 206/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8263\n",
      "Epoch 00206: val_loss did not improve from 0.40067\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3801 - accuracy: 0.8260 - val_loss: 0.4069 - val_accuracy: 0.8123\n",
      "Epoch 207/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.8275\n",
      "Epoch 00207: val_loss did not improve from 0.40067\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3794 - accuracy: 0.8272 - val_loss: 0.4196 - val_accuracy: 0.8025\n",
      "Epoch 208/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8291\n",
      "Epoch 00208: val_loss did not improve from 0.40067\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3742 - accuracy: 0.8286 - val_loss: 0.4136 - val_accuracy: 0.8072\n",
      "Epoch 209/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8294\n",
      "Epoch 00209: val_loss did not improve from 0.40067\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3755 - accuracy: 0.8289 - val_loss: 0.4268 - val_accuracy: 0.7954\n",
      "Epoch 210/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8243\n",
      "Epoch 00210: val_loss did not improve from 0.40067\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3777 - accuracy: 0.8241 - val_loss: 0.4118 - val_accuracy: 0.8093\n",
      "Epoch 211/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8310\n",
      "Epoch 00211: val_loss improved from 0.40067 to 0.40045, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3745 - accuracy: 0.8311 - val_loss: 0.4005 - val_accuracy: 0.8132\n",
      "Epoch 212/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8328\n",
      "Epoch 00212: val_loss did not improve from 0.40045\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3712 - accuracy: 0.8326 - val_loss: 0.4128 - val_accuracy: 0.8093\n",
      "Epoch 213/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8339\n",
      "Epoch 00213: val_loss improved from 0.40045 to 0.39860, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3696 - accuracy: 0.8338 - val_loss: 0.3986 - val_accuracy: 0.8153\n",
      "Epoch 214/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8302\n",
      "Epoch 00214: val_loss improved from 0.39860 to 0.39269, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3701 - accuracy: 0.8299 - val_loss: 0.3927 - val_accuracy: 0.8149\n",
      "Epoch 215/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8302\n",
      "Epoch 00215: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3715 - accuracy: 0.8300 - val_loss: 0.3982 - val_accuracy: 0.8138\n",
      "Epoch 216/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8265\n",
      "Epoch 00216: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3729 - accuracy: 0.8260 - val_loss: 0.4091 - val_accuracy: 0.8080\n",
      "Epoch 217/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8340\n",
      "Epoch 00217: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3690 - accuracy: 0.8338 - val_loss: 0.4035 - val_accuracy: 0.8136\n",
      "Epoch 218/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3663 - accuracy: 0.8340\n",
      "Epoch 00218: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3667 - accuracy: 0.8339 - val_loss: 0.4074 - val_accuracy: 0.8128\n",
      "Epoch 219/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8333\n",
      "Epoch 00219: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3655 - accuracy: 0.8333 - val_loss: 0.3939 - val_accuracy: 0.8173\n",
      "Epoch 220/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3674 - accuracy: 0.8314\n",
      "Epoch 00220: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3684 - accuracy: 0.8310 - val_loss: 0.4036 - val_accuracy: 0.8141\n",
      "Epoch 221/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8300\n",
      "Epoch 00221: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3660 - accuracy: 0.8294 - val_loss: 0.4142 - val_accuracy: 0.8061\n",
      "Epoch 222/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8303\n",
      "Epoch 00222: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3666 - accuracy: 0.8300 - val_loss: 0.4236 - val_accuracy: 0.8022\n",
      "Epoch 223/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8316\n",
      "Epoch 00223: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3629 - accuracy: 0.8314 - val_loss: 0.4076 - val_accuracy: 0.8106\n",
      "Epoch 224/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3610 - accuracy: 0.8368\n",
      "Epoch 00224: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3620 - accuracy: 0.8362 - val_loss: 0.4079 - val_accuracy: 0.8100\n",
      "Epoch 225/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8377\n",
      "Epoch 00225: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3603 - accuracy: 0.8374 - val_loss: 0.4106 - val_accuracy: 0.8065\n",
      "Epoch 226/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8379\n",
      "Epoch 00226: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3540 - accuracy: 0.8377 - val_loss: 0.3979 - val_accuracy: 0.8143\n",
      "Epoch 227/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8347\n",
      "Epoch 00227: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3631 - accuracy: 0.8344 - val_loss: 0.4077 - val_accuracy: 0.8113\n",
      "Epoch 228/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.8365\n",
      "Epoch 00228: val_loss did not improve from 0.39269\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3600 - accuracy: 0.8364 - val_loss: 0.4187 - val_accuracy: 0.8014\n",
      "Epoch 229/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8369\n",
      "Epoch 00229: val_loss improved from 0.39269 to 0.39229, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3595 - accuracy: 0.8366 - val_loss: 0.3923 - val_accuracy: 0.8192\n",
      "Epoch 230/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8368\n",
      "Epoch 00230: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3628 - accuracy: 0.8364 - val_loss: 0.4081 - val_accuracy: 0.8087\n",
      "Epoch 231/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8405\n",
      "Epoch 00231: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3550 - accuracy: 0.8402 - val_loss: 0.3941 - val_accuracy: 0.8194\n",
      "Epoch 232/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8382\n",
      "Epoch 00232: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3601 - accuracy: 0.8381 - val_loss: 0.3947 - val_accuracy: 0.8158\n",
      "Epoch 233/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8386\n",
      "Epoch 00233: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3558 - accuracy: 0.8385 - val_loss: 0.3946 - val_accuracy: 0.8188\n",
      "Epoch 234/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8432\n",
      "Epoch 00234: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3514 - accuracy: 0.8431 - val_loss: 0.3992 - val_accuracy: 0.8143\n",
      "Epoch 235/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8415\n",
      "Epoch 00235: val_loss did not improve from 0.39229\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3548 - accuracy: 0.8413 - val_loss: 0.3995 - val_accuracy: 0.8136\n",
      "Epoch 236/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8376\n",
      "Epoch 00236: val_loss improved from 0.39229 to 0.38975, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3555 - accuracy: 0.8372 - val_loss: 0.3898 - val_accuracy: 0.8222\n",
      "Epoch 237/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8408\n",
      "Epoch 00237: val_loss did not improve from 0.38975\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3514 - accuracy: 0.8402 - val_loss: 0.3944 - val_accuracy: 0.8158\n",
      "Epoch 238/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8400\n",
      "Epoch 00238: val_loss did not improve from 0.38975\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3529 - accuracy: 0.8399 - val_loss: 0.4015 - val_accuracy: 0.8134\n",
      "Epoch 239/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.8417\n",
      "Epoch 00239: val_loss improved from 0.38975 to 0.38514, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3482 - accuracy: 0.8415 - val_loss: 0.3851 - val_accuracy: 0.8233\n",
      "Epoch 240/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8408\n",
      "Epoch 00240: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3511 - accuracy: 0.8409 - val_loss: 0.4018 - val_accuracy: 0.8171\n",
      "Epoch 241/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8417\n",
      "Epoch 00241: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3523 - accuracy: 0.8415 - val_loss: 0.4113 - val_accuracy: 0.8055\n",
      "Epoch 242/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8445\n",
      "Epoch 00242: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3460 - accuracy: 0.8443 - val_loss: 0.3916 - val_accuracy: 0.8166\n",
      "Epoch 243/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8416\n",
      "Epoch 00243: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3523 - accuracy: 0.8416 - val_loss: 0.3997 - val_accuracy: 0.8128\n",
      "Epoch 244/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3431 - accuracy: 0.8472\n",
      "Epoch 00244: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3442 - accuracy: 0.8469 - val_loss: 0.3949 - val_accuracy: 0.8149\n",
      "Epoch 245/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8402\n",
      "Epoch 00245: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3508 - accuracy: 0.8399 - val_loss: 0.4304 - val_accuracy: 0.7945\n",
      "Epoch 246/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8446\n",
      "Epoch 00246: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3454 - accuracy: 0.8446 - val_loss: 0.4066 - val_accuracy: 0.8126\n",
      "Epoch 247/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8457\n",
      "Epoch 00247: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3482 - accuracy: 0.8456 - val_loss: 0.4418 - val_accuracy: 0.7891\n",
      "Epoch 248/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8456\n",
      "Epoch 00248: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3449 - accuracy: 0.8455 - val_loss: 0.4118 - val_accuracy: 0.8050\n",
      "Epoch 249/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8416\n",
      "Epoch 00249: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3453 - accuracy: 0.8414 - val_loss: 0.3995 - val_accuracy: 0.8171\n",
      "Epoch 250/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8438\n",
      "Epoch 00250: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3442 - accuracy: 0.8435 - val_loss: 0.3956 - val_accuracy: 0.8222\n",
      "Epoch 251/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.8461\n",
      "Epoch 00251: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3456 - accuracy: 0.8458 - val_loss: 0.3929 - val_accuracy: 0.8190\n",
      "Epoch 252/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8481\n",
      "Epoch 00252: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3376 - accuracy: 0.8480 - val_loss: 0.3966 - val_accuracy: 0.8115\n",
      "Epoch 253/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8392\n",
      "Epoch 00253: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3444 - accuracy: 0.8392 - val_loss: 0.3885 - val_accuracy: 0.8169\n",
      "Epoch 254/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8475\n",
      "Epoch 00254: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3396 - accuracy: 0.8475 - val_loss: 0.4027 - val_accuracy: 0.8117\n",
      "Epoch 255/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8447\n",
      "Epoch 00255: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3419 - accuracy: 0.8447 - val_loss: 0.3927 - val_accuracy: 0.8181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8467\n",
      "Epoch 00256: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3402 - accuracy: 0.8461 - val_loss: 0.4014 - val_accuracy: 0.8128\n",
      "Epoch 257/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8522\n",
      "Epoch 00257: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3355 - accuracy: 0.8514 - val_loss: 0.3911 - val_accuracy: 0.8164\n",
      "Epoch 258/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.8513\n",
      "Epoch 00258: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3340 - accuracy: 0.8513 - val_loss: 0.4017 - val_accuracy: 0.8130\n",
      "Epoch 259/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8489\n",
      "Epoch 00259: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3385 - accuracy: 0.8486 - val_loss: 0.4102 - val_accuracy: 0.8083\n",
      "Epoch 260/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8482\n",
      "Epoch 00260: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3359 - accuracy: 0.8481 - val_loss: 0.4052 - val_accuracy: 0.8121\n",
      "Epoch 261/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8472\n",
      "Epoch 00261: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3418 - accuracy: 0.8471 - val_loss: 0.4056 - val_accuracy: 0.8134\n",
      "Epoch 262/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8512\n",
      "Epoch 00262: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3350 - accuracy: 0.8510 - val_loss: 0.3946 - val_accuracy: 0.8143\n",
      "Epoch 263/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3320 - accuracy: 0.8499\n",
      "Epoch 00263: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3322 - accuracy: 0.8497 - val_loss: 0.4141 - val_accuracy: 0.8057\n",
      "Epoch 264/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8482\n",
      "Epoch 00264: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3353 - accuracy: 0.8479 - val_loss: 0.4019 - val_accuracy: 0.8110\n",
      "Epoch 265/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8505\n",
      "Epoch 00265: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3315 - accuracy: 0.8503 - val_loss: 0.3984 - val_accuracy: 0.8130\n",
      "Epoch 266/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8499\n",
      "Epoch 00266: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3342 - accuracy: 0.8498 - val_loss: 0.4159 - val_accuracy: 0.8025\n",
      "Epoch 267/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8526\n",
      "Epoch 00267: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3293 - accuracy: 0.8525 - val_loss: 0.3939 - val_accuracy: 0.8175\n",
      "Epoch 268/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8514\n",
      "Epoch 00268: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3292 - accuracy: 0.8512 - val_loss: 0.4011 - val_accuracy: 0.8102\n",
      "Epoch 269/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8509\n",
      "Epoch 00269: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3290 - accuracy: 0.8506 - val_loss: 0.4071 - val_accuracy: 0.8121\n",
      "Epoch 270/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8514\n",
      "Epoch 00270: val_loss did not improve from 0.38514\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3309 - accuracy: 0.8508 - val_loss: 0.4011 - val_accuracy: 0.8098\n",
      "Epoch 271/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8528\n",
      "Epoch 00271: val_loss improved from 0.38514 to 0.38509, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3309 - accuracy: 0.8527 - val_loss: 0.3851 - val_accuracy: 0.8179\n",
      "Epoch 272/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3263 - accuracy: 0.8550\n",
      "Epoch 00272: val_loss did not improve from 0.38509\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3268 - accuracy: 0.8549 - val_loss: 0.4003 - val_accuracy: 0.8143\n",
      "Epoch 273/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8545\n",
      "Epoch 00273: val_loss did not improve from 0.38509\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3233 - accuracy: 0.8542 - val_loss: 0.3901 - val_accuracy: 0.8190\n",
      "Epoch 274/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8532\n",
      "Epoch 00274: val_loss improved from 0.38509 to 0.38422, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3281 - accuracy: 0.8526 - val_loss: 0.3842 - val_accuracy: 0.8205\n",
      "Epoch 275/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8545\n",
      "Epoch 00275: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3222 - accuracy: 0.8544 - val_loss: 0.3973 - val_accuracy: 0.8117\n",
      "Epoch 276/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8589\n",
      "Epoch 00276: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3232 - accuracy: 0.8586 - val_loss: 0.4029 - val_accuracy: 0.8087\n",
      "Epoch 277/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8573\n",
      "Epoch 00277: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3260 - accuracy: 0.8571 - val_loss: 0.3962 - val_accuracy: 0.8136\n",
      "Epoch 278/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8564\n",
      "Epoch 00278: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3186 - accuracy: 0.8562 - val_loss: 0.4087 - val_accuracy: 0.8093\n",
      "Epoch 279/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8597\n",
      "Epoch 00279: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3190 - accuracy: 0.8595 - val_loss: 0.3927 - val_accuracy: 0.8186\n",
      "Epoch 280/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8550\n",
      "Epoch 00280: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3223 - accuracy: 0.8548 - val_loss: 0.4025 - val_accuracy: 0.8138\n",
      "Epoch 281/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8534\n",
      "Epoch 00281: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3260 - accuracy: 0.8535 - val_loss: 0.3971 - val_accuracy: 0.8141\n",
      "Epoch 282/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8580\n",
      "Epoch 00282: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3208 - accuracy: 0.8575 - val_loss: 0.3963 - val_accuracy: 0.8117\n",
      "Epoch 283/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8563\n",
      "Epoch 00283: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3185 - accuracy: 0.8563 - val_loss: 0.3937 - val_accuracy: 0.8151\n",
      "Epoch 284/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8578\n",
      "Epoch 00284: val_loss did not improve from 0.38422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3210 - accuracy: 0.8575 - val_loss: 0.4214 - val_accuracy: 0.7986\n",
      "Epoch 285/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.8603\n",
      "Epoch 00285: val_loss improved from 0.38422 to 0.38295, saving model to pickled_objects/batch_size_64_lr_0.01_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3170 - accuracy: 0.8600 - val_loss: 0.3829 - val_accuracy: 0.8224\n",
      "Epoch 286/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8581\n",
      "Epoch 00286: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3173 - accuracy: 0.8580 - val_loss: 0.3963 - val_accuracy: 0.8147\n",
      "Epoch 287/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8572\n",
      "Epoch 00287: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3195 - accuracy: 0.8569 - val_loss: 0.3903 - val_accuracy: 0.8184\n",
      "Epoch 288/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8616\n",
      "Epoch 00288: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3154 - accuracy: 0.8614 - val_loss: 0.3987 - val_accuracy: 0.8123\n",
      "Epoch 289/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.8611\n",
      "Epoch 00289: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3166 - accuracy: 0.8608 - val_loss: 0.3940 - val_accuracy: 0.8188\n",
      "Epoch 290/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8613\n",
      "Epoch 00290: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3093 - accuracy: 0.8610 - val_loss: 0.3890 - val_accuracy: 0.8212\n",
      "Epoch 291/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8624\n",
      "Epoch 00291: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3115 - accuracy: 0.8622 - val_loss: 0.3980 - val_accuracy: 0.8158\n",
      "Epoch 292/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8609\n",
      "Epoch 00292: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3134 - accuracy: 0.8608 - val_loss: 0.3859 - val_accuracy: 0.8216\n",
      "Epoch 293/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8620\n",
      "Epoch 00293: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3094 - accuracy: 0.8620 - val_loss: 0.4244 - val_accuracy: 0.8027\n",
      "Epoch 294/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8631\n",
      "Epoch 00294: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3131 - accuracy: 0.8630 - val_loss: 0.3907 - val_accuracy: 0.8207\n",
      "Epoch 295/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8632\n",
      "Epoch 00295: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3088 - accuracy: 0.8630 - val_loss: 0.3878 - val_accuracy: 0.8218\n",
      "Epoch 296/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8658\n",
      "Epoch 00296: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3044 - accuracy: 0.8654 - val_loss: 0.3952 - val_accuracy: 0.8138\n",
      "Epoch 297/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8611\n",
      "Epoch 00297: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3106 - accuracy: 0.8609 - val_loss: 0.3930 - val_accuracy: 0.8186\n",
      "Epoch 298/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8633\n",
      "Epoch 00298: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3086 - accuracy: 0.8632 - val_loss: 0.3978 - val_accuracy: 0.8126\n",
      "Epoch 299/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8631\n",
      "Epoch 00299: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3082 - accuracy: 0.8628 - val_loss: 0.4082 - val_accuracy: 0.8100\n",
      "Epoch 300/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8641\n",
      "Epoch 00300: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.3062 - accuracy: 0.8637 - val_loss: 0.3963 - val_accuracy: 0.8151\n",
      "Epoch 301/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8638\n",
      "Epoch 00301: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3077 - accuracy: 0.8637 - val_loss: 0.3888 - val_accuracy: 0.8201\n",
      "Epoch 302/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8656\n",
      "Epoch 00302: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3055 - accuracy: 0.8653 - val_loss: 0.3951 - val_accuracy: 0.8162\n",
      "Epoch 303/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8687\n",
      "Epoch 00303: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3037 - accuracy: 0.8683 - val_loss: 0.4299 - val_accuracy: 0.7986\n",
      "Epoch 304/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8665\n",
      "Epoch 00304: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3007 - accuracy: 0.8663 - val_loss: 0.4057 - val_accuracy: 0.8093\n",
      "Epoch 305/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8679\n",
      "Epoch 00305: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3001 - accuracy: 0.8679 - val_loss: 0.4124 - val_accuracy: 0.8059\n",
      "Epoch 306/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8694\n",
      "Epoch 00306: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2972 - accuracy: 0.8692 - val_loss: 0.3914 - val_accuracy: 0.8177\n",
      "Epoch 307/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8668\n",
      "Epoch 00307: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3018 - accuracy: 0.8666 - val_loss: 0.3831 - val_accuracy: 0.8227\n",
      "Epoch 308/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8729\n",
      "Epoch 00308: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2945 - accuracy: 0.8723 - val_loss: 0.3997 - val_accuracy: 0.8143\n",
      "Epoch 309/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8683\n",
      "Epoch 00309: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2974 - accuracy: 0.8685 - val_loss: 0.4020 - val_accuracy: 0.8117\n",
      "Epoch 310/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8732\n",
      "Epoch 00310: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2964 - accuracy: 0.8731 - val_loss: 0.4028 - val_accuracy: 0.8098\n",
      "Epoch 311/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8693\n",
      "Epoch 00311: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2988 - accuracy: 0.8688 - val_loss: 0.3868 - val_accuracy: 0.8186\n",
      "Epoch 312/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8681\n",
      "Epoch 00312: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2978 - accuracy: 0.8678 - val_loss: 0.3902 - val_accuracy: 0.8153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8716\n",
      "Epoch 00313: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2940 - accuracy: 0.8711 - val_loss: 0.3867 - val_accuracy: 0.8184\n",
      "Epoch 314/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8728\n",
      "Epoch 00314: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2935 - accuracy: 0.8725 - val_loss: 0.4128 - val_accuracy: 0.8085\n",
      "Epoch 315/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8694\n",
      "Epoch 00315: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2952 - accuracy: 0.8693 - val_loss: 0.3991 - val_accuracy: 0.8104\n",
      "Epoch 316/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8679\n",
      "Epoch 00316: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2953 - accuracy: 0.8678 - val_loss: 0.3943 - val_accuracy: 0.8160\n",
      "Epoch 317/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8734\n",
      "Epoch 00317: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2934 - accuracy: 0.8732 - val_loss: 0.4028 - val_accuracy: 0.8128\n",
      "Epoch 318/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8737\n",
      "Epoch 00318: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2903 - accuracy: 0.8733 - val_loss: 0.3918 - val_accuracy: 0.8158\n",
      "Epoch 319/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.8747\n",
      "Epoch 00319: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2901 - accuracy: 0.8746 - val_loss: 0.4118 - val_accuracy: 0.8083\n",
      "Epoch 320/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.8729\n",
      "Epoch 00320: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2880 - accuracy: 0.8731 - val_loss: 0.3985 - val_accuracy: 0.8158\n",
      "Epoch 321/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8712\n",
      "Epoch 00321: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2927 - accuracy: 0.8713 - val_loss: 0.4254 - val_accuracy: 0.8007\n",
      "Epoch 322/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.8746\n",
      "Epoch 00322: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2842 - accuracy: 0.8742 - val_loss: 0.4047 - val_accuracy: 0.8098\n",
      "Epoch 323/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8752\n",
      "Epoch 00323: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2869 - accuracy: 0.8749 - val_loss: 0.4051 - val_accuracy: 0.8138\n",
      "Epoch 324/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8735\n",
      "Epoch 00324: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2876 - accuracy: 0.8732 - val_loss: 0.3928 - val_accuracy: 0.8164\n",
      "Epoch 325/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8747\n",
      "Epoch 00325: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2876 - accuracy: 0.8745 - val_loss: 0.3961 - val_accuracy: 0.8166\n",
      "Epoch 326/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.8755\n",
      "Epoch 00326: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2857 - accuracy: 0.8752 - val_loss: 0.3886 - val_accuracy: 0.8175\n",
      "Epoch 327/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.8799\n",
      "Epoch 00327: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2817 - accuracy: 0.8794 - val_loss: 0.4010 - val_accuracy: 0.8138\n",
      "Epoch 328/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8754\n",
      "Epoch 00328: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2834 - accuracy: 0.8750 - val_loss: 0.3917 - val_accuracy: 0.8169\n",
      "Epoch 329/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8779\n",
      "Epoch 00329: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2844 - accuracy: 0.8780 - val_loss: 0.4025 - val_accuracy: 0.8128\n",
      "Epoch 330/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2796 - accuracy: 0.8769\n",
      "Epoch 00330: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2797 - accuracy: 0.8770 - val_loss: 0.4173 - val_accuracy: 0.8078\n",
      "Epoch 331/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.8768\n",
      "Epoch 00331: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2803 - accuracy: 0.8762 - val_loss: 0.3871 - val_accuracy: 0.8199\n",
      "Epoch 332/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2778 - accuracy: 0.8789\n",
      "Epoch 00332: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2784 - accuracy: 0.8787 - val_loss: 0.3970 - val_accuracy: 0.8175\n",
      "Epoch 333/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.8709\n",
      "Epoch 00333: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2830 - accuracy: 0.8709 - val_loss: 0.4095 - val_accuracy: 0.8126\n",
      "Epoch 334/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.8789\n",
      "Epoch 00334: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2812 - accuracy: 0.8790 - val_loss: 0.4042 - val_accuracy: 0.8132\n",
      "Epoch 335/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.8810\n",
      "Epoch 00335: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2750 - accuracy: 0.8809 - val_loss: 0.4094 - val_accuracy: 0.8151\n",
      "Epoch 336/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8778\n",
      "Epoch 00336: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2780 - accuracy: 0.8776 - val_loss: 0.4249 - val_accuracy: 0.8070\n",
      "Epoch 337/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.8776\n",
      "Epoch 00337: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2798 - accuracy: 0.8776 - val_loss: 0.4070 - val_accuracy: 0.8117\n",
      "Epoch 338/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.8760\n",
      "Epoch 00338: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2790 - accuracy: 0.8761 - val_loss: 0.4068 - val_accuracy: 0.8153\n",
      "Epoch 339/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2780 - accuracy: 0.8781\n",
      "Epoch 00339: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2783 - accuracy: 0.8781 - val_loss: 0.3991 - val_accuracy: 0.8149\n",
      "Epoch 340/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8820\n",
      "Epoch 00340: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2786 - accuracy: 0.8818 - val_loss: 0.3972 - val_accuracy: 0.8132\n",
      "Epoch 341/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.8808\n",
      "Epoch 00341: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2731 - accuracy: 0.8808 - val_loss: 0.3967 - val_accuracy: 0.8184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8844\n",
      "Epoch 00342: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2717 - accuracy: 0.8845 - val_loss: 0.4022 - val_accuracy: 0.8132\n",
      "Epoch 343/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8851\n",
      "Epoch 00343: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2681 - accuracy: 0.8851 - val_loss: 0.4182 - val_accuracy: 0.8113\n",
      "Epoch 344/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.8828\n",
      "Epoch 00344: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2706 - accuracy: 0.8826 - val_loss: 0.3935 - val_accuracy: 0.8186\n",
      "Epoch 345/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.8844\n",
      "Epoch 00345: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2680 - accuracy: 0.8842 - val_loss: 0.3899 - val_accuracy: 0.8246\n",
      "Epoch 346/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.8855\n",
      "Epoch 00346: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2687 - accuracy: 0.8853 - val_loss: 0.4058 - val_accuracy: 0.8169\n",
      "Epoch 347/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.8809\n",
      "Epoch 00347: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2710 - accuracy: 0.8810 - val_loss: 0.4114 - val_accuracy: 0.8117\n",
      "Epoch 348/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8804\n",
      "Epoch 00348: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2718 - accuracy: 0.8804 - val_loss: 0.4152 - val_accuracy: 0.8083\n",
      "Epoch 349/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2697 - accuracy: 0.8825\n",
      "Epoch 00349: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2705 - accuracy: 0.8820 - val_loss: 0.4043 - val_accuracy: 0.8149\n",
      "Epoch 350/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.8854\n",
      "Epoch 00350: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2646 - accuracy: 0.8854 - val_loss: 0.4230 - val_accuracy: 0.8057\n",
      "Epoch 351/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.8852\n",
      "Epoch 00351: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2652 - accuracy: 0.8852 - val_loss: 0.4253 - val_accuracy: 0.8067\n",
      "Epoch 352/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2632 - accuracy: 0.8884\n",
      "Epoch 00352: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2634 - accuracy: 0.8884 - val_loss: 0.3961 - val_accuracy: 0.8153\n",
      "Epoch 353/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2635 - accuracy: 0.8865\n",
      "Epoch 00353: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2636 - accuracy: 0.8866 - val_loss: 0.4178 - val_accuracy: 0.8087\n",
      "Epoch 354/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.8834\n",
      "Epoch 00354: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2643 - accuracy: 0.8832 - val_loss: 0.4248 - val_accuracy: 0.8074\n",
      "Epoch 355/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.8864\n",
      "Epoch 00355: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2649 - accuracy: 0.8862 - val_loss: 0.4123 - val_accuracy: 0.8115\n",
      "Epoch 356/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.8834\n",
      "Epoch 00356: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2679 - accuracy: 0.8832 - val_loss: 0.3935 - val_accuracy: 0.8222\n",
      "Epoch 357/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.8843\n",
      "Epoch 00357: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2593 - accuracy: 0.8839 - val_loss: 0.4089 - val_accuracy: 0.8158\n",
      "Epoch 358/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.8861\n",
      "Epoch 00358: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2630 - accuracy: 0.8859 - val_loss: 0.4053 - val_accuracy: 0.8147\n",
      "Epoch 359/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.8885\n",
      "Epoch 00359: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2613 - accuracy: 0.8884 - val_loss: 0.4047 - val_accuracy: 0.8147\n",
      "Epoch 360/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.8855\n",
      "Epoch 00360: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2623 - accuracy: 0.8853 - val_loss: 0.4008 - val_accuracy: 0.8194\n",
      "Epoch 361/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.8867\n",
      "Epoch 00361: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2616 - accuracy: 0.8865 - val_loss: 0.4028 - val_accuracy: 0.8169\n",
      "Epoch 362/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8859\n",
      "Epoch 00362: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2594 - accuracy: 0.8858 - val_loss: 0.4315 - val_accuracy: 0.8037\n",
      "Epoch 363/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.8870\n",
      "Epoch 00363: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2582 - accuracy: 0.8867 - val_loss: 0.3986 - val_accuracy: 0.8184\n",
      "Epoch 364/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.8904\n",
      "Epoch 00364: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2566 - accuracy: 0.8902 - val_loss: 0.4502 - val_accuracy: 0.8005\n",
      "Epoch 365/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.8879\n",
      "Epoch 00365: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2615 - accuracy: 0.8878 - val_loss: 0.4454 - val_accuracy: 0.8020\n",
      "Epoch 366/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.8906\n",
      "Epoch 00366: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2553 - accuracy: 0.8903 - val_loss: 0.4223 - val_accuracy: 0.8091\n",
      "Epoch 367/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.8922\n",
      "Epoch 00367: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2557 - accuracy: 0.8922 - val_loss: 0.4052 - val_accuracy: 0.8166\n",
      "Epoch 368/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.8887\n",
      "Epoch 00368: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2549 - accuracy: 0.8886 - val_loss: 0.4428 - val_accuracy: 0.8035\n",
      "Epoch 369/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.8907\n",
      "Epoch 00369: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2552 - accuracy: 0.8901 - val_loss: 0.4079 - val_accuracy: 0.8199\n",
      "Epoch 370/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.8923\n",
      "Epoch 00370: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2539 - accuracy: 0.8922 - val_loss: 0.3946 - val_accuracy: 0.8214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.8958\n",
      "Epoch 00371: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2498 - accuracy: 0.8958 - val_loss: 0.3924 - val_accuracy: 0.8237\n",
      "Epoch 372/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2483 - accuracy: 0.8929\n",
      "Epoch 00372: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2483 - accuracy: 0.8930 - val_loss: 0.4075 - val_accuracy: 0.8173\n",
      "Epoch 373/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.8926\n",
      "Epoch 00373: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2513 - accuracy: 0.8923 - val_loss: 0.4665 - val_accuracy: 0.7945\n",
      "Epoch 374/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.8895\n",
      "Epoch 00374: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2505 - accuracy: 0.8893 - val_loss: 0.4083 - val_accuracy: 0.8158\n",
      "Epoch 375/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.8937\n",
      "Epoch 00375: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2494 - accuracy: 0.8933 - val_loss: 0.3980 - val_accuracy: 0.8209\n",
      "Epoch 376/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.8927\n",
      "Epoch 00376: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2461 - accuracy: 0.8929 - val_loss: 0.4027 - val_accuracy: 0.8222\n",
      "Epoch 377/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.8920\n",
      "Epoch 00377: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2492 - accuracy: 0.8919 - val_loss: 0.3972 - val_accuracy: 0.8246\n",
      "Epoch 378/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.8915\n",
      "Epoch 00378: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2486 - accuracy: 0.8915 - val_loss: 0.4280 - val_accuracy: 0.8065\n",
      "Epoch 379/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.8952\n",
      "Epoch 00379: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2477 - accuracy: 0.8951 - val_loss: 0.4014 - val_accuracy: 0.8188\n",
      "Epoch 380/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.8952\n",
      "Epoch 00380: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2451 - accuracy: 0.8948 - val_loss: 0.4315 - val_accuracy: 0.8100\n",
      "Epoch 381/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.8955\n",
      "Epoch 00381: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2429 - accuracy: 0.8954 - val_loss: 0.3994 - val_accuracy: 0.8250\n",
      "Epoch 382/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.8948\n",
      "Epoch 00382: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2464 - accuracy: 0.8948 - val_loss: 0.4386 - val_accuracy: 0.8046\n",
      "Epoch 383/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.8953\n",
      "Epoch 00383: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2423 - accuracy: 0.8952 - val_loss: 0.4157 - val_accuracy: 0.8136\n",
      "Epoch 384/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.8954\n",
      "Epoch 00384: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2445 - accuracy: 0.8950 - val_loss: 0.4355 - val_accuracy: 0.8005\n",
      "Epoch 385/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.8957\n",
      "Epoch 00385: val_loss did not improve from 0.38295\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2414 - accuracy: 0.8957 - val_loss: 0.4260 - val_accuracy: 0.8121\n",
      "Epoch 00385: early stopping\n",
      "Epoch 1/10000\n",
      "    291/Unknown - 10s 34ms/step - loss: 0.6920 - accuracy: 0.5234\n",
      "Epoch 00001: val_loss improved from inf to 0.69132, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 11s 38ms/step - loss: 0.6920 - accuracy: 0.5234 - val_loss: 0.6913 - val_accuracy: 0.5049\n",
      "Epoch 2/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5688\n",
      "Epoch 00002: val_loss improved from 0.69132 to 0.68808, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6861 - accuracy: 0.5690 - val_loss: 0.6881 - val_accuracy: 0.5185\n",
      "Epoch 3/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6785 - accuracy: 0.5770\n",
      "Epoch 00003: val_loss did not improve from 0.68808\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6785 - accuracy: 0.5767 - val_loss: 0.6923 - val_accuracy: 0.5086\n",
      "Epoch 4/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6729 - accuracy: 0.5837\n",
      "Epoch 00004: val_loss improved from 0.68808 to 0.67970, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6728 - accuracy: 0.5838 - val_loss: 0.6797 - val_accuracy: 0.5593\n",
      "Epoch 5/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6679 - accuracy: 0.5898\n",
      "Epoch 00005: val_loss improved from 0.67970 to 0.66542, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6677 - accuracy: 0.5902 - val_loss: 0.6654 - val_accuracy: 0.6000\n",
      "Epoch 6/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6636 - accuracy: 0.5969\n",
      "Epoch 00006: val_loss did not improve from 0.66542\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6634 - accuracy: 0.5970 - val_loss: 0.6680 - val_accuracy: 0.5862\n",
      "Epoch 7/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.6022\n",
      "Epoch 00007: val_loss improved from 0.66542 to 0.65767, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6584 - accuracy: 0.6024 - val_loss: 0.6577 - val_accuracy: 0.6068\n",
      "Epoch 8/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6536 - accuracy: 0.6075\n",
      "Epoch 00008: val_loss improved from 0.65767 to 0.65605, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6534 - accuracy: 0.6078 - val_loss: 0.6560 - val_accuracy: 0.6066\n",
      "Epoch 9/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6140\n",
      "Epoch 00009: val_loss improved from 0.65605 to 0.64680, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6492 - accuracy: 0.6143 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
      "Epoch 10/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6469 - accuracy: 0.6225\n",
      "Epoch 00010: val_loss improved from 0.64680 to 0.63926, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.6466 - accuracy: 0.6228 - val_loss: 0.6393 - val_accuracy: 0.6356\n",
      "Epoch 11/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6419 - accuracy: 0.6228\n",
      "Epoch 00011: val_loss improved from 0.63926 to 0.63308, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6417 - accuracy: 0.6229 - val_loss: 0.6331 - val_accuracy: 0.6369\n",
      "Epoch 12/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6407 - accuracy: 0.6247\n",
      "Epoch 00012: val_loss improved from 0.63308 to 0.63268, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6404 - accuracy: 0.6252 - val_loss: 0.6327 - val_accuracy: 0.6391\n",
      "Epoch 13/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6349 - accuracy: 0.6306\n",
      "Epoch 00013: val_loss did not improve from 0.63268\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6348 - accuracy: 0.6307 - val_loss: 0.6327 - val_accuracy: 0.6371\n",
      "Epoch 14/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6311 - accuracy: 0.6375\n",
      "Epoch 00014: val_loss improved from 0.63268 to 0.62363, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6308 - accuracy: 0.6380 - val_loss: 0.6236 - val_accuracy: 0.6442\n",
      "Epoch 15/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6250 - accuracy: 0.6462\n",
      "Epoch 00015: val_loss improved from 0.62363 to 0.61756, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6249 - accuracy: 0.6464 - val_loss: 0.6176 - val_accuracy: 0.6500\n",
      "Epoch 16/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.6558\n",
      "Epoch 00016: val_loss improved from 0.61756 to 0.60870, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6162 - accuracy: 0.6561 - val_loss: 0.6087 - val_accuracy: 0.6608\n",
      "Epoch 17/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6047 - accuracy: 0.6688\n",
      "Epoch 00017: val_loss improved from 0.60870 to 0.59776, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6047 - accuracy: 0.6686 - val_loss: 0.5978 - val_accuracy: 0.6735\n",
      "Epoch 18/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.6838\n",
      "Epoch 00018: val_loss improved from 0.59776 to 0.58553, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5895 - accuracy: 0.6836 - val_loss: 0.5855 - val_accuracy: 0.6892\n",
      "Epoch 19/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.6973\n",
      "Epoch 00019: val_loss improved from 0.58553 to 0.56948, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5783 - accuracy: 0.6972 - val_loss: 0.5695 - val_accuracy: 0.7001\n",
      "Epoch 20/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7080\n",
      "Epoch 00020: val_loss improved from 0.56948 to 0.56894, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5645 - accuracy: 0.7080 - val_loss: 0.5689 - val_accuracy: 0.6926\n",
      "Epoch 21/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7199\n",
      "Epoch 00021: val_loss improved from 0.56894 to 0.54810, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5496 - accuracy: 0.7194 - val_loss: 0.5481 - val_accuracy: 0.7173\n",
      "Epoch 22/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5406 - accuracy: 0.7280\n",
      "Epoch 00022: val_loss improved from 0.54810 to 0.52651, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5409 - accuracy: 0.7280 - val_loss: 0.5265 - val_accuracy: 0.7345\n",
      "Epoch 23/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5312 - accuracy: 0.7332\n",
      "Epoch 00023: val_loss improved from 0.52651 to 0.51234, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5317 - accuracy: 0.7329 - val_loss: 0.5123 - val_accuracy: 0.7515\n",
      "Epoch 24/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7381\n",
      "Epoch 00024: val_loss improved from 0.51234 to 0.50437, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5257 - accuracy: 0.7376 - val_loss: 0.5044 - val_accuracy: 0.7601\n",
      "Epoch 25/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5157 - accuracy: 0.7443\n",
      "Epoch 00025: val_loss improved from 0.50437 to 0.50229, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5163 - accuracy: 0.7437 - val_loss: 0.5023 - val_accuracy: 0.7655\n",
      "Epoch 26/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.7433\n",
      "Epoch 00026: val_loss improved from 0.50229 to 0.48620, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5144 - accuracy: 0.7433 - val_loss: 0.4862 - val_accuracy: 0.7683\n",
      "Epoch 27/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5047 - accuracy: 0.7514\n",
      "Epoch 00027: val_loss improved from 0.48620 to 0.48227, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5049 - accuracy: 0.7512 - val_loss: 0.4823 - val_accuracy: 0.7754\n",
      "Epoch 28/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5031 - accuracy: 0.7545\n",
      "Epoch 00028: val_loss did not improve from 0.48227\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5033 - accuracy: 0.7542 - val_loss: 0.4830 - val_accuracy: 0.7687\n",
      "Epoch 29/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7554\n",
      "Epoch 00029: val_loss improved from 0.48227 to 0.47922, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4979 - accuracy: 0.7550 - val_loss: 0.4792 - val_accuracy: 0.7756\n",
      "Epoch 30/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.7623\n",
      "Epoch 00030: val_loss improved from 0.47922 to 0.47305, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4919 - accuracy: 0.7617 - val_loss: 0.4731 - val_accuracy: 0.7844\n",
      "Epoch 31/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4908 - accuracy: 0.7641\n",
      "Epoch 00031: val_loss improved from 0.47305 to 0.46783, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4911 - accuracy: 0.7638 - val_loss: 0.4678 - val_accuracy: 0.7820\n",
      "Epoch 32/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7634\n",
      "Epoch 00032: val_loss improved from 0.46783 to 0.46489, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4852 - accuracy: 0.7631 - val_loss: 0.4649 - val_accuracy: 0.7861\n",
      "Epoch 33/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.7678\n",
      "Epoch 00033: val_loss did not improve from 0.46489\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4832 - accuracy: 0.7672 - val_loss: 0.4661 - val_accuracy: 0.7874\n",
      "Epoch 34/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4819 - accuracy: 0.7670\n",
      "Epoch 00034: val_loss improved from 0.46489 to 0.46028, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4822 - accuracy: 0.7667 - val_loss: 0.4603 - val_accuracy: 0.7837\n",
      "Epoch 35/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/291 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.7714\n",
      "Epoch 00035: val_loss did not improve from 0.46028\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4739 - accuracy: 0.7714 - val_loss: 0.4620 - val_accuracy: 0.7814\n",
      "Epoch 36/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4695 - accuracy: 0.7762\n",
      "Epoch 00036: val_loss improved from 0.46028 to 0.45223, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4702 - accuracy: 0.7756 - val_loss: 0.4522 - val_accuracy: 0.7923\n",
      "Epoch 37/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.7736\n",
      "Epoch 00037: val_loss did not improve from 0.45223\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4728 - accuracy: 0.7733 - val_loss: 0.4566 - val_accuracy: 0.7868\n",
      "Epoch 38/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.7784\n",
      "Epoch 00038: val_loss improved from 0.45223 to 0.44394, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4650 - accuracy: 0.7780 - val_loss: 0.4439 - val_accuracy: 0.7936\n",
      "Epoch 39/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.7797\n",
      "Epoch 00039: val_loss did not improve from 0.44394\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4638 - accuracy: 0.7789 - val_loss: 0.4472 - val_accuracy: 0.7951\n",
      "Epoch 40/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4615 - accuracy: 0.7787\n",
      "Epoch 00040: val_loss improved from 0.44394 to 0.44374, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4621 - accuracy: 0.7785 - val_loss: 0.4437 - val_accuracy: 0.7973\n",
      "Epoch 41/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.7836\n",
      "Epoch 00041: val_loss did not improve from 0.44374\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4565 - accuracy: 0.7833 - val_loss: 0.4491 - val_accuracy: 0.7917\n",
      "Epoch 42/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.7783\n",
      "Epoch 00042: val_loss did not improve from 0.44374\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4607 - accuracy: 0.7780 - val_loss: 0.4538 - val_accuracy: 0.7865\n",
      "Epoch 43/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4556 - accuracy: 0.7854\n",
      "Epoch 00043: val_loss improved from 0.44374 to 0.44047, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4560 - accuracy: 0.7850 - val_loss: 0.4405 - val_accuracy: 0.7945\n",
      "Epoch 44/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.7843\n",
      "Epoch 00044: val_loss did not improve from 0.44047\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4545 - accuracy: 0.7839 - val_loss: 0.4417 - val_accuracy: 0.7962\n",
      "Epoch 45/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.7827\n",
      "Epoch 00045: val_loss did not improve from 0.44047\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4526 - accuracy: 0.7821 - val_loss: 0.4462 - val_accuracy: 0.7945\n",
      "Epoch 46/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4439 - accuracy: 0.7907\n",
      "Epoch 00046: val_loss improved from 0.44047 to 0.43731, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4443 - accuracy: 0.7904 - val_loss: 0.4373 - val_accuracy: 0.7949\n",
      "Epoch 47/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.7882\n",
      "Epoch 00047: val_loss improved from 0.43731 to 0.42775, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4469 - accuracy: 0.7876 - val_loss: 0.4277 - val_accuracy: 0.8044\n",
      "Epoch 48/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4439 - accuracy: 0.7909\n",
      "Epoch 00048: val_loss did not improve from 0.42775\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4441 - accuracy: 0.7908 - val_loss: 0.4327 - val_accuracy: 0.7988\n",
      "Epoch 49/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7904\n",
      "Epoch 00049: val_loss improved from 0.42775 to 0.42654, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4439 - accuracy: 0.7901 - val_loss: 0.4265 - val_accuracy: 0.8029\n",
      "Epoch 50/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.7911\n",
      "Epoch 00050: val_loss improved from 0.42654 to 0.42080, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4405 - accuracy: 0.7910 - val_loss: 0.4208 - val_accuracy: 0.8083\n",
      "Epoch 51/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.7931\n",
      "Epoch 00051: val_loss did not improve from 0.42080\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4374 - accuracy: 0.7930 - val_loss: 0.4311 - val_accuracy: 0.7988\n",
      "Epoch 52/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7912\n",
      "Epoch 00052: val_loss did not improve from 0.42080\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4393 - accuracy: 0.7913 - val_loss: 0.4258 - val_accuracy: 0.8035\n",
      "Epoch 53/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.7971\n",
      "Epoch 00053: val_loss did not improve from 0.42080\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4338 - accuracy: 0.7970 - val_loss: 0.4246 - val_accuracy: 0.8040\n",
      "Epoch 54/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7987\n",
      "Epoch 00054: val_loss improved from 0.42080 to 0.41263, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4298 - accuracy: 0.7984 - val_loss: 0.4126 - val_accuracy: 0.8093\n",
      "Epoch 55/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.7962\n",
      "Epoch 00055: val_loss did not improve from 0.41263\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4307 - accuracy: 0.7959 - val_loss: 0.4202 - val_accuracy: 0.8025\n",
      "Epoch 56/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.7988\n",
      "Epoch 00056: val_loss did not improve from 0.41263\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4268 - accuracy: 0.7985 - val_loss: 0.4162 - val_accuracy: 0.8074\n",
      "Epoch 57/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8026\n",
      "Epoch 00057: val_loss did not improve from 0.41263\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4247 - accuracy: 0.8026 - val_loss: 0.4151 - val_accuracy: 0.8061\n",
      "Epoch 58/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4230 - accuracy: 0.8015\n",
      "Epoch 00058: val_loss did not improve from 0.41263\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4235 - accuracy: 0.8010 - val_loss: 0.4180 - val_accuracy: 0.8072\n",
      "Epoch 59/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.7994\n",
      "Epoch 00059: val_loss improved from 0.41263 to 0.40573, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4213 - accuracy: 0.7995 - val_loss: 0.4057 - val_accuracy: 0.8126\n",
      "Epoch 60/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8046\n",
      "Epoch 00060: val_loss did not improve from 0.40573\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4168 - accuracy: 0.8041 - val_loss: 0.4257 - val_accuracy: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4166 - accuracy: 0.8061\n",
      "Epoch 00061: val_loss did not improve from 0.40573\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4168 - accuracy: 0.8060 - val_loss: 0.4077 - val_accuracy: 0.8113\n",
      "Epoch 62/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8061\n",
      "Epoch 00062: val_loss did not improve from 0.40573\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4179 - accuracy: 0.8062 - val_loss: 0.4200 - val_accuracy: 0.8012\n",
      "Epoch 63/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4122 - accuracy: 0.8102\n",
      "Epoch 00063: val_loss improved from 0.40573 to 0.40416, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4125 - accuracy: 0.8101 - val_loss: 0.4042 - val_accuracy: 0.8128\n",
      "Epoch 64/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8116\n",
      "Epoch 00064: val_loss improved from 0.40416 to 0.40402, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4119 - accuracy: 0.8115 - val_loss: 0.4040 - val_accuracy: 0.8087\n",
      "Epoch 65/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8115\n",
      "Epoch 00065: val_loss did not improve from 0.40402\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4081 - accuracy: 0.8116 - val_loss: 0.4045 - val_accuracy: 0.8117\n",
      "Epoch 66/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4094 - accuracy: 0.8102\n",
      "Epoch 00066: val_loss improved from 0.40402 to 0.39941, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4097 - accuracy: 0.8099 - val_loss: 0.3994 - val_accuracy: 0.8184\n",
      "Epoch 67/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8113\n",
      "Epoch 00067: val_loss did not improve from 0.39941\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4083 - accuracy: 0.8111 - val_loss: 0.4034 - val_accuracy: 0.8123\n",
      "Epoch 68/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4041 - accuracy: 0.8140\n",
      "Epoch 00068: val_loss did not improve from 0.39941\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4043 - accuracy: 0.8140 - val_loss: 0.4016 - val_accuracy: 0.8169\n",
      "Epoch 69/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8116\n",
      "Epoch 00069: val_loss improved from 0.39941 to 0.39554, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4048 - accuracy: 0.8112 - val_loss: 0.3955 - val_accuracy: 0.8169\n",
      "Epoch 70/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8175\n",
      "Epoch 00070: val_loss did not improve from 0.39554\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3992 - accuracy: 0.8174 - val_loss: 0.4045 - val_accuracy: 0.8087\n",
      "Epoch 71/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8153\n",
      "Epoch 00071: val_loss did not improve from 0.39554\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3997 - accuracy: 0.8152 - val_loss: 0.3991 - val_accuracy: 0.8177\n",
      "Epoch 72/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8195\n",
      "Epoch 00072: val_loss did not improve from 0.39554\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3953 - accuracy: 0.8196 - val_loss: 0.4061 - val_accuracy: 0.8091\n",
      "Epoch 73/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.8153\n",
      "Epoch 00073: val_loss did not improve from 0.39554\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3992 - accuracy: 0.8150 - val_loss: 0.3994 - val_accuracy: 0.8169\n",
      "Epoch 74/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8172\n",
      "Epoch 00074: val_loss did not improve from 0.39554\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3942 - accuracy: 0.8170 - val_loss: 0.3973 - val_accuracy: 0.8104\n",
      "Epoch 75/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8219\n",
      "Epoch 00075: val_loss improved from 0.39554 to 0.39418, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3913 - accuracy: 0.8217 - val_loss: 0.3942 - val_accuracy: 0.8164\n",
      "Epoch 76/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3930 - accuracy: 0.8203\n",
      "Epoch 00076: val_loss did not improve from 0.39418\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3937 - accuracy: 0.8199 - val_loss: 0.3972 - val_accuracy: 0.8143\n",
      "Epoch 77/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8207\n",
      "Epoch 00077: val_loss improved from 0.39418 to 0.39013, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3877 - accuracy: 0.8208 - val_loss: 0.3901 - val_accuracy: 0.8205\n",
      "Epoch 78/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8241\n",
      "Epoch 00078: val_loss did not improve from 0.39013\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3857 - accuracy: 0.8240 - val_loss: 0.3913 - val_accuracy: 0.8237\n",
      "Epoch 79/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3879 - accuracy: 0.8204\n",
      "Epoch 00079: val_loss improved from 0.39013 to 0.38916, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3880 - accuracy: 0.8203 - val_loss: 0.3892 - val_accuracy: 0.8214\n",
      "Epoch 80/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3857 - accuracy: 0.8222\n",
      "Epoch 00080: val_loss improved from 0.38916 to 0.38356, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3858 - accuracy: 0.8222 - val_loss: 0.3836 - val_accuracy: 0.8274\n",
      "Epoch 81/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8209\n",
      "Epoch 00081: val_loss did not improve from 0.38356\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3849 - accuracy: 0.8208 - val_loss: 0.3848 - val_accuracy: 0.8227\n",
      "Epoch 82/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8248\n",
      "Epoch 00082: val_loss did not improve from 0.38356\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3827 - accuracy: 0.8248 - val_loss: 0.3841 - val_accuracy: 0.8263\n",
      "Epoch 83/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8241\n",
      "Epoch 00083: val_loss improved from 0.38356 to 0.38033, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3840 - accuracy: 0.8235 - val_loss: 0.3803 - val_accuracy: 0.8265\n",
      "Epoch 84/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8254\n",
      "Epoch 00084: val_loss did not improve from 0.38033\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3831 - accuracy: 0.8256 - val_loss: 0.3924 - val_accuracy: 0.8201\n",
      "Epoch 85/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8268\n",
      "Epoch 00085: val_loss improved from 0.38033 to 0.37891, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3789 - accuracy: 0.8269 - val_loss: 0.3789 - val_accuracy: 0.8272\n",
      "Epoch 86/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8289\n",
      "Epoch 00086: val_loss improved from 0.37891 to 0.37810, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3743 - accuracy: 0.8286 - val_loss: 0.3781 - val_accuracy: 0.8270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.8304\n",
      "Epoch 00087: val_loss improved from 0.37810 to 0.37701, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3737 - accuracy: 0.8304 - val_loss: 0.3770 - val_accuracy: 0.8280\n",
      "Epoch 88/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8294\n",
      "Epoch 00088: val_loss improved from 0.37701 to 0.37317, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3772 - accuracy: 0.8292 - val_loss: 0.3732 - val_accuracy: 0.8278\n",
      "Epoch 89/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8316\n",
      "Epoch 00089: val_loss did not improve from 0.37317\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3647 - accuracy: 0.8316 - val_loss: 0.3766 - val_accuracy: 0.8250\n",
      "Epoch 90/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.8345\n",
      "Epoch 00090: val_loss did not improve from 0.37317\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3699 - accuracy: 0.8342 - val_loss: 0.3744 - val_accuracy: 0.8287\n",
      "Epoch 91/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8316\n",
      "Epoch 00091: val_loss did not improve from 0.37317\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3703 - accuracy: 0.8315 - val_loss: 0.3758 - val_accuracy: 0.8293\n",
      "Epoch 92/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3674 - accuracy: 0.8337\n",
      "Epoch 00092: val_loss improved from 0.37317 to 0.37210, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3679 - accuracy: 0.8336 - val_loss: 0.3721 - val_accuracy: 0.8313\n",
      "Epoch 93/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8346\n",
      "Epoch 00093: val_loss did not improve from 0.37210\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3636 - accuracy: 0.8344 - val_loss: 0.3744 - val_accuracy: 0.8285\n",
      "Epoch 94/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8351\n",
      "Epoch 00094: val_loss did not improve from 0.37210\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3670 - accuracy: 0.8348 - val_loss: 0.3739 - val_accuracy: 0.8308\n",
      "Epoch 95/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8352\n",
      "Epoch 00095: val_loss improved from 0.37210 to 0.36967, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3605 - accuracy: 0.8352 - val_loss: 0.3697 - val_accuracy: 0.8295\n",
      "Epoch 96/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8381\n",
      "Epoch 00096: val_loss did not improve from 0.36967\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3587 - accuracy: 0.8382 - val_loss: 0.3845 - val_accuracy: 0.8214\n",
      "Epoch 97/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8371\n",
      "Epoch 00097: val_loss did not improve from 0.36967\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3614 - accuracy: 0.8370 - val_loss: 0.3715 - val_accuracy: 0.8285\n",
      "Epoch 98/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8388\n",
      "Epoch 00098: val_loss improved from 0.36967 to 0.36763, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3565 - accuracy: 0.8388 - val_loss: 0.3676 - val_accuracy: 0.8313\n",
      "Epoch 99/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.8380\n",
      "Epoch 00099: val_loss improved from 0.36763 to 0.36742, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3604 - accuracy: 0.8377 - val_loss: 0.3674 - val_accuracy: 0.8338\n",
      "Epoch 100/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.8378\n",
      "Epoch 00100: val_loss did not improve from 0.36742\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3572 - accuracy: 0.8377 - val_loss: 0.3687 - val_accuracy: 0.8330\n",
      "Epoch 101/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8394\n",
      "Epoch 00101: val_loss did not improve from 0.36742\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3553 - accuracy: 0.8392 - val_loss: 0.3676 - val_accuracy: 0.8291\n",
      "Epoch 102/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8435\n",
      "Epoch 00102: val_loss did not improve from 0.36742\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3506 - accuracy: 0.8431 - val_loss: 0.3708 - val_accuracy: 0.8255\n",
      "Epoch 103/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8438\n",
      "Epoch 00103: val_loss did not improve from 0.36742\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3524 - accuracy: 0.8435 - val_loss: 0.3702 - val_accuracy: 0.8270\n",
      "Epoch 104/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8423\n",
      "Epoch 00104: val_loss did not improve from 0.36742\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.3495 - accuracy: 0.8423 - val_loss: 0.3714 - val_accuracy: 0.8300\n",
      "Epoch 105/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8422\n",
      "Epoch 00105: val_loss improved from 0.36742 to 0.36572, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3502 - accuracy: 0.8421 - val_loss: 0.3657 - val_accuracy: 0.8360\n",
      "Epoch 106/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.8464\n",
      "Epoch 00106: val_loss improved from 0.36572 to 0.36500, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3474 - accuracy: 0.8463 - val_loss: 0.3650 - val_accuracy: 0.8345\n",
      "Epoch 107/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8460\n",
      "Epoch 00107: val_loss improved from 0.36500 to 0.36318, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3450 - accuracy: 0.8456 - val_loss: 0.3632 - val_accuracy: 0.8345\n",
      "Epoch 108/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8455\n",
      "Epoch 00108: val_loss did not improve from 0.36318\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3446 - accuracy: 0.8455 - val_loss: 0.3749 - val_accuracy: 0.8270\n",
      "Epoch 109/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3423 - accuracy: 0.8467\n",
      "Epoch 00109: val_loss did not improve from 0.36318\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3424 - accuracy: 0.8467 - val_loss: 0.3655 - val_accuracy: 0.8336\n",
      "Epoch 110/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8469\n",
      "Epoch 00110: val_loss improved from 0.36318 to 0.36087, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3390 - accuracy: 0.8466 - val_loss: 0.3609 - val_accuracy: 0.8330\n",
      "Epoch 111/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8484\n",
      "Epoch 00111: val_loss did not improve from 0.36087\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3395 - accuracy: 0.8483 - val_loss: 0.3635 - val_accuracy: 0.8310\n",
      "Epoch 112/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8503\n",
      "Epoch 00112: val_loss did not improve from 0.36087\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3353 - accuracy: 0.8506 - val_loss: 0.3677 - val_accuracy: 0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8480\n",
      "Epoch 00113: val_loss improved from 0.36087 to 0.35839, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3362 - accuracy: 0.8477 - val_loss: 0.3584 - val_accuracy: 0.8353\n",
      "Epoch 114/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8519\n",
      "Epoch 00114: val_loss did not improve from 0.35839\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3337 - accuracy: 0.8521 - val_loss: 0.3630 - val_accuracy: 0.8356\n",
      "Epoch 115/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.8539\n",
      "Epoch 00115: val_loss did not improve from 0.35839\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3307 - accuracy: 0.8539 - val_loss: 0.3666 - val_accuracy: 0.8289\n",
      "Epoch 116/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8518\n",
      "Epoch 00116: val_loss did not improve from 0.35839\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3344 - accuracy: 0.8516 - val_loss: 0.3595 - val_accuracy: 0.8336\n",
      "Epoch 117/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.8522\n",
      "Epoch 00117: val_loss did not improve from 0.35839\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3323 - accuracy: 0.8521 - val_loss: 0.3669 - val_accuracy: 0.8332\n",
      "Epoch 118/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8522\n",
      "Epoch 00118: val_loss improved from 0.35839 to 0.35619, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3313 - accuracy: 0.8523 - val_loss: 0.3562 - val_accuracy: 0.8349\n",
      "Epoch 119/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8528\n",
      "Epoch 00119: val_loss did not improve from 0.35619\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3254 - accuracy: 0.8526 - val_loss: 0.3578 - val_accuracy: 0.8345\n",
      "Epoch 120/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8525\n",
      "Epoch 00120: val_loss did not improve from 0.35619\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3289 - accuracy: 0.8523 - val_loss: 0.3569 - val_accuracy: 0.8347\n",
      "Epoch 121/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8566\n",
      "Epoch 00121: val_loss improved from 0.35619 to 0.35547, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3234 - accuracy: 0.8566 - val_loss: 0.3555 - val_accuracy: 0.8349\n",
      "Epoch 122/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8563\n",
      "Epoch 00122: val_loss did not improve from 0.35547\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3225 - accuracy: 0.8563 - val_loss: 0.3604 - val_accuracy: 0.8317\n",
      "Epoch 123/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8577\n",
      "Epoch 00123: val_loss did not improve from 0.35547\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3216 - accuracy: 0.8575 - val_loss: 0.3673 - val_accuracy: 0.8349\n",
      "Epoch 124/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8563\n",
      "Epoch 00124: val_loss did not improve from 0.35547\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3222 - accuracy: 0.8560 - val_loss: 0.3590 - val_accuracy: 0.8368\n",
      "Epoch 125/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8561\n",
      "Epoch 00125: val_loss did not improve from 0.35547\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3236 - accuracy: 0.8560 - val_loss: 0.3585 - val_accuracy: 0.8401\n",
      "Epoch 126/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8591\n",
      "Epoch 00126: val_loss improved from 0.35547 to 0.35399, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3204 - accuracy: 0.8592 - val_loss: 0.3540 - val_accuracy: 0.8360\n",
      "Epoch 127/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8594\n",
      "Epoch 00127: val_loss did not improve from 0.35399\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3196 - accuracy: 0.8592 - val_loss: 0.3565 - val_accuracy: 0.8418\n",
      "Epoch 128/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8669\n",
      "Epoch 00128: val_loss did not improve from 0.35399\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3062 - accuracy: 0.8670 - val_loss: 0.3600 - val_accuracy: 0.8401\n",
      "Epoch 129/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8629\n",
      "Epoch 00129: val_loss improved from 0.35399 to 0.35117, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3137 - accuracy: 0.8628 - val_loss: 0.3512 - val_accuracy: 0.8403\n",
      "Epoch 130/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8612\n",
      "Epoch 00130: val_loss did not improve from 0.35117\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3132 - accuracy: 0.8610 - val_loss: 0.3610 - val_accuracy: 0.8394\n",
      "Epoch 131/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3113 - accuracy: 0.8627\n",
      "Epoch 00131: val_loss did not improve from 0.35117\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3112 - accuracy: 0.8627 - val_loss: 0.3639 - val_accuracy: 0.8300\n",
      "Epoch 132/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8646\n",
      "Epoch 00132: val_loss did not improve from 0.35117\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3117 - accuracy: 0.8646 - val_loss: 0.3638 - val_accuracy: 0.8351\n",
      "Epoch 133/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8612\n",
      "Epoch 00133: val_loss improved from 0.35117 to 0.34970, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3157 - accuracy: 0.8614 - val_loss: 0.3497 - val_accuracy: 0.8411\n",
      "Epoch 134/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8658\n",
      "Epoch 00134: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3065 - accuracy: 0.8658 - val_loss: 0.3523 - val_accuracy: 0.8437\n",
      "Epoch 135/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8660\n",
      "Epoch 00135: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3039 - accuracy: 0.8663 - val_loss: 0.3594 - val_accuracy: 0.8399\n",
      "Epoch 136/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8660\n",
      "Epoch 00136: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3046 - accuracy: 0.8664 - val_loss: 0.3568 - val_accuracy: 0.8377\n",
      "Epoch 137/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8660\n",
      "Epoch 00137: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3038 - accuracy: 0.8658 - val_loss: 0.3574 - val_accuracy: 0.8392\n",
      "Epoch 138/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8648\n",
      "Epoch 00138: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3059 - accuracy: 0.8649 - val_loss: 0.3511 - val_accuracy: 0.8418\n",
      "Epoch 139/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8651\n",
      "Epoch 00139: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3016 - accuracy: 0.8651 - val_loss: 0.3580 - val_accuracy: 0.8371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8671\n",
      "Epoch 00140: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3015 - accuracy: 0.8670 - val_loss: 0.3579 - val_accuracy: 0.8330\n",
      "Epoch 141/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8674\n",
      "Epoch 00141: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2971 - accuracy: 0.8673 - val_loss: 0.3544 - val_accuracy: 0.8396\n",
      "Epoch 142/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8673\n",
      "Epoch 00142: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2987 - accuracy: 0.8675 - val_loss: 0.3667 - val_accuracy: 0.8302\n",
      "Epoch 143/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8740\n",
      "Epoch 00143: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2949 - accuracy: 0.8737 - val_loss: 0.3574 - val_accuracy: 0.8390\n",
      "Epoch 144/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8716\n",
      "Epoch 00144: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2952 - accuracy: 0.8714 - val_loss: 0.3568 - val_accuracy: 0.8375\n",
      "Epoch 145/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8704\n",
      "Epoch 00145: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2950 - accuracy: 0.8705 - val_loss: 0.3641 - val_accuracy: 0.8379\n",
      "Epoch 146/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8739\n",
      "Epoch 00146: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2932 - accuracy: 0.8737 - val_loss: 0.3613 - val_accuracy: 0.8358\n",
      "Epoch 147/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8717\n",
      "Epoch 00147: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2924 - accuracy: 0.8714 - val_loss: 0.3522 - val_accuracy: 0.8379\n",
      "Epoch 148/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8720\n",
      "Epoch 00148: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2949 - accuracy: 0.8722 - val_loss: 0.3560 - val_accuracy: 0.8444\n",
      "Epoch 149/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.8739\n",
      "Epoch 00149: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2882 - accuracy: 0.8742 - val_loss: 0.3521 - val_accuracy: 0.8405\n",
      "Epoch 150/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8739\n",
      "Epoch 00150: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2905 - accuracy: 0.8737 - val_loss: 0.3572 - val_accuracy: 0.8377\n",
      "Epoch 151/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.8757\n",
      "Epoch 00151: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2883 - accuracy: 0.8758 - val_loss: 0.3515 - val_accuracy: 0.8452\n",
      "Epoch 152/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.8754\n",
      "Epoch 00152: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2821 - accuracy: 0.8756 - val_loss: 0.3519 - val_accuracy: 0.8446\n",
      "Epoch 153/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.8778\n",
      "Epoch 00153: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2812 - accuracy: 0.8779 - val_loss: 0.3539 - val_accuracy: 0.8454\n",
      "Epoch 154/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.8782\n",
      "Epoch 00154: val_loss did not improve from 0.34970\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2807 - accuracy: 0.8783 - val_loss: 0.3528 - val_accuracy: 0.8465\n",
      "Epoch 155/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2816 - accuracy: 0.8767\n",
      "Epoch 00155: val_loss improved from 0.34970 to 0.34969, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2820 - accuracy: 0.8765 - val_loss: 0.3497 - val_accuracy: 0.8467\n",
      "Epoch 156/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.8753\n",
      "Epoch 00156: val_loss did not improve from 0.34969\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2833 - accuracy: 0.8756 - val_loss: 0.3618 - val_accuracy: 0.8356\n",
      "Epoch 157/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2760 - accuracy: 0.8793\n",
      "Epoch 00157: val_loss did not improve from 0.34969\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2762 - accuracy: 0.8790 - val_loss: 0.3589 - val_accuracy: 0.8401\n",
      "Epoch 158/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8798\n",
      "Epoch 00158: val_loss did not improve from 0.34969\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2769 - accuracy: 0.8798 - val_loss: 0.3570 - val_accuracy: 0.8409\n",
      "Epoch 159/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2781 - accuracy: 0.8810\n",
      "Epoch 00159: val_loss did not improve from 0.34969\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2780 - accuracy: 0.8810 - val_loss: 0.3683 - val_accuracy: 0.8323\n",
      "Epoch 160/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8809\n",
      "Epoch 00160: val_loss did not improve from 0.34969\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2738 - accuracy: 0.8809 - val_loss: 0.3554 - val_accuracy: 0.8459\n",
      "Epoch 161/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.8821\n",
      "Epoch 00161: val_loss improved from 0.34969 to 0.34904, saving model to pickled_objects/batch_size_64_lr_0.02_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2699 - accuracy: 0.8823 - val_loss: 0.3490 - val_accuracy: 0.8474\n",
      "Epoch 162/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.8773\n",
      "Epoch 00162: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2797 - accuracy: 0.8775 - val_loss: 0.3682 - val_accuracy: 0.8334\n",
      "Epoch 163/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.8842\n",
      "Epoch 00163: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2685 - accuracy: 0.8843 - val_loss: 0.3565 - val_accuracy: 0.8375\n",
      "Epoch 164/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.8875\n",
      "Epoch 00164: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2672 - accuracy: 0.8873 - val_loss: 0.3655 - val_accuracy: 0.8407\n",
      "Epoch 165/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8833\n",
      "Epoch 00165: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2680 - accuracy: 0.8835 - val_loss: 0.3552 - val_accuracy: 0.8495\n",
      "Epoch 166/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.8826\n",
      "Epoch 00166: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2705 - accuracy: 0.8825 - val_loss: 0.3552 - val_accuracy: 0.8480\n",
      "Epoch 167/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.8884\n",
      "Epoch 00167: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2622 - accuracy: 0.8883 - val_loss: 0.3551 - val_accuracy: 0.8497\n",
      "Epoch 168/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2691 - accuracy: 0.8811\n",
      "Epoch 00168: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2696 - accuracy: 0.8808 - val_loss: 0.3809 - val_accuracy: 0.8381\n",
      "Epoch 169/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8887\n",
      "Epoch 00169: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2616 - accuracy: 0.8888 - val_loss: 0.3537 - val_accuracy: 0.8452\n",
      "Epoch 170/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.8864\n",
      "Epoch 00170: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2625 - accuracy: 0.8860 - val_loss: 0.3531 - val_accuracy: 0.8450\n",
      "Epoch 171/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.8868\n",
      "Epoch 00171: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2636 - accuracy: 0.8867 - val_loss: 0.3598 - val_accuracy: 0.8409\n",
      "Epoch 172/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8881\n",
      "Epoch 00172: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2597 - accuracy: 0.8878 - val_loss: 0.3665 - val_accuracy: 0.8429\n",
      "Epoch 173/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8861\n",
      "Epoch 00173: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2637 - accuracy: 0.8860 - val_loss: 0.3605 - val_accuracy: 0.8377\n",
      "Epoch 174/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.8866\n",
      "Epoch 00174: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2600 - accuracy: 0.8866 - val_loss: 0.3621 - val_accuracy: 0.8381\n",
      "Epoch 175/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.8867\n",
      "Epoch 00175: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2614 - accuracy: 0.8866 - val_loss: 0.3603 - val_accuracy: 0.8409\n",
      "Epoch 176/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.8919\n",
      "Epoch 00176: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2525 - accuracy: 0.8917 - val_loss: 0.3530 - val_accuracy: 0.8476\n",
      "Epoch 177/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.8903\n",
      "Epoch 00177: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2566 - accuracy: 0.8903 - val_loss: 0.3587 - val_accuracy: 0.8390\n",
      "Epoch 178/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.8913\n",
      "Epoch 00178: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2493 - accuracy: 0.8915 - val_loss: 0.3670 - val_accuracy: 0.8358\n",
      "Epoch 179/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.8920\n",
      "Epoch 00179: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2521 - accuracy: 0.8921 - val_loss: 0.3568 - val_accuracy: 0.8439\n",
      "Epoch 180/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.8940\n",
      "Epoch 00180: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2456 - accuracy: 0.8941 - val_loss: 0.3624 - val_accuracy: 0.8459\n",
      "Epoch 181/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8927\n",
      "Epoch 00181: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2510 - accuracy: 0.8928 - val_loss: 0.3570 - val_accuracy: 0.8463\n",
      "Epoch 182/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.8942\n",
      "Epoch 00182: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2502 - accuracy: 0.8941 - val_loss: 0.3634 - val_accuracy: 0.8463\n",
      "Epoch 183/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.8955\n",
      "Epoch 00183: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2486 - accuracy: 0.8958 - val_loss: 0.3685 - val_accuracy: 0.8381\n",
      "Epoch 184/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.8935\n",
      "Epoch 00184: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2482 - accuracy: 0.8932 - val_loss: 0.3592 - val_accuracy: 0.8457\n",
      "Epoch 185/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.8946\n",
      "Epoch 00185: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2467 - accuracy: 0.8946 - val_loss: 0.3688 - val_accuracy: 0.8373\n",
      "Epoch 186/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8967\n",
      "Epoch 00186: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2444 - accuracy: 0.8967 - val_loss: 0.3627 - val_accuracy: 0.8444\n",
      "Epoch 187/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.8968\n",
      "Epoch 00187: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2426 - accuracy: 0.8969 - val_loss: 0.3824 - val_accuracy: 0.8321\n",
      "Epoch 188/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.8987\n",
      "Epoch 00188: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2385 - accuracy: 0.8988 - val_loss: 0.3666 - val_accuracy: 0.8377\n",
      "Epoch 189/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.8961\n",
      "Epoch 00189: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2452 - accuracy: 0.8962 - val_loss: 0.3706 - val_accuracy: 0.8336\n",
      "Epoch 190/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9020\n",
      "Epoch 00190: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2325 - accuracy: 0.9022 - val_loss: 0.3772 - val_accuracy: 0.8377\n",
      "Epoch 191/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9026\n",
      "Epoch 00191: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2353 - accuracy: 0.9026 - val_loss: 0.3668 - val_accuracy: 0.8424\n",
      "Epoch 192/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.8988\n",
      "Epoch 00192: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2380 - accuracy: 0.8985 - val_loss: 0.3785 - val_accuracy: 0.8360\n",
      "Epoch 193/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9005\n",
      "Epoch 00193: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2355 - accuracy: 0.9004 - val_loss: 0.3767 - val_accuracy: 0.8330\n",
      "Epoch 194/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9031\n",
      "Epoch 00194: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2327 - accuracy: 0.9031 - val_loss: 0.3678 - val_accuracy: 0.8411\n",
      "Epoch 195/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.8976\n",
      "Epoch 00195: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2370 - accuracy: 0.8978 - val_loss: 0.3760 - val_accuracy: 0.8340\n",
      "Epoch 196/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2309 - accuracy: 0.9024\n",
      "Epoch 00196: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2313 - accuracy: 0.9023 - val_loss: 0.3679 - val_accuracy: 0.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9040\n",
      "Epoch 00197: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2303 - accuracy: 0.9041 - val_loss: 0.3734 - val_accuracy: 0.8381\n",
      "Epoch 198/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.9045\n",
      "Epoch 00198: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.2290 - accuracy: 0.9046 - val_loss: 0.3700 - val_accuracy: 0.8407\n",
      "Epoch 199/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9057\n",
      "Epoch 00199: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2271 - accuracy: 0.9058 - val_loss: 0.3828 - val_accuracy: 0.8366\n",
      "Epoch 200/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9033\n",
      "Epoch 00200: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2283 - accuracy: 0.9032 - val_loss: 0.3661 - val_accuracy: 0.8405\n",
      "Epoch 201/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9044\n",
      "Epoch 00201: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2283 - accuracy: 0.9042 - val_loss: 0.3722 - val_accuracy: 0.8422\n",
      "Epoch 202/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9064\n",
      "Epoch 00202: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2267 - accuracy: 0.9064 - val_loss: 0.3769 - val_accuracy: 0.8390\n",
      "Epoch 203/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9079\n",
      "Epoch 00203: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2202 - accuracy: 0.9075 - val_loss: 0.3812 - val_accuracy: 0.8300\n",
      "Epoch 204/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9042\n",
      "Epoch 00204: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2280 - accuracy: 0.9042 - val_loss: 0.3808 - val_accuracy: 0.8407\n",
      "Epoch 205/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9030\n",
      "Epoch 00205: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2260 - accuracy: 0.9030 - val_loss: 0.3769 - val_accuracy: 0.8394\n",
      "Epoch 206/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9051\n",
      "Epoch 00206: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2246 - accuracy: 0.9051 - val_loss: 0.3740 - val_accuracy: 0.8364\n",
      "Epoch 207/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.9068\n",
      "Epoch 00207: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2228 - accuracy: 0.9069 - val_loss: 0.3815 - val_accuracy: 0.8291\n",
      "Epoch 208/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2227 - accuracy: 0.9063\n",
      "Epoch 00208: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2231 - accuracy: 0.9062 - val_loss: 0.3708 - val_accuracy: 0.8429\n",
      "Epoch 209/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9108\n",
      "Epoch 00209: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2155 - accuracy: 0.9110 - val_loss: 0.3799 - val_accuracy: 0.8411\n",
      "Epoch 210/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9077\n",
      "Epoch 00210: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2206 - accuracy: 0.9075 - val_loss: 0.3889 - val_accuracy: 0.8310\n",
      "Epoch 211/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9112\n",
      "Epoch 00211: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2106 - accuracy: 0.9112 - val_loss: 0.3828 - val_accuracy: 0.8448\n",
      "Epoch 212/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2140 - accuracy: 0.9112\n",
      "Epoch 00212: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2141 - accuracy: 0.9111 - val_loss: 0.3835 - val_accuracy: 0.8368\n",
      "Epoch 213/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9100\n",
      "Epoch 00213: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2144 - accuracy: 0.9100 - val_loss: 0.3908 - val_accuracy: 0.8377\n",
      "Epoch 214/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9072\n",
      "Epoch 00214: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2168 - accuracy: 0.9072 - val_loss: 0.3759 - val_accuracy: 0.8390\n",
      "Epoch 215/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9117\n",
      "Epoch 00215: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2109 - accuracy: 0.9116 - val_loss: 0.3847 - val_accuracy: 0.8343\n",
      "Epoch 216/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9141\n",
      "Epoch 00216: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2094 - accuracy: 0.9141 - val_loss: 0.3768 - val_accuracy: 0.8375\n",
      "Epoch 217/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9108\n",
      "Epoch 00217: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2129 - accuracy: 0.9111 - val_loss: 0.3884 - val_accuracy: 0.8405\n",
      "Epoch 218/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9133\n",
      "Epoch 00218: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2090 - accuracy: 0.9133 - val_loss: 0.3939 - val_accuracy: 0.8340\n",
      "Epoch 219/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9154\n",
      "Epoch 00219: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2035 - accuracy: 0.9153 - val_loss: 0.3790 - val_accuracy: 0.8446\n",
      "Epoch 220/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9141\n",
      "Epoch 00220: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2046 - accuracy: 0.9139 - val_loss: 0.3870 - val_accuracy: 0.8390\n",
      "Epoch 221/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9113\n",
      "Epoch 00221: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2076 - accuracy: 0.9116 - val_loss: 0.3874 - val_accuracy: 0.8321\n",
      "Epoch 222/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9144\n",
      "Epoch 00222: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2070 - accuracy: 0.9145 - val_loss: 0.4018 - val_accuracy: 0.8289\n",
      "Epoch 223/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2062 - accuracy: 0.9141\n",
      "Epoch 00223: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2056 - accuracy: 0.9145 - val_loss: 0.3860 - val_accuracy: 0.8362\n",
      "Epoch 224/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9196\n",
      "Epoch 00224: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1945 - accuracy: 0.9195 - val_loss: 0.3826 - val_accuracy: 0.8416\n",
      "Epoch 225/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2024 - accuracy: 0.9168\n",
      "Epoch 00225: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2028 - accuracy: 0.9166 - val_loss: 0.3831 - val_accuracy: 0.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9191\n",
      "Epoch 00226: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1974 - accuracy: 0.9189 - val_loss: 0.3747 - val_accuracy: 0.8407\n",
      "Epoch 227/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9153\n",
      "Epoch 00227: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2021 - accuracy: 0.9152 - val_loss: 0.3834 - val_accuracy: 0.8388\n",
      "Epoch 228/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9197\n",
      "Epoch 00228: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1948 - accuracy: 0.9196 - val_loss: 0.4054 - val_accuracy: 0.8300\n",
      "Epoch 229/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9184\n",
      "Epoch 00229: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1975 - accuracy: 0.9183 - val_loss: 0.3860 - val_accuracy: 0.8377\n",
      "Epoch 230/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9166\n",
      "Epoch 00230: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1996 - accuracy: 0.9165 - val_loss: 0.3814 - val_accuracy: 0.8409\n",
      "Epoch 231/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9190\n",
      "Epoch 00231: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1956 - accuracy: 0.9188 - val_loss: 0.3814 - val_accuracy: 0.8401\n",
      "Epoch 232/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9190\n",
      "Epoch 00232: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1949 - accuracy: 0.9189 - val_loss: 0.3763 - val_accuracy: 0.8454\n",
      "Epoch 233/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9213\n",
      "Epoch 00233: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1945 - accuracy: 0.9212 - val_loss: 0.3857 - val_accuracy: 0.8373\n",
      "Epoch 234/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9202\n",
      "Epoch 00234: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1926 - accuracy: 0.9202 - val_loss: 0.3980 - val_accuracy: 0.8315\n",
      "Epoch 235/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9221\n",
      "Epoch 00235: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1903 - accuracy: 0.9220 - val_loss: 0.3824 - val_accuracy: 0.8381\n",
      "Epoch 236/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9208\n",
      "Epoch 00236: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1915 - accuracy: 0.9206 - val_loss: 0.3867 - val_accuracy: 0.8379\n",
      "Epoch 237/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9208\n",
      "Epoch 00237: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1897 - accuracy: 0.9211 - val_loss: 0.3905 - val_accuracy: 0.8420\n",
      "Epoch 238/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9203\n",
      "Epoch 00238: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1948 - accuracy: 0.9203 - val_loss: 0.3855 - val_accuracy: 0.8435\n",
      "Epoch 239/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9256\n",
      "Epoch 00239: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1833 - accuracy: 0.9257 - val_loss: 0.3932 - val_accuracy: 0.8375\n",
      "Epoch 240/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9224\n",
      "Epoch 00240: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1890 - accuracy: 0.9223 - val_loss: 0.3851 - val_accuracy: 0.8411\n",
      "Epoch 241/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9243\n",
      "Epoch 00241: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1837 - accuracy: 0.9244 - val_loss: 0.4038 - val_accuracy: 0.8351\n",
      "Epoch 242/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9250\n",
      "Epoch 00242: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1817 - accuracy: 0.9250 - val_loss: 0.3864 - val_accuracy: 0.8426\n",
      "Epoch 243/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9245\n",
      "Epoch 00243: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1873 - accuracy: 0.9244 - val_loss: 0.3871 - val_accuracy: 0.8426\n",
      "Epoch 244/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9235\n",
      "Epoch 00244: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1824 - accuracy: 0.9235 - val_loss: 0.3980 - val_accuracy: 0.8332\n",
      "Epoch 245/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9239\n",
      "Epoch 00245: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1857 - accuracy: 0.9239 - val_loss: 0.3912 - val_accuracy: 0.8351\n",
      "Epoch 246/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9257\n",
      "Epoch 00246: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1816 - accuracy: 0.9256 - val_loss: 0.3866 - val_accuracy: 0.8371\n",
      "Epoch 247/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9265\n",
      "Epoch 00247: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1806 - accuracy: 0.9264 - val_loss: 0.3922 - val_accuracy: 0.8390\n",
      "Epoch 248/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9266\n",
      "Epoch 00248: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1774 - accuracy: 0.9265 - val_loss: 0.3994 - val_accuracy: 0.8349\n",
      "Epoch 249/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9262\n",
      "Epoch 00249: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1810 - accuracy: 0.9264 - val_loss: 0.4200 - val_accuracy: 0.8353\n",
      "Epoch 250/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9262\n",
      "Epoch 00250: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1814 - accuracy: 0.9263 - val_loss: 0.4201 - val_accuracy: 0.8267\n",
      "Epoch 251/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9298\n",
      "Epoch 00251: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1727 - accuracy: 0.9297 - val_loss: 0.3968 - val_accuracy: 0.8401\n",
      "Epoch 252/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.9278\n",
      "Epoch 00252: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1821 - accuracy: 0.9279 - val_loss: 0.4048 - val_accuracy: 0.8330\n",
      "Epoch 253/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9304\n",
      "Epoch 00253: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1745 - accuracy: 0.9298 - val_loss: 0.3961 - val_accuracy: 0.8356\n",
      "Epoch 254/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1753 - accuracy: 0.9278\n",
      "Epoch 00254: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1749 - accuracy: 0.9280 - val_loss: 0.3989 - val_accuracy: 0.8360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9321\n",
      "Epoch 00255: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1686 - accuracy: 0.9321 - val_loss: 0.4194 - val_accuracy: 0.8280\n",
      "Epoch 256/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9298\n",
      "Epoch 00256: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1722 - accuracy: 0.9300 - val_loss: 0.3927 - val_accuracy: 0.8399\n",
      "Epoch 257/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9311\n",
      "Epoch 00257: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1698 - accuracy: 0.9311 - val_loss: 0.4186 - val_accuracy: 0.8353\n",
      "Epoch 258/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9317\n",
      "Epoch 00258: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1706 - accuracy: 0.9316 - val_loss: 0.4081 - val_accuracy: 0.8302\n",
      "Epoch 259/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9257\n",
      "Epoch 00259: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1786 - accuracy: 0.9257 - val_loss: 0.4011 - val_accuracy: 0.8319\n",
      "Epoch 260/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9314\n",
      "Epoch 00260: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1678 - accuracy: 0.9315 - val_loss: 0.4040 - val_accuracy: 0.8317\n",
      "Epoch 261/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9338\n",
      "Epoch 00261: val_loss did not improve from 0.34904\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.1646 - accuracy: 0.9336 - val_loss: 0.4207 - val_accuracy: 0.8282\n",
      "Epoch 00261: early stopping\n",
      "Epoch 1/10000\n",
      "    291/Unknown - 10s 34ms/step - loss: 0.6900 - accuracy: 0.5330\n",
      "Epoch 00001: val_loss improved from inf to 0.68799, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 11s 38ms/step - loss: 0.6900 - accuracy: 0.5330 - val_loss: 0.6880 - val_accuracy: 0.5286\n",
      "Epoch 2/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5706\n",
      "Epoch 00002: val_loss improved from 0.68799 to 0.67077, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6801 - accuracy: 0.5707 - val_loss: 0.6708 - val_accuracy: 0.6111\n",
      "Epoch 3/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6748 - accuracy: 0.5798\n",
      "Epoch 00003: val_loss improved from 0.67077 to 0.66953, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6747 - accuracy: 0.5799 - val_loss: 0.6695 - val_accuracy: 0.5948\n",
      "Epoch 4/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6671 - accuracy: 0.5916\n",
      "Epoch 00004: val_loss improved from 0.66953 to 0.65976, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6670 - accuracy: 0.5918 - val_loss: 0.6598 - val_accuracy: 0.6105\n",
      "Epoch 5/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.6087\n",
      "Epoch 00005: val_loss improved from 0.65976 to 0.64494, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6576 - accuracy: 0.6091 - val_loss: 0.6449 - val_accuracy: 0.6382\n",
      "Epoch 6/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6510 - accuracy: 0.6136\n",
      "Epoch 00006: val_loss improved from 0.64494 to 0.64155, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6509 - accuracy: 0.6138 - val_loss: 0.6415 - val_accuracy: 0.6341\n",
      "Epoch 7/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.6284\n",
      "Epoch 00007: val_loss improved from 0.64155 to 0.62540, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6406 - accuracy: 0.6289 - val_loss: 0.6254 - val_accuracy: 0.6533\n",
      "Epoch 8/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6254 - accuracy: 0.6481\n",
      "Epoch 00008: val_loss improved from 0.62540 to 0.60982, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6252 - accuracy: 0.6484 - val_loss: 0.6098 - val_accuracy: 0.6668\n",
      "Epoch 9/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.6734\n",
      "Epoch 00009: val_loss improved from 0.60982 to 0.58055, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6023 - accuracy: 0.6735 - val_loss: 0.5806 - val_accuracy: 0.7003\n",
      "Epoch 10/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5771 - accuracy: 0.6964\n",
      "Epoch 00010: val_loss improved from 0.58055 to 0.55452, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5772 - accuracy: 0.6963 - val_loss: 0.5545 - val_accuracy: 0.7272\n",
      "Epoch 11/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5586 - accuracy: 0.7159\n",
      "Epoch 00011: val_loss improved from 0.55452 to 0.52746, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5590 - accuracy: 0.7155 - val_loss: 0.5275 - val_accuracy: 0.7444\n",
      "Epoch 12/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5443 - accuracy: 0.7245\n",
      "Epoch 00012: val_loss improved from 0.52746 to 0.51931, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5444 - accuracy: 0.7243 - val_loss: 0.5193 - val_accuracy: 0.7496\n",
      "Epoch 13/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5300 - accuracy: 0.7381\n",
      "Epoch 00013: val_loss improved from 0.51931 to 0.51287, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5304 - accuracy: 0.7375 - val_loss: 0.5129 - val_accuracy: 0.7485\n",
      "Epoch 14/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5215 - accuracy: 0.7384\n",
      "Epoch 00014: val_loss improved from 0.51287 to 0.49536, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5216 - accuracy: 0.7382 - val_loss: 0.4954 - val_accuracy: 0.7601\n",
      "Epoch 15/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7464\n",
      "Epoch 00015: val_loss did not improve from 0.49536\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5111 - accuracy: 0.7458 - val_loss: 0.5023 - val_accuracy: 0.7642\n",
      "Epoch 16/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7520\n",
      "Epoch 00016: val_loss improved from 0.49536 to 0.47735, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5041 - accuracy: 0.7520 - val_loss: 0.4773 - val_accuracy: 0.7797\n",
      "Epoch 17/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.7566\n",
      "Epoch 00017: val_loss did not improve from 0.47735\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4969 - accuracy: 0.7563 - val_loss: 0.4904 - val_accuracy: 0.7762\n",
      "Epoch 18/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.7631\n",
      "Epoch 00018: val_loss improved from 0.47735 to 0.47298, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4896 - accuracy: 0.7623 - val_loss: 0.4730 - val_accuracy: 0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4820 - accuracy: 0.7665\n",
      "Epoch 00019: val_loss did not improve from 0.47298\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4824 - accuracy: 0.7664 - val_loss: 0.4755 - val_accuracy: 0.7803\n",
      "Epoch 20/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4800 - accuracy: 0.7682\n",
      "Epoch 00020: val_loss improved from 0.47298 to 0.46616, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4802 - accuracy: 0.7679 - val_loss: 0.4662 - val_accuracy: 0.7833\n",
      "Epoch 21/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4724 - accuracy: 0.7727\n",
      "Epoch 00021: val_loss improved from 0.46616 to 0.45819, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4729 - accuracy: 0.7719 - val_loss: 0.4582 - val_accuracy: 0.7902\n",
      "Epoch 22/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.7713\n",
      "Epoch 00022: val_loss did not improve from 0.45819\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4729 - accuracy: 0.7709 - val_loss: 0.4606 - val_accuracy: 0.7868\n",
      "Epoch 23/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4646 - accuracy: 0.7741\n",
      "Epoch 00023: val_loss improved from 0.45819 to 0.44424, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4651 - accuracy: 0.7739 - val_loss: 0.4442 - val_accuracy: 0.7954\n",
      "Epoch 24/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4621 - accuracy: 0.7813\n",
      "Epoch 00024: val_loss did not improve from 0.44424\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4626 - accuracy: 0.7808 - val_loss: 0.4589 - val_accuracy: 0.7919\n",
      "Epoch 25/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4562 - accuracy: 0.7822\n",
      "Epoch 00025: val_loss did not improve from 0.44424\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4569 - accuracy: 0.7815 - val_loss: 0.4539 - val_accuracy: 0.7945\n",
      "Epoch 26/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.7824\n",
      "Epoch 00026: val_loss improved from 0.44424 to 0.43630, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4566 - accuracy: 0.7824 - val_loss: 0.4363 - val_accuracy: 0.7934\n",
      "Epoch 27/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4494 - accuracy: 0.7884\n",
      "Epoch 00027: val_loss improved from 0.43630 to 0.43461, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4497 - accuracy: 0.7880 - val_loss: 0.4346 - val_accuracy: 0.8003\n",
      "Epoch 28/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7902\n",
      "Epoch 00028: val_loss improved from 0.43461 to 0.43218, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.4322 - val_accuracy: 0.8014\n",
      "Epoch 29/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.7924\n",
      "Epoch 00029: val_loss did not improve from 0.43218\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4420 - accuracy: 0.7921 - val_loss: 0.4393 - val_accuracy: 0.7979\n",
      "Epoch 30/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4389 - accuracy: 0.7931\n",
      "Epoch 00030: val_loss improved from 0.43218 to 0.42514, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4392 - accuracy: 0.7926 - val_loss: 0.4251 - val_accuracy: 0.8072\n",
      "Epoch 31/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4351 - accuracy: 0.7973\n",
      "Epoch 00031: val_loss improved from 0.42514 to 0.42492, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4353 - accuracy: 0.7969 - val_loss: 0.4249 - val_accuracy: 0.8065\n",
      "Epoch 32/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.7963\n",
      "Epoch 00032: val_loss improved from 0.42492 to 0.42345, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4326 - accuracy: 0.7963 - val_loss: 0.4235 - val_accuracy: 0.8050\n",
      "Epoch 33/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7992\n",
      "Epoch 00033: val_loss improved from 0.42345 to 0.42154, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4293 - accuracy: 0.7990 - val_loss: 0.4215 - val_accuracy: 0.7988\n",
      "Epoch 34/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4284 - accuracy: 0.8011\n",
      "Epoch 00034: val_loss improved from 0.42154 to 0.41859, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4287 - accuracy: 0.8010 - val_loss: 0.4186 - val_accuracy: 0.8031\n",
      "Epoch 35/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8026\n",
      "Epoch 00035: val_loss improved from 0.41859 to 0.41511, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4232 - accuracy: 0.8021 - val_loss: 0.4151 - val_accuracy: 0.8085\n",
      "Epoch 36/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8044\n",
      "Epoch 00036: val_loss improved from 0.41511 to 0.41224, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4194 - accuracy: 0.8039 - val_loss: 0.4122 - val_accuracy: 0.8093\n",
      "Epoch 37/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8014\n",
      "Epoch 00037: val_loss did not improve from 0.41224\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4225 - accuracy: 0.8014 - val_loss: 0.4196 - val_accuracy: 0.8022\n",
      "Epoch 38/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4131 - accuracy: 0.8099\n",
      "Epoch 00038: val_loss did not improve from 0.41224\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4135 - accuracy: 0.8097 - val_loss: 0.4222 - val_accuracy: 0.8001\n",
      "Epoch 39/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8080\n",
      "Epoch 00039: val_loss improved from 0.41224 to 0.40689, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4096 - accuracy: 0.8074 - val_loss: 0.4069 - val_accuracy: 0.8119\n",
      "Epoch 40/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4069 - accuracy: 0.8109\n",
      "Epoch 00040: val_loss improved from 0.40689 to 0.40659, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4073 - accuracy: 0.8108 - val_loss: 0.4066 - val_accuracy: 0.8100\n",
      "Epoch 41/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4041 - accuracy: 0.8139\n",
      "Epoch 00041: val_loss improved from 0.40659 to 0.40626, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4046 - accuracy: 0.8134 - val_loss: 0.4063 - val_accuracy: 0.8171\n",
      "Epoch 42/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8135\n",
      "Epoch 00042: val_loss improved from 0.40626 to 0.39302, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4048 - accuracy: 0.8132 - val_loss: 0.3930 - val_accuracy: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8136\n",
      "Epoch 00043: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3989 - accuracy: 0.8132 - val_loss: 0.4157 - val_accuracy: 0.8093\n",
      "Epoch 44/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3982 - accuracy: 0.8165\n",
      "Epoch 00044: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3983 - accuracy: 0.8164 - val_loss: 0.3985 - val_accuracy: 0.8166\n",
      "Epoch 45/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8170\n",
      "Epoch 00045: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3960 - accuracy: 0.8170 - val_loss: 0.4010 - val_accuracy: 0.8091\n",
      "Epoch 46/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8232\n",
      "Epoch 00046: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3912 - accuracy: 0.8231 - val_loss: 0.4045 - val_accuracy: 0.8130\n",
      "Epoch 47/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8197\n",
      "Epoch 00047: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3923 - accuracy: 0.8197 - val_loss: 0.3976 - val_accuracy: 0.8130\n",
      "Epoch 48/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3861 - accuracy: 0.8255\n",
      "Epoch 00048: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3858 - accuracy: 0.8255 - val_loss: 0.4014 - val_accuracy: 0.8160\n",
      "Epoch 49/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8215\n",
      "Epoch 00049: val_loss did not improve from 0.39302\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3858 - accuracy: 0.8215 - val_loss: 0.3978 - val_accuracy: 0.8132\n",
      "Epoch 50/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8241\n",
      "Epoch 00050: val_loss improved from 0.39302 to 0.38267, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3846 - accuracy: 0.8239 - val_loss: 0.3827 - val_accuracy: 0.8212\n",
      "Epoch 51/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8288\n",
      "Epoch 00051: val_loss did not improve from 0.38267\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3777 - accuracy: 0.8285 - val_loss: 0.3921 - val_accuracy: 0.8214\n",
      "Epoch 52/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8273\n",
      "Epoch 00052: val_loss improved from 0.38267 to 0.37959, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3776 - accuracy: 0.8273 - val_loss: 0.3796 - val_accuracy: 0.8244\n",
      "Epoch 53/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.8274\n",
      "Epoch 00053: val_loss did not improve from 0.37959\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3752 - accuracy: 0.8272 - val_loss: 0.3899 - val_accuracy: 0.8145\n",
      "Epoch 54/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8326\n",
      "Epoch 00054: val_loss did not improve from 0.37959\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3698 - accuracy: 0.8323 - val_loss: 0.3812 - val_accuracy: 0.8248\n",
      "Epoch 55/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8328\n",
      "Epoch 00055: val_loss did not improve from 0.37959\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3680 - accuracy: 0.8327 - val_loss: 0.4056 - val_accuracy: 0.8143\n",
      "Epoch 56/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8317\n",
      "Epoch 00056: val_loss did not improve from 0.37959\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3675 - accuracy: 0.8315 - val_loss: 0.3872 - val_accuracy: 0.8209\n",
      "Epoch 57/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8366\n",
      "Epoch 00057: val_loss did not improve from 0.37959\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3645 - accuracy: 0.8366 - val_loss: 0.3941 - val_accuracy: 0.8169\n",
      "Epoch 58/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8372\n",
      "Epoch 00058: val_loss improved from 0.37959 to 0.37135, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3614 - accuracy: 0.8371 - val_loss: 0.3713 - val_accuracy: 0.8291\n",
      "Epoch 59/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8377\n",
      "Epoch 00059: val_loss did not improve from 0.37135\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3604 - accuracy: 0.8377 - val_loss: 0.3749 - val_accuracy: 0.8308\n",
      "Epoch 60/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3559 - accuracy: 0.8375\n",
      "Epoch 00060: val_loss did not improve from 0.37135\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3565 - accuracy: 0.8375 - val_loss: 0.3825 - val_accuracy: 0.8229\n",
      "Epoch 61/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8419\n",
      "Epoch 00061: val_loss improved from 0.37135 to 0.36715, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3509 - accuracy: 0.8419 - val_loss: 0.3671 - val_accuracy: 0.8325\n",
      "Epoch 62/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8442\n",
      "Epoch 00062: val_loss did not improve from 0.36715\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3524 - accuracy: 0.8441 - val_loss: 0.3693 - val_accuracy: 0.8317\n",
      "Epoch 63/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8454\n",
      "Epoch 00063: val_loss did not improve from 0.36715\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3473 - accuracy: 0.8454 - val_loss: 0.3766 - val_accuracy: 0.8263\n",
      "Epoch 64/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.8417\n",
      "Epoch 00064: val_loss improved from 0.36715 to 0.36553, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3477 - accuracy: 0.8416 - val_loss: 0.3655 - val_accuracy: 0.8323\n",
      "Epoch 65/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8434\n",
      "Epoch 00065: val_loss did not improve from 0.36553\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3466 - accuracy: 0.8430 - val_loss: 0.3772 - val_accuracy: 0.8298\n",
      "Epoch 66/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8468\n",
      "Epoch 00066: val_loss did not improve from 0.36553\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3402 - accuracy: 0.8465 - val_loss: 0.3729 - val_accuracy: 0.8242\n",
      "Epoch 67/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8491\n",
      "Epoch 00067: val_loss did not improve from 0.36553\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3420 - accuracy: 0.8491 - val_loss: 0.3682 - val_accuracy: 0.8276\n",
      "Epoch 68/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8488\n",
      "Epoch 00068: val_loss did not improve from 0.36553\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3380 - accuracy: 0.8489 - val_loss: 0.3714 - val_accuracy: 0.8308\n",
      "Epoch 69/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3389 - accuracy: 0.8480\n",
      "Epoch 00069: val_loss improved from 0.36553 to 0.36322, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3390 - accuracy: 0.8478 - val_loss: 0.3632 - val_accuracy: 0.8336\n",
      "Epoch 70/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8508\n",
      "Epoch 00070: val_loss did not improve from 0.36322\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3315 - accuracy: 0.8508 - val_loss: 0.3722 - val_accuracy: 0.8265\n",
      "Epoch 71/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8505\n",
      "Epoch 00071: val_loss did not improve from 0.36322\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3347 - accuracy: 0.8504 - val_loss: 0.3683 - val_accuracy: 0.8330\n",
      "Epoch 72/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.8516\n",
      "Epoch 00072: val_loss did not improve from 0.36322\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3307 - accuracy: 0.8516 - val_loss: 0.3682 - val_accuracy: 0.8248\n",
      "Epoch 73/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8525\n",
      "Epoch 00073: val_loss improved from 0.36322 to 0.36267, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3290 - accuracy: 0.8523 - val_loss: 0.3627 - val_accuracy: 0.8289\n",
      "Epoch 74/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8574\n",
      "Epoch 00074: val_loss did not improve from 0.36267\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3218 - accuracy: 0.8573 - val_loss: 0.3636 - val_accuracy: 0.8306\n",
      "Epoch 75/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8556\n",
      "Epoch 00075: val_loss did not improve from 0.36267\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3221 - accuracy: 0.8558 - val_loss: 0.3757 - val_accuracy: 0.8244\n",
      "Epoch 76/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8583\n",
      "Epoch 00076: val_loss did not improve from 0.36267\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3197 - accuracy: 0.8585 - val_loss: 0.3708 - val_accuracy: 0.8332\n",
      "Epoch 77/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8599\n",
      "Epoch 00077: val_loss improved from 0.36267 to 0.35540, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3169 - accuracy: 0.8597 - val_loss: 0.3554 - val_accuracy: 0.8364\n",
      "Epoch 78/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8591\n",
      "Epoch 00078: val_loss improved from 0.35540 to 0.35351, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3160 - accuracy: 0.8589 - val_loss: 0.3535 - val_accuracy: 0.8358\n",
      "Epoch 79/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8623\n",
      "Epoch 00079: val_loss did not improve from 0.35351\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3141 - accuracy: 0.8623 - val_loss: 0.3637 - val_accuracy: 0.8356\n",
      "Epoch 80/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8616\n",
      "Epoch 00080: val_loss improved from 0.35351 to 0.35049, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3155 - accuracy: 0.8616 - val_loss: 0.3505 - val_accuracy: 0.8388\n",
      "Epoch 81/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8610\n",
      "Epoch 00081: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3115 - accuracy: 0.8614 - val_loss: 0.3593 - val_accuracy: 0.8338\n",
      "Epoch 82/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8662\n",
      "Epoch 00082: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3057 - accuracy: 0.8661 - val_loss: 0.3633 - val_accuracy: 0.8317\n",
      "Epoch 83/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8640\n",
      "Epoch 00083: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3072 - accuracy: 0.8638 - val_loss: 0.3568 - val_accuracy: 0.8368\n",
      "Epoch 84/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8642\n",
      "Epoch 00084: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3051 - accuracy: 0.8641 - val_loss: 0.3611 - val_accuracy: 0.8267\n",
      "Epoch 85/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8680\n",
      "Epoch 00085: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3046 - accuracy: 0.8683 - val_loss: 0.3625 - val_accuracy: 0.8276\n",
      "Epoch 86/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8683\n",
      "Epoch 00086: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2970 - accuracy: 0.8681 - val_loss: 0.3668 - val_accuracy: 0.8278\n",
      "Epoch 87/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8674\n",
      "Epoch 00087: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2966 - accuracy: 0.8675 - val_loss: 0.3576 - val_accuracy: 0.8358\n",
      "Epoch 88/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8724\n",
      "Epoch 00088: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2944 - accuracy: 0.8723 - val_loss: 0.3719 - val_accuracy: 0.8310\n",
      "Epoch 89/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8760\n",
      "Epoch 00089: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2879 - accuracy: 0.8761 - val_loss: 0.3588 - val_accuracy: 0.8362\n",
      "Epoch 90/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.8764\n",
      "Epoch 00090: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2886 - accuracy: 0.8761 - val_loss: 0.3687 - val_accuracy: 0.8313\n",
      "Epoch 91/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8720\n",
      "Epoch 00091: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2940 - accuracy: 0.8723 - val_loss: 0.3646 - val_accuracy: 0.8308\n",
      "Epoch 92/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.8733\n",
      "Epoch 00092: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2856 - accuracy: 0.8734 - val_loss: 0.3556 - val_accuracy: 0.8371\n",
      "Epoch 93/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.8773\n",
      "Epoch 00093: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2826 - accuracy: 0.8773 - val_loss: 0.3606 - val_accuracy: 0.8317\n",
      "Epoch 94/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8782\n",
      "Epoch 00094: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2835 - accuracy: 0.8781 - val_loss: 0.3635 - val_accuracy: 0.8328\n",
      "Epoch 95/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8771\n",
      "Epoch 00095: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2845 - accuracy: 0.8773 - val_loss: 0.3583 - val_accuracy: 0.8336\n",
      "Epoch 96/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8800\n",
      "Epoch 00096: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2767 - accuracy: 0.8802 - val_loss: 0.3710 - val_accuracy: 0.8289\n",
      "Epoch 97/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.8790\n",
      "Epoch 00097: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2772 - accuracy: 0.8790 - val_loss: 0.3545 - val_accuracy: 0.8405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.8809\n",
      "Epoch 00098: val_loss did not improve from 0.35049\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2748 - accuracy: 0.8809 - val_loss: 0.3575 - val_accuracy: 0.8386\n",
      "Epoch 99/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8821\n",
      "Epoch 00099: val_loss improved from 0.35049 to 0.35010, saving model to pickled_objects/batch_size_64_lr_0.04_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2760 - accuracy: 0.8818 - val_loss: 0.3501 - val_accuracy: 0.8386\n",
      "Epoch 100/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8804\n",
      "Epoch 00100: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2762 - accuracy: 0.8805 - val_loss: 0.3602 - val_accuracy: 0.8310\n",
      "Epoch 101/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2652 - accuracy: 0.8859\n",
      "Epoch 00101: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2648 - accuracy: 0.8860 - val_loss: 0.3577 - val_accuracy: 0.8347\n",
      "Epoch 102/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8852\n",
      "Epoch 00102: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2674 - accuracy: 0.8847 - val_loss: 0.3535 - val_accuracy: 0.8399\n",
      "Epoch 103/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.8845\n",
      "Epoch 00103: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2671 - accuracy: 0.8843 - val_loss: 0.3547 - val_accuracy: 0.8383\n",
      "Epoch 104/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.8864\n",
      "Epoch 00104: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2657 - accuracy: 0.8862 - val_loss: 0.3561 - val_accuracy: 0.8414\n",
      "Epoch 105/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.8869\n",
      "Epoch 00105: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2638 - accuracy: 0.8869 - val_loss: 0.3611 - val_accuracy: 0.8388\n",
      "Epoch 106/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8878\n",
      "Epoch 00106: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2578 - accuracy: 0.8880 - val_loss: 0.3557 - val_accuracy: 0.8396\n",
      "Epoch 107/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.8861\n",
      "Epoch 00107: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2649 - accuracy: 0.8860 - val_loss: 0.3633 - val_accuracy: 0.8383\n",
      "Epoch 108/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.8926\n",
      "Epoch 00108: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2533 - accuracy: 0.8929 - val_loss: 0.3513 - val_accuracy: 0.8450\n",
      "Epoch 109/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.8894\n",
      "Epoch 00109: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2544 - accuracy: 0.8893 - val_loss: 0.3651 - val_accuracy: 0.8356\n",
      "Epoch 110/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.8891\n",
      "Epoch 00110: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2528 - accuracy: 0.8893 - val_loss: 0.3714 - val_accuracy: 0.8368\n",
      "Epoch 111/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.8935\n",
      "Epoch 00111: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2530 - accuracy: 0.8934 - val_loss: 0.3669 - val_accuracy: 0.8373\n",
      "Epoch 112/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.8933\n",
      "Epoch 00112: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2527 - accuracy: 0.8937 - val_loss: 0.3748 - val_accuracy: 0.8336\n",
      "Epoch 113/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.8972\n",
      "Epoch 00113: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2468 - accuracy: 0.8974 - val_loss: 0.3700 - val_accuracy: 0.8343\n",
      "Epoch 114/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.8947\n",
      "Epoch 00114: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2439 - accuracy: 0.8948 - val_loss: 0.3645 - val_accuracy: 0.8377\n",
      "Epoch 115/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.8995\n",
      "Epoch 00115: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2379 - accuracy: 0.8995 - val_loss: 0.3745 - val_accuracy: 0.8358\n",
      "Epoch 116/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.8969\n",
      "Epoch 00116: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2421 - accuracy: 0.8968 - val_loss: 0.3642 - val_accuracy: 0.8366\n",
      "Epoch 117/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.8988\n",
      "Epoch 00117: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2413 - accuracy: 0.8987 - val_loss: 0.3556 - val_accuracy: 0.8431\n",
      "Epoch 118/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.9010\n",
      "Epoch 00118: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2337 - accuracy: 0.9010 - val_loss: 0.3748 - val_accuracy: 0.8338\n",
      "Epoch 119/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.8991\n",
      "Epoch 00119: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2408 - accuracy: 0.8990 - val_loss: 0.3704 - val_accuracy: 0.8325\n",
      "Epoch 120/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9014\n",
      "Epoch 00120: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2378 - accuracy: 0.9015 - val_loss: 0.3644 - val_accuracy: 0.8366\n",
      "Epoch 121/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9038\n",
      "Epoch 00121: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2302 - accuracy: 0.9037 - val_loss: 0.3602 - val_accuracy: 0.8442\n",
      "Epoch 122/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9023\n",
      "Epoch 00122: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2282 - accuracy: 0.9022 - val_loss: 0.3677 - val_accuracy: 0.8383\n",
      "Epoch 123/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9047\n",
      "Epoch 00123: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2277 - accuracy: 0.9047 - val_loss: 0.3709 - val_accuracy: 0.8360\n",
      "Epoch 124/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9037\n",
      "Epoch 00124: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2308 - accuracy: 0.9034 - val_loss: 0.3713 - val_accuracy: 0.8381\n",
      "Epoch 125/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2304 - accuracy: 0.9012\n",
      "Epoch 00125: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2304 - accuracy: 0.9010 - val_loss: 0.3682 - val_accuracy: 0.8401\n",
      "Epoch 126/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9072\n",
      "Epoch 00126: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2243 - accuracy: 0.9073 - val_loss: 0.3890 - val_accuracy: 0.8265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.9066\n",
      "Epoch 00127: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2225 - accuracy: 0.9067 - val_loss: 0.3656 - val_accuracy: 0.8424\n",
      "Epoch 128/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9061\n",
      "Epoch 00128: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2212 - accuracy: 0.9061 - val_loss: 0.3693 - val_accuracy: 0.8433\n",
      "Epoch 129/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2200 - accuracy: 0.9077\n",
      "Epoch 00129: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2201 - accuracy: 0.9077 - val_loss: 0.3780 - val_accuracy: 0.8334\n",
      "Epoch 130/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2136 - accuracy: 0.9101\n",
      "Epoch 00130: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2138 - accuracy: 0.9101 - val_loss: 0.3894 - val_accuracy: 0.8371\n",
      "Epoch 131/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9088\n",
      "Epoch 00131: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2167 - accuracy: 0.9087 - val_loss: 0.3881 - val_accuracy: 0.8310\n",
      "Epoch 132/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2157 - accuracy: 0.9092\n",
      "Epoch 00132: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2155 - accuracy: 0.9092 - val_loss: 0.3851 - val_accuracy: 0.8347\n",
      "Epoch 133/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9068\n",
      "Epoch 00133: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2245 - accuracy: 0.9065 - val_loss: 0.3693 - val_accuracy: 0.8392\n",
      "Epoch 134/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9135\n",
      "Epoch 00134: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2082 - accuracy: 0.9134 - val_loss: 0.3798 - val_accuracy: 0.8340\n",
      "Epoch 135/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9150\n",
      "Epoch 00135: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2071 - accuracy: 0.9149 - val_loss: 0.3667 - val_accuracy: 0.8448\n",
      "Epoch 136/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9113\n",
      "Epoch 00136: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2089 - accuracy: 0.9111 - val_loss: 0.3848 - val_accuracy: 0.8325\n",
      "Epoch 137/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.9111\n",
      "Epoch 00137: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2096 - accuracy: 0.9112 - val_loss: 0.3751 - val_accuracy: 0.8386\n",
      "Epoch 138/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9153\n",
      "Epoch 00138: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2045 - accuracy: 0.9155 - val_loss: 0.3827 - val_accuracy: 0.8343\n",
      "Epoch 139/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9147\n",
      "Epoch 00139: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2009 - accuracy: 0.9148 - val_loss: 0.3828 - val_accuracy: 0.8386\n",
      "Epoch 140/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9188\n",
      "Epoch 00140: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1993 - accuracy: 0.9189 - val_loss: 0.3867 - val_accuracy: 0.8345\n",
      "Epoch 141/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9148\n",
      "Epoch 00141: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2035 - accuracy: 0.9148 - val_loss: 0.3890 - val_accuracy: 0.8328\n",
      "Epoch 142/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9142\n",
      "Epoch 00142: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2038 - accuracy: 0.9143 - val_loss: 0.3940 - val_accuracy: 0.8295\n",
      "Epoch 143/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9201\n",
      "Epoch 00143: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1953 - accuracy: 0.9200 - val_loss: 0.3968 - val_accuracy: 0.8285\n",
      "Epoch 144/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9172\n",
      "Epoch 00144: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1969 - accuracy: 0.9170 - val_loss: 0.3874 - val_accuracy: 0.8332\n",
      "Epoch 145/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9180\n",
      "Epoch 00145: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1999 - accuracy: 0.9182 - val_loss: 0.3774 - val_accuracy: 0.8351\n",
      "Epoch 146/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9174\n",
      "Epoch 00146: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1972 - accuracy: 0.9175 - val_loss: 0.3793 - val_accuracy: 0.8345\n",
      "Epoch 147/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9230\n",
      "Epoch 00147: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1921 - accuracy: 0.9226 - val_loss: 0.3853 - val_accuracy: 0.8345\n",
      "Epoch 148/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9178\n",
      "Epoch 00148: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1981 - accuracy: 0.9179 - val_loss: 0.3988 - val_accuracy: 0.8332\n",
      "Epoch 149/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9217\n",
      "Epoch 00149: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1904 - accuracy: 0.9215 - val_loss: 0.3969 - val_accuracy: 0.8343\n",
      "Epoch 150/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9206\n",
      "Epoch 00150: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1915 - accuracy: 0.9208 - val_loss: 0.3995 - val_accuracy: 0.8347\n",
      "Epoch 151/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9209\n",
      "Epoch 00151: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1887 - accuracy: 0.9212 - val_loss: 0.3903 - val_accuracy: 0.8330\n",
      "Epoch 152/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.9225\n",
      "Epoch 00152: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1856 - accuracy: 0.9226 - val_loss: 0.3879 - val_accuracy: 0.8332\n",
      "Epoch 153/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9265\n",
      "Epoch 00153: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1782 - accuracy: 0.9264 - val_loss: 0.4071 - val_accuracy: 0.8270\n",
      "Epoch 154/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9245\n",
      "Epoch 00154: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1838 - accuracy: 0.9246 - val_loss: 0.4013 - val_accuracy: 0.8265\n",
      "Epoch 155/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9256\n",
      "Epoch 00155: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1812 - accuracy: 0.9254 - val_loss: 0.3842 - val_accuracy: 0.8392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9230\n",
      "Epoch 00156: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1848 - accuracy: 0.9231 - val_loss: 0.3996 - val_accuracy: 0.8224\n",
      "Epoch 157/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9290\n",
      "Epoch 00157: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1792 - accuracy: 0.9293 - val_loss: 0.3883 - val_accuracy: 0.8340\n",
      "Epoch 158/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9300\n",
      "Epoch 00158: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1787 - accuracy: 0.9300 - val_loss: 0.4005 - val_accuracy: 0.8248\n",
      "Epoch 159/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9285\n",
      "Epoch 00159: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1768 - accuracy: 0.9286 - val_loss: 0.4041 - val_accuracy: 0.8255\n",
      "Epoch 160/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9300\n",
      "Epoch 00160: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1740 - accuracy: 0.9300 - val_loss: 0.4113 - val_accuracy: 0.8293\n",
      "Epoch 161/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9276\n",
      "Epoch 00161: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1751 - accuracy: 0.9276 - val_loss: 0.4031 - val_accuracy: 0.8364\n",
      "Epoch 162/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9293\n",
      "Epoch 00162: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1775 - accuracy: 0.9292 - val_loss: 0.3951 - val_accuracy: 0.8310\n",
      "Epoch 163/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9291\n",
      "Epoch 00163: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1699 - accuracy: 0.9293 - val_loss: 0.4004 - val_accuracy: 0.8358\n",
      "Epoch 164/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9313\n",
      "Epoch 00164: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1744 - accuracy: 0.9311 - val_loss: 0.4106 - val_accuracy: 0.8289\n",
      "Epoch 165/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9290\n",
      "Epoch 00165: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1695 - accuracy: 0.9290 - val_loss: 0.4019 - val_accuracy: 0.8345\n",
      "Epoch 166/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9302\n",
      "Epoch 00166: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1729 - accuracy: 0.9304 - val_loss: 0.4075 - val_accuracy: 0.8259\n",
      "Epoch 167/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9285\n",
      "Epoch 00167: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1737 - accuracy: 0.9286 - val_loss: 0.4174 - val_accuracy: 0.8287\n",
      "Epoch 168/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9293\n",
      "Epoch 00168: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1727 - accuracy: 0.9293 - val_loss: 0.3943 - val_accuracy: 0.8394\n",
      "Epoch 169/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9320\n",
      "Epoch 00169: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1649 - accuracy: 0.9320 - val_loss: 0.4059 - val_accuracy: 0.8371\n",
      "Epoch 170/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9340\n",
      "Epoch 00170: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1702 - accuracy: 0.9336 - val_loss: 0.4052 - val_accuracy: 0.8325\n",
      "Epoch 171/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9320\n",
      "Epoch 00171: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1648 - accuracy: 0.9322 - val_loss: 0.4249 - val_accuracy: 0.8270\n",
      "Epoch 172/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.9356\n",
      "Epoch 00172: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1593 - accuracy: 0.9354 - val_loss: 0.4262 - val_accuracy: 0.8265\n",
      "Epoch 173/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9350\n",
      "Epoch 00173: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1586 - accuracy: 0.9349 - val_loss: 0.4162 - val_accuracy: 0.8313\n",
      "Epoch 174/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9347\n",
      "Epoch 00174: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1618 - accuracy: 0.9346 - val_loss: 0.4115 - val_accuracy: 0.8315\n",
      "Epoch 175/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9341\n",
      "Epoch 00175: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1663 - accuracy: 0.9342 - val_loss: 0.4148 - val_accuracy: 0.8259\n",
      "Epoch 176/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1575 - accuracy: 0.9353\n",
      "Epoch 00176: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1575 - accuracy: 0.9352 - val_loss: 0.4121 - val_accuracy: 0.8317\n",
      "Epoch 177/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9352\n",
      "Epoch 00177: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1628 - accuracy: 0.9354 - val_loss: 0.4155 - val_accuracy: 0.8310\n",
      "Epoch 178/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1575 - accuracy: 0.9359\n",
      "Epoch 00178: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1570 - accuracy: 0.9361 - val_loss: 0.4171 - val_accuracy: 0.8315\n",
      "Epoch 179/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9352\n",
      "Epoch 00179: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1597 - accuracy: 0.9354 - val_loss: 0.4282 - val_accuracy: 0.8248\n",
      "Epoch 180/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9403\n",
      "Epoch 00180: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1503 - accuracy: 0.9401 - val_loss: 0.4063 - val_accuracy: 0.8366\n",
      "Epoch 181/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9390\n",
      "Epoch 00181: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1544 - accuracy: 0.9389 - val_loss: 0.4296 - val_accuracy: 0.8272\n",
      "Epoch 182/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9382\n",
      "Epoch 00182: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1529 - accuracy: 0.9384 - val_loss: 0.4123 - val_accuracy: 0.8317\n",
      "Epoch 183/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9372\n",
      "Epoch 00183: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1527 - accuracy: 0.9374 - val_loss: 0.4106 - val_accuracy: 0.8340\n",
      "Epoch 184/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9385\n",
      "Epoch 00184: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1513 - accuracy: 0.9385 - val_loss: 0.4298 - val_accuracy: 0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9401\n",
      "Epoch 00185: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1520 - accuracy: 0.9400 - val_loss: 0.4170 - val_accuracy: 0.8310\n",
      "Epoch 186/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9418\n",
      "Epoch 00186: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1482 - accuracy: 0.9416 - val_loss: 0.4147 - val_accuracy: 0.8353\n",
      "Epoch 187/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9391\n",
      "Epoch 00187: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1520 - accuracy: 0.9394 - val_loss: 0.4179 - val_accuracy: 0.8345\n",
      "Epoch 188/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9411\n",
      "Epoch 00188: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1473 - accuracy: 0.9412 - val_loss: 0.4308 - val_accuracy: 0.8267\n",
      "Epoch 189/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9404\n",
      "Epoch 00189: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1528 - accuracy: 0.9404 - val_loss: 0.4197 - val_accuracy: 0.8330\n",
      "Epoch 190/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9392\n",
      "Epoch 00190: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1492 - accuracy: 0.9391 - val_loss: 0.4328 - val_accuracy: 0.8340\n",
      "Epoch 191/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.9421\n",
      "Epoch 00191: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1430 - accuracy: 0.9420 - val_loss: 0.4340 - val_accuracy: 0.8222\n",
      "Epoch 192/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9423\n",
      "Epoch 00192: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1417 - accuracy: 0.9422 - val_loss: 0.4414 - val_accuracy: 0.8224\n",
      "Epoch 193/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1485 - accuracy: 0.9398\n",
      "Epoch 00193: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1480 - accuracy: 0.9400 - val_loss: 0.4212 - val_accuracy: 0.8239\n",
      "Epoch 194/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9420\n",
      "Epoch 00194: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1453 - accuracy: 0.9420 - val_loss: 0.4455 - val_accuracy: 0.8203\n",
      "Epoch 195/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9426\n",
      "Epoch 00195: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1431 - accuracy: 0.9427 - val_loss: 0.4129 - val_accuracy: 0.8379\n",
      "Epoch 196/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9436\n",
      "Epoch 00196: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1441 - accuracy: 0.9437 - val_loss: 0.4260 - val_accuracy: 0.8291\n",
      "Epoch 197/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.9419\n",
      "Epoch 00197: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1451 - accuracy: 0.9421 - val_loss: 0.4240 - val_accuracy: 0.8304\n",
      "Epoch 198/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9463\n",
      "Epoch 00198: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1381 - accuracy: 0.9462 - val_loss: 0.4278 - val_accuracy: 0.8272\n",
      "Epoch 199/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9450\n",
      "Epoch 00199: val_loss did not improve from 0.35010\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1408 - accuracy: 0.9450 - val_loss: 0.4242 - val_accuracy: 0.8358\n",
      "Epoch 00199: early stopping\n",
      "Epoch 1/10000\n",
      "    291/Unknown - 10s 34ms/step - loss: 0.6875 - accuracy: 0.5437\n",
      "Epoch 00001: val_loss improved from inf to 0.67641, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 11s 37ms/step - loss: 0.6875 - accuracy: 0.5437 - val_loss: 0.6764 - val_accuracy: 0.6109\n",
      "Epoch 2/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5681\n",
      "Epoch 00002: val_loss improved from 0.67641 to 0.66432, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6801 - accuracy: 0.5684 - val_loss: 0.6643 - val_accuracy: 0.6212\n",
      "Epoch 3/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6677 - accuracy: 0.5953\n",
      "Epoch 00003: val_loss improved from 0.66432 to 0.65110, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6677 - accuracy: 0.5953 - val_loss: 0.6511 - val_accuracy: 0.6359\n",
      "Epoch 4/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6524 - accuracy: 0.6193\n",
      "Epoch 00004: val_loss improved from 0.65110 to 0.63906, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6524 - accuracy: 0.6189 - val_loss: 0.6391 - val_accuracy: 0.6485\n",
      "Epoch 5/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6312 - accuracy: 0.6449\n",
      "Epoch 00005: val_loss improved from 0.63906 to 0.61151, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.6311 - accuracy: 0.6450 - val_loss: 0.6115 - val_accuracy: 0.6831\n",
      "Epoch 6/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6075 - accuracy: 0.6706\n",
      "Epoch 00006: val_loss improved from 0.61151 to 0.58406, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6075 - accuracy: 0.6702 - val_loss: 0.5841 - val_accuracy: 0.6928\n",
      "Epoch 7/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.6951\n",
      "Epoch 00007: val_loss improved from 0.58406 to 0.55728, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5801 - accuracy: 0.6950 - val_loss: 0.5573 - val_accuracy: 0.7242\n",
      "Epoch 8/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5579 - accuracy: 0.7131\n",
      "Epoch 00008: val_loss improved from 0.55728 to 0.55055, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5583 - accuracy: 0.7127 - val_loss: 0.5505 - val_accuracy: 0.7296\n",
      "Epoch 9/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7250\n",
      "Epoch 00009: val_loss improved from 0.55055 to 0.51746, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5436 - accuracy: 0.7247 - val_loss: 0.5175 - val_accuracy: 0.7517\n",
      "Epoch 10/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5297 - accuracy: 0.7276\n",
      "Epoch 00010: val_loss did not improve from 0.51746\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5301 - accuracy: 0.7271 - val_loss: 0.5211 - val_accuracy: 0.7436\n",
      "Epoch 11/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5175 - accuracy: 0.7389\n",
      "Epoch 00011: val_loss improved from 0.51746 to 0.47554, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5176 - accuracy: 0.7388 - val_loss: 0.4755 - val_accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7446\n",
      "Epoch 00012: val_loss improved from 0.47554 to 0.47532, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5077 - accuracy: 0.7444 - val_loss: 0.4753 - val_accuracy: 0.7769\n",
      "Epoch 13/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4973 - accuracy: 0.7566\n",
      "Epoch 00013: val_loss did not improve from 0.47532\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4977 - accuracy: 0.7562 - val_loss: 0.4765 - val_accuracy: 0.7633\n",
      "Epoch 14/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7594\n",
      "Epoch 00014: val_loss improved from 0.47532 to 0.47185, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4876 - accuracy: 0.7593 - val_loss: 0.4719 - val_accuracy: 0.7747\n",
      "Epoch 15/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7686\n",
      "Epoch 00015: val_loss improved from 0.47185 to 0.46616, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4756 - accuracy: 0.7684 - val_loss: 0.4662 - val_accuracy: 0.7797\n",
      "Epoch 16/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.7724\n",
      "Epoch 00016: val_loss improved from 0.46616 to 0.44281, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4708 - accuracy: 0.7724 - val_loss: 0.4428 - val_accuracy: 0.7943\n",
      "Epoch 17/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.7754\n",
      "Epoch 00017: val_loss did not improve from 0.44281\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4678 - accuracy: 0.7750 - val_loss: 0.4628 - val_accuracy: 0.7764\n",
      "Epoch 18/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.7819\n",
      "Epoch 00018: val_loss did not improve from 0.44281\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4552 - accuracy: 0.7814 - val_loss: 0.4442 - val_accuracy: 0.7889\n",
      "Epoch 19/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7838\n",
      "Epoch 00019: val_loss improved from 0.44281 to 0.44219, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4501 - accuracy: 0.7834 - val_loss: 0.4422 - val_accuracy: 0.7951\n",
      "Epoch 20/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7892\n",
      "Epoch 00020: val_loss improved from 0.44219 to 0.42284, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4447 - accuracy: 0.7889 - val_loss: 0.4228 - val_accuracy: 0.8012\n",
      "Epoch 21/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.7955\n",
      "Epoch 00021: val_loss improved from 0.42284 to 0.41307, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4383 - accuracy: 0.7950 - val_loss: 0.4131 - val_accuracy: 0.8093\n",
      "Epoch 22/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7935\n",
      "Epoch 00022: val_loss did not improve from 0.41307\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4335 - accuracy: 0.7933 - val_loss: 0.4190 - val_accuracy: 0.8048\n",
      "Epoch 23/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8004\n",
      "Epoch 00023: val_loss improved from 0.41307 to 0.40993, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4247 - accuracy: 0.8001 - val_loss: 0.4099 - val_accuracy: 0.8110\n",
      "Epoch 24/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8009\n",
      "Epoch 00024: val_loss did not improve from 0.40993\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4229 - accuracy: 0.8005 - val_loss: 0.4188 - val_accuracy: 0.8067\n",
      "Epoch 25/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8094\n",
      "Epoch 00025: val_loss improved from 0.40993 to 0.39413, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4148 - accuracy: 0.8089 - val_loss: 0.3941 - val_accuracy: 0.8196\n",
      "Epoch 26/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8067\n",
      "Epoch 00026: val_loss did not improve from 0.39413\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4129 - accuracy: 0.8068 - val_loss: 0.3980 - val_accuracy: 0.8201\n",
      "Epoch 27/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8127\n",
      "Epoch 00027: val_loss improved from 0.39413 to 0.38989, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4049 - accuracy: 0.8126 - val_loss: 0.3899 - val_accuracy: 0.8205\n",
      "Epoch 28/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4050 - accuracy: 0.8115\n",
      "Epoch 00028: val_loss did not improve from 0.38989\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4050 - accuracy: 0.8116 - val_loss: 0.3972 - val_accuracy: 0.8184\n",
      "Epoch 29/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8149\n",
      "Epoch 00029: val_loss improved from 0.38989 to 0.37879, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3995 - accuracy: 0.8147 - val_loss: 0.3788 - val_accuracy: 0.8315\n",
      "Epoch 30/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3975 - accuracy: 0.8159\n",
      "Epoch 00030: val_loss did not improve from 0.37879\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3975 - accuracy: 0.8157 - val_loss: 0.3859 - val_accuracy: 0.8250\n",
      "Epoch 31/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3924 - accuracy: 0.8212\n",
      "Epoch 00031: val_loss improved from 0.37879 to 0.37473, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3925 - accuracy: 0.8214 - val_loss: 0.3747 - val_accuracy: 0.8317\n",
      "Epoch 32/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8227\n",
      "Epoch 00032: val_loss did not improve from 0.37473\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3856 - accuracy: 0.8228 - val_loss: 0.3811 - val_accuracy: 0.8276\n",
      "Epoch 33/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8251\n",
      "Epoch 00033: val_loss improved from 0.37473 to 0.37434, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3828 - accuracy: 0.8248 - val_loss: 0.3743 - val_accuracy: 0.8285\n",
      "Epoch 34/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.8269\n",
      "Epoch 00034: val_loss did not improve from 0.37434\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3802 - accuracy: 0.8264 - val_loss: 0.3949 - val_accuracy: 0.8164\n",
      "Epoch 35/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8310\n",
      "Epoch 00035: val_loss improved from 0.37434 to 0.36718, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3777 - accuracy: 0.8305 - val_loss: 0.3672 - val_accuracy: 0.8345\n",
      "Epoch 36/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8317\n",
      "Epoch 00036: val_loss did not improve from 0.36718\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3697 - accuracy: 0.8315 - val_loss: 0.3756 - val_accuracy: 0.8282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8315\n",
      "Epoch 00037: val_loss did not improve from 0.36718\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3710 - accuracy: 0.8311 - val_loss: 0.3820 - val_accuracy: 0.8257\n",
      "Epoch 38/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8342\n",
      "Epoch 00038: val_loss did not improve from 0.36718\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3654 - accuracy: 0.8339 - val_loss: 0.4136 - val_accuracy: 0.8067\n",
      "Epoch 39/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8372\n",
      "Epoch 00039: val_loss improved from 0.36718 to 0.36598, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3594 - accuracy: 0.8370 - val_loss: 0.3660 - val_accuracy: 0.8358\n",
      "Epoch 40/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8377\n",
      "Epoch 00040: val_loss did not improve from 0.36598\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3572 - accuracy: 0.8375 - val_loss: 0.3662 - val_accuracy: 0.8358\n",
      "Epoch 41/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8402\n",
      "Epoch 00041: val_loss did not improve from 0.36598\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3538 - accuracy: 0.8401 - val_loss: 0.3880 - val_accuracy: 0.8224\n",
      "Epoch 42/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8425\n",
      "Epoch 00042: val_loss did not improve from 0.36598\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3546 - accuracy: 0.8427 - val_loss: 0.3661 - val_accuracy: 0.8366\n",
      "Epoch 43/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8430\n",
      "Epoch 00043: val_loss improved from 0.36598 to 0.36431, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3470 - accuracy: 0.8430 - val_loss: 0.3643 - val_accuracy: 0.8368\n",
      "Epoch 44/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.8442\n",
      "Epoch 00044: val_loss did not improve from 0.36431\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3466 - accuracy: 0.8441 - val_loss: 0.3741 - val_accuracy: 0.8300\n",
      "Epoch 45/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8465\n",
      "Epoch 00045: val_loss did not improve from 0.36431\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3432 - accuracy: 0.8461 - val_loss: 0.3707 - val_accuracy: 0.8334\n",
      "Epoch 46/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8491\n",
      "Epoch 00046: val_loss did not improve from 0.36431\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3389 - accuracy: 0.8490 - val_loss: 0.3665 - val_accuracy: 0.8336\n",
      "Epoch 47/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.8504\n",
      "Epoch 00047: val_loss did not improve from 0.36431\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3338 - accuracy: 0.8506 - val_loss: 0.3666 - val_accuracy: 0.8373\n",
      "Epoch 48/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8548\n",
      "Epoch 00048: val_loss improved from 0.36431 to 0.35738, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3297 - accuracy: 0.8549 - val_loss: 0.3574 - val_accuracy: 0.8444\n",
      "Epoch 49/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8535\n",
      "Epoch 00049: val_loss improved from 0.35738 to 0.35645, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3304 - accuracy: 0.8536 - val_loss: 0.3564 - val_accuracy: 0.8401\n",
      "Epoch 50/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8529\n",
      "Epoch 00050: val_loss did not improve from 0.35645\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3318 - accuracy: 0.8528 - val_loss: 0.3747 - val_accuracy: 0.8287\n",
      "Epoch 51/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8562\n",
      "Epoch 00051: val_loss did not improve from 0.35645\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3256 - accuracy: 0.8562 - val_loss: 0.3664 - val_accuracy: 0.8358\n",
      "Epoch 52/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8553\n",
      "Epoch 00052: val_loss improved from 0.35645 to 0.35170, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3268 - accuracy: 0.8554 - val_loss: 0.3517 - val_accuracy: 0.8409\n",
      "Epoch 53/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8615\n",
      "Epoch 00053: val_loss did not improve from 0.35170\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3167 - accuracy: 0.8613 - val_loss: 0.3551 - val_accuracy: 0.8392\n",
      "Epoch 54/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3113 - accuracy: 0.8616\n",
      "Epoch 00054: val_loss did not improve from 0.35170\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.3113 - accuracy: 0.8613 - val_loss: 0.3771 - val_accuracy: 0.8323\n",
      "Epoch 55/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8608\n",
      "Epoch 00055: val_loss did not improve from 0.35170\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3135 - accuracy: 0.8607 - val_loss: 0.3706 - val_accuracy: 0.8334\n",
      "Epoch 56/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8625\n",
      "Epoch 00056: val_loss did not improve from 0.35170\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3112 - accuracy: 0.8627 - val_loss: 0.3647 - val_accuracy: 0.8325\n",
      "Epoch 57/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8678\n",
      "Epoch 00057: val_loss did not improve from 0.35170\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3040 - accuracy: 0.8679 - val_loss: 0.3767 - val_accuracy: 0.8280\n",
      "Epoch 58/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8650\n",
      "Epoch 00058: val_loss improved from 0.35170 to 0.34422, saving model to pickled_objects/batch_size_64_lr_0.08_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3053 - accuracy: 0.8649 - val_loss: 0.3442 - val_accuracy: 0.8463\n",
      "Epoch 59/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8659\n",
      "Epoch 00059: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3031 - accuracy: 0.8655 - val_loss: 0.3851 - val_accuracy: 0.8233\n",
      "Epoch 60/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.8706\n",
      "Epoch 00060: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2987 - accuracy: 0.8704 - val_loss: 0.3610 - val_accuracy: 0.8353\n",
      "Epoch 61/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8710\n",
      "Epoch 00061: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2944 - accuracy: 0.8713 - val_loss: 0.3677 - val_accuracy: 0.8343\n",
      "Epoch 62/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8734\n",
      "Epoch 00062: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2943 - accuracy: 0.8735 - val_loss: 0.3631 - val_accuracy: 0.8377\n",
      "Epoch 63/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8715\n",
      "Epoch 00063: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2893 - accuracy: 0.8714 - val_loss: 0.3842 - val_accuracy: 0.8212\n",
      "Epoch 64/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8760\n",
      "Epoch 00064: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2834 - accuracy: 0.8758 - val_loss: 0.3567 - val_accuracy: 0.8426\n",
      "Epoch 65/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8719\n",
      "Epoch 00065: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2886 - accuracy: 0.8720 - val_loss: 0.3572 - val_accuracy: 0.8396\n",
      "Epoch 66/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8808\n",
      "Epoch 00066: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2786 - accuracy: 0.8811 - val_loss: 0.3577 - val_accuracy: 0.8349\n",
      "Epoch 67/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8777\n",
      "Epoch 00067: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2817 - accuracy: 0.8774 - val_loss: 0.3739 - val_accuracy: 0.8298\n",
      "Epoch 68/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2804 - accuracy: 0.8778\n",
      "Epoch 00068: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2805 - accuracy: 0.8776 - val_loss: 0.3765 - val_accuracy: 0.8319\n",
      "Epoch 69/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8798\n",
      "Epoch 00069: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2786 - accuracy: 0.8800 - val_loss: 0.3771 - val_accuracy: 0.8298\n",
      "Epoch 70/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8838\n",
      "Epoch 00070: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2695 - accuracy: 0.8838 - val_loss: 0.3599 - val_accuracy: 0.8368\n",
      "Epoch 71/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.8835\n",
      "Epoch 00071: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2679 - accuracy: 0.8835 - val_loss: 0.3697 - val_accuracy: 0.8381\n",
      "Epoch 72/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8833\n",
      "Epoch 00072: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2688 - accuracy: 0.8833 - val_loss: 0.3511 - val_accuracy: 0.8444\n",
      "Epoch 73/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.8835\n",
      "Epoch 00073: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2705 - accuracy: 0.8836 - val_loss: 0.3525 - val_accuracy: 0.8379\n",
      "Epoch 74/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8839\n",
      "Epoch 00074: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2641 - accuracy: 0.8839 - val_loss: 0.3555 - val_accuracy: 0.8420\n",
      "Epoch 75/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.8902\n",
      "Epoch 00075: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2559 - accuracy: 0.8901 - val_loss: 0.3597 - val_accuracy: 0.8358\n",
      "Epoch 76/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.8891\n",
      "Epoch 00076: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2573 - accuracy: 0.8892 - val_loss: 0.3737 - val_accuracy: 0.8321\n",
      "Epoch 77/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.8920\n",
      "Epoch 00077: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2533 - accuracy: 0.8919 - val_loss: 0.3717 - val_accuracy: 0.8358\n",
      "Epoch 78/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.8931\n",
      "Epoch 00078: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2523 - accuracy: 0.8932 - val_loss: 0.3744 - val_accuracy: 0.8325\n",
      "Epoch 79/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.8900\n",
      "Epoch 00079: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2566 - accuracy: 0.8901 - val_loss: 0.3640 - val_accuracy: 0.8396\n",
      "Epoch 80/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.8958\n",
      "Epoch 00080: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2491 - accuracy: 0.8955 - val_loss: 0.3513 - val_accuracy: 0.8463\n",
      "Epoch 81/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.8964\n",
      "Epoch 00081: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2461 - accuracy: 0.8962 - val_loss: 0.3682 - val_accuracy: 0.8338\n",
      "Epoch 82/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.8950\n",
      "Epoch 00082: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2441 - accuracy: 0.8953 - val_loss: 0.3577 - val_accuracy: 0.8433\n",
      "Epoch 83/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.8951\n",
      "Epoch 00083: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2399 - accuracy: 0.8952 - val_loss: 0.3666 - val_accuracy: 0.8383\n",
      "Epoch 84/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.8986\n",
      "Epoch 00084: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2394 - accuracy: 0.8985 - val_loss: 0.3702 - val_accuracy: 0.8340\n",
      "Epoch 85/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.8968\n",
      "Epoch 00085: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2401 - accuracy: 0.8969 - val_loss: 0.3639 - val_accuracy: 0.8407\n",
      "Epoch 86/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.8964\n",
      "Epoch 00086: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2388 - accuracy: 0.8965 - val_loss: 0.3998 - val_accuracy: 0.8194\n",
      "Epoch 87/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9008\n",
      "Epoch 00087: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2368 - accuracy: 0.9006 - val_loss: 0.3694 - val_accuracy: 0.8366\n",
      "Epoch 88/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9003\n",
      "Epoch 00088: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2337 - accuracy: 0.9003 - val_loss: 0.3526 - val_accuracy: 0.8491\n",
      "Epoch 89/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9058\n",
      "Epoch 00089: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2207 - accuracy: 0.9061 - val_loss: 0.3655 - val_accuracy: 0.8377\n",
      "Epoch 90/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9055\n",
      "Epoch 00090: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2228 - accuracy: 0.9053 - val_loss: 0.4045 - val_accuracy: 0.8220\n",
      "Epoch 91/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9041\n",
      "Epoch 00091: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2276 - accuracy: 0.9040 - val_loss: 0.3544 - val_accuracy: 0.8487\n",
      "Epoch 92/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9086\n",
      "Epoch 00092: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2253 - accuracy: 0.9084 - val_loss: 0.3721 - val_accuracy: 0.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9058\n",
      "Epoch 00093: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2260 - accuracy: 0.9057 - val_loss: 0.4005 - val_accuracy: 0.8261\n",
      "Epoch 94/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9048\n",
      "Epoch 00094: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2251 - accuracy: 0.9048 - val_loss: 0.3761 - val_accuracy: 0.8364\n",
      "Epoch 95/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9077\n",
      "Epoch 00095: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2144 - accuracy: 0.9079 - val_loss: 0.3754 - val_accuracy: 0.8263\n",
      "Epoch 96/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9086\n",
      "Epoch 00096: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2178 - accuracy: 0.9087 - val_loss: 0.3837 - val_accuracy: 0.8242\n",
      "Epoch 97/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2116 - accuracy: 0.9101\n",
      "Epoch 00097: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2113 - accuracy: 0.9103 - val_loss: 0.3748 - val_accuracy: 0.8368\n",
      "Epoch 98/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9096\n",
      "Epoch 00098: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2158 - accuracy: 0.9097 - val_loss: 0.3699 - val_accuracy: 0.8317\n",
      "Epoch 99/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2076 - accuracy: 0.9138\n",
      "Epoch 00099: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2080 - accuracy: 0.9137 - val_loss: 0.3792 - val_accuracy: 0.8388\n",
      "Epoch 100/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9164\n",
      "Epoch 00100: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2040 - accuracy: 0.9162 - val_loss: 0.3866 - val_accuracy: 0.8315\n",
      "Epoch 101/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.9128\n",
      "Epoch 00101: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2102 - accuracy: 0.9126 - val_loss: 0.3759 - val_accuracy: 0.8356\n",
      "Epoch 102/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9144\n",
      "Epoch 00102: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2050 - accuracy: 0.9144 - val_loss: 0.3854 - val_accuracy: 0.8356\n",
      "Epoch 103/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9153\n",
      "Epoch 00103: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2027 - accuracy: 0.9152 - val_loss: 0.3693 - val_accuracy: 0.8433\n",
      "Epoch 104/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9150\n",
      "Epoch 00104: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2022 - accuracy: 0.9148 - val_loss: 0.3838 - val_accuracy: 0.8287\n",
      "Epoch 105/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9164\n",
      "Epoch 00105: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2008 - accuracy: 0.9162 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 106/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9188\n",
      "Epoch 00106: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1974 - accuracy: 0.9188 - val_loss: 0.4075 - val_accuracy: 0.8289\n",
      "Epoch 107/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9201\n",
      "Epoch 00107: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1948 - accuracy: 0.9202 - val_loss: 0.3933 - val_accuracy: 0.8261\n",
      "Epoch 108/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9193\n",
      "Epoch 00108: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1952 - accuracy: 0.9193 - val_loss: 0.3765 - val_accuracy: 0.8371\n",
      "Epoch 109/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1951 - accuracy: 0.9176\n",
      "Epoch 00109: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1952 - accuracy: 0.9175 - val_loss: 0.3892 - val_accuracy: 0.8336\n",
      "Epoch 110/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9212\n",
      "Epoch 00110: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1919 - accuracy: 0.9213 - val_loss: 0.3848 - val_accuracy: 0.8356\n",
      "Epoch 111/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9230\n",
      "Epoch 00111: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1902 - accuracy: 0.9227 - val_loss: 0.4057 - val_accuracy: 0.8285\n",
      "Epoch 112/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1852 - accuracy: 0.9250\n",
      "Epoch 00112: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1854 - accuracy: 0.9249 - val_loss: 0.3785 - val_accuracy: 0.8394\n",
      "Epoch 113/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9260\n",
      "Epoch 00113: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1836 - accuracy: 0.9261 - val_loss: 0.3925 - val_accuracy: 0.8328\n",
      "Epoch 114/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9250\n",
      "Epoch 00114: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1848 - accuracy: 0.9250 - val_loss: 0.4320 - val_accuracy: 0.8188\n",
      "Epoch 115/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1865 - accuracy: 0.9234\n",
      "Epoch 00115: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1868 - accuracy: 0.9231 - val_loss: 0.3908 - val_accuracy: 0.8317\n",
      "Epoch 116/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9223\n",
      "Epoch 00116: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1873 - accuracy: 0.9224 - val_loss: 0.3834 - val_accuracy: 0.8366\n",
      "Epoch 117/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9268\n",
      "Epoch 00117: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1792 - accuracy: 0.9269 - val_loss: 0.4001 - val_accuracy: 0.8259\n",
      "Epoch 118/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9276\n",
      "Epoch 00118: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1802 - accuracy: 0.9277 - val_loss: 0.4139 - val_accuracy: 0.8242\n",
      "Epoch 119/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9261\n",
      "Epoch 00119: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1784 - accuracy: 0.9262 - val_loss: 0.4125 - val_accuracy: 0.8209\n",
      "Epoch 120/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9285\n",
      "Epoch 00120: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1766 - accuracy: 0.9283 - val_loss: 0.3826 - val_accuracy: 0.8381\n",
      "Epoch 121/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9273\n",
      "Epoch 00121: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1796 - accuracy: 0.9274 - val_loss: 0.3967 - val_accuracy: 0.8349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9293\n",
      "Epoch 00122: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1750 - accuracy: 0.9293 - val_loss: 0.4003 - val_accuracy: 0.8390\n",
      "Epoch 123/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9285\n",
      "Epoch 00123: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1735 - accuracy: 0.9285 - val_loss: 0.4013 - val_accuracy: 0.8323\n",
      "Epoch 124/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9308\n",
      "Epoch 00124: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1698 - accuracy: 0.9307 - val_loss: 0.3917 - val_accuracy: 0.8431\n",
      "Epoch 125/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9322\n",
      "Epoch 00125: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1693 - accuracy: 0.9322 - val_loss: 0.3919 - val_accuracy: 0.8386\n",
      "Epoch 126/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9337\n",
      "Epoch 00126: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1648 - accuracy: 0.9338 - val_loss: 0.4052 - val_accuracy: 0.8313\n",
      "Epoch 127/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9306\n",
      "Epoch 00127: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1691 - accuracy: 0.9308 - val_loss: 0.3905 - val_accuracy: 0.8351\n",
      "Epoch 128/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9306\n",
      "Epoch 00128: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1667 - accuracy: 0.9307 - val_loss: 0.3971 - val_accuracy: 0.8310\n",
      "Epoch 129/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9307\n",
      "Epoch 00129: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1715 - accuracy: 0.9307 - val_loss: 0.3996 - val_accuracy: 0.8381\n",
      "Epoch 130/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9340\n",
      "Epoch 00130: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1623 - accuracy: 0.9340 - val_loss: 0.4283 - val_accuracy: 0.8278\n",
      "Epoch 131/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9359\n",
      "Epoch 00131: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1574 - accuracy: 0.9357 - val_loss: 0.4267 - val_accuracy: 0.8259\n",
      "Epoch 132/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9337\n",
      "Epoch 00132: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1681 - accuracy: 0.9336 - val_loss: 0.4050 - val_accuracy: 0.8248\n",
      "Epoch 133/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.9366\n",
      "Epoch 00133: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1584 - accuracy: 0.9369 - val_loss: 0.4199 - val_accuracy: 0.8291\n",
      "Epoch 134/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9350\n",
      "Epoch 00134: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1626 - accuracy: 0.9351 - val_loss: 0.4395 - val_accuracy: 0.8205\n",
      "Epoch 135/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9370\n",
      "Epoch 00135: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1557 - accuracy: 0.9370 - val_loss: 0.4068 - val_accuracy: 0.8418\n",
      "Epoch 136/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.9368\n",
      "Epoch 00136: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1593 - accuracy: 0.9367 - val_loss: 0.4300 - val_accuracy: 0.8304\n",
      "Epoch 137/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9373\n",
      "Epoch 00137: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1531 - accuracy: 0.9376 - val_loss: 0.4403 - val_accuracy: 0.8306\n",
      "Epoch 138/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9387\n",
      "Epoch 00138: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1557 - accuracy: 0.9388 - val_loss: 0.4059 - val_accuracy: 0.8375\n",
      "Epoch 139/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.9372\n",
      "Epoch 00139: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1531 - accuracy: 0.9374 - val_loss: 0.4325 - val_accuracy: 0.8233\n",
      "Epoch 140/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9389\n",
      "Epoch 00140: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1539 - accuracy: 0.9386 - val_loss: 0.4081 - val_accuracy: 0.8319\n",
      "Epoch 141/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9396\n",
      "Epoch 00141: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1534 - accuracy: 0.9395 - val_loss: 0.4543 - val_accuracy: 0.8188\n",
      "Epoch 142/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9393\n",
      "Epoch 00142: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1505 - accuracy: 0.9392 - val_loss: 0.4036 - val_accuracy: 0.8332\n",
      "Epoch 143/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9401\n",
      "Epoch 00143: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1498 - accuracy: 0.9401 - val_loss: 0.4249 - val_accuracy: 0.8248\n",
      "Epoch 144/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.9423\n",
      "Epoch 00144: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1462 - accuracy: 0.9424 - val_loss: 0.4327 - val_accuracy: 0.8246\n",
      "Epoch 145/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9398\n",
      "Epoch 00145: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1487 - accuracy: 0.9393 - val_loss: 0.4332 - val_accuracy: 0.8231\n",
      "Epoch 146/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.9410\n",
      "Epoch 00146: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1488 - accuracy: 0.9410 - val_loss: 0.4182 - val_accuracy: 0.8276\n",
      "Epoch 147/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9389\n",
      "Epoch 00147: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1531 - accuracy: 0.9389 - val_loss: 0.4061 - val_accuracy: 0.8317\n",
      "Epoch 148/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9416\n",
      "Epoch 00148: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1446 - accuracy: 0.9415 - val_loss: 0.4191 - val_accuracy: 0.8334\n",
      "Epoch 149/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9416\n",
      "Epoch 00149: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.1443 - accuracy: 0.9416 - val_loss: 0.4548 - val_accuracy: 0.8220\n",
      "Epoch 150/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9431\n",
      "Epoch 00150: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1438 - accuracy: 0.9430 - val_loss: 0.4400 - val_accuracy: 0.8233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9441\n",
      "Epoch 00151: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1438 - accuracy: 0.9441 - val_loss: 0.4079 - val_accuracy: 0.8332\n",
      "Epoch 152/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.9456\n",
      "Epoch 00152: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1366 - accuracy: 0.9457 - val_loss: 0.4402 - val_accuracy: 0.8261\n",
      "Epoch 153/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9483\n",
      "Epoch 00153: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1350 - accuracy: 0.9480 - val_loss: 0.4136 - val_accuracy: 0.8343\n",
      "Epoch 154/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9498\n",
      "Epoch 00154: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1329 - accuracy: 0.9496 - val_loss: 0.4282 - val_accuracy: 0.8287\n",
      "Epoch 155/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9458\n",
      "Epoch 00155: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1398 - accuracy: 0.9458 - val_loss: 0.4255 - val_accuracy: 0.8229\n",
      "Epoch 156/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9461\n",
      "Epoch 00156: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1402 - accuracy: 0.9459 - val_loss: 0.4312 - val_accuracy: 0.8321\n",
      "Epoch 157/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.9472\n",
      "Epoch 00157: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1351 - accuracy: 0.9471 - val_loss: 0.4142 - val_accuracy: 0.8345\n",
      "Epoch 158/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.9462\n",
      "Epoch 00158: val_loss did not improve from 0.34422\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1365 - accuracy: 0.9463 - val_loss: 0.4374 - val_accuracy: 0.8255\n",
      "Epoch 00158: early stopping\n",
      "Epoch 1/10000\n",
      "    291/Unknown - 10s 34ms/step - loss: 0.6883 - accuracy: 0.5384\n",
      "Epoch 00001: val_loss improved from inf to 0.67737, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 11s 38ms/step - loss: 0.6883 - accuracy: 0.5384 - val_loss: 0.6774 - val_accuracy: 0.5868\n",
      "Epoch 2/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5155\n",
      "Epoch 00002: val_loss did not improve from 0.67737\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6925 - accuracy: 0.5159 - val_loss: 0.6896 - val_accuracy: 0.5942\n",
      "Epoch 3/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5757\n",
      "Epoch 00003: val_loss improved from 0.67737 to 0.65745, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6764 - accuracy: 0.5752 - val_loss: 0.6575 - val_accuracy: 0.6217\n",
      "Epoch 4/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6607 - accuracy: 0.6063\n",
      "Epoch 00004: val_loss improved from 0.65745 to 0.63847, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6609 - accuracy: 0.6061 - val_loss: 0.6385 - val_accuracy: 0.6513\n",
      "Epoch 5/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.6365\n",
      "Epoch 00005: val_loss improved from 0.63847 to 0.61096, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6390 - accuracy: 0.6365 - val_loss: 0.6110 - val_accuracy: 0.6705\n",
      "Epoch 6/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6187 - accuracy: 0.6628\n",
      "Epoch 00006: val_loss improved from 0.61096 to 0.59875, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6187 - accuracy: 0.6628 - val_loss: 0.5987 - val_accuracy: 0.6801\n",
      "Epoch 7/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.6781\n",
      "Epoch 00007: val_loss improved from 0.59875 to 0.57714, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.6026 - accuracy: 0.6784 - val_loss: 0.5771 - val_accuracy: 0.6928\n",
      "Epoch 8/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5846 - accuracy: 0.6916\n",
      "Epoch 00008: val_loss did not improve from 0.57714\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5845 - accuracy: 0.6916 - val_loss: 0.6476 - val_accuracy: 0.6187\n",
      "Epoch 9/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7060\n",
      "Epoch 00009: val_loss improved from 0.57714 to 0.57457, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5684 - accuracy: 0.7060 - val_loss: 0.5746 - val_accuracy: 0.6978\n",
      "Epoch 10/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7193\n",
      "Epoch 00010: val_loss improved from 0.57457 to 0.55460, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5499 - accuracy: 0.7190 - val_loss: 0.5546 - val_accuracy: 0.7092\n",
      "Epoch 11/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.7304\n",
      "Epoch 00011: val_loss improved from 0.55460 to 0.50056, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5352 - accuracy: 0.7307 - val_loss: 0.5006 - val_accuracy: 0.7556\n",
      "Epoch 12/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5229 - accuracy: 0.7432\n",
      "Epoch 00012: val_loss improved from 0.50056 to 0.48219, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.5229 - accuracy: 0.7429 - val_loss: 0.4822 - val_accuracy: 0.7719\n",
      "Epoch 13/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7462\n",
      "Epoch 00013: val_loss improved from 0.48219 to 0.48172, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5111 - accuracy: 0.7462 - val_loss: 0.4817 - val_accuracy: 0.7721\n",
      "Epoch 14/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7577\n",
      "Epoch 00014: val_loss improved from 0.48172 to 0.47613, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.5015 - accuracy: 0.7581 - val_loss: 0.4761 - val_accuracy: 0.7683\n",
      "Epoch 15/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4892 - accuracy: 0.7621\n",
      "Epoch 00015: val_loss improved from 0.47613 to 0.47333, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4896 - accuracy: 0.7616 - val_loss: 0.4733 - val_accuracy: 0.7752\n",
      "Epoch 16/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.7628\n",
      "Epoch 00016: val_loss improved from 0.47333 to 0.44834, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4887 - accuracy: 0.7626 - val_loss: 0.4483 - val_accuracy: 0.7893\n",
      "Epoch 17/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4817 - accuracy: 0.7689\n",
      "Epoch 00017: val_loss improved from 0.44834 to 0.43919, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4815 - accuracy: 0.7690 - val_loss: 0.4392 - val_accuracy: 0.7964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7722\n",
      "Epoch 00018: val_loss improved from 0.43919 to 0.43596, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4721 - accuracy: 0.7722 - val_loss: 0.4360 - val_accuracy: 0.7994\n",
      "Epoch 19/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.7731\n",
      "Epoch 00019: val_loss improved from 0.43596 to 0.43561, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4679 - accuracy: 0.7729 - val_loss: 0.4356 - val_accuracy: 0.7975\n",
      "Epoch 20/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4591 - accuracy: 0.7814\n",
      "Epoch 00020: val_loss improved from 0.43561 to 0.43261, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4591 - accuracy: 0.7813 - val_loss: 0.4326 - val_accuracy: 0.8009\n",
      "Epoch 21/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.7789\n",
      "Epoch 00021: val_loss did not improve from 0.43261\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4585 - accuracy: 0.7789 - val_loss: 0.4336 - val_accuracy: 0.8005\n",
      "Epoch 22/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.7880\n",
      "Epoch 00022: val_loss did not improve from 0.43261\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.4520 - accuracy: 0.7880 - val_loss: 0.4522 - val_accuracy: 0.7958\n",
      "Epoch 23/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4452 - accuracy: 0.7903\n",
      "Epoch 00023: val_loss improved from 0.43261 to 0.41602, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4454 - accuracy: 0.7903 - val_loss: 0.4160 - val_accuracy: 0.8029\n",
      "Epoch 24/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.7908\n",
      "Epoch 00024: val_loss improved from 0.41602 to 0.41594, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4416 - accuracy: 0.7909 - val_loss: 0.4159 - val_accuracy: 0.8044\n",
      "Epoch 25/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4353 - accuracy: 0.7960\n",
      "Epoch 00025: val_loss improved from 0.41594 to 0.40953, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4354 - accuracy: 0.7959 - val_loss: 0.4095 - val_accuracy: 0.8143\n",
      "Epoch 26/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4296 - accuracy: 0.8010\n",
      "Epoch 00026: val_loss did not improve from 0.40953\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4297 - accuracy: 0.8013 - val_loss: 0.4098 - val_accuracy: 0.8091\n",
      "Epoch 27/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4243 - accuracy: 0.8011\n",
      "Epoch 00027: val_loss did not improve from 0.40953\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4244 - accuracy: 0.8012 - val_loss: 0.4157 - val_accuracy: 0.8042\n",
      "Epoch 28/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8037\n",
      "Epoch 00028: val_loss did not improve from 0.40953\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.4217 - accuracy: 0.8035 - val_loss: 0.4154 - val_accuracy: 0.8065\n",
      "Epoch 29/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8066\n",
      "Epoch 00029: val_loss did not improve from 0.40953\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4164 - accuracy: 0.8064 - val_loss: 0.4152 - val_accuracy: 0.8061\n",
      "Epoch 30/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4133 - accuracy: 0.8081\n",
      "Epoch 00030: val_loss improved from 0.40953 to 0.40420, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4134 - accuracy: 0.8081 - val_loss: 0.4042 - val_accuracy: 0.8158\n",
      "Epoch 31/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8114\n",
      "Epoch 00031: val_loss improved from 0.40420 to 0.39791, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4117 - accuracy: 0.8114 - val_loss: 0.3979 - val_accuracy: 0.8153\n",
      "Epoch 32/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8103\n",
      "Epoch 00032: val_loss did not improve from 0.39791\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4069 - accuracy: 0.8099 - val_loss: 0.4321 - val_accuracy: 0.8027\n",
      "Epoch 33/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.4041 - accuracy: 0.8128\n",
      "Epoch 00033: val_loss did not improve from 0.39791\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.4043 - accuracy: 0.8127 - val_loss: 0.4028 - val_accuracy: 0.8143\n",
      "Epoch 34/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8148\n",
      "Epoch 00034: val_loss improved from 0.39791 to 0.39765, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3980 - accuracy: 0.8147 - val_loss: 0.3976 - val_accuracy: 0.8166\n",
      "Epoch 35/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8170\n",
      "Epoch 00035: val_loss improved from 0.39765 to 0.39491, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3941 - accuracy: 0.8169 - val_loss: 0.3949 - val_accuracy: 0.8235\n",
      "Epoch 36/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8176\n",
      "Epoch 00036: val_loss did not improve from 0.39491\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3916 - accuracy: 0.8177 - val_loss: 0.4081 - val_accuracy: 0.8106\n",
      "Epoch 37/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3849 - accuracy: 0.8232\n",
      "Epoch 00037: val_loss did not improve from 0.39491\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3847 - accuracy: 0.8230 - val_loss: 0.3961 - val_accuracy: 0.8158\n",
      "Epoch 38/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8199\n",
      "Epoch 00038: val_loss did not improve from 0.39491\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3887 - accuracy: 0.8198 - val_loss: 0.4157 - val_accuracy: 0.8089\n",
      "Epoch 39/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8256\n",
      "Epoch 00039: val_loss did not improve from 0.39491\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3845 - accuracy: 0.8257 - val_loss: 0.4015 - val_accuracy: 0.8147\n",
      "Epoch 40/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8261\n",
      "Epoch 00040: val_loss improved from 0.39491 to 0.39098, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3810 - accuracy: 0.8262 - val_loss: 0.3910 - val_accuracy: 0.8141\n",
      "Epoch 41/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8316\n",
      "Epoch 00041: val_loss did not improve from 0.39098\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3726 - accuracy: 0.8319 - val_loss: 0.4112 - val_accuracy: 0.8089\n",
      "Epoch 42/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8346\n",
      "Epoch 00042: val_loss did not improve from 0.39098\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3666 - accuracy: 0.8343 - val_loss: 0.4354 - val_accuracy: 0.7975\n",
      "Epoch 43/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3668 - accuracy: 0.8321\n",
      "Epoch 00043: val_loss did not improve from 0.39098\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3665 - accuracy: 0.8322 - val_loss: 0.4118 - val_accuracy: 0.8108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8358\n",
      "Epoch 00044: val_loss did not improve from 0.39098\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3683 - accuracy: 0.8357 - val_loss: 0.4099 - val_accuracy: 0.8046\n",
      "Epoch 45/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3626 - accuracy: 0.8323\n",
      "Epoch 00045: val_loss improved from 0.39098 to 0.38738, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3626 - accuracy: 0.8324 - val_loss: 0.3874 - val_accuracy: 0.8231\n",
      "Epoch 46/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8413\n",
      "Epoch 00046: val_loss did not improve from 0.38738\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3547 - accuracy: 0.8416 - val_loss: 0.4048 - val_accuracy: 0.8151\n",
      "Epoch 47/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.8386\n",
      "Epoch 00047: val_loss did not improve from 0.38738\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3519 - accuracy: 0.8389 - val_loss: 0.3901 - val_accuracy: 0.8212\n",
      "Epoch 48/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8435\n",
      "Epoch 00048: val_loss did not improve from 0.38738\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3499 - accuracy: 0.8437 - val_loss: 0.3970 - val_accuracy: 0.8162\n",
      "Epoch 49/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8417\n",
      "Epoch 00049: val_loss improved from 0.38738 to 0.38363, saving model to pickled_objects/batch_size_64_lr_0.16_best_weights_trial_4.h5\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3461 - accuracy: 0.8419 - val_loss: 0.3836 - val_accuracy: 0.8242\n",
      "Epoch 50/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8425\n",
      "Epoch 00050: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3459 - accuracy: 0.8426 - val_loss: 0.3869 - val_accuracy: 0.8205\n",
      "Epoch 51/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.8504\n",
      "Epoch 00051: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3351 - accuracy: 0.8503 - val_loss: 0.4293 - val_accuracy: 0.8037\n",
      "Epoch 52/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.8504\n",
      "Epoch 00052: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3379 - accuracy: 0.8504 - val_loss: 0.4149 - val_accuracy: 0.8130\n",
      "Epoch 53/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8500\n",
      "Epoch 00053: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3342 - accuracy: 0.8496 - val_loss: 0.4016 - val_accuracy: 0.8126\n",
      "Epoch 54/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8542\n",
      "Epoch 00054: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3313 - accuracy: 0.8540 - val_loss: 0.4016 - val_accuracy: 0.8151\n",
      "Epoch 55/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8547\n",
      "Epoch 00055: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3251 - accuracy: 0.8551 - val_loss: 0.3957 - val_accuracy: 0.8169\n",
      "Epoch 56/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8576\n",
      "Epoch 00056: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3247 - accuracy: 0.8578 - val_loss: 0.4341 - val_accuracy: 0.8016\n",
      "Epoch 57/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8597\n",
      "Epoch 00057: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3213 - accuracy: 0.8598 - val_loss: 0.4112 - val_accuracy: 0.8117\n",
      "Epoch 58/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8588\n",
      "Epoch 00058: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3194 - accuracy: 0.8591 - val_loss: 0.3900 - val_accuracy: 0.8184\n",
      "Epoch 59/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8596\n",
      "Epoch 00059: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.3170 - accuracy: 0.8599 - val_loss: 0.3949 - val_accuracy: 0.8214\n",
      "Epoch 60/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8635\n",
      "Epoch 00060: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3110 - accuracy: 0.8637 - val_loss: 0.3951 - val_accuracy: 0.8177\n",
      "Epoch 61/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8626\n",
      "Epoch 00061: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3089 - accuracy: 0.8624 - val_loss: 0.4021 - val_accuracy: 0.8160\n",
      "Epoch 62/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8679\n",
      "Epoch 00062: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3033 - accuracy: 0.8680 - val_loss: 0.3942 - val_accuracy: 0.8267\n",
      "Epoch 63/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8668\n",
      "Epoch 00063: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.3033 - accuracy: 0.8671 - val_loss: 0.3878 - val_accuracy: 0.8233\n",
      "Epoch 64/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8675\n",
      "Epoch 00064: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2977 - accuracy: 0.8677 - val_loss: 0.4010 - val_accuracy: 0.8171\n",
      "Epoch 65/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8705\n",
      "Epoch 00065: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2981 - accuracy: 0.8707 - val_loss: 0.4007 - val_accuracy: 0.8119\n",
      "Epoch 66/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8701\n",
      "Epoch 00066: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2950 - accuracy: 0.8702 - val_loss: 0.3884 - val_accuracy: 0.8216\n",
      "Epoch 67/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8748\n",
      "Epoch 00067: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2883 - accuracy: 0.8750 - val_loss: 0.3964 - val_accuracy: 0.8126\n",
      "Epoch 68/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8785\n",
      "Epoch 00068: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2846 - accuracy: 0.8786 - val_loss: 0.4071 - val_accuracy: 0.8169\n",
      "Epoch 69/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.8765\n",
      "Epoch 00069: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2868 - accuracy: 0.8767 - val_loss: 0.3925 - val_accuracy: 0.8199\n",
      "Epoch 70/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8741\n",
      "Epoch 00070: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2879 - accuracy: 0.8739 - val_loss: 0.3911 - val_accuracy: 0.8199\n",
      "Epoch 71/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.8798\n",
      "Epoch 00071: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2781 - accuracy: 0.8795 - val_loss: 0.4015 - val_accuracy: 0.8173\n",
      "Epoch 72/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.8765\n",
      "Epoch 00072: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2857 - accuracy: 0.8765 - val_loss: 0.4032 - val_accuracy: 0.8233\n",
      "Epoch 73/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.8834\n",
      "Epoch 00073: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2743 - accuracy: 0.8835 - val_loss: 0.4120 - val_accuracy: 0.8149\n",
      "Epoch 74/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8812\n",
      "Epoch 00074: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2775 - accuracy: 0.8812 - val_loss: 0.4409 - val_accuracy: 0.8046\n",
      "Epoch 75/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.8830\n",
      "Epoch 00075: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2739 - accuracy: 0.8829 - val_loss: 0.4068 - val_accuracy: 0.8151\n",
      "Epoch 76/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.8860\n",
      "Epoch 00076: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2674 - accuracy: 0.8863 - val_loss: 0.4000 - val_accuracy: 0.8188\n",
      "Epoch 77/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8893\n",
      "Epoch 00077: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2692 - accuracy: 0.8891 - val_loss: 0.3982 - val_accuracy: 0.8192\n",
      "Epoch 78/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.8865\n",
      "Epoch 00078: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2647 - accuracy: 0.8867 - val_loss: 0.4340 - val_accuracy: 0.8044\n",
      "Epoch 79/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8871\n",
      "Epoch 00079: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2630 - accuracy: 0.8871 - val_loss: 0.4231 - val_accuracy: 0.8046\n",
      "Epoch 80/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.8905\n",
      "Epoch 00080: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2594 - accuracy: 0.8906 - val_loss: 0.4317 - val_accuracy: 0.8055\n",
      "Epoch 81/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.8914\n",
      "Epoch 00081: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2582 - accuracy: 0.8916 - val_loss: 0.4064 - val_accuracy: 0.8171\n",
      "Epoch 82/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8880\n",
      "Epoch 00082: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2590 - accuracy: 0.8883 - val_loss: 0.4082 - val_accuracy: 0.8164\n",
      "Epoch 83/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.8968\n",
      "Epoch 00083: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2516 - accuracy: 0.8969 - val_loss: 0.4450 - val_accuracy: 0.7994\n",
      "Epoch 84/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.8940\n",
      "Epoch 00084: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2511 - accuracy: 0.8943 - val_loss: 0.4411 - val_accuracy: 0.8059\n",
      "Epoch 85/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.8947\n",
      "Epoch 00085: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2526 - accuracy: 0.8948 - val_loss: 0.4095 - val_accuracy: 0.8149\n",
      "Epoch 86/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.8956\n",
      "Epoch 00086: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2503 - accuracy: 0.8955 - val_loss: 0.4211 - val_accuracy: 0.8108\n",
      "Epoch 87/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.8945\n",
      "Epoch 00087: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2499 - accuracy: 0.8945 - val_loss: 0.4127 - val_accuracy: 0.8104\n",
      "Epoch 88/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.8990\n",
      "Epoch 00088: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2437 - accuracy: 0.8991 - val_loss: 0.4216 - val_accuracy: 0.8061\n",
      "Epoch 89/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.8969\n",
      "Epoch 00089: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2456 - accuracy: 0.8970 - val_loss: 0.4034 - val_accuracy: 0.8171\n",
      "Epoch 90/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.8980\n",
      "Epoch 00090: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2445 - accuracy: 0.8983 - val_loss: 0.4302 - val_accuracy: 0.8108\n",
      "Epoch 91/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.8982\n",
      "Epoch 00091: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2415 - accuracy: 0.8982 - val_loss: 0.4058 - val_accuracy: 0.8227\n",
      "Epoch 92/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2409 - accuracy: 0.8972\n",
      "Epoch 00092: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2402 - accuracy: 0.8975 - val_loss: 0.4089 - val_accuracy: 0.8201\n",
      "Epoch 93/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9030\n",
      "Epoch 00093: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2304 - accuracy: 0.9029 - val_loss: 0.4311 - val_accuracy: 0.8158\n",
      "Epoch 94/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9031\n",
      "Epoch 00094: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2304 - accuracy: 0.9032 - val_loss: 0.4425 - val_accuracy: 0.8033\n",
      "Epoch 95/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9008\n",
      "Epoch 00095: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2351 - accuracy: 0.9009 - val_loss: 0.4543 - val_accuracy: 0.7999\n",
      "Epoch 96/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9024\n",
      "Epoch 00096: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2313 - accuracy: 0.9026 - val_loss: 0.4107 - val_accuracy: 0.8141\n",
      "Epoch 97/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2260 - accuracy: 0.9067\n",
      "Epoch 00097: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2265 - accuracy: 0.9067 - val_loss: 0.4232 - val_accuracy: 0.8102\n",
      "Epoch 98/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9064\n",
      "Epoch 00098: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2261 - accuracy: 0.9064 - val_loss: 0.4281 - val_accuracy: 0.8177\n",
      "Epoch 99/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2215 - accuracy: 0.9098\n",
      "Epoch 00099: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2215 - accuracy: 0.9099 - val_loss: 0.4126 - val_accuracy: 0.8214\n",
      "Epoch 100/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9071\n",
      "Epoch 00100: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2228 - accuracy: 0.9072 - val_loss: 0.4454 - val_accuracy: 0.8121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9093\n",
      "Epoch 00101: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2210 - accuracy: 0.9092 - val_loss: 0.4210 - val_accuracy: 0.8184\n",
      "Epoch 102/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.9061\n",
      "Epoch 00102: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2292 - accuracy: 0.9059 - val_loss: 0.4119 - val_accuracy: 0.8119\n",
      "Epoch 103/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9101\n",
      "Epoch 00103: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2194 - accuracy: 0.9102 - val_loss: 0.4218 - val_accuracy: 0.8087\n",
      "Epoch 104/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.9078\n",
      "Epoch 00104: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2180 - accuracy: 0.9077 - val_loss: 0.3990 - val_accuracy: 0.8222\n",
      "Epoch 105/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9134\n",
      "Epoch 00105: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.2155 - accuracy: 0.9134 - val_loss: 0.4300 - val_accuracy: 0.8050\n",
      "Epoch 106/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9116\n",
      "Epoch 00106: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2105 - accuracy: 0.9119 - val_loss: 0.4402 - val_accuracy: 0.8037\n",
      "Epoch 107/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9099\n",
      "Epoch 00107: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2175 - accuracy: 0.9100 - val_loss: 0.3956 - val_accuracy: 0.8216\n",
      "Epoch 108/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9125\n",
      "Epoch 00108: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2099 - accuracy: 0.9125 - val_loss: 0.4264 - val_accuracy: 0.8162\n",
      "Epoch 109/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9108\n",
      "Epoch 00109: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2126 - accuracy: 0.9107 - val_loss: 0.4354 - val_accuracy: 0.8085\n",
      "Epoch 110/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9149\n",
      "Epoch 00110: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2077 - accuracy: 0.9150 - val_loss: 0.4124 - val_accuracy: 0.8145\n",
      "Epoch 111/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9127\n",
      "Epoch 00111: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2131 - accuracy: 0.9128 - val_loss: 0.4232 - val_accuracy: 0.8136\n",
      "Epoch 112/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9184\n",
      "Epoch 00112: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2033 - accuracy: 0.9183 - val_loss: 0.4577 - val_accuracy: 0.8029\n",
      "Epoch 113/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9168\n",
      "Epoch 00113: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2073 - accuracy: 0.9171 - val_loss: 0.4281 - val_accuracy: 0.8175\n",
      "Epoch 114/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9169\n",
      "Epoch 00114: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2066 - accuracy: 0.9168 - val_loss: 0.4217 - val_accuracy: 0.8201\n",
      "Epoch 115/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9176\n",
      "Epoch 00115: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2046 - accuracy: 0.9177 - val_loss: 0.4700 - val_accuracy: 0.8057\n",
      "Epoch 116/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9139\n",
      "Epoch 00116: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2068 - accuracy: 0.9140 - val_loss: 0.4491 - val_accuracy: 0.8106\n",
      "Epoch 117/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9174\n",
      "Epoch 00117: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 35ms/step - loss: 0.2044 - accuracy: 0.9175 - val_loss: 0.4444 - val_accuracy: 0.8145\n",
      "Epoch 118/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2026 - accuracy: 0.9181\n",
      "Epoch 00118: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2027 - accuracy: 0.9180 - val_loss: 0.4331 - val_accuracy: 0.8181\n",
      "Epoch 119/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9184\n",
      "Epoch 00119: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2009 - accuracy: 0.9184 - val_loss: 0.4762 - val_accuracy: 0.8074\n",
      "Epoch 120/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1951 - accuracy: 0.9207\n",
      "Epoch 00120: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1948 - accuracy: 0.9209 - val_loss: 0.4241 - val_accuracy: 0.8115\n",
      "Epoch 121/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9186\n",
      "Epoch 00121: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1998 - accuracy: 0.9188 - val_loss: 0.4514 - val_accuracy: 0.8042\n",
      "Epoch 122/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9179\n",
      "Epoch 00122: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.2040 - accuracy: 0.9179 - val_loss: 0.4844 - val_accuracy: 0.7984\n",
      "Epoch 123/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9184\n",
      "Epoch 00123: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1984 - accuracy: 0.9185 - val_loss: 0.4261 - val_accuracy: 0.8162\n",
      "Epoch 124/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9216\n",
      "Epoch 00124: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1894 - accuracy: 0.9217 - val_loss: 0.4539 - val_accuracy: 0.8136\n",
      "Epoch 125/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9220\n",
      "Epoch 00125: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1921 - accuracy: 0.9221 - val_loss: 0.4556 - val_accuracy: 0.8128\n",
      "Epoch 126/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.9199\n",
      "Epoch 00126: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1963 - accuracy: 0.9201 - val_loss: 0.4462 - val_accuracy: 0.8143\n",
      "Epoch 127/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9237\n",
      "Epoch 00127: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1896 - accuracy: 0.9236 - val_loss: 0.4798 - val_accuracy: 0.8134\n",
      "Epoch 128/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9259\n",
      "Epoch 00128: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1900 - accuracy: 0.9263 - val_loss: 0.4523 - val_accuracy: 0.8138\n",
      "Epoch 129/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9263\n",
      "Epoch 00129: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1881 - accuracy: 0.9264 - val_loss: 0.4768 - val_accuracy: 0.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9245\n",
      "Epoch 00130: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1892 - accuracy: 0.9244 - val_loss: 0.4556 - val_accuracy: 0.8063\n",
      "Epoch 131/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9264\n",
      "Epoch 00131: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1845 - accuracy: 0.9263 - val_loss: 0.4512 - val_accuracy: 0.8102\n",
      "Epoch 132/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9279\n",
      "Epoch 00132: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1811 - accuracy: 0.9280 - val_loss: 0.4834 - val_accuracy: 0.8136\n",
      "Epoch 133/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9227\n",
      "Epoch 00133: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1890 - accuracy: 0.9229 - val_loss: 0.5030 - val_accuracy: 0.8007\n",
      "Epoch 134/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1865 - accuracy: 0.9253\n",
      "Epoch 00134: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1867 - accuracy: 0.9251 - val_loss: 0.5186 - val_accuracy: 0.7917\n",
      "Epoch 135/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9271\n",
      "Epoch 00135: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1854 - accuracy: 0.9271 - val_loss: 0.4613 - val_accuracy: 0.8098\n",
      "Epoch 136/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9276\n",
      "Epoch 00136: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1837 - accuracy: 0.9278 - val_loss: 0.4502 - val_accuracy: 0.8190\n",
      "Epoch 137/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9264\n",
      "Epoch 00137: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1846 - accuracy: 0.9264 - val_loss: 0.4327 - val_accuracy: 0.8171\n",
      "Epoch 138/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9233\n",
      "Epoch 00138: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1889 - accuracy: 0.9232 - val_loss: 0.5258 - val_accuracy: 0.7835\n",
      "Epoch 139/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9265\n",
      "Epoch 00139: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1845 - accuracy: 0.9267 - val_loss: 0.4626 - val_accuracy: 0.8083\n",
      "Epoch 140/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9277\n",
      "Epoch 00140: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1774 - accuracy: 0.9278 - val_loss: 0.4302 - val_accuracy: 0.8181\n",
      "Epoch 141/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9292\n",
      "Epoch 00141: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1775 - accuracy: 0.9290 - val_loss: 0.4257 - val_accuracy: 0.8246\n",
      "Epoch 142/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1853 - accuracy: 0.9260\n",
      "Epoch 00142: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1848 - accuracy: 0.9262 - val_loss: 0.4684 - val_accuracy: 0.8134\n",
      "Epoch 143/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9312\n",
      "Epoch 00143: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1790 - accuracy: 0.9313 - val_loss: 0.4252 - val_accuracy: 0.8179\n",
      "Epoch 144/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9293\n",
      "Epoch 00144: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1768 - accuracy: 0.9293 - val_loss: 0.4820 - val_accuracy: 0.7975\n",
      "Epoch 145/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9286\n",
      "Epoch 00145: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1842 - accuracy: 0.9285 - val_loss: 0.4485 - val_accuracy: 0.8098\n",
      "Epoch 146/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9301\n",
      "Epoch 00146: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1744 - accuracy: 0.9301 - val_loss: 0.4787 - val_accuracy: 0.8057\n",
      "Epoch 147/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9267\n",
      "Epoch 00147: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 33ms/step - loss: 0.1807 - accuracy: 0.9268 - val_loss: 0.4521 - val_accuracy: 0.8136\n",
      "Epoch 148/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9311\n",
      "Epoch 00148: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1767 - accuracy: 0.9312 - val_loss: 0.4104 - val_accuracy: 0.8224\n",
      "Epoch 149/10000\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9284\n",
      "Epoch 00149: val_loss did not improve from 0.38363\n",
      "291/291 [==============================] - 10s 34ms/step - loss: 0.1787 - accuracy: 0.9282 - val_loss: 0.4287 - val_accuracy: 0.8171\n",
      "Epoch 00149: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "    146/Unknown - 23s 159ms/step - loss: 0.6935 - accuracy: 0.5035\n",
      "Epoch 00001: val_loss improved from inf to 0.69275, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 29s 201ms/step - loss: 0.6935 - accuracy: 0.5035 - val_loss: 0.6928 - val_accuracy: 0.5400\n",
      "Epoch 2/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5154\n",
      "Epoch 00002: val_loss improved from 0.69275 to 0.69259, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6922 - accuracy: 0.5153 - val_loss: 0.6926 - val_accuracy: 0.4942\n",
      "Epoch 3/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6915 - accuracy: 0.5264\n",
      "Epoch 00003: val_loss did not improve from 0.69259\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6916 - accuracy: 0.5263 - val_loss: 0.6928 - val_accuracy: 0.4910\n",
      "Epoch 4/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5421\n",
      "Epoch 00004: val_loss did not improve from 0.69259\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6907 - accuracy: 0.5420 - val_loss: 0.6931 - val_accuracy: 0.4912\n",
      "Epoch 5/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5599\n",
      "Epoch 00005: val_loss did not improve from 0.69259\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6891 - accuracy: 0.5599 - val_loss: 0.6927 - val_accuracy: 0.4920\n",
      "Epoch 6/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.5678\n",
      "Epoch 00006: val_loss did not improve from 0.69259\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6871 - accuracy: 0.5678 - val_loss: 0.6930 - val_accuracy: 0.4920\n",
      "Epoch 7/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5766\n",
      "Epoch 00007: val_loss did not improve from 0.69259\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6851 - accuracy: 0.5764 - val_loss: 0.6927 - val_accuracy: 0.4936\n",
      "Epoch 8/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/146 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5824\n",
      "Epoch 00008: val_loss improved from 0.69259 to 0.69237, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6826 - accuracy: 0.5823 - val_loss: 0.6924 - val_accuracy: 0.4961\n",
      "Epoch 9/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5835\n",
      "Epoch 00009: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6805 - accuracy: 0.5835 - val_loss: 0.6934 - val_accuracy: 0.4968\n",
      "Epoch 10/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6778 - accuracy: 0.5870\n",
      "Epoch 00010: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6776 - accuracy: 0.5870 - val_loss: 0.6933 - val_accuracy: 0.4998\n",
      "Epoch 11/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6759 - accuracy: 0.5884\n",
      "Epoch 00011: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6758 - accuracy: 0.5883 - val_loss: 0.6954 - val_accuracy: 0.5009\n",
      "Epoch 12/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.5927\n",
      "Epoch 00012: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6724 - accuracy: 0.5927 - val_loss: 0.6981 - val_accuracy: 0.5019\n",
      "Epoch 13/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6696 - accuracy: 0.5955\n",
      "Epoch 00013: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6697 - accuracy: 0.5953 - val_loss: 0.6971 - val_accuracy: 0.5095\n",
      "Epoch 14/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6662 - accuracy: 0.6009\n",
      "Epoch 00014: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6660 - accuracy: 0.6012 - val_loss: 0.6970 - val_accuracy: 0.5142\n",
      "Epoch 15/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6648 - accuracy: 0.6012\n",
      "Epoch 00015: val_loss did not improve from 0.69237\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6648 - accuracy: 0.6010 - val_loss: 0.6964 - val_accuracy: 0.5211\n",
      "Epoch 16/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6067 ETA: 0s - loss: 0.6624 - accuracy\n",
      "Epoch 00016: val_loss improved from 0.69237 to 0.68707, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6623 - accuracy: 0.6065 - val_loss: 0.6871 - val_accuracy: 0.5439\n",
      "Epoch 17/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6596 - accuracy: 0.6051\n",
      "Epoch 00017: val_loss did not improve from 0.68707\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6595 - accuracy: 0.6050 - val_loss: 0.7004 - val_accuracy: 0.5236\n",
      "Epoch 18/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6593 - accuracy: 0.6059\n",
      "Epoch 00018: val_loss did not improve from 0.68707\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6593 - accuracy: 0.6059 - val_loss: 0.6917 - val_accuracy: 0.5415\n",
      "Epoch 19/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6564 - accuracy: 0.6074\n",
      "Epoch 00019: val_loss improved from 0.68707 to 0.68500, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6561 - accuracy: 0.6078 - val_loss: 0.6850 - val_accuracy: 0.5533\n",
      "Epoch 20/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6564 - accuracy: 0.6062\n",
      "Epoch 00020: val_loss improved from 0.68500 to 0.68345, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6562 - accuracy: 0.6066 - val_loss: 0.6834 - val_accuracy: 0.5567\n",
      "Epoch 21/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6538 - accuracy: 0.6122\n",
      "Epoch 00021: val_loss improved from 0.68345 to 0.67440, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6536 - accuracy: 0.6122 - val_loss: 0.6744 - val_accuracy: 0.5765\n",
      "Epoch 22/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6528 - accuracy: 0.6109\n",
      "Epoch 00022: val_loss did not improve from 0.67440\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6525 - accuracy: 0.6115 - val_loss: 0.6801 - val_accuracy: 0.5656\n",
      "Epoch 23/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.6099\n",
      "Epoch 00023: val_loss did not improve from 0.67440\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6515 - accuracy: 0.6098 - val_loss: 0.6768 - val_accuracy: 0.5733\n",
      "Epoch 24/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6510 - accuracy: 0.6116\n",
      "Epoch 00024: val_loss improved from 0.67440 to 0.66901, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6509 - accuracy: 0.6117 - val_loss: 0.6690 - val_accuracy: 0.5856\n",
      "Epoch 25/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6505 - accuracy: 0.6143\n",
      "Epoch 00025: val_loss improved from 0.66901 to 0.66029, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6500 - accuracy: 0.6148 - val_loss: 0.6603 - val_accuracy: 0.6004\n",
      "Epoch 26/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6142\n",
      "Epoch 00026: val_loss did not improve from 0.66029\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6493 - accuracy: 0.6141 - val_loss: 0.6656 - val_accuracy: 0.5896\n",
      "Epoch 27/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6471 - accuracy: 0.6160\n",
      "Epoch 00027: val_loss did not improve from 0.66029\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6470 - accuracy: 0.6159 - val_loss: 0.6607 - val_accuracy: 0.5995\n",
      "Epoch 28/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6471 - accuracy: 0.6156\n",
      "Epoch 00028: val_loss did not improve from 0.66029\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6467 - accuracy: 0.6160 - val_loss: 0.6636 - val_accuracy: 0.5924\n",
      "Epoch 29/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6453 - accuracy: 0.6197\n",
      "Epoch 00029: val_loss improved from 0.66029 to 0.65667, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6450 - accuracy: 0.6200 - val_loss: 0.6567 - val_accuracy: 0.6049\n",
      "Epoch 30/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6445 - accuracy: 0.6189\n",
      "Epoch 00030: val_loss did not improve from 0.65667\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6442 - accuracy: 0.6193 - val_loss: 0.6610 - val_accuracy: 0.5965\n",
      "Epoch 31/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.6227\n",
      "Epoch 00031: val_loss improved from 0.65667 to 0.65297, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6430 - accuracy: 0.6227 - val_loss: 0.6530 - val_accuracy: 0.6122\n",
      "Epoch 32/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6417 - accuracy: 0.6202\n",
      "Epoch 00032: val_loss improved from 0.65297 to 0.65292, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6417 - accuracy: 0.6200 - val_loss: 0.6529 - val_accuracy: 0.6107\n",
      "Epoch 33/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.6230\n",
      "Epoch 00033: val_loss improved from 0.65292 to 0.65002, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6411 - accuracy: 0.6228 - val_loss: 0.6500 - val_accuracy: 0.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6407 - accuracy: 0.6254\n",
      "Epoch 00034: val_loss improved from 0.65002 to 0.63971, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6406 - accuracy: 0.6253 - val_loss: 0.6397 - val_accuracy: 0.6303\n",
      "Epoch 35/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6392 - accuracy: 0.6255\n",
      "Epoch 00035: val_loss did not improve from 0.63971\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6394 - accuracy: 0.6253 - val_loss: 0.6464 - val_accuracy: 0.6180\n",
      "Epoch 36/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6385 - accuracy: 0.6270\n",
      "Epoch 00036: val_loss improved from 0.63971 to 0.63875, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6383 - accuracy: 0.6269 - val_loss: 0.6388 - val_accuracy: 0.6270\n",
      "Epoch 37/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6361 - accuracy: 0.6301\n",
      "Epoch 00037: val_loss improved from 0.63875 to 0.63811, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6361 - accuracy: 0.6300 - val_loss: 0.6381 - val_accuracy: 0.6292\n",
      "Epoch 38/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6350 - accuracy: 0.6306\n",
      "Epoch 00038: val_loss improved from 0.63811 to 0.63406, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6346 - accuracy: 0.6310 - val_loss: 0.6341 - val_accuracy: 0.6359\n",
      "Epoch 39/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6341 - accuracy: 0.6317\n",
      "Epoch 00039: val_loss did not improve from 0.63406\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6341 - accuracy: 0.6316 - val_loss: 0.6360 - val_accuracy: 0.6311\n",
      "Epoch 40/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6328 - accuracy: 0.6356\n",
      "Epoch 00040: val_loss improved from 0.63406 to 0.62939, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6327 - accuracy: 0.6355 - val_loss: 0.6294 - val_accuracy: 0.6423\n",
      "Epoch 41/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6327 - accuracy: 0.6317\n",
      "Epoch 00041: val_loss improved from 0.62939 to 0.62630, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6328 - accuracy: 0.6315 - val_loss: 0.6263 - val_accuracy: 0.6468\n",
      "Epoch 42/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6304 - accuracy: 0.6395\n",
      "Epoch 00042: val_loss improved from 0.62630 to 0.62460, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6300 - accuracy: 0.6397 - val_loss: 0.6246 - val_accuracy: 0.6490\n",
      "Epoch 43/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6289 - accuracy: 0.6354\n",
      "Epoch 00043: val_loss did not improve from 0.62460\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6286 - accuracy: 0.6356 - val_loss: 0.6259 - val_accuracy: 0.6404\n",
      "Epoch 44/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.6387\n",
      "Epoch 00044: val_loss improved from 0.62460 to 0.62395, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6282 - accuracy: 0.6385 - val_loss: 0.6239 - val_accuracy: 0.6453\n",
      "Epoch 45/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6260 - accuracy: 0.6429\n",
      "Epoch 00045: val_loss improved from 0.62395 to 0.62074, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6262 - accuracy: 0.6427 - val_loss: 0.6207 - val_accuracy: 0.6496\n",
      "Epoch 46/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6258 - accuracy: 0.6416\n",
      "Epoch 00046: val_loss improved from 0.62074 to 0.61810, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6259 - accuracy: 0.6416 - val_loss: 0.6181 - val_accuracy: 0.6541\n",
      "Epoch 47/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6260 - accuracy: 0.6419\n",
      "Epoch 00047: val_loss improved from 0.61810 to 0.61640, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6260 - accuracy: 0.6419 - val_loss: 0.6164 - val_accuracy: 0.6567\n",
      "Epoch 48/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.6460\n",
      "Epoch 00048: val_loss improved from 0.61640 to 0.61401, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6218 - accuracy: 0.6458 - val_loss: 0.6140 - val_accuracy: 0.6599\n",
      "Epoch 49/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.6479\n",
      "Epoch 00049: val_loss improved from 0.61401 to 0.61334, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6212 - accuracy: 0.6480 - val_loss: 0.6133 - val_accuracy: 0.6576\n",
      "Epoch 50/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.6483\n",
      "Epoch 00050: val_loss improved from 0.61334 to 0.61088, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6206 - accuracy: 0.6482 - val_loss: 0.6109 - val_accuracy: 0.6632\n",
      "Epoch 51/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6186 - accuracy: 0.6469\n",
      "Epoch 00051: val_loss did not improve from 0.61088\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6188 - accuracy: 0.6466 - val_loss: 0.6130 - val_accuracy: 0.6563\n",
      "Epoch 52/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6177 - accuracy: 0.6539\n",
      "Epoch 00052: val_loss improved from 0.61088 to 0.60954, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6178 - accuracy: 0.6537 - val_loss: 0.6095 - val_accuracy: 0.6597\n",
      "Epoch 53/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6148 - accuracy: 0.6520\n",
      "Epoch 00053: val_loss improved from 0.60954 to 0.60887, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6148 - accuracy: 0.6519 - val_loss: 0.6089 - val_accuracy: 0.6569\n",
      "Epoch 54/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6133 - accuracy: 0.6557\n",
      "Epoch 00054: val_loss improved from 0.60887 to 0.60683, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6134 - accuracy: 0.6557 - val_loss: 0.6068 - val_accuracy: 0.6634\n",
      "Epoch 55/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6130 - accuracy: 0.6554\n",
      "Epoch 00055: val_loss did not improve from 0.60683\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6128 - accuracy: 0.6555 - val_loss: 0.6076 - val_accuracy: 0.6597\n",
      "Epoch 56/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.6589\n",
      "Epoch 00056: val_loss did not improve from 0.60683\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6099 - accuracy: 0.6587 - val_loss: 0.6115 - val_accuracy: 0.6518\n",
      "Epoch 57/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6082 - accuracy: 0.6608\n",
      "Epoch 00057: val_loss did not improve from 0.60683\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6086 - accuracy: 0.6604 - val_loss: 0.6157 - val_accuracy: 0.6447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.6626\n",
      "Epoch 00058: val_loss improved from 0.60683 to 0.60055, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6060 - accuracy: 0.6624 - val_loss: 0.6006 - val_accuracy: 0.6660\n",
      "Epoch 59/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6040 - accuracy: 0.6662\n",
      "Epoch 00059: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6044 - accuracy: 0.6657 - val_loss: 0.6061 - val_accuracy: 0.6606\n",
      "Epoch 60/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5997 - accuracy: 0.6689\n",
      "Epoch 00060: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6001 - accuracy: 0.6684 - val_loss: 0.6103 - val_accuracy: 0.6511\n",
      "Epoch 61/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5953 - accuracy: 0.6714\n",
      "Epoch 00061: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5956 - accuracy: 0.6713 - val_loss: 0.6045 - val_accuracy: 0.6574\n",
      "Epoch 62/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.6769\n",
      "Epoch 00062: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5946 - accuracy: 0.6762 - val_loss: 0.6120 - val_accuracy: 0.6479\n",
      "Epoch 63/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5882 - accuracy: 0.6816\n",
      "Epoch 00063: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5887 - accuracy: 0.6811 - val_loss: 0.6115 - val_accuracy: 0.6498\n",
      "Epoch 64/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.6852\n",
      "Epoch 00064: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5855 - accuracy: 0.6850 - val_loss: 0.6146 - val_accuracy: 0.6442\n",
      "Epoch 65/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5823 - accuracy: 0.6867\n",
      "Epoch 00065: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5835 - accuracy: 0.6858 - val_loss: 0.6137 - val_accuracy: 0.6425\n",
      "Epoch 66/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.6889\n",
      "Epoch 00066: val_loss did not improve from 0.60055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5807 - accuracy: 0.6887 - val_loss: 0.6202 - val_accuracy: 0.6380\n",
      "Epoch 67/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.6913\n",
      "Epoch 00067: val_loss improved from 0.60055 to 0.59174, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5757 - accuracy: 0.6910 - val_loss: 0.5917 - val_accuracy: 0.6675\n",
      "Epoch 68/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.6965\n",
      "Epoch 00068: val_loss did not improve from 0.59174\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5735 - accuracy: 0.6962 - val_loss: 0.5971 - val_accuracy: 0.6580\n",
      "Epoch 69/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.6959\n",
      "Epoch 00069: val_loss did not improve from 0.59174\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5716 - accuracy: 0.6955 - val_loss: 0.5982 - val_accuracy: 0.6550\n",
      "Epoch 70/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7010\n",
      "Epoch 00070: val_loss did not improve from 0.59174\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5678 - accuracy: 0.7008 - val_loss: 0.6018 - val_accuracy: 0.6546\n",
      "Epoch 71/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7051\n",
      "Epoch 00071: val_loss did not improve from 0.59174\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5637 - accuracy: 0.7049 - val_loss: 0.5984 - val_accuracy: 0.6638\n",
      "Epoch 72/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7047\n",
      "Epoch 00072: val_loss improved from 0.59174 to 0.57370, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5610 - accuracy: 0.7042 - val_loss: 0.5737 - val_accuracy: 0.6851\n",
      "Epoch 73/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7076\n",
      "Epoch 00073: val_loss did not improve from 0.57370\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5613 - accuracy: 0.7071 - val_loss: 0.5742 - val_accuracy: 0.6939\n",
      "Epoch 74/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.7079\n",
      "Epoch 00074: val_loss did not improve from 0.57370\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5594 - accuracy: 0.7075 - val_loss: 0.5797 - val_accuracy: 0.6771\n",
      "Epoch 75/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5547 - accuracy: 0.7122\n",
      "Epoch 00075: val_loss improved from 0.57370 to 0.57346, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5552 - accuracy: 0.7121 - val_loss: 0.5735 - val_accuracy: 0.6971\n",
      "Epoch 76/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5542 - accuracy: 0.7137\n",
      "Epoch 00076: val_loss improved from 0.57346 to 0.54521, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5554 - accuracy: 0.7128 - val_loss: 0.5452 - val_accuracy: 0.7334\n",
      "Epoch 77/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7141\n",
      "Epoch 00077: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5518 - accuracy: 0.7141 - val_loss: 0.5663 - val_accuracy: 0.6902\n",
      "Epoch 78/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.7176\n",
      "Epoch 00078: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5496 - accuracy: 0.7170 - val_loss: 0.5664 - val_accuracy: 0.6900\n",
      "Epoch 79/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7198\n",
      "Epoch 00079: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5464 - accuracy: 0.7197 - val_loss: 0.6093 - val_accuracy: 0.6604\n",
      "Epoch 80/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5444 - accuracy: 0.7214\n",
      "Epoch 00080: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5452 - accuracy: 0.7211 - val_loss: 0.5486 - val_accuracy: 0.7225\n",
      "Epoch 81/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7175\n",
      "Epoch 00081: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5470 - accuracy: 0.7172 - val_loss: 0.5553 - val_accuracy: 0.7025\n",
      "Epoch 82/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7237\n",
      "Epoch 00082: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5440 - accuracy: 0.7235 - val_loss: 0.5586 - val_accuracy: 0.7085\n",
      "Epoch 83/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7224\n",
      "Epoch 00083: val_loss did not improve from 0.54521\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5419 - accuracy: 0.7221 - val_loss: 0.5493 - val_accuracy: 0.7079\n",
      "Epoch 84/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7238\n",
      "Epoch 00084: val_loss improved from 0.54521 to 0.54484, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5407 - accuracy: 0.7234 - val_loss: 0.5448 - val_accuracy: 0.7246\n",
      "Epoch 85/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7263\n",
      "Epoch 00085: val_loss improved from 0.54484 to 0.52921, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5396 - accuracy: 0.7260 - val_loss: 0.5292 - val_accuracy: 0.7341\n",
      "Epoch 86/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7270\n",
      "Epoch 00086: val_loss did not improve from 0.52921\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5364 - accuracy: 0.7266 - val_loss: 0.5333 - val_accuracy: 0.7261\n",
      "Epoch 87/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5352 - accuracy: 0.7276\n",
      "Epoch 00087: val_loss did not improve from 0.52921\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5361 - accuracy: 0.7275 - val_loss: 0.5401 - val_accuracy: 0.7463\n",
      "Epoch 88/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7287\n",
      "Epoch 00088: val_loss did not improve from 0.52921\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5348 - accuracy: 0.7286 - val_loss: 0.5582 - val_accuracy: 0.7012\n",
      "Epoch 89/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.7309\n",
      "Epoch 00089: val_loss did not improve from 0.52921\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5337 - accuracy: 0.7305 - val_loss: 0.5497 - val_accuracy: 0.7081\n",
      "Epoch 90/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.7348\n",
      "Epoch 00090: val_loss improved from 0.52921 to 0.52556, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5307 - accuracy: 0.7344 - val_loss: 0.5256 - val_accuracy: 0.7322\n",
      "Epoch 91/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5306 - accuracy: 0.7341\n",
      "Epoch 00091: val_loss did not improve from 0.52556\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5315 - accuracy: 0.7336 - val_loss: 0.5344 - val_accuracy: 0.7227\n",
      "Epoch 92/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5296 - accuracy: 0.7322\n",
      "Epoch 00092: val_loss improved from 0.52556 to 0.52163, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5305 - accuracy: 0.7316 - val_loss: 0.5216 - val_accuracy: 0.7603\n",
      "Epoch 93/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7343\n",
      "Epoch 00093: val_loss did not improve from 0.52163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5283 - accuracy: 0.7339 - val_loss: 0.5347 - val_accuracy: 0.7212\n",
      "Epoch 94/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5262 - accuracy: 0.7386\n",
      "Epoch 00094: val_loss did not improve from 0.52163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5273 - accuracy: 0.7383 - val_loss: 0.5279 - val_accuracy: 0.7373\n",
      "Epoch 95/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7336\n",
      "Epoch 00095: val_loss did not improve from 0.52163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5275 - accuracy: 0.7333 - val_loss: 0.5412 - val_accuracy: 0.7141\n",
      "Epoch 96/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7373\n",
      "Epoch 00096: val_loss did not improve from 0.52163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5248 - accuracy: 0.7369 - val_loss: 0.5319 - val_accuracy: 0.7227\n",
      "Epoch 97/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7390\n",
      "Epoch 00097: val_loss did not improve from 0.52163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5230 - accuracy: 0.7388 - val_loss: 0.5586 - val_accuracy: 0.7023\n",
      "Epoch 98/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7410\n",
      "Epoch 00098: val_loss improved from 0.52163 to 0.51804, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5231 - accuracy: 0.7406 - val_loss: 0.5180 - val_accuracy: 0.7545\n",
      "Epoch 99/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7355\n",
      "Epoch 00099: val_loss improved from 0.51804 to 0.51774, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5226 - accuracy: 0.7352 - val_loss: 0.5177 - val_accuracy: 0.7414\n",
      "Epoch 100/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7424\n",
      "Epoch 00100: val_loss did not improve from 0.51774\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5206 - accuracy: 0.7422 - val_loss: 0.5197 - val_accuracy: 0.7388\n",
      "Epoch 101/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5177 - accuracy: 0.7412\n",
      "Epoch 00101: val_loss did not improve from 0.51774\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5195 - accuracy: 0.7405 - val_loss: 0.5281 - val_accuracy: 0.7270\n",
      "Epoch 102/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5170 - accuracy: 0.7456\n",
      "Epoch 00102: val_loss did not improve from 0.51774\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5184 - accuracy: 0.7452 - val_loss: 0.5189 - val_accuracy: 0.7414\n",
      "Epoch 103/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.7400\n",
      "Epoch 00103: val_loss improved from 0.51774 to 0.51534, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5182 - accuracy: 0.7398 - val_loss: 0.5153 - val_accuracy: 0.7476\n",
      "Epoch 104/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7438\n",
      "Epoch 00104: val_loss did not improve from 0.51534\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5156 - accuracy: 0.7435 - val_loss: 0.5211 - val_accuracy: 0.7337\n",
      "Epoch 105/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5175 - accuracy: 0.7427\n",
      "Epoch 00105: val_loss did not improve from 0.51534\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5187 - accuracy: 0.7422 - val_loss: 0.5284 - val_accuracy: 0.7274\n",
      "Epoch 106/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7448\n",
      "Epoch 00106: val_loss improved from 0.51534 to 0.49730, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5161 - accuracy: 0.7443 - val_loss: 0.4973 - val_accuracy: 0.7661\n",
      "Epoch 107/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7471\n",
      "Epoch 00107: val_loss improved from 0.49730 to 0.49215, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5146 - accuracy: 0.7468 - val_loss: 0.4922 - val_accuracy: 0.7620\n",
      "Epoch 108/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7449\n",
      "Epoch 00108: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5112 - accuracy: 0.7449 - val_loss: 0.5055 - val_accuracy: 0.7506\n",
      "Epoch 109/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7462\n",
      "Epoch 00109: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5129 - accuracy: 0.7459 - val_loss: 0.5029 - val_accuracy: 0.7545\n",
      "Epoch 110/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5082 - accuracy: 0.7488\n",
      "Epoch 00110: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5086 - accuracy: 0.7485 - val_loss: 0.5371 - val_accuracy: 0.7173\n",
      "Epoch 111/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5099 - accuracy: 0.7466\n",
      "Epoch 00111: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5104 - accuracy: 0.7463 - val_loss: 0.5225 - val_accuracy: 0.7317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5097 - accuracy: 0.7466\n",
      "Epoch 00112: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5104 - accuracy: 0.7464 - val_loss: 0.5001 - val_accuracy: 0.7539\n",
      "Epoch 113/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5052 - accuracy: 0.7513\n",
      "Epoch 00113: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5062 - accuracy: 0.7510 - val_loss: 0.5000 - val_accuracy: 0.7554\n",
      "Epoch 114/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7485\n",
      "Epoch 00114: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5067 - accuracy: 0.7483 - val_loss: 0.4982 - val_accuracy: 0.7620\n",
      "Epoch 115/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.7518\n",
      "Epoch 00115: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5039 - accuracy: 0.7519 - val_loss: 0.5270 - val_accuracy: 0.7218\n",
      "Epoch 116/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7472\n",
      "Epoch 00116: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5074 - accuracy: 0.7468 - val_loss: 0.4992 - val_accuracy: 0.7683\n",
      "Epoch 117/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7490\n",
      "Epoch 00117: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5070 - accuracy: 0.7487 - val_loss: 0.4942 - val_accuracy: 0.7635\n",
      "Epoch 118/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5053 - accuracy: 0.7480\n",
      "Epoch 00118: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5059 - accuracy: 0.7477 - val_loss: 0.5010 - val_accuracy: 0.7526\n",
      "Epoch 119/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7531\n",
      "Epoch 00119: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5018 - accuracy: 0.7526 - val_loss: 0.4922 - val_accuracy: 0.7638\n",
      "Epoch 120/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7520\n",
      "Epoch 00120: val_loss did not improve from 0.49215\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5017 - accuracy: 0.7518 - val_loss: 0.4975 - val_accuracy: 0.7560\n",
      "Epoch 121/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5016 - accuracy: 0.7513\n",
      "Epoch 00121: val_loss improved from 0.49215 to 0.49072, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5025 - accuracy: 0.7511 - val_loss: 0.4907 - val_accuracy: 0.7631\n",
      "Epoch 122/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4970 - accuracy: 0.7539\n",
      "Epoch 00122: val_loss improved from 0.49072 to 0.48794, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4982 - accuracy: 0.7535 - val_loss: 0.4879 - val_accuracy: 0.7674\n",
      "Epoch 123/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.7537\n",
      "Epoch 00123: val_loss did not improve from 0.48794\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5026 - accuracy: 0.7536 - val_loss: 0.5205 - val_accuracy: 0.7291\n",
      "Epoch 124/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4972 - accuracy: 0.7581\n",
      "Epoch 00124: val_loss did not improve from 0.48794\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4979 - accuracy: 0.7579 - val_loss: 0.4971 - val_accuracy: 0.7541\n",
      "Epoch 125/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4976 - accuracy: 0.7558\n",
      "Epoch 00125: val_loss did not improve from 0.48794\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4982 - accuracy: 0.7557 - val_loss: 0.5288 - val_accuracy: 0.7244\n",
      "Epoch 126/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7547\n",
      "Epoch 00126: val_loss did not improve from 0.48794\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4989 - accuracy: 0.7544 - val_loss: 0.4958 - val_accuracy: 0.7567\n",
      "Epoch 127/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7576\n",
      "Epoch 00127: val_loss improved from 0.48794 to 0.48673, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4965 - accuracy: 0.7572 - val_loss: 0.4867 - val_accuracy: 0.7638\n",
      "Epoch 128/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4964 - accuracy: 0.7576\n",
      "Epoch 00128: val_loss did not improve from 0.48673\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4969 - accuracy: 0.7575 - val_loss: 0.4995 - val_accuracy: 0.7509\n",
      "Epoch 129/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7557\n",
      "Epoch 00129: val_loss improved from 0.48673 to 0.48009, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4983 - accuracy: 0.7553 - val_loss: 0.4801 - val_accuracy: 0.7698\n",
      "Epoch 130/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7574\n",
      "Epoch 00130: val_loss improved from 0.48009 to 0.47843, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4960 - accuracy: 0.7570 - val_loss: 0.4784 - val_accuracy: 0.7730\n",
      "Epoch 131/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.7561\n",
      "Epoch 00131: val_loss did not improve from 0.47843\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4965 - accuracy: 0.7556 - val_loss: 0.5011 - val_accuracy: 0.7506\n",
      "Epoch 132/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4930 - accuracy: 0.7605\n",
      "Epoch 00132: val_loss improved from 0.47843 to 0.47751, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4938 - accuracy: 0.7602 - val_loss: 0.4775 - val_accuracy: 0.7709\n",
      "Epoch 133/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7552\n",
      "Epoch 00133: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4965 - accuracy: 0.7548 - val_loss: 0.5045 - val_accuracy: 0.7451\n",
      "Epoch 134/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.7578\n",
      "Epoch 00134: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4943 - accuracy: 0.7573 - val_loss: 0.4813 - val_accuracy: 0.7704\n",
      "Epoch 135/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7585\n",
      "Epoch 00135: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4912 - accuracy: 0.7582 - val_loss: 0.4874 - val_accuracy: 0.7582\n",
      "Epoch 136/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4917 - accuracy: 0.7595\n",
      "Epoch 00136: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4920 - accuracy: 0.7593 - val_loss: 0.5424 - val_accuracy: 0.7206\n",
      "Epoch 137/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4922 - accuracy: 0.7597\n",
      "Epoch 00137: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4930 - accuracy: 0.7592 - val_loss: 0.5267 - val_accuracy: 0.7272\n",
      "Epoch 138/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4910 - accuracy: 0.7619\n",
      "Epoch 00138: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4920 - accuracy: 0.7615 - val_loss: 0.4892 - val_accuracy: 0.7575\n",
      "Epoch 139/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.7636\n",
      "Epoch 00139: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4877 - accuracy: 0.7633 - val_loss: 0.4857 - val_accuracy: 0.7605\n",
      "Epoch 140/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.7623\n",
      "Epoch 00140: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4878 - accuracy: 0.7623 - val_loss: 0.4891 - val_accuracy: 0.7575\n",
      "Epoch 141/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7599\n",
      "Epoch 00141: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4875 - accuracy: 0.7599 - val_loss: 0.4889 - val_accuracy: 0.7577\n",
      "Epoch 142/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.7631\n",
      "Epoch 00142: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4878 - accuracy: 0.7629 - val_loss: 0.5054 - val_accuracy: 0.7463\n",
      "Epoch 143/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4884 - accuracy: 0.7594\n",
      "Epoch 00143: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4894 - accuracy: 0.7589 - val_loss: 0.5035 - val_accuracy: 0.7431\n",
      "Epoch 144/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7645\n",
      "Epoch 00144: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4872 - accuracy: 0.7642 - val_loss: 0.5074 - val_accuracy: 0.7423\n",
      "Epoch 145/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.7615\n",
      "Epoch 00145: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4848 - accuracy: 0.7615 - val_loss: 0.4985 - val_accuracy: 0.7500\n",
      "Epoch 146/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4869 - accuracy: 0.7584\n",
      "Epoch 00146: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4876 - accuracy: 0.7583 - val_loss: 0.5144 - val_accuracy: 0.7380\n",
      "Epoch 147/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.7631\n",
      "Epoch 00147: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4830 - accuracy: 0.7628 - val_loss: 0.4776 - val_accuracy: 0.7668\n",
      "Epoch 148/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4834 - accuracy: 0.7654\n",
      "Epoch 00148: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4845 - accuracy: 0.7650 - val_loss: 0.5036 - val_accuracy: 0.7433\n",
      "Epoch 149/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4833 - accuracy: 0.7641\n",
      "Epoch 00149: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4838 - accuracy: 0.7639 - val_loss: 0.5067 - val_accuracy: 0.7444\n",
      "Epoch 150/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4835 - accuracy: 0.7633 ETA: 0s - loss: 0.4823 \n",
      "Epoch 00150: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4841 - accuracy: 0.7630 - val_loss: 0.5035 - val_accuracy: 0.7455\n",
      "Epoch 151/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4805 - accuracy: 0.7669\n",
      "Epoch 00151: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4812 - accuracy: 0.7666 - val_loss: 0.5275 - val_accuracy: 0.7261\n",
      "Epoch 152/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7647\n",
      "Epoch 00152: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4803 - accuracy: 0.7643 - val_loss: 0.5161 - val_accuracy: 0.7332\n",
      "Epoch 153/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7656\n",
      "Epoch 00153: val_loss did not improve from 0.47751\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4825 - accuracy: 0.7653 - val_loss: 0.5057 - val_accuracy: 0.7427\n",
      "Epoch 154/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4783 - accuracy: 0.7700 ETA: 0s - l\n",
      "Epoch 00154: val_loss improved from 0.47751 to 0.47181, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4791 - accuracy: 0.7699 - val_loss: 0.4718 - val_accuracy: 0.7691\n",
      "Epoch 155/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4791 - accuracy: 0.7674\n",
      "Epoch 00155: val_loss did not improve from 0.47181\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4798 - accuracy: 0.7673 - val_loss: 0.5151 - val_accuracy: 0.7352\n",
      "Epoch 156/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4789 - accuracy: 0.7698\n",
      "Epoch 00156: val_loss did not improve from 0.47181\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4796 - accuracy: 0.7695 - val_loss: 0.4762 - val_accuracy: 0.7623\n",
      "Epoch 157/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.7673\n",
      "Epoch 00157: val_loss did not improve from 0.47181\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4784 - accuracy: 0.7671 - val_loss: 0.4792 - val_accuracy: 0.7605\n",
      "Epoch 158/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4768 - accuracy: 0.7699\n",
      "Epoch 00158: val_loss improved from 0.47181 to 0.46878, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4774 - accuracy: 0.7699 - val_loss: 0.4688 - val_accuracy: 0.7704\n",
      "Epoch 159/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4777 - accuracy: 0.7688\n",
      "Epoch 00159: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4783 - accuracy: 0.7687 - val_loss: 0.5044 - val_accuracy: 0.7455\n",
      "Epoch 160/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7650\n",
      "Epoch 00160: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4801 - accuracy: 0.7649 - val_loss: 0.4839 - val_accuracy: 0.7577\n",
      "Epoch 161/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7719\n",
      "Epoch 00161: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4783 - accuracy: 0.7720 - val_loss: 0.4988 - val_accuracy: 0.7472\n",
      "Epoch 162/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.7709\n",
      "Epoch 00162: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4764 - accuracy: 0.7708 - val_loss: 0.4779 - val_accuracy: 0.7605\n",
      "Epoch 163/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7642\n",
      "Epoch 00163: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4785 - accuracy: 0.7639 - val_loss: 0.4964 - val_accuracy: 0.7491\n",
      "Epoch 164/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7702\n",
      "Epoch 00164: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4749 - accuracy: 0.7699 - val_loss: 0.5228 - val_accuracy: 0.7309\n",
      "Epoch 165/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.7739\n",
      "Epoch 00165: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4750 - accuracy: 0.7735 - val_loss: 0.4978 - val_accuracy: 0.7509\n",
      "Epoch 166/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7659\n",
      "Epoch 00166: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4763 - accuracy: 0.7656 - val_loss: 0.4882 - val_accuracy: 0.7554\n",
      "Epoch 167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/146 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.7725\n",
      "Epoch 00167: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4747 - accuracy: 0.7723 - val_loss: 0.5151 - val_accuracy: 0.7375\n",
      "Epoch 168/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4702 - accuracy: 0.7738\n",
      "Epoch 00168: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4705 - accuracy: 0.7738 - val_loss: 0.4851 - val_accuracy: 0.7586\n",
      "Epoch 169/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.7681\n",
      "Epoch 00169: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4736 - accuracy: 0.7678 - val_loss: 0.4842 - val_accuracy: 0.7571\n",
      "Epoch 170/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7716\n",
      "Epoch 00170: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4712 - accuracy: 0.7715 - val_loss: 0.5397 - val_accuracy: 0.7197\n",
      "Epoch 171/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7710\n",
      "Epoch 00171: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4729 - accuracy: 0.7708 - val_loss: 0.4832 - val_accuracy: 0.7595\n",
      "Epoch 172/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7696\n",
      "Epoch 00172: val_loss did not improve from 0.46878\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4744 - accuracy: 0.7696 - val_loss: 0.4779 - val_accuracy: 0.7629\n",
      "Epoch 173/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7714\n",
      "Epoch 00173: val_loss improved from 0.46878 to 0.45880, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4737 - accuracy: 0.7710 - val_loss: 0.4588 - val_accuracy: 0.7782\n",
      "Epoch 174/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7761\n",
      "Epoch 00174: val_loss did not improve from 0.45880\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4713 - accuracy: 0.7759 - val_loss: 0.4878 - val_accuracy: 0.7569\n",
      "Epoch 175/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.7766\n",
      "Epoch 00175: val_loss did not improve from 0.45880\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4684 - accuracy: 0.7763 - val_loss: 0.4875 - val_accuracy: 0.7562\n",
      "Epoch 176/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7726\n",
      "Epoch 00176: val_loss did not improve from 0.45880\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4701 - accuracy: 0.7724 - val_loss: 0.4760 - val_accuracy: 0.7616\n",
      "Epoch 177/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4653 - accuracy: 0.7762\n",
      "Epoch 00177: val_loss did not improve from 0.45880\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4660 - accuracy: 0.7762 - val_loss: 0.4694 - val_accuracy: 0.7646\n",
      "Epoch 178/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4669 - accuracy: 0.7763\n",
      "Epoch 00178: val_loss did not improve from 0.45880\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4673 - accuracy: 0.7762 - val_loss: 0.4674 - val_accuracy: 0.7678\n",
      "Epoch 179/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.7758\n",
      "Epoch 00179: val_loss improved from 0.45880 to 0.45696, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4692 - accuracy: 0.7757 - val_loss: 0.4570 - val_accuracy: 0.7771\n",
      "Epoch 180/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4695 - accuracy: 0.7737\n",
      "Epoch 00180: val_loss did not improve from 0.45696\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4698 - accuracy: 0.7738 - val_loss: 0.4895 - val_accuracy: 0.7541\n",
      "Epoch 181/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.7762\n",
      "Epoch 00181: val_loss did not improve from 0.45696\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4647 - accuracy: 0.7759 - val_loss: 0.4815 - val_accuracy: 0.7575\n",
      "Epoch 182/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4659 - accuracy: 0.7740\n",
      "Epoch 00182: val_loss did not improve from 0.45696\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4660 - accuracy: 0.7743 - val_loss: 0.4864 - val_accuracy: 0.7599\n",
      "Epoch 183/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4670 - accuracy: 0.7744\n",
      "Epoch 00183: val_loss improved from 0.45696 to 0.45236, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4674 - accuracy: 0.7746 - val_loss: 0.4524 - val_accuracy: 0.7803\n",
      "Epoch 184/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4681 - accuracy: 0.7730\n",
      "Epoch 00184: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4689 - accuracy: 0.7730 - val_loss: 0.4862 - val_accuracy: 0.7567\n",
      "Epoch 185/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.7741\n",
      "Epoch 00185: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4674 - accuracy: 0.7738 - val_loss: 0.4712 - val_accuracy: 0.7653\n",
      "Epoch 186/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.7760\n",
      "Epoch 00186: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4671 - accuracy: 0.7759 - val_loss: 0.4820 - val_accuracy: 0.7595\n",
      "Epoch 187/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4660 - accuracy: 0.7757\n",
      "Epoch 00187: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4661 - accuracy: 0.7757 - val_loss: 0.4941 - val_accuracy: 0.7524\n",
      "Epoch 188/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4624 - accuracy: 0.7748\n",
      "Epoch 00188: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4628 - accuracy: 0.7750 - val_loss: 0.4850 - val_accuracy: 0.7562\n",
      "Epoch 189/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.7779\n",
      "Epoch 00189: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4634 - accuracy: 0.7778 - val_loss: 0.4641 - val_accuracy: 0.7724\n",
      "Epoch 190/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4638 - accuracy: 0.7770\n",
      "Epoch 00190: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4647 - accuracy: 0.7769 - val_loss: 0.5074 - val_accuracy: 0.7408\n",
      "Epoch 191/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.7799\n",
      "Epoch 00191: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4615 - accuracy: 0.7797 - val_loss: 0.5024 - val_accuracy: 0.7433\n",
      "Epoch 192/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4622 - accuracy: 0.7792\n",
      "Epoch 00192: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4621 - accuracy: 0.7791 - val_loss: 0.5044 - val_accuracy: 0.7440\n",
      "Epoch 193/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.7776\n",
      "Epoch 00193: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4866 - val_accuracy: 0.7564\n",
      "Epoch 194/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4600 - accuracy: 0.7777\n",
      "Epoch 00194: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4603 - accuracy: 0.7777 - val_loss: 0.4629 - val_accuracy: 0.7724\n",
      "Epoch 195/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/146 [============================>.] - ETA: 0s - loss: 0.4614 - accuracy: 0.7750\n",
      "Epoch 00195: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4617 - accuracy: 0.7748 - val_loss: 0.4816 - val_accuracy: 0.7577\n",
      "Epoch 196/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.7790\n",
      "Epoch 00196: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4568 - accuracy: 0.7787 - val_loss: 0.4993 - val_accuracy: 0.7459\n",
      "Epoch 197/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7822\n",
      "Epoch 00197: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4558 - accuracy: 0.7820 - val_loss: 0.5209 - val_accuracy: 0.7341\n",
      "Epoch 198/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4589 - accuracy: 0.7797\n",
      "Epoch 00198: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4597 - accuracy: 0.7799 - val_loss: 0.5086 - val_accuracy: 0.7425\n",
      "Epoch 199/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7767\n",
      "Epoch 00199: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4591 - accuracy: 0.7765 - val_loss: 0.5023 - val_accuracy: 0.7448\n",
      "Epoch 200/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7790\n",
      "Epoch 00200: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4586 - accuracy: 0.7786 - val_loss: 0.4727 - val_accuracy: 0.7655\n",
      "Epoch 201/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4589 - accuracy: 0.7809\n",
      "Epoch 00201: val_loss did not improve from 0.45236\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4598 - accuracy: 0.7807 - val_loss: 0.4640 - val_accuracy: 0.7741\n",
      "Epoch 202/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.7811\n",
      "Epoch 00202: val_loss improved from 0.45236 to 0.45013, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4585 - accuracy: 0.7808 - val_loss: 0.4501 - val_accuracy: 0.7827\n",
      "Epoch 203/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4554 - accuracy: 0.7812\n",
      "Epoch 00203: val_loss did not improve from 0.45013\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4562 - accuracy: 0.7807 - val_loss: 0.5002 - val_accuracy: 0.7463\n",
      "Epoch 204/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4580 - accuracy: 0.7827\n",
      "Epoch 00204: val_loss did not improve from 0.45013\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4589 - accuracy: 0.7824 - val_loss: 0.4916 - val_accuracy: 0.7571\n",
      "Epoch 205/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7790\n",
      "Epoch 00205: val_loss improved from 0.45013 to 0.44321, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4565 - accuracy: 0.7790 - val_loss: 0.4432 - val_accuracy: 0.7915\n",
      "Epoch 206/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4570 - accuracy: 0.7805 ETA: 0s - loss: 0\n",
      "Epoch 00206: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4581 - accuracy: 0.7802 - val_loss: 0.4463 - val_accuracy: 0.7840\n",
      "Epoch 207/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4562 - accuracy: 0.7792\n",
      "Epoch 00207: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4567 - accuracy: 0.7789 - val_loss: 0.4943 - val_accuracy: 0.7519\n",
      "Epoch 208/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7834\n",
      "Epoch 00208: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4533 - accuracy: 0.7833 - val_loss: 0.4451 - val_accuracy: 0.7840\n",
      "Epoch 209/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.7859\n",
      "Epoch 00209: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4512 - accuracy: 0.7857 - val_loss: 0.4605 - val_accuracy: 0.7713\n",
      "Epoch 210/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.7843\n",
      "Epoch 00210: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4519 - accuracy: 0.7844 - val_loss: 0.4990 - val_accuracy: 0.7476\n",
      "Epoch 211/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.7814\n",
      "Epoch 00211: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4551 - accuracy: 0.7813 - val_loss: 0.4524 - val_accuracy: 0.7829\n",
      "Epoch 212/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.7841\n",
      "Epoch 00212: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4548 - accuracy: 0.7839 - val_loss: 0.4751 - val_accuracy: 0.7648\n",
      "Epoch 213/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4532 - accuracy: 0.7828\n",
      "Epoch 00213: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4538 - accuracy: 0.7826 - val_loss: 0.4670 - val_accuracy: 0.7732\n",
      "Epoch 214/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4532 - accuracy: 0.7839\n",
      "Epoch 00214: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4541 - accuracy: 0.7836 - val_loss: 0.4673 - val_accuracy: 0.7717\n",
      "Epoch 215/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.7814\n",
      "Epoch 00215: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4545 - accuracy: 0.7812 - val_loss: 0.4495 - val_accuracy: 0.7859\n",
      "Epoch 216/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.7862\n",
      "Epoch 00216: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4515 - accuracy: 0.7858 - val_loss: 0.4454 - val_accuracy: 0.7842\n",
      "Epoch 217/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7859\n",
      "Epoch 00217: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4507 - accuracy: 0.7858 - val_loss: 0.4691 - val_accuracy: 0.7709\n",
      "Epoch 218/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4493 - accuracy: 0.7837\n",
      "Epoch 00218: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4499 - accuracy: 0.7836 - val_loss: 0.4864 - val_accuracy: 0.7564\n",
      "Epoch 219/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7848\n",
      "Epoch 00219: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4496 - accuracy: 0.7847 - val_loss: 0.4690 - val_accuracy: 0.7691\n",
      "Epoch 220/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4458 - accuracy: 0.7877\n",
      "Epoch 00220: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4471 - accuracy: 0.7874 - val_loss: 0.5276 - val_accuracy: 0.7261\n",
      "Epoch 221/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.7863\n",
      "Epoch 00221: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4485 - accuracy: 0.7862 - val_loss: 0.4851 - val_accuracy: 0.7577\n",
      "Epoch 222/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.7849\n",
      "Epoch 00222: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4517 - accuracy: 0.7847 - val_loss: 0.4866 - val_accuracy: 0.7584\n",
      "Epoch 223/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.7835\n",
      "Epoch 00223: val_loss did not improve from 0.44321\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4486 - accuracy: 0.7833 - val_loss: 0.4767 - val_accuracy: 0.7663\n",
      "Epoch 224/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7886\n",
      "Epoch 00224: val_loss improved from 0.44321 to 0.44189, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4473 - accuracy: 0.7883 - val_loss: 0.4419 - val_accuracy: 0.7893\n",
      "Epoch 225/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7897\n",
      "Epoch 00225: val_loss did not improve from 0.44189\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4465 - accuracy: 0.7894 - val_loss: 0.4504 - val_accuracy: 0.7810\n",
      "Epoch 226/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7856\n",
      "Epoch 00226: val_loss did not improve from 0.44189\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4431 - accuracy: 0.7854 - val_loss: 0.4673 - val_accuracy: 0.7724\n",
      "Epoch 227/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7888\n",
      "Epoch 00227: val_loss did not improve from 0.44189\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4479 - accuracy: 0.7882 - val_loss: 0.4982 - val_accuracy: 0.7491\n",
      "Epoch 228/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.7889\n",
      "Epoch 00228: val_loss did not improve from 0.44189\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4457 - accuracy: 0.7887 - val_loss: 0.4472 - val_accuracy: 0.7831\n",
      "Epoch 229/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7879\n",
      "Epoch 00229: val_loss did not improve from 0.44189\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4454 - accuracy: 0.7879 - val_loss: 0.4472 - val_accuracy: 0.7855\n",
      "Epoch 230/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.7881\n",
      "Epoch 00230: val_loss improved from 0.44189 to 0.43296, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4469 - accuracy: 0.7880 - val_loss: 0.4330 - val_accuracy: 0.7949\n",
      "Epoch 231/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.7912\n",
      "Epoch 00231: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4424 - accuracy: 0.7911 - val_loss: 0.4528 - val_accuracy: 0.7825\n",
      "Epoch 232/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.7896 ETA: 0s - loss: 0.4440 - accuracy: 0.\n",
      "Epoch 00232: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4442 - accuracy: 0.7896 - val_loss: 0.4507 - val_accuracy: 0.7857\n",
      "Epoch 233/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.7910\n",
      "Epoch 00233: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4423 - accuracy: 0.7909 - val_loss: 0.4418 - val_accuracy: 0.7885\n",
      "Epoch 234/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4406 - accuracy: 0.7906\n",
      "Epoch 00234: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4410 - accuracy: 0.7904 - val_loss: 0.4536 - val_accuracy: 0.7816\n",
      "Epoch 235/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7887\n",
      "Epoch 00235: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4440 - accuracy: 0.7884 - val_loss: 0.4528 - val_accuracy: 0.7816\n",
      "Epoch 236/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7899\n",
      "Epoch 00236: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4457 - accuracy: 0.7898 - val_loss: 0.4515 - val_accuracy: 0.7822\n",
      "Epoch 237/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4403 - accuracy: 0.7936\n",
      "Epoch 00237: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4410 - accuracy: 0.7933 - val_loss: 0.4458 - val_accuracy: 0.7872\n",
      "Epoch 238/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.7896\n",
      "Epoch 00238: val_loss did not improve from 0.43296\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4403 - accuracy: 0.7896 - val_loss: 0.4601 - val_accuracy: 0.7747\n",
      "Epoch 239/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4376 - accuracy: 0.7912 ETA: 0s - los\n",
      "Epoch 00239: val_loss improved from 0.43296 to 0.43158, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4384 - accuracy: 0.7909 - val_loss: 0.4316 - val_accuracy: 0.7934\n",
      "Epoch 240/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.7922\n",
      "Epoch 00240: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4423 - accuracy: 0.7920 - val_loss: 0.4647 - val_accuracy: 0.7721\n",
      "Epoch 241/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4408 - accuracy: 0.7922\n",
      "Epoch 00241: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4409 - accuracy: 0.7923 - val_loss: 0.4771 - val_accuracy: 0.7661\n",
      "Epoch 242/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4373 - accuracy: 0.7937\n",
      "Epoch 00242: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4378 - accuracy: 0.7936 - val_loss: 0.4660 - val_accuracy: 0.7726\n",
      "Epoch 243/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.7909\n",
      "Epoch 00243: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4405 - accuracy: 0.7907 - val_loss: 0.4436 - val_accuracy: 0.7880\n",
      "Epoch 244/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.7915\n",
      "Epoch 00244: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4377 - accuracy: 0.7912 - val_loss: 0.4647 - val_accuracy: 0.7721\n",
      "Epoch 245/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.7949\n",
      "Epoch 00245: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4370 - accuracy: 0.7948 - val_loss: 0.5103 - val_accuracy: 0.7418\n",
      "Epoch 246/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.7898\n",
      "Epoch 00246: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4395 - accuracy: 0.7896 - val_loss: 0.4380 - val_accuracy: 0.7928\n",
      "Epoch 247/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.7886\n",
      "Epoch 00247: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4403 - accuracy: 0.7882 - val_loss: 0.4723 - val_accuracy: 0.7666\n",
      "Epoch 248/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.7955\n",
      "Epoch 00248: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4362 - accuracy: 0.7955 - val_loss: 0.4711 - val_accuracy: 0.7687\n",
      "Epoch 249/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.7938\n",
      "Epoch 00249: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4361 - accuracy: 0.7937 - val_loss: 0.4479 - val_accuracy: 0.7861\n",
      "Epoch 250/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.7937\n",
      "Epoch 00250: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4350 - accuracy: 0.7936 - val_loss: 0.4540 - val_accuracy: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.7950\n",
      "Epoch 00251: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4357 - accuracy: 0.7947 - val_loss: 0.4508 - val_accuracy: 0.7807\n",
      "Epoch 252/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4362 - accuracy: 0.7960\n",
      "Epoch 00252: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4366 - accuracy: 0.7959 - val_loss: 0.4426 - val_accuracy: 0.7865\n",
      "Epoch 253/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.7942\n",
      "Epoch 00253: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4357 - accuracy: 0.7940 - val_loss: 0.4494 - val_accuracy: 0.7827\n",
      "Epoch 254/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7933 ETA: 0s - loss: 0.4\n",
      "Epoch 00254: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4372 - accuracy: 0.7932 - val_loss: 0.4743 - val_accuracy: 0.7646\n",
      "Epoch 255/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.7957\n",
      "Epoch 00255: val_loss did not improve from 0.43158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4348 - accuracy: 0.7961 - val_loss: 0.4557 - val_accuracy: 0.7771\n",
      "Epoch 256/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7975\n",
      "Epoch 00256: val_loss improved from 0.43158 to 0.42352, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 0.4337 - accuracy: 0.7975 - val_loss: 0.4235 - val_accuracy: 0.8003\n",
      "Epoch 257/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.7988\n",
      "Epoch 00257: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4326 - accuracy: 0.7988 - val_loss: 0.4402 - val_accuracy: 0.7891\n",
      "Epoch 258/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7961\n",
      "Epoch 00258: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4303 - accuracy: 0.7960 - val_loss: 0.4457 - val_accuracy: 0.7872\n",
      "Epoch 259/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7954\n",
      "Epoch 00259: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4335 - accuracy: 0.7952 - val_loss: 0.4559 - val_accuracy: 0.7786\n",
      "Epoch 260/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.7982\n",
      "Epoch 00260: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4311 - accuracy: 0.7981 - val_loss: 0.4335 - val_accuracy: 0.7898\n",
      "Epoch 261/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.7916\n",
      "Epoch 00261: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4372 - accuracy: 0.7916 - val_loss: 0.4769 - val_accuracy: 0.7640\n",
      "Epoch 262/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4308 - accuracy: 0.7954\n",
      "Epoch 00262: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4310 - accuracy: 0.7954 - val_loss: 0.4488 - val_accuracy: 0.7831\n",
      "Epoch 263/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4288 - accuracy: 0.7996\n",
      "Epoch 00263: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4293 - accuracy: 0.7992 - val_loss: 0.4403 - val_accuracy: 0.7876\n",
      "Epoch 264/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.7950\n",
      "Epoch 00264: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4331 - accuracy: 0.7947 - val_loss: 0.4266 - val_accuracy: 0.7966\n",
      "Epoch 265/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7959\n",
      "Epoch 00265: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4338 - accuracy: 0.7956 - val_loss: 0.4329 - val_accuracy: 0.7960\n",
      "Epoch 266/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.7977\n",
      "Epoch 00266: val_loss did not improve from 0.42352\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4323 - accuracy: 0.7977 - val_loss: 0.4441 - val_accuracy: 0.7868\n",
      "Epoch 267/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.7982\n",
      "Epoch 00267: val_loss improved from 0.42352 to 0.41975, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4273 - accuracy: 0.7980 - val_loss: 0.4198 - val_accuracy: 0.8014\n",
      "Epoch 268/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.7987\n",
      "Epoch 00268: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4301 - accuracy: 0.7985 - val_loss: 0.4898 - val_accuracy: 0.7577\n",
      "Epoch 269/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4283 - accuracy: 0.7963\n",
      "Epoch 00269: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4287 - accuracy: 0.7962 - val_loss: 0.4879 - val_accuracy: 0.7558\n",
      "Epoch 270/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8017\n",
      "Epoch 00270: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4279 - accuracy: 0.8016 - val_loss: 0.4461 - val_accuracy: 0.7883\n",
      "Epoch 271/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.7956\n",
      "Epoch 00271: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4326 - accuracy: 0.7955 - val_loss: 0.4665 - val_accuracy: 0.7730\n",
      "Epoch 272/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.7940\n",
      "Epoch 00272: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4311 - accuracy: 0.7939 - val_loss: 0.4656 - val_accuracy: 0.7743\n",
      "Epoch 273/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4300 - accuracy: 0.7990\n",
      "Epoch 00273: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4309 - accuracy: 0.7987 - val_loss: 0.4251 - val_accuracy: 0.8003\n",
      "Epoch 274/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.7994\n",
      "Epoch 00274: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4268 - accuracy: 0.7991 - val_loss: 0.4304 - val_accuracy: 0.7917\n",
      "Epoch 275/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8023\n",
      "Epoch 00275: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4260 - accuracy: 0.8023 - val_loss: 0.4403 - val_accuracy: 0.7876\n",
      "Epoch 276/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8002\n",
      "Epoch 00276: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4267 - accuracy: 0.8004 - val_loss: 0.4445 - val_accuracy: 0.7861\n",
      "Epoch 277/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4239 - accuracy: 0.8011\n",
      "Epoch 00277: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4240 - accuracy: 0.8009 - val_loss: 0.4715 - val_accuracy: 0.7666\n",
      "Epoch 278/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.7990\n",
      "Epoch 00278: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4253 - accuracy: 0.7989 - val_loss: 0.4464 - val_accuracy: 0.7820\n",
      "Epoch 279/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8030\n",
      "Epoch 00279: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4223 - accuracy: 0.8029 - val_loss: 0.4389 - val_accuracy: 0.7893\n",
      "Epoch 280/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8011\n",
      "Epoch 00280: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4260 - accuracy: 0.8010 - val_loss: 0.4214 - val_accuracy: 0.8031\n",
      "Epoch 281/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4273 - accuracy: 0.7985\n",
      "Epoch 00281: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4274 - accuracy: 0.7984 - val_loss: 0.4554 - val_accuracy: 0.7762\n",
      "Epoch 282/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8010\n",
      "Epoch 00282: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4241 - accuracy: 0.8007 - val_loss: 0.4579 - val_accuracy: 0.7736\n",
      "Epoch 283/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8009\n",
      "Epoch 00283: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4240 - accuracy: 0.8009 - val_loss: 0.4228 - val_accuracy: 0.8020\n",
      "Epoch 284/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8040\n",
      "Epoch 00284: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4250 - accuracy: 0.8040 - val_loss: 0.4354 - val_accuracy: 0.7923\n",
      "Epoch 285/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8031\n",
      "Epoch 00285: val_loss did not improve from 0.41975\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4230 - accuracy: 0.8028 - val_loss: 0.4241 - val_accuracy: 0.7977\n",
      "Epoch 286/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8011\n",
      "Epoch 00286: val_loss improved from 0.41975 to 0.41594, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4251 - accuracy: 0.8010 - val_loss: 0.4159 - val_accuracy: 0.8093\n",
      "Epoch 287/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.7989\n",
      "Epoch 00287: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4251 - accuracy: 0.7989 - val_loss: 0.4492 - val_accuracy: 0.7803\n",
      "Epoch 288/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8028\n",
      "Epoch 00288: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4223 - accuracy: 0.8026 - val_loss: 0.4331 - val_accuracy: 0.7904\n",
      "Epoch 289/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8011\n",
      "Epoch 00289: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4205 - accuracy: 0.8013 - val_loss: 0.4507 - val_accuracy: 0.7788\n",
      "Epoch 290/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8052\n",
      "Epoch 00290: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4167 - accuracy: 0.8052 - val_loss: 0.4358 - val_accuracy: 0.7885\n",
      "Epoch 291/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8064\n",
      "Epoch 00291: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4168 - accuracy: 0.8063 - val_loss: 0.4424 - val_accuracy: 0.7878\n",
      "Epoch 292/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8032\n",
      "Epoch 00292: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4186 - accuracy: 0.8035 - val_loss: 0.4259 - val_accuracy: 0.7984\n",
      "Epoch 293/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8009\n",
      "Epoch 00293: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4190 - accuracy: 0.8011 - val_loss: 0.4867 - val_accuracy: 0.7590\n",
      "Epoch 294/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8021\n",
      "Epoch 00294: val_loss did not improve from 0.41594\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4191 - accuracy: 0.8019 - val_loss: 0.4548 - val_accuracy: 0.7767\n",
      "Epoch 295/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8036\n",
      "Epoch 00295: val_loss improved from 0.41594 to 0.41473, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4235 - accuracy: 0.8035 - val_loss: 0.4147 - val_accuracy: 0.8050\n",
      "Epoch 296/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8040\n",
      "Epoch 00296: val_loss did not improve from 0.41473\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4200 - accuracy: 0.8039 - val_loss: 0.4388 - val_accuracy: 0.7889\n",
      "Epoch 297/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8032\n",
      "Epoch 00297: val_loss did not improve from 0.41473\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4194 - accuracy: 0.8031 - val_loss: 0.4412 - val_accuracy: 0.7876\n",
      "Epoch 298/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8036\n",
      "Epoch 00298: val_loss did not improve from 0.41473\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4204 - accuracy: 0.8034 - val_loss: 0.4309 - val_accuracy: 0.7930\n",
      "Epoch 299/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8026\n",
      "Epoch 00299: val_loss did not improve from 0.41473\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4193 - accuracy: 0.8024 - val_loss: 0.4369 - val_accuracy: 0.7904\n",
      "Epoch 300/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.7997\n",
      "Epoch 00300: val_loss did not improve from 0.41473\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4196 - accuracy: 0.7996 - val_loss: 0.4504 - val_accuracy: 0.7792\n",
      "Epoch 301/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8041\n",
      "Epoch 00301: val_loss improved from 0.41473 to 0.41184, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4166 - accuracy: 0.8039 - val_loss: 0.4118 - val_accuracy: 0.8104\n",
      "Epoch 302/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8010\n",
      "Epoch 00302: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4199 - accuracy: 0.8008 - val_loss: 0.4310 - val_accuracy: 0.7958\n",
      "Epoch 303/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8041\n",
      "Epoch 00303: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4185 - accuracy: 0.8040 - val_loss: 0.4401 - val_accuracy: 0.7850\n",
      "Epoch 304/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8045\n",
      "Epoch 00304: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4164 - accuracy: 0.8042 - val_loss: 0.4413 - val_accuracy: 0.7853\n",
      "Epoch 305/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4165 - accuracy: 0.8060\n",
      "Epoch 00305: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4170 - accuracy: 0.8058 - val_loss: 0.4742 - val_accuracy: 0.7638\n",
      "Epoch 306/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8059\n",
      "Epoch 00306: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4163 - accuracy: 0.8059 - val_loss: 0.4401 - val_accuracy: 0.7878\n",
      "Epoch 307/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/146 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8006\n",
      "Epoch 00307: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4193 - accuracy: 0.8006 - val_loss: 0.4498 - val_accuracy: 0.7790\n",
      "Epoch 308/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8102\n",
      "Epoch 00308: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4122 - accuracy: 0.8100 - val_loss: 0.4518 - val_accuracy: 0.7777\n",
      "Epoch 309/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8068\n",
      "Epoch 00309: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4108 - accuracy: 0.8068 - val_loss: 0.4713 - val_accuracy: 0.7663\n",
      "Epoch 310/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8098\n",
      "Epoch 00310: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4124 - accuracy: 0.8096 - val_loss: 0.4343 - val_accuracy: 0.7887\n",
      "Epoch 311/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8094\n",
      "Epoch 00311: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4103 - accuracy: 0.8092 - val_loss: 0.4372 - val_accuracy: 0.7919\n",
      "Epoch 312/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8062\n",
      "Epoch 00312: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4139 - accuracy: 0.8063 - val_loss: 0.4910 - val_accuracy: 0.7562\n",
      "Epoch 313/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.8035\n",
      "Epoch 00313: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4140 - accuracy: 0.8037 - val_loss: 0.4279 - val_accuracy: 0.7984\n",
      "Epoch 314/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4142 - accuracy: 0.8094\n",
      "Epoch 00314: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4145 - accuracy: 0.8093 - val_loss: 0.4205 - val_accuracy: 0.8044\n",
      "Epoch 315/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8075\n",
      "Epoch 00315: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4117 - accuracy: 0.8074 - val_loss: 0.4183 - val_accuracy: 0.8003\n",
      "Epoch 316/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8117\n",
      "Epoch 00316: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4130 - accuracy: 0.8114 - val_loss: 0.4323 - val_accuracy: 0.7945\n",
      "Epoch 317/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8022\n",
      "Epoch 00317: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4177 - accuracy: 0.8023 - val_loss: 0.4460 - val_accuracy: 0.7818\n",
      "Epoch 318/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.8083\n",
      "Epoch 00318: val_loss did not improve from 0.41184\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4116 - accuracy: 0.8082 - val_loss: 0.4465 - val_accuracy: 0.7801\n",
      "Epoch 319/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8064\n",
      "Epoch 00319: val_loss improved from 0.41184 to 0.41163, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4111 - accuracy: 0.8063 - val_loss: 0.4116 - val_accuracy: 0.8065\n",
      "Epoch 320/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8112\n",
      "Epoch 00320: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4067 - accuracy: 0.8110 - val_loss: 0.4246 - val_accuracy: 0.7986\n",
      "Epoch 321/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8080\n",
      "Epoch 00321: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4082 - accuracy: 0.8078 - val_loss: 0.4532 - val_accuracy: 0.7764\n",
      "Epoch 322/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8124\n",
      "Epoch 00322: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4074 - accuracy: 0.8124 - val_loss: 0.4189 - val_accuracy: 0.8022\n",
      "Epoch 323/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8117\n",
      "Epoch 00323: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4048 - accuracy: 0.8117 - val_loss: 0.4642 - val_accuracy: 0.7685\n",
      "Epoch 324/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.8084\n",
      "Epoch 00324: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4075 - accuracy: 0.8085 - val_loss: 0.4208 - val_accuracy: 0.7988\n",
      "Epoch 325/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8100\n",
      "Epoch 00325: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4066 - accuracy: 0.8102 - val_loss: 0.4333 - val_accuracy: 0.7906\n",
      "Epoch 326/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8135\n",
      "Epoch 00326: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4055 - accuracy: 0.8134 - val_loss: 0.4338 - val_accuracy: 0.7919\n",
      "Epoch 327/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8121\n",
      "Epoch 00327: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4061 - accuracy: 0.8121 - val_loss: 0.4593 - val_accuracy: 0.7730\n",
      "Epoch 328/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8105\n",
      "Epoch 00328: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4078 - accuracy: 0.8104 - val_loss: 0.4540 - val_accuracy: 0.7762\n",
      "Epoch 329/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8131\n",
      "Epoch 00329: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4051 - accuracy: 0.8130 - val_loss: 0.4319 - val_accuracy: 0.7932\n",
      "Epoch 330/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8140\n",
      "Epoch 00330: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4027 - accuracy: 0.8141 - val_loss: 0.4824 - val_accuracy: 0.7595\n",
      "Epoch 331/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4028 - accuracy: 0.8129\n",
      "Epoch 00331: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4033 - accuracy: 0.8128 - val_loss: 0.4386 - val_accuracy: 0.7853\n",
      "Epoch 332/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4073 - accuracy: 0.8115\n",
      "Epoch 00332: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4080 - accuracy: 0.8115 - val_loss: 0.4321 - val_accuracy: 0.7921\n",
      "Epoch 333/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8116\n",
      "Epoch 00333: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4078 - accuracy: 0.8117 - val_loss: 0.4527 - val_accuracy: 0.7747\n",
      "Epoch 334/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4041 - accuracy: 0.8120\n",
      "Epoch 00334: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4045 - accuracy: 0.8118 - val_loss: 0.4258 - val_accuracy: 0.7982\n",
      "Epoch 335/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8109\n",
      "Epoch 00335: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4053 - accuracy: 0.8105 - val_loss: 0.4716 - val_accuracy: 0.7642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4006 - accuracy: 0.8122\n",
      "Epoch 00336: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4010 - accuracy: 0.8120 - val_loss: 0.4258 - val_accuracy: 0.7975\n",
      "Epoch 337/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8172\n",
      "Epoch 00337: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4023 - accuracy: 0.8170 - val_loss: 0.4225 - val_accuracy: 0.7977\n",
      "Epoch 338/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8134\n",
      "Epoch 00338: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4010 - accuracy: 0.8134 - val_loss: 0.4432 - val_accuracy: 0.7837\n",
      "Epoch 339/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8119 ETA: 1s -\n",
      "Epoch 00339: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4012 - accuracy: 0.8117 - val_loss: 0.4185 - val_accuracy: 0.7992\n",
      "Epoch 340/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8118\n",
      "Epoch 00340: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4033 - accuracy: 0.8117 - val_loss: 0.4259 - val_accuracy: 0.7994\n",
      "Epoch 341/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8132\n",
      "Epoch 00341: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4007 - accuracy: 0.8130 - val_loss: 0.4348 - val_accuracy: 0.7915\n",
      "Epoch 342/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3982 - accuracy: 0.8167\n",
      "Epoch 00342: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3980 - accuracy: 0.8167 - val_loss: 0.4352 - val_accuracy: 0.7889\n",
      "Epoch 343/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3984 - accuracy: 0.8103\n",
      "Epoch 00343: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3986 - accuracy: 0.8103 - val_loss: 0.4250 - val_accuracy: 0.7973\n",
      "Epoch 344/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8161\n",
      "Epoch 00344: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4007 - accuracy: 0.8159 - val_loss: 0.4392 - val_accuracy: 0.7855\n",
      "Epoch 345/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8145\n",
      "Epoch 00345: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3982 - accuracy: 0.8144 - val_loss: 0.4150 - val_accuracy: 0.8057\n",
      "Epoch 346/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8178\n",
      "Epoch 00346: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3981 - accuracy: 0.8179 - val_loss: 0.4740 - val_accuracy: 0.7620\n",
      "Epoch 347/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8154\n",
      "Epoch 00347: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3998 - accuracy: 0.8156 - val_loss: 0.4448 - val_accuracy: 0.7803\n",
      "Epoch 348/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3972 - accuracy: 0.8144\n",
      "Epoch 00348: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3976 - accuracy: 0.8143 - val_loss: 0.4512 - val_accuracy: 0.7767\n",
      "Epoch 349/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8159\n",
      "Epoch 00349: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4011 - accuracy: 0.8157 - val_loss: 0.4142 - val_accuracy: 0.8048\n",
      "Epoch 350/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3965 - accuracy: 0.8142\n",
      "Epoch 00350: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3971 - accuracy: 0.8142 - val_loss: 0.4141 - val_accuracy: 0.8020\n",
      "Epoch 351/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3972 - accuracy: 0.8152\n",
      "Epoch 00351: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3972 - accuracy: 0.8150 - val_loss: 0.4328 - val_accuracy: 0.7921\n",
      "Epoch 352/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.8187\n",
      "Epoch 00352: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3963 - accuracy: 0.8188 - val_loss: 0.4158 - val_accuracy: 0.8037\n",
      "Epoch 353/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8155\n",
      "Epoch 00353: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3960 - accuracy: 0.8156 - val_loss: 0.4281 - val_accuracy: 0.7939\n",
      "Epoch 354/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8148\n",
      "Epoch 00354: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3981 - accuracy: 0.8148 - val_loss: 0.4527 - val_accuracy: 0.7736\n",
      "Epoch 355/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8148\n",
      "Epoch 00355: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3971 - accuracy: 0.8147 - val_loss: 0.4630 - val_accuracy: 0.7663\n",
      "Epoch 356/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8155\n",
      "Epoch 00356: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3974 - accuracy: 0.8153 - val_loss: 0.4471 - val_accuracy: 0.7799\n",
      "Epoch 357/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8177\n",
      "Epoch 00357: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3979 - accuracy: 0.8179 - val_loss: 0.4318 - val_accuracy: 0.7911\n",
      "Epoch 358/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8158\n",
      "Epoch 00358: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3988 - accuracy: 0.8157 - val_loss: 0.4362 - val_accuracy: 0.7908\n",
      "Epoch 359/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8150\n",
      "Epoch 00359: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3959 - accuracy: 0.8148 - val_loss: 0.4402 - val_accuracy: 0.7850\n",
      "Epoch 360/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3953 - accuracy: 0.8175\n",
      "Epoch 00360: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3961 - accuracy: 0.8172 - val_loss: 0.4710 - val_accuracy: 0.7638\n",
      "Epoch 361/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8175\n",
      "Epoch 00361: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3910 - accuracy: 0.8174 - val_loss: 0.4184 - val_accuracy: 0.8009\n",
      "Epoch 362/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8168\n",
      "Epoch 00362: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3966 - accuracy: 0.8166 - val_loss: 0.4639 - val_accuracy: 0.7681\n",
      "Epoch 363/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8188\n",
      "Epoch 00363: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3947 - accuracy: 0.8187 - val_loss: 0.4138 - val_accuracy: 0.8085\n",
      "Epoch 364/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3922 - accuracy: 0.8178\n",
      "Epoch 00364: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3920 - accuracy: 0.8179 - val_loss: 0.4627 - val_accuracy: 0.7698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8202\n",
      "Epoch 00365: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3919 - accuracy: 0.8199 - val_loss: 0.4787 - val_accuracy: 0.7580\n",
      "Epoch 366/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.8210\n",
      "Epoch 00366: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3903 - accuracy: 0.8209 - val_loss: 0.4483 - val_accuracy: 0.7812\n",
      "Epoch 367/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8170\n",
      "Epoch 00367: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3948 - accuracy: 0.8169 - val_loss: 0.4421 - val_accuracy: 0.7801\n",
      "Epoch 368/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8212\n",
      "Epoch 00368: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3906 - accuracy: 0.8212 - val_loss: 0.4835 - val_accuracy: 0.7582\n",
      "Epoch 369/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8193\n",
      "Epoch 00369: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3922 - accuracy: 0.8192 - val_loss: 0.4498 - val_accuracy: 0.7782\n",
      "Epoch 370/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8198\n",
      "Epoch 00370: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3933 - accuracy: 0.8200 - val_loss: 0.4181 - val_accuracy: 0.8044\n",
      "Epoch 371/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8182\n",
      "Epoch 00371: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3922 - accuracy: 0.8178 - val_loss: 0.4450 - val_accuracy: 0.7827\n",
      "Epoch 372/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8161\n",
      "Epoch 00372: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3924 - accuracy: 0.8161 - val_loss: 0.4254 - val_accuracy: 0.7979\n",
      "Epoch 373/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8218\n",
      "Epoch 00373: val_loss did not improve from 0.41163\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3865 - accuracy: 0.8218 - val_loss: 0.4274 - val_accuracy: 0.7943\n",
      "Epoch 374/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8225\n",
      "Epoch 00374: val_loss improved from 0.41163 to 0.41067, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3897 - accuracy: 0.8226 - val_loss: 0.4107 - val_accuracy: 0.8085\n",
      "Epoch 375/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8185\n",
      "Epoch 00375: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3875 - accuracy: 0.8182 - val_loss: 0.4138 - val_accuracy: 0.8098\n",
      "Epoch 376/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3915 - accuracy: 0.8205\n",
      "Epoch 00376: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3919 - accuracy: 0.8203 - val_loss: 0.4520 - val_accuracy: 0.7732\n",
      "Epoch 377/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8207\n",
      "Epoch 00377: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3859 - accuracy: 0.8207 - val_loss: 0.4150 - val_accuracy: 0.8027\n",
      "Epoch 378/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8221\n",
      "Epoch 00378: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3889 - accuracy: 0.8221 - val_loss: 0.4140 - val_accuracy: 0.8046\n",
      "Epoch 379/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8201\n",
      "Epoch 00379: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3893 - accuracy: 0.8203 - val_loss: 0.4160 - val_accuracy: 0.8025\n",
      "Epoch 380/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8218\n",
      "Epoch 00380: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3859 - accuracy: 0.8219 - val_loss: 0.4366 - val_accuracy: 0.7889\n",
      "Epoch 381/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8226\n",
      "Epoch 00381: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3895 - accuracy: 0.8226 - val_loss: 0.4117 - val_accuracy: 0.8076\n",
      "Epoch 382/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8221\n",
      "Epoch 00382: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3871 - accuracy: 0.8221 - val_loss: 0.4213 - val_accuracy: 0.7986\n",
      "Epoch 383/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8247\n",
      "Epoch 00383: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3857 - accuracy: 0.8247 - val_loss: 0.4158 - val_accuracy: 0.8031\n",
      "Epoch 384/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3843 - accuracy: 0.8211\n",
      "Epoch 00384: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3847 - accuracy: 0.8207 - val_loss: 0.4475 - val_accuracy: 0.7764\n",
      "Epoch 385/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8219\n",
      "Epoch 00385: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3844 - accuracy: 0.8219 - val_loss: 0.4227 - val_accuracy: 0.7997\n",
      "Epoch 386/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8208\n",
      "Epoch 00386: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3895 - accuracy: 0.8206 - val_loss: 0.4172 - val_accuracy: 0.8050\n",
      "Epoch 387/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3861 - accuracy: 0.8245\n",
      "Epoch 00387: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3857 - accuracy: 0.8246 - val_loss: 0.4130 - val_accuracy: 0.8059\n",
      "Epoch 388/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8239\n",
      "Epoch 00388: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3841 - accuracy: 0.8238 - val_loss: 0.4623 - val_accuracy: 0.7661\n",
      "Epoch 389/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.8219\n",
      "Epoch 00389: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3854 - accuracy: 0.8220 - val_loss: 0.4207 - val_accuracy: 0.7986\n",
      "Epoch 390/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8226\n",
      "Epoch 00390: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3823 - accuracy: 0.8226 - val_loss: 0.4332 - val_accuracy: 0.7913\n",
      "Epoch 391/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8184\n",
      "Epoch 00391: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3881 - accuracy: 0.8184 - val_loss: 0.4271 - val_accuracy: 0.7939\n",
      "Epoch 392/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8258\n",
      "Epoch 00392: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3795 - accuracy: 0.8256 - val_loss: 0.4339 - val_accuracy: 0.7896\n",
      "Epoch 393/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8305\n",
      "Epoch 00393: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3776 - accuracy: 0.8304 - val_loss: 0.4348 - val_accuracy: 0.7904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8232\n",
      "Epoch 00394: val_loss did not improve from 0.41067\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3791 - accuracy: 0.8232 - val_loss: 0.4284 - val_accuracy: 0.7926\n",
      "Epoch 395/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8301\n",
      "Epoch 00395: val_loss improved from 0.41067 to 0.40726, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3789 - accuracy: 0.8297 - val_loss: 0.4073 - val_accuracy: 0.8108\n",
      "Epoch 396/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8234\n",
      "Epoch 00396: val_loss improved from 0.40726 to 0.40291, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3846 - accuracy: 0.8230 - val_loss: 0.4029 - val_accuracy: 0.8098\n",
      "Epoch 397/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8252\n",
      "Epoch 00397: val_loss did not improve from 0.40291\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3799 - accuracy: 0.8252 - val_loss: 0.4235 - val_accuracy: 0.7954\n",
      "Epoch 398/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8244\n",
      "Epoch 00398: val_loss did not improve from 0.40291\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3785 - accuracy: 0.8243 - val_loss: 0.4128 - val_accuracy: 0.8044\n",
      "Epoch 399/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8287\n",
      "Epoch 00399: val_loss did not improve from 0.40291\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3758 - accuracy: 0.8286 - val_loss: 0.4189 - val_accuracy: 0.8014\n",
      "Epoch 400/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8238\n",
      "Epoch 00400: val_loss did not improve from 0.40291\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3797 - accuracy: 0.8237 - val_loss: 0.4035 - val_accuracy: 0.8093\n",
      "Epoch 401/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8276\n",
      "Epoch 00401: val_loss did not improve from 0.40291\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3756 - accuracy: 0.8275 - val_loss: 0.4278 - val_accuracy: 0.7956\n",
      "Epoch 402/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8255\n",
      "Epoch 00402: val_loss improved from 0.40291 to 0.40024, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3778 - accuracy: 0.8254 - val_loss: 0.4002 - val_accuracy: 0.8117\n",
      "Epoch 403/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8330\n",
      "Epoch 00403: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3739 - accuracy: 0.8329 - val_loss: 0.4148 - val_accuracy: 0.8046\n",
      "Epoch 404/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8228\n",
      "Epoch 00404: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3794 - accuracy: 0.8231 - val_loss: 0.4599 - val_accuracy: 0.7719\n",
      "Epoch 405/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3778 - accuracy: 0.8249\n",
      "Epoch 00405: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3781 - accuracy: 0.8249 - val_loss: 0.4657 - val_accuracy: 0.7674\n",
      "Epoch 406/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8260\n",
      "Epoch 00406: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3764 - accuracy: 0.8260 - val_loss: 0.4220 - val_accuracy: 0.7977\n",
      "Epoch 407/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8239\n",
      "Epoch 00407: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3793 - accuracy: 0.8244 - val_loss: 0.4372 - val_accuracy: 0.7840\n",
      "Epoch 408/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8259\n",
      "Epoch 00408: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3783 - accuracy: 0.8258 - val_loss: 0.4451 - val_accuracy: 0.7844\n",
      "Epoch 409/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8264\n",
      "Epoch 00409: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3782 - accuracy: 0.8266 - val_loss: 0.4517 - val_accuracy: 0.7767\n",
      "Epoch 410/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8255\n",
      "Epoch 00410: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3770 - accuracy: 0.8258 - val_loss: 0.4424 - val_accuracy: 0.7850\n",
      "Epoch 411/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8318\n",
      "Epoch 00411: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3715 - accuracy: 0.8316 - val_loss: 0.4260 - val_accuracy: 0.7969\n",
      "Epoch 412/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8281\n",
      "Epoch 00412: val_loss did not improve from 0.40024\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3733 - accuracy: 0.8281 - val_loss: 0.4351 - val_accuracy: 0.7878\n",
      "Epoch 413/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8272\n",
      "Epoch 00413: val_loss improved from 0.40024 to 0.39434, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3756 - accuracy: 0.8270 - val_loss: 0.3943 - val_accuracy: 0.8143\n",
      "Epoch 414/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8304\n",
      "Epoch 00414: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3751 - accuracy: 0.8303 - val_loss: 0.4107 - val_accuracy: 0.8085\n",
      "Epoch 415/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8282\n",
      "Epoch 00415: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3756 - accuracy: 0.8282 - val_loss: 0.4352 - val_accuracy: 0.7891\n",
      "Epoch 416/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3730 - accuracy: 0.8281\n",
      "Epoch 00416: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3727 - accuracy: 0.8283 - val_loss: 0.4417 - val_accuracy: 0.7837\n",
      "Epoch 417/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8305\n",
      "Epoch 00417: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3721 - accuracy: 0.8304 - val_loss: 0.4061 - val_accuracy: 0.8095\n",
      "Epoch 418/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3741 - accuracy: 0.8284\n",
      "Epoch 00418: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3753 - accuracy: 0.8282 - val_loss: 0.4052 - val_accuracy: 0.8074\n",
      "Epoch 419/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8273\n",
      "Epoch 00419: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3726 - accuracy: 0.8274 - val_loss: 0.4091 - val_accuracy: 0.8087\n",
      "Epoch 420/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3730 - accuracy: 0.8314\n",
      "Epoch 00420: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3728 - accuracy: 0.8315 - val_loss: 0.4256 - val_accuracy: 0.7986\n",
      "Epoch 421/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8321\n",
      "Epoch 00421: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3696 - accuracy: 0.8318 - val_loss: 0.4188 - val_accuracy: 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8294\n",
      "Epoch 00422: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3730 - accuracy: 0.8296 - val_loss: 0.4075 - val_accuracy: 0.8063\n",
      "Epoch 423/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8315\n",
      "Epoch 00423: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3687 - accuracy: 0.8318 - val_loss: 0.4034 - val_accuracy: 0.8128\n",
      "Epoch 424/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8318\n",
      "Epoch 00424: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3709 - accuracy: 0.8317 - val_loss: 0.4290 - val_accuracy: 0.7921\n",
      "Epoch 425/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8269\n",
      "Epoch 00425: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3725 - accuracy: 0.8269 - val_loss: 0.4397 - val_accuracy: 0.7846\n",
      "Epoch 426/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8300\n",
      "Epoch 00426: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3701 - accuracy: 0.8299 - val_loss: 0.4019 - val_accuracy: 0.8115\n",
      "Epoch 427/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8310\n",
      "Epoch 00427: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3691 - accuracy: 0.8310 - val_loss: 0.4373 - val_accuracy: 0.7891\n",
      "Epoch 428/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3636 - accuracy: 0.8350\n",
      "Epoch 00428: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3636 - accuracy: 0.8350 - val_loss: 0.4152 - val_accuracy: 0.8044\n",
      "Epoch 429/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8299\n",
      "Epoch 00429: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3697 - accuracy: 0.8301 - val_loss: 0.4411 - val_accuracy: 0.7850\n",
      "Epoch 430/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8314\n",
      "Epoch 00430: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3646 - accuracy: 0.8315 - val_loss: 0.4320 - val_accuracy: 0.7908\n",
      "Epoch 431/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8321\n",
      "Epoch 00431: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3698 - accuracy: 0.8320 - val_loss: 0.4455 - val_accuracy: 0.7831\n",
      "Epoch 432/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3680 - accuracy: 0.8328\n",
      "Epoch 00432: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3678 - accuracy: 0.8329 - val_loss: 0.4130 - val_accuracy: 0.8033\n",
      "Epoch 433/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8294\n",
      "Epoch 00433: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3690 - accuracy: 0.8292 - val_loss: 0.4409 - val_accuracy: 0.7835\n",
      "Epoch 434/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8330\n",
      "Epoch 00434: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3644 - accuracy: 0.8331 - val_loss: 0.3966 - val_accuracy: 0.8151\n",
      "Epoch 435/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8322\n",
      "Epoch 00435: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3636 - accuracy: 0.8323 - val_loss: 0.4037 - val_accuracy: 0.8095\n",
      "Epoch 436/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8327\n",
      "Epoch 00436: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3655 - accuracy: 0.8326 - val_loss: 0.4557 - val_accuracy: 0.7741\n",
      "Epoch 437/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8337\n",
      "Epoch 00437: val_loss did not improve from 0.39434\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3632 - accuracy: 0.8339 - val_loss: 0.4220 - val_accuracy: 0.7977\n",
      "Epoch 438/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8326\n",
      "Epoch 00438: val_loss improved from 0.39434 to 0.39160, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3647 - accuracy: 0.8327 - val_loss: 0.3916 - val_accuracy: 0.8175\n",
      "Epoch 439/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3663 - accuracy: 0.8322\n",
      "Epoch 00439: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3667 - accuracy: 0.8322 - val_loss: 0.4124 - val_accuracy: 0.8033\n",
      "Epoch 440/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3620 - accuracy: 0.8362\n",
      "Epoch 00440: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3622 - accuracy: 0.8360 - val_loss: 0.4179 - val_accuracy: 0.7997\n",
      "Epoch 441/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8352\n",
      "Epoch 00441: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3652 - accuracy: 0.8349 - val_loss: 0.4344 - val_accuracy: 0.7904\n",
      "Epoch 442/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3637 - accuracy: 0.8335\n",
      "Epoch 00442: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3646 - accuracy: 0.8334 - val_loss: 0.4323 - val_accuracy: 0.7904\n",
      "Epoch 443/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8377\n",
      "Epoch 00443: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3609 - accuracy: 0.8380 - val_loss: 0.4096 - val_accuracy: 0.8046\n",
      "Epoch 444/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8309\n",
      "Epoch 00444: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3671 - accuracy: 0.8310 - val_loss: 0.4392 - val_accuracy: 0.7883\n",
      "Epoch 445/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8360\n",
      "Epoch 00445: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3628 - accuracy: 0.8357 - val_loss: 0.4356 - val_accuracy: 0.7872\n",
      "Epoch 446/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8356\n",
      "Epoch 00446: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3605 - accuracy: 0.8356 - val_loss: 0.3979 - val_accuracy: 0.8141\n",
      "Epoch 447/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8345\n",
      "Epoch 00447: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3611 - accuracy: 0.8344 - val_loss: 0.3944 - val_accuracy: 0.8149\n",
      "Epoch 448/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8379\n",
      "Epoch 00448: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3588 - accuracy: 0.8377 - val_loss: 0.4032 - val_accuracy: 0.8098\n",
      "Epoch 449/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3578 - accuracy: 0.8378\n",
      "Epoch 00449: val_loss did not improve from 0.39160\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3579 - accuracy: 0.8377 - val_loss: 0.4253 - val_accuracy: 0.7962\n",
      "Epoch 450/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8359\n",
      "Epoch 00450: val_loss improved from 0.39160 to 0.39134, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3588 - accuracy: 0.8358 - val_loss: 0.3913 - val_accuracy: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8346 ETA: 0s - loss: 0.3597 - accuracy: 0.83\n",
      "Epoch 00451: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3593 - accuracy: 0.8346 - val_loss: 0.4191 - val_accuracy: 0.7999\n",
      "Epoch 452/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8379\n",
      "Epoch 00452: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3610 - accuracy: 0.8377 - val_loss: 0.3947 - val_accuracy: 0.8151\n",
      "Epoch 453/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8343\n",
      "Epoch 00453: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3575 - accuracy: 0.8342 - val_loss: 0.4029 - val_accuracy: 0.8091\n",
      "Epoch 454/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8364\n",
      "Epoch 00454: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3615 - accuracy: 0.8363 - val_loss: 0.4016 - val_accuracy: 0.8100\n",
      "Epoch 455/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8355\n",
      "Epoch 00455: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3564 - accuracy: 0.8356 - val_loss: 0.4376 - val_accuracy: 0.7891\n",
      "Epoch 456/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.8370\n",
      "Epoch 00456: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3597 - accuracy: 0.8370 - val_loss: 0.4115 - val_accuracy: 0.8035\n",
      "Epoch 457/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8352\n",
      "Epoch 00457: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3591 - accuracy: 0.8354 - val_loss: 0.4321 - val_accuracy: 0.7913\n",
      "Epoch 458/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.8349\n",
      "Epoch 00458: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3595 - accuracy: 0.8347 - val_loss: 0.4022 - val_accuracy: 0.8095\n",
      "Epoch 459/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8362\n",
      "Epoch 00459: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3575 - accuracy: 0.8362 - val_loss: 0.4274 - val_accuracy: 0.7954\n",
      "Epoch 460/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8384\n",
      "Epoch 00460: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3563 - accuracy: 0.8384 - val_loss: 0.4638 - val_accuracy: 0.7719\n",
      "Epoch 461/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8378\n",
      "Epoch 00461: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3588 - accuracy: 0.8380 - val_loss: 0.4256 - val_accuracy: 0.7908\n",
      "Epoch 462/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8418\n",
      "Epoch 00462: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3489 - accuracy: 0.8420 - val_loss: 0.3948 - val_accuracy: 0.8143\n",
      "Epoch 463/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3557 - accuracy: 0.8411\n",
      "Epoch 00463: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3561 - accuracy: 0.8410 - val_loss: 0.4078 - val_accuracy: 0.8065\n",
      "Epoch 464/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8357\n",
      "Epoch 00464: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3575 - accuracy: 0.8357 - val_loss: 0.4003 - val_accuracy: 0.8100\n",
      "Epoch 465/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8409\n",
      "Epoch 00465: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3484 - accuracy: 0.8411 - val_loss: 0.4240 - val_accuracy: 0.7956\n",
      "Epoch 466/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8400\n",
      "Epoch 00466: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3516 - accuracy: 0.8399 - val_loss: 0.4301 - val_accuracy: 0.7945\n",
      "Epoch 467/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3505 - accuracy: 0.8411\n",
      "Epoch 00467: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3504 - accuracy: 0.8411 - val_loss: 0.4462 - val_accuracy: 0.7829\n",
      "Epoch 468/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8418\n",
      "Epoch 00468: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3497 - accuracy: 0.8416 - val_loss: 0.4316 - val_accuracy: 0.7913\n",
      "Epoch 469/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.8388\n",
      "Epoch 00469: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3548 - accuracy: 0.8389 - val_loss: 0.4063 - val_accuracy: 0.8057\n",
      "Epoch 470/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8427\n",
      "Epoch 00470: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3494 - accuracy: 0.8425 - val_loss: 0.4161 - val_accuracy: 0.7988\n",
      "Epoch 471/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8448\n",
      "Epoch 00471: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3497 - accuracy: 0.8453 - val_loss: 0.4442 - val_accuracy: 0.7829\n",
      "Epoch 472/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3559 - accuracy: 0.8385\n",
      "Epoch 00472: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3566 - accuracy: 0.8383 - val_loss: 0.3970 - val_accuracy: 0.8134\n",
      "Epoch 473/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.8430\n",
      "Epoch 00473: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3527 - accuracy: 0.8432 - val_loss: 0.4282 - val_accuracy: 0.7951\n",
      "Epoch 474/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8440\n",
      "Epoch 00474: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3504 - accuracy: 0.8442 - val_loss: 0.4068 - val_accuracy: 0.8063\n",
      "Epoch 475/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8373\n",
      "Epoch 00475: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3534 - accuracy: 0.8375 - val_loss: 0.4149 - val_accuracy: 0.7988\n",
      "Epoch 476/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8379\n",
      "Epoch 00476: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3545 - accuracy: 0.8380 - val_loss: 0.4154 - val_accuracy: 0.8001\n",
      "Epoch 477/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.8381\n",
      "Epoch 00477: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3554 - accuracy: 0.8378 - val_loss: 0.3951 - val_accuracy: 0.8113\n",
      "Epoch 478/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8423\n",
      "Epoch 00478: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3461 - accuracy: 0.8421 - val_loss: 0.4260 - val_accuracy: 0.7954\n",
      "Epoch 479/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8397\n",
      "Epoch 00479: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3492 - accuracy: 0.8400 - val_loss: 0.4327 - val_accuracy: 0.7872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8435\n",
      "Epoch 00480: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3487 - accuracy: 0.8433 - val_loss: 0.3950 - val_accuracy: 0.8130\n",
      "Epoch 481/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8419\n",
      "Epoch 00481: val_loss did not improve from 0.39134\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3495 - accuracy: 0.8419 - val_loss: 0.4221 - val_accuracy: 0.7939\n",
      "Epoch 482/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8421\n",
      "Epoch 00482: val_loss improved from 0.39134 to 0.39130, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3484 - accuracy: 0.8421 - val_loss: 0.3913 - val_accuracy: 0.8162\n",
      "Epoch 483/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8442\n",
      "Epoch 00483: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3454 - accuracy: 0.8444 - val_loss: 0.4109 - val_accuracy: 0.8027\n",
      "Epoch 484/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8423\n",
      "Epoch 00484: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3500 - accuracy: 0.8422 - val_loss: 0.4251 - val_accuracy: 0.7921\n",
      "Epoch 485/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8401\n",
      "Epoch 00485: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3480 - accuracy: 0.8401 - val_loss: 0.4104 - val_accuracy: 0.8025\n",
      "Epoch 486/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8431\n",
      "Epoch 00486: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3458 - accuracy: 0.8432 - val_loss: 0.4135 - val_accuracy: 0.7992\n",
      "Epoch 487/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8454\n",
      "Epoch 00487: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3423 - accuracy: 0.8454 - val_loss: 0.4256 - val_accuracy: 0.7919\n",
      "Epoch 488/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8415\n",
      "Epoch 00488: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3491 - accuracy: 0.8416 - val_loss: 0.4234 - val_accuracy: 0.7956\n",
      "Epoch 489/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8449\n",
      "Epoch 00489: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3406 - accuracy: 0.8449 - val_loss: 0.3972 - val_accuracy: 0.8128\n",
      "Epoch 490/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8438\n",
      "Epoch 00490: val_loss did not improve from 0.39130\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3489 - accuracy: 0.8438 - val_loss: 0.4210 - val_accuracy: 0.7982\n",
      "Epoch 491/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8439 E\n",
      "Epoch 00491: val_loss improved from 0.39130 to 0.38601, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3447 - accuracy: 0.8439 - val_loss: 0.3860 - val_accuracy: 0.8212\n",
      "Epoch 492/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3420 - accuracy: 0.8475\n",
      "Epoch 00492: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3418 - accuracy: 0.8474 - val_loss: 0.4095 - val_accuracy: 0.8014\n",
      "Epoch 493/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.8438\n",
      "Epoch 00493: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3474 - accuracy: 0.8439 - val_loss: 0.4303 - val_accuracy: 0.7902\n",
      "Epoch 494/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8438\n",
      "Epoch 00494: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3454 - accuracy: 0.8439 - val_loss: 0.4074 - val_accuracy: 0.8061\n",
      "Epoch 495/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8475\n",
      "Epoch 00495: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3396 - accuracy: 0.8474 - val_loss: 0.4253 - val_accuracy: 0.7954\n",
      "Epoch 496/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8434\n",
      "Epoch 00496: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3479 - accuracy: 0.8433 - val_loss: 0.4155 - val_accuracy: 0.8020\n",
      "Epoch 497/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8460\n",
      "Epoch 00497: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3402 - accuracy: 0.8459 - val_loss: 0.4349 - val_accuracy: 0.7880\n",
      "Epoch 498/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8503\n",
      "Epoch 00498: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3392 - accuracy: 0.8503 - val_loss: 0.3886 - val_accuracy: 0.8169\n",
      "Epoch 499/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8484\n",
      "Epoch 00499: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3417 - accuracy: 0.8484 - val_loss: 0.4028 - val_accuracy: 0.8067\n",
      "Epoch 500/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.8443\n",
      "Epoch 00500: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3444 - accuracy: 0.8443 - val_loss: 0.3947 - val_accuracy: 0.8151\n",
      "Epoch 501/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3395 - accuracy: 0.8467\n",
      "Epoch 00501: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3394 - accuracy: 0.8464 - val_loss: 0.4010 - val_accuracy: 0.8091\n",
      "Epoch 502/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8457\n",
      "Epoch 00502: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3412 - accuracy: 0.8457 - val_loss: 0.3950 - val_accuracy: 0.8147\n",
      "Epoch 503/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8459\n",
      "Epoch 00503: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3406 - accuracy: 0.8458 - val_loss: 0.4053 - val_accuracy: 0.8067\n",
      "Epoch 504/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3431 - accuracy: 0.8472\n",
      "Epoch 00504: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3426 - accuracy: 0.8473 - val_loss: 0.4288 - val_accuracy: 0.7900\n",
      "Epoch 505/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8468\n",
      "Epoch 00505: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3380 - accuracy: 0.8466 - val_loss: 0.4066 - val_accuracy: 0.8078\n",
      "Epoch 506/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8470\n",
      "Epoch 00506: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3412 - accuracy: 0.8469 - val_loss: 0.4349 - val_accuracy: 0.7874\n",
      "Epoch 507/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8501\n",
      "Epoch 00507: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3362 - accuracy: 0.8502 - val_loss: 0.4205 - val_accuracy: 0.7969\n",
      "Epoch 508/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8472\n",
      "Epoch 00508: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3393 - accuracy: 0.8472 - val_loss: 0.4142 - val_accuracy: 0.7990\n",
      "Epoch 509/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8507\n",
      "Epoch 00509: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3361 - accuracy: 0.8507 - val_loss: 0.3966 - val_accuracy: 0.8130\n",
      "Epoch 510/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8464\n",
      "Epoch 00510: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3390 - accuracy: 0.8465 - val_loss: 0.4414 - val_accuracy: 0.7872\n",
      "Epoch 511/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8493\n",
      "Epoch 00511: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3358 - accuracy: 0.8495 - val_loss: 0.4166 - val_accuracy: 0.7994\n",
      "Epoch 512/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8485\n",
      "Epoch 00512: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3364 - accuracy: 0.8487 - val_loss: 0.4013 - val_accuracy: 0.8093\n",
      "Epoch 513/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8487\n",
      "Epoch 00513: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3374 - accuracy: 0.8487 - val_loss: 0.3967 - val_accuracy: 0.8147\n",
      "Epoch 514/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8491\n",
      "Epoch 00514: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3364 - accuracy: 0.8489 - val_loss: 0.3995 - val_accuracy: 0.8121\n",
      "Epoch 515/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8474\n",
      "Epoch 00515: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3362 - accuracy: 0.8470 - val_loss: 0.3986 - val_accuracy: 0.8136\n",
      "Epoch 516/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.8519\n",
      "Epoch 00516: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3305 - accuracy: 0.8519 - val_loss: 0.4251 - val_accuracy: 0.7919\n",
      "Epoch 517/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8509\n",
      "Epoch 00517: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 0.3342 - accuracy: 0.8509 - val_loss: 0.4396 - val_accuracy: 0.7846\n",
      "Epoch 518/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8509\n",
      "Epoch 00518: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3349 - accuracy: 0.8512 - val_loss: 0.4152 - val_accuracy: 0.7984\n",
      "Epoch 519/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8514\n",
      "Epoch 00519: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3307 - accuracy: 0.8514 - val_loss: 0.4058 - val_accuracy: 0.8063\n",
      "Epoch 520/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8515\n",
      "Epoch 00520: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3323 - accuracy: 0.8516 - val_loss: 0.4173 - val_accuracy: 0.7992\n",
      "Epoch 521/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8505\n",
      "Epoch 00521: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3341 - accuracy: 0.8503 - val_loss: 0.4049 - val_accuracy: 0.8085\n",
      "Epoch 522/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8485\n",
      "Epoch 00522: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3338 - accuracy: 0.8490 - val_loss: 0.4024 - val_accuracy: 0.8080\n",
      "Epoch 523/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.8499\n",
      "Epoch 00523: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3332 - accuracy: 0.8501 - val_loss: 0.4143 - val_accuracy: 0.8018\n",
      "Epoch 524/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8526\n",
      "Epoch 00524: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3317 - accuracy: 0.8525 - val_loss: 0.4000 - val_accuracy: 0.8106\n",
      "Epoch 525/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8489\n",
      "Epoch 00525: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3337 - accuracy: 0.8491 - val_loss: 0.4288 - val_accuracy: 0.7934\n",
      "Epoch 526/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8500\n",
      "Epoch 00526: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3347 - accuracy: 0.8503 - val_loss: 0.4215 - val_accuracy: 0.7977\n",
      "Epoch 527/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8522\n",
      "Epoch 00527: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3327 - accuracy: 0.8521 - val_loss: 0.4476 - val_accuracy: 0.7868\n",
      "Epoch 528/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8538\n",
      "Epoch 00528: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3300 - accuracy: 0.8538 - val_loss: 0.4190 - val_accuracy: 0.7984\n",
      "Epoch 529/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3389 - accuracy: 0.8451\n",
      "Epoch 00529: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3389 - accuracy: 0.8451 - val_loss: 0.4019 - val_accuracy: 0.8113\n",
      "Epoch 530/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.8567\n",
      "Epoch 00530: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3285 - accuracy: 0.8565 - val_loss: 0.3897 - val_accuracy: 0.8186\n",
      "Epoch 531/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8537\n",
      "Epoch 00531: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3276 - accuracy: 0.8536 - val_loss: 0.4065 - val_accuracy: 0.8061\n",
      "Epoch 532/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8539\n",
      "Epoch 00532: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3290 - accuracy: 0.8537 - val_loss: 0.4300 - val_accuracy: 0.7904\n",
      "Epoch 533/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8504\n",
      "Epoch 00533: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3305 - accuracy: 0.8506 - val_loss: 0.4087 - val_accuracy: 0.8070\n",
      "Epoch 534/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.8544\n",
      "Epoch 00534: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3226 - accuracy: 0.8545 - val_loss: 0.4383 - val_accuracy: 0.7868\n",
      "Epoch 535/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8563\n",
      "Epoch 00535: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3270 - accuracy: 0.8564 - val_loss: 0.3915 - val_accuracy: 0.8199\n",
      "Epoch 536/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8546\n",
      "Epoch 00536: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3274 - accuracy: 0.8545 - val_loss: 0.4183 - val_accuracy: 0.7979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 537/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8523\n",
      "Epoch 00537: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3277 - accuracy: 0.8527 - val_loss: 0.4577 - val_accuracy: 0.7810\n",
      "Epoch 538/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.8528\n",
      "Epoch 00538: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3308 - accuracy: 0.8529 - val_loss: 0.4159 - val_accuracy: 0.8018\n",
      "Epoch 539/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.85 - ETA: 0s - loss: 0.3268 - accuracy: 0.8547\n",
      "Epoch 00539: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3268 - accuracy: 0.8548 - val_loss: 0.3886 - val_accuracy: 0.8209\n",
      "Epoch 540/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.8529\n",
      "Epoch 00540: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3262 - accuracy: 0.8529 - val_loss: 0.3880 - val_accuracy: 0.8186\n",
      "Epoch 541/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8520\n",
      "Epoch 00541: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3227 - accuracy: 0.8522 - val_loss: 0.4101 - val_accuracy: 0.8052\n",
      "Epoch 542/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8544\n",
      "Epoch 00542: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3268 - accuracy: 0.8544 - val_loss: 0.4303 - val_accuracy: 0.7928\n",
      "Epoch 543/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8550\n",
      "Epoch 00543: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3265 - accuracy: 0.8551 - val_loss: 0.3975 - val_accuracy: 0.8128\n",
      "Epoch 544/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8552\n",
      "Epoch 00544: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3256 - accuracy: 0.8552 - val_loss: 0.4232 - val_accuracy: 0.7921\n",
      "Epoch 545/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3274 - accuracy: 0.8537\n",
      "Epoch 00545: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3275 - accuracy: 0.8536 - val_loss: 0.4358 - val_accuracy: 0.7880\n",
      "Epoch 546/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8579\n",
      "Epoch 00546: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3195 - accuracy: 0.8580 - val_loss: 0.4326 - val_accuracy: 0.7898\n",
      "Epoch 547/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8568\n",
      "Epoch 00547: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3221 - accuracy: 0.8567 - val_loss: 0.4059 - val_accuracy: 0.8072\n",
      "Epoch 548/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8566\n",
      "Epoch 00548: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3230 - accuracy: 0.8566 - val_loss: 0.3894 - val_accuracy: 0.8186\n",
      "Epoch 549/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8539\n",
      "Epoch 00549: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3247 - accuracy: 0.8537 - val_loss: 0.4248 - val_accuracy: 0.7969\n",
      "Epoch 550/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8559\n",
      "Epoch 00550: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3253 - accuracy: 0.8558 - val_loss: 0.3957 - val_accuracy: 0.8136\n",
      "Epoch 551/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8570\n",
      "Epoch 00551: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3215 - accuracy: 0.8569 - val_loss: 0.3906 - val_accuracy: 0.8184\n",
      "Epoch 552/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8568\n",
      "Epoch 00552: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3254 - accuracy: 0.8568 - val_loss: 0.3997 - val_accuracy: 0.8134\n",
      "Epoch 553/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8553\n",
      "Epoch 00553: val_loss did not improve from 0.38601\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3256 - accuracy: 0.8554 - val_loss: 0.4038 - val_accuracy: 0.8067\n",
      "Epoch 554/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8575\n",
      "Epoch 00554: val_loss improved from 0.38601 to 0.38303, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3179 - accuracy: 0.8577 - val_loss: 0.3830 - val_accuracy: 0.8250\n",
      "Epoch 555/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8572\n",
      "Epoch 00555: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3198 - accuracy: 0.8572 - val_loss: 0.4207 - val_accuracy: 0.7962\n",
      "Epoch 556/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8574\n",
      "Epoch 00556: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3188 - accuracy: 0.8575 - val_loss: 0.4261 - val_accuracy: 0.7919\n",
      "Epoch 557/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8592\n",
      "Epoch 00557: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3174 - accuracy: 0.8592 - val_loss: 0.3987 - val_accuracy: 0.8134\n",
      "Epoch 558/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8562\n",
      "Epoch 00558: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3202 - accuracy: 0.8563 - val_loss: 0.4013 - val_accuracy: 0.8110\n",
      "Epoch 559/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8551\n",
      "Epoch 00559: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3201 - accuracy: 0.8552 - val_loss: 0.4215 - val_accuracy: 0.7977\n",
      "Epoch 560/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8591\n",
      "Epoch 00560: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3217 - accuracy: 0.8592 - val_loss: 0.4011 - val_accuracy: 0.8104\n",
      "Epoch 561/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8578\n",
      "Epoch 00561: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3208 - accuracy: 0.8579 - val_loss: 0.3965 - val_accuracy: 0.8162\n",
      "Epoch 562/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8602\n",
      "Epoch 00562: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3126 - accuracy: 0.8601 - val_loss: 0.4642 - val_accuracy: 0.7756\n",
      "Epoch 563/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8595\n",
      "Epoch 00563: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3154 - accuracy: 0.8593 - val_loss: 0.4072 - val_accuracy: 0.8074\n",
      "Epoch 564/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8601\n",
      "Epoch 00564: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3154 - accuracy: 0.8600 - val_loss: 0.4351 - val_accuracy: 0.7921\n",
      "Epoch 565/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8606\n",
      "Epoch 00565: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3158 - accuracy: 0.8607 - val_loss: 0.4172 - val_accuracy: 0.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8613\n",
      "Epoch 00566: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3139 - accuracy: 0.8614 - val_loss: 0.3984 - val_accuracy: 0.8121\n",
      "Epoch 567/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8541\n",
      "Epoch 00567: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3235 - accuracy: 0.8545 - val_loss: 0.4468 - val_accuracy: 0.7861\n",
      "Epoch 568/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8594\n",
      "Epoch 00568: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3167 - accuracy: 0.8595 - val_loss: 0.4081 - val_accuracy: 0.8067\n",
      "Epoch 569/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8596\n",
      "Epoch 00569: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3143 - accuracy: 0.8595 - val_loss: 0.3999 - val_accuracy: 0.8113\n",
      "Epoch 570/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8609\n",
      "Epoch 00570: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3141 - accuracy: 0.8610 - val_loss: 0.4208 - val_accuracy: 0.7928\n",
      "Epoch 571/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8593\n",
      "Epoch 00571: val_loss did not improve from 0.38303\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3184 - accuracy: 0.8591 - val_loss: 0.3839 - val_accuracy: 0.8218\n",
      "Epoch 572/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8620 ETA: 0s - loss: 0.3106 - accuracy: 0.86\n",
      "Epoch 00572: val_loss improved from 0.38303 to 0.38281, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3107 - accuracy: 0.8620 - val_loss: 0.3828 - val_accuracy: 0.8235\n",
      "Epoch 573/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8600\n",
      "Epoch 00573: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3125 - accuracy: 0.8598 - val_loss: 0.3848 - val_accuracy: 0.8220\n",
      "Epoch 574/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8588\n",
      "Epoch 00574: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3196 - accuracy: 0.8589 - val_loss: 0.4093 - val_accuracy: 0.8014\n",
      "Epoch 575/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8555\n",
      "Epoch 00575: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3167 - accuracy: 0.8560 - val_loss: 0.4118 - val_accuracy: 0.8035\n",
      "Epoch 576/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8583\n",
      "Epoch 00576: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3198 - accuracy: 0.8583 - val_loss: 0.3974 - val_accuracy: 0.8134\n",
      "Epoch 577/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8625\n",
      "Epoch 00577: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3099 - accuracy: 0.8623 - val_loss: 0.4133 - val_accuracy: 0.8027\n",
      "Epoch 578/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8648\n",
      "Epoch 00578: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3100 - accuracy: 0.8648 - val_loss: 0.3865 - val_accuracy: 0.8218\n",
      "Epoch 579/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8602 ETA: 0s - loss: 0.3157 - ac\n",
      "Epoch 00579: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3157 - accuracy: 0.8602 - val_loss: 0.3996 - val_accuracy: 0.8085\n",
      "Epoch 580/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8627\n",
      "Epoch 00580: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3098 - accuracy: 0.8626 - val_loss: 0.4303 - val_accuracy: 0.7941\n",
      "Epoch 581/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8640\n",
      "Epoch 00581: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3067 - accuracy: 0.8641 - val_loss: 0.4315 - val_accuracy: 0.7902\n",
      "Epoch 582/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8625\n",
      "Epoch 00582: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3092 - accuracy: 0.8626 - val_loss: 0.4265 - val_accuracy: 0.7969\n",
      "Epoch 583/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.8599\n",
      "Epoch 00583: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3146 - accuracy: 0.8599 - val_loss: 0.4165 - val_accuracy: 0.8007\n",
      "Epoch 584/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8626\n",
      "Epoch 00584: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3099 - accuracy: 0.8628 - val_loss: 0.4075 - val_accuracy: 0.8057\n",
      "Epoch 585/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8631\n",
      "Epoch 00585: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3078 - accuracy: 0.8630 - val_loss: 0.3954 - val_accuracy: 0.8158\n",
      "Epoch 586/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8605\n",
      "Epoch 00586: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3097 - accuracy: 0.8605 - val_loss: 0.4103 - val_accuracy: 0.8042\n",
      "Epoch 587/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8626\n",
      "Epoch 00587: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3075 - accuracy: 0.8627 - val_loss: 0.4132 - val_accuracy: 0.8042\n",
      "Epoch 588/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8674\n",
      "Epoch 00588: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3049 - accuracy: 0.8675 - val_loss: 0.3944 - val_accuracy: 0.8171\n",
      "Epoch 589/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8628\n",
      "Epoch 00589: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3080 - accuracy: 0.8631 - val_loss: 0.4293 - val_accuracy: 0.7964\n",
      "Epoch 590/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8651\n",
      "Epoch 00590: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3040 - accuracy: 0.8650 - val_loss: 0.4345 - val_accuracy: 0.7928\n",
      "Epoch 591/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8628\n",
      "Epoch 00591: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3103 - accuracy: 0.8628 - val_loss: 0.3873 - val_accuracy: 0.8218\n",
      "Epoch 592/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8647\n",
      "Epoch 00592: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3042 - accuracy: 0.8648 - val_loss: 0.4491 - val_accuracy: 0.7855\n",
      "Epoch 593/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8670\n",
      "Epoch 00593: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3064 - accuracy: 0.8671 - val_loss: 0.4074 - val_accuracy: 0.8078\n",
      "Epoch 594/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8652\n",
      "Epoch 00594: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3015 - accuracy: 0.8653 - val_loss: 0.4311 - val_accuracy: 0.7958\n",
      "Epoch 595/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8625\n",
      "Epoch 00595: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3069 - accuracy: 0.8624 - val_loss: 0.4188 - val_accuracy: 0.8031\n",
      "Epoch 596/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8623\n",
      "Epoch 00596: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3059 - accuracy: 0.8627 - val_loss: 0.4183 - val_accuracy: 0.7988\n",
      "Epoch 597/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8657\n",
      "Epoch 00597: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3047 - accuracy: 0.8661 - val_loss: 0.3964 - val_accuracy: 0.8149\n",
      "Epoch 598/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8636\n",
      "Epoch 00598: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3050 - accuracy: 0.8637 - val_loss: 0.4156 - val_accuracy: 0.7997\n",
      "Epoch 599/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8652\n",
      "Epoch 00599: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3018 - accuracy: 0.8652 - val_loss: 0.4207 - val_accuracy: 0.8014\n",
      "Epoch 600/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8661\n",
      "Epoch 00600: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3003 - accuracy: 0.8664 - val_loss: 0.4251 - val_accuracy: 0.7975\n",
      "Epoch 601/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8643\n",
      "Epoch 00601: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3019 - accuracy: 0.8644 - val_loss: 0.4127 - val_accuracy: 0.8025\n",
      "Epoch 602/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8685\n",
      "Epoch 00602: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3008 - accuracy: 0.8686 - val_loss: 0.4125 - val_accuracy: 0.8007\n",
      "Epoch 603/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8673\n",
      "Epoch 00603: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3015 - accuracy: 0.8673 - val_loss: 0.3884 - val_accuracy: 0.8190\n",
      "Epoch 604/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8679\n",
      "Epoch 00604: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3005 - accuracy: 0.8679 - val_loss: 0.4091 - val_accuracy: 0.8052\n",
      "Epoch 605/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8675\n",
      "Epoch 00605: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3003 - accuracy: 0.8677 - val_loss: 0.4235 - val_accuracy: 0.7984\n",
      "Epoch 606/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8665\n",
      "Epoch 00606: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3010 - accuracy: 0.8663 - val_loss: 0.3985 - val_accuracy: 0.8115\n",
      "Epoch 607/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8679\n",
      "Epoch 00607: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2998 - accuracy: 0.8681 - val_loss: 0.4239 - val_accuracy: 0.8007\n",
      "Epoch 608/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8660\n",
      "Epoch 00608: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3011 - accuracy: 0.8663 - val_loss: 0.4501 - val_accuracy: 0.7848\n",
      "Epoch 609/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8665\n",
      "Epoch 00609: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2989 - accuracy: 0.8667 - val_loss: 0.4279 - val_accuracy: 0.7984\n",
      "Epoch 610/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.8697\n",
      "Epoch 00610: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3004 - accuracy: 0.8699 - val_loss: 0.4240 - val_accuracy: 0.7975\n",
      "Epoch 611/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8662\n",
      "Epoch 00611: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3013 - accuracy: 0.8663 - val_loss: 0.4094 - val_accuracy: 0.8078\n",
      "Epoch 612/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8659\n",
      "Epoch 00612: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3019 - accuracy: 0.8654 - val_loss: 0.4117 - val_accuracy: 0.8042\n",
      "Epoch 613/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8669\n",
      "Epoch 00613: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3031 - accuracy: 0.8672 - val_loss: 0.4221 - val_accuracy: 0.8035\n",
      "Epoch 614/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8685\n",
      "Epoch 00614: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2971 - accuracy: 0.8685 - val_loss: 0.4157 - val_accuracy: 0.8067\n",
      "Epoch 615/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8719\n",
      "Epoch 00615: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2953 - accuracy: 0.8715 - val_loss: 0.3976 - val_accuracy: 0.8153\n",
      "Epoch 616/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8711\n",
      "Epoch 00616: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2928 - accuracy: 0.8711 - val_loss: 0.4015 - val_accuracy: 0.8145\n",
      "Epoch 617/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8680\n",
      "Epoch 00617: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2965 - accuracy: 0.8682 - val_loss: 0.4178 - val_accuracy: 0.8031\n",
      "Epoch 618/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8706\n",
      "Epoch 00618: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2949 - accuracy: 0.8708 - val_loss: 0.4100 - val_accuracy: 0.8093\n",
      "Epoch 619/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8700\n",
      "Epoch 00619: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2950 - accuracy: 0.8699 - val_loss: 0.4306 - val_accuracy: 0.7977\n",
      "Epoch 620/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8734\n",
      "Epoch 00620: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2891 - accuracy: 0.8736 - val_loss: 0.3962 - val_accuracy: 0.8153\n",
      "Epoch 621/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.8682\n",
      "Epoch 00621: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2991 - accuracy: 0.8680 - val_loss: 0.4099 - val_accuracy: 0.8061\n",
      "Epoch 622/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8726\n",
      "Epoch 00622: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2903 - accuracy: 0.8727 - val_loss: 0.4280 - val_accuracy: 0.7960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8737\n",
      "Epoch 00623: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2920 - accuracy: 0.8739 - val_loss: 0.4135 - val_accuracy: 0.8046\n",
      "Epoch 624/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8698\n",
      "Epoch 00624: val_loss did not improve from 0.38281\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2942 - accuracy: 0.8697 - val_loss: 0.4447 - val_accuracy: 0.7911\n",
      "Epoch 625/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.8717\n",
      "Epoch 00625: val_loss improved from 0.38281 to 0.38265, saving model to pickled_objects/batch_size_128_lr_0.01_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2922 - accuracy: 0.8716 - val_loss: 0.3826 - val_accuracy: 0.8257\n",
      "Epoch 626/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8716\n",
      "Epoch 00626: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2936 - accuracy: 0.8716 - val_loss: 0.4145 - val_accuracy: 0.8048\n",
      "Epoch 627/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8690\n",
      "Epoch 00627: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2945 - accuracy: 0.8691 - val_loss: 0.3986 - val_accuracy: 0.8117\n",
      "Epoch 628/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8699\n",
      "Epoch 00628: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2931 - accuracy: 0.8700 - val_loss: 0.4239 - val_accuracy: 0.7962\n",
      "Epoch 629/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8716\n",
      "Epoch 00629: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 0.2889 - accuracy: 0.8715 - val_loss: 0.4149 - val_accuracy: 0.8018\n",
      "Epoch 630/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8719\n",
      "Epoch 00630: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2904 - accuracy: 0.8722 - val_loss: 0.4101 - val_accuracy: 0.8098\n",
      "Epoch 631/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8714\n",
      "Epoch 00631: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2933 - accuracy: 0.8715 - val_loss: 0.4128 - val_accuracy: 0.8061\n",
      "Epoch 632/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8739\n",
      "Epoch 00632: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2874 - accuracy: 0.8739 - val_loss: 0.3861 - val_accuracy: 0.8216\n",
      "Epoch 633/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8712\n",
      "Epoch 00633: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2926 - accuracy: 0.8713 - val_loss: 0.4104 - val_accuracy: 0.8061\n",
      "Epoch 634/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.8719\n",
      "Epoch 00634: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2900 - accuracy: 0.8720 - val_loss: 0.3891 - val_accuracy: 0.8192\n",
      "Epoch 635/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8710\n",
      "Epoch 00635: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2908 - accuracy: 0.8711 - val_loss: 0.4084 - val_accuracy: 0.8110\n",
      "Epoch 636/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8733\n",
      "Epoch 00636: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2887 - accuracy: 0.8736 - val_loss: 0.4371 - val_accuracy: 0.7945\n",
      "Epoch 637/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8710\n",
      "Epoch 00637: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2927 - accuracy: 0.8712 - val_loss: 0.4039 - val_accuracy: 0.8098\n",
      "Epoch 638/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8731\n",
      "Epoch 00638: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2901 - accuracy: 0.8732 - val_loss: 0.4011 - val_accuracy: 0.8132\n",
      "Epoch 639/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.8743\n",
      "Epoch 00639: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2868 - accuracy: 0.8744 - val_loss: 0.4050 - val_accuracy: 0.8100\n",
      "Epoch 640/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8767\n",
      "Epoch 00640: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2803 - accuracy: 0.8767 - val_loss: 0.4050 - val_accuracy: 0.8104\n",
      "Epoch 641/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8737\n",
      "Epoch 00641: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2887 - accuracy: 0.8738 - val_loss: 0.4086 - val_accuracy: 0.8108\n",
      "Epoch 642/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8733\n",
      "Epoch 00642: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2884 - accuracy: 0.8734 - val_loss: 0.4049 - val_accuracy: 0.8115\n",
      "Epoch 643/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8734\n",
      "Epoch 00643: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2847 - accuracy: 0.8736 - val_loss: 0.4352 - val_accuracy: 0.7928\n",
      "Epoch 644/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8761\n",
      "Epoch 00644: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2810 - accuracy: 0.8763 - val_loss: 0.3967 - val_accuracy: 0.8164\n",
      "Epoch 645/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8724\n",
      "Epoch 00645: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2829 - accuracy: 0.8724 - val_loss: 0.4171 - val_accuracy: 0.8074\n",
      "Epoch 646/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8762\n",
      "Epoch 00646: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2882 - accuracy: 0.8761 - val_loss: 0.3865 - val_accuracy: 0.8257\n",
      "Epoch 647/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.8721\n",
      "Epoch 00647: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2887 - accuracy: 0.8724 - val_loss: 0.4084 - val_accuracy: 0.8080\n",
      "Epoch 648/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8740\n",
      "Epoch 00648: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2842 - accuracy: 0.8743 - val_loss: 0.4284 - val_accuracy: 0.7992\n",
      "Epoch 649/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.8743\n",
      "Epoch 00649: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2852 - accuracy: 0.8744 - val_loss: 0.4011 - val_accuracy: 0.8130\n",
      "Epoch 650/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.8763\n",
      "Epoch 00650: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2847 - accuracy: 0.8764 - val_loss: 0.4051 - val_accuracy: 0.8089\n",
      "Epoch 651/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8774\n",
      "Epoch 00651: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2827 - accuracy: 0.8774 - val_loss: 0.4250 - val_accuracy: 0.8033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 652/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8748\n",
      "Epoch 00652: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2816 - accuracy: 0.8750 - val_loss: 0.4341 - val_accuracy: 0.7979\n",
      "Epoch 653/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.8737\n",
      "Epoch 00653: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2877 - accuracy: 0.8737 - val_loss: 0.4365 - val_accuracy: 0.7962\n",
      "Epoch 654/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.8732\n",
      "Epoch 00654: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2858 - accuracy: 0.8733 - val_loss: 0.4117 - val_accuracy: 0.8078\n",
      "Epoch 655/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.8770\n",
      "Epoch 00655: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2826 - accuracy: 0.8771 - val_loss: 0.4212 - val_accuracy: 0.7990\n",
      "Epoch 656/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8747\n",
      "Epoch 00656: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2851 - accuracy: 0.8749 - val_loss: 0.3937 - val_accuracy: 0.8177\n",
      "Epoch 657/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.8783\n",
      "Epoch 00657: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2770 - accuracy: 0.8785 - val_loss: 0.4217 - val_accuracy: 0.8003\n",
      "Epoch 658/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8750\n",
      "Epoch 00658: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2848 - accuracy: 0.8752 - val_loss: 0.4306 - val_accuracy: 0.7973\n",
      "Epoch 659/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8779\n",
      "Epoch 00659: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2797 - accuracy: 0.8779 - val_loss: 0.4336 - val_accuracy: 0.7954\n",
      "Epoch 660/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.8788\n",
      "Epoch 00660: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2783 - accuracy: 0.8790 - val_loss: 0.3966 - val_accuracy: 0.8153\n",
      "Epoch 661/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2786 - accuracy: 0.8768\n",
      "Epoch 00661: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2781 - accuracy: 0.8769 - val_loss: 0.4014 - val_accuracy: 0.8160\n",
      "Epoch 662/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8780\n",
      "Epoch 00662: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2825 - accuracy: 0.8782 - val_loss: 0.4065 - val_accuracy: 0.8108\n",
      "Epoch 663/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8759\n",
      "Epoch 00663: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2801 - accuracy: 0.8759 - val_loss: 0.4294 - val_accuracy: 0.8009\n",
      "Epoch 664/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.8747\n",
      "Epoch 00664: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2822 - accuracy: 0.8748 - val_loss: 0.4123 - val_accuracy: 0.8057\n",
      "Epoch 665/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.8760\n",
      "Epoch 00665: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2813 - accuracy: 0.8760 - val_loss: 0.4137 - val_accuracy: 0.8052\n",
      "Epoch 666/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.8801\n",
      "Epoch 00666: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2734 - accuracy: 0.8803 - val_loss: 0.4347 - val_accuracy: 0.7962\n",
      "Epoch 667/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2769 - accuracy: 0.8785\n",
      "Epoch 00667: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2766 - accuracy: 0.8787 - val_loss: 0.4007 - val_accuracy: 0.8132\n",
      "Epoch 668/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8766\n",
      "Epoch 00668: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2785 - accuracy: 0.8769 - val_loss: 0.4081 - val_accuracy: 0.8083\n",
      "Epoch 669/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8756\n",
      "Epoch 00669: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2786 - accuracy: 0.8757 - val_loss: 0.4338 - val_accuracy: 0.7975\n",
      "Epoch 670/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.8789\n",
      "Epoch 00670: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2769 - accuracy: 0.8788 - val_loss: 0.4066 - val_accuracy: 0.8110\n",
      "Epoch 671/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.8795\n",
      "Epoch 00671: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2730 - accuracy: 0.8798 - val_loss: 0.4236 - val_accuracy: 0.8052\n",
      "Epoch 672/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.8787\n",
      "Epoch 00672: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2745 - accuracy: 0.8789 - val_loss: 0.4310 - val_accuracy: 0.8007\n",
      "Epoch 673/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8812\n",
      "Epoch 00673: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2779 - accuracy: 0.8813 - val_loss: 0.3943 - val_accuracy: 0.8190\n",
      "Epoch 674/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.8785\n",
      "Epoch 00674: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2755 - accuracy: 0.8785 - val_loss: 0.4222 - val_accuracy: 0.8035\n",
      "Epoch 675/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.8841\n",
      "Epoch 00675: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2693 - accuracy: 0.8842 - val_loss: 0.4167 - val_accuracy: 0.8065\n",
      "Epoch 676/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8825\n",
      "Epoch 00676: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2720 - accuracy: 0.8825 - val_loss: 0.4128 - val_accuracy: 0.8117\n",
      "Epoch 677/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.8797\n",
      "Epoch 00677: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2735 - accuracy: 0.8799 - val_loss: 0.4132 - val_accuracy: 0.8119\n",
      "Epoch 678/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.8828\n",
      "Epoch 00678: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2709 - accuracy: 0.8829 - val_loss: 0.4067 - val_accuracy: 0.8134\n",
      "Epoch 679/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.8856\n",
      "Epoch 00679: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2670 - accuracy: 0.8857 - val_loss: 0.4000 - val_accuracy: 0.8171\n",
      "Epoch 680/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.8859\n",
      "Epoch 00680: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2628 - accuracy: 0.8864 - val_loss: 0.4015 - val_accuracy: 0.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 681/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8837\n",
      "Epoch 00681: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2690 - accuracy: 0.8839 - val_loss: 0.4114 - val_accuracy: 0.8121\n",
      "Epoch 682/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8836\n",
      "Epoch 00682: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2684 - accuracy: 0.8838 - val_loss: 0.3961 - val_accuracy: 0.8179\n",
      "Epoch 683/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8849\n",
      "Epoch 00683: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2698 - accuracy: 0.8852 - val_loss: 0.4155 - val_accuracy: 0.8091\n",
      "Epoch 684/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.8815\n",
      "Epoch 00684: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2722 - accuracy: 0.8815 - val_loss: 0.3984 - val_accuracy: 0.8166\n",
      "Epoch 685/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.8857\n",
      "Epoch 00685: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2690 - accuracy: 0.8859 - val_loss: 0.4166 - val_accuracy: 0.8055\n",
      "Epoch 686/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8802\n",
      "Epoch 00686: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2686 - accuracy: 0.8802 - val_loss: 0.4334 - val_accuracy: 0.8007\n",
      "Epoch 687/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8814\n",
      "Epoch 00687: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2706 - accuracy: 0.8812 - val_loss: 0.4263 - val_accuracy: 0.8085\n",
      "Epoch 688/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8824\n",
      "Epoch 00688: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2662 - accuracy: 0.8825 - val_loss: 0.4414 - val_accuracy: 0.7999\n",
      "Epoch 689/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.8832\n",
      "Epoch 00689: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2678 - accuracy: 0.8833 - val_loss: 0.4097 - val_accuracy: 0.8089\n",
      "Epoch 690/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.8824\n",
      "Epoch 00690: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2727 - accuracy: 0.8823 - val_loss: 0.4323 - val_accuracy: 0.7994\n",
      "Epoch 691/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.8808\n",
      "Epoch 00691: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2711 - accuracy: 0.8809 - val_loss: 0.4234 - val_accuracy: 0.8029\n",
      "Epoch 692/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8837\n",
      "Epoch 00692: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2649 - accuracy: 0.8838 - val_loss: 0.4116 - val_accuracy: 0.8074\n",
      "Epoch 693/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.8824\n",
      "Epoch 00693: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2686 - accuracy: 0.8823 - val_loss: 0.4188 - val_accuracy: 0.8065\n",
      "Epoch 694/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.8850\n",
      "Epoch 00694: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2648 - accuracy: 0.8852 - val_loss: 0.4394 - val_accuracy: 0.7960\n",
      "Epoch 695/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8826\n",
      "Epoch 00695: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2673 - accuracy: 0.8826 - val_loss: 0.4468 - val_accuracy: 0.7928\n",
      "Epoch 696/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.8870\n",
      "Epoch 00696: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2646 - accuracy: 0.8873 - val_loss: 0.4354 - val_accuracy: 0.8031\n",
      "Epoch 697/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.8838\n",
      "Epoch 00697: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2669 - accuracy: 0.8838 - val_loss: 0.4011 - val_accuracy: 0.8130\n",
      "Epoch 698/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.8850\n",
      "Epoch 00698: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2659 - accuracy: 0.8851 - val_loss: 0.4042 - val_accuracy: 0.8175\n",
      "Epoch 699/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8830\n",
      "Epoch 00699: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2681 - accuracy: 0.8832 - val_loss: 0.4092 - val_accuracy: 0.8113\n",
      "Epoch 700/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.8845\n",
      "Epoch 00700: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2645 - accuracy: 0.8847 - val_loss: 0.4179 - val_accuracy: 0.8087\n",
      "Epoch 701/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.8847\n",
      "Epoch 00701: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2616 - accuracy: 0.8850 - val_loss: 0.4279 - val_accuracy: 0.8033\n",
      "Epoch 702/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2667 - accuracy: 0.8834\n",
      "Epoch 00702: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2663 - accuracy: 0.8834 - val_loss: 0.4302 - val_accuracy: 0.8027\n",
      "Epoch 703/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.8855\n",
      "Epoch 00703: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2609 - accuracy: 0.8855 - val_loss: 0.4207 - val_accuracy: 0.8067\n",
      "Epoch 704/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8841\n",
      "Epoch 00704: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2673 - accuracy: 0.8839 - val_loss: 0.4096 - val_accuracy: 0.8128\n",
      "Epoch 705/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2660 - accuracy: 0.8813\n",
      "Epoch 00705: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2660 - accuracy: 0.8812 - val_loss: 0.3949 - val_accuracy: 0.8186\n",
      "Epoch 706/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.8851\n",
      "Epoch 00706: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2650 - accuracy: 0.8854 - val_loss: 0.4213 - val_accuracy: 0.8076\n",
      "Epoch 707/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8839\n",
      "Epoch 00707: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2657 - accuracy: 0.8836 - val_loss: 0.4072 - val_accuracy: 0.8162\n",
      "Epoch 708/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.8891\n",
      "Epoch 00708: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2601 - accuracy: 0.8892 - val_loss: 0.4128 - val_accuracy: 0.8102\n",
      "Epoch 709/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8843\n",
      "Epoch 00709: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2662 - accuracy: 0.8845 - val_loss: 0.4408 - val_accuracy: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.8857\n",
      "Epoch 00710: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2635 - accuracy: 0.8860 - val_loss: 0.4781 - val_accuracy: 0.7829\n",
      "Epoch 711/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8886\n",
      "Epoch 00711: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2595 - accuracy: 0.8886 - val_loss: 0.4198 - val_accuracy: 0.8061\n",
      "Epoch 712/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.8874\n",
      "Epoch 00712: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2636 - accuracy: 0.8875 - val_loss: 0.4249 - val_accuracy: 0.8052\n",
      "Epoch 713/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.8890\n",
      "Epoch 00713: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2580 - accuracy: 0.8891 - val_loss: 0.4196 - val_accuracy: 0.8070\n",
      "Epoch 714/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8857\n",
      "Epoch 00714: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2603 - accuracy: 0.8857 - val_loss: 0.4340 - val_accuracy: 0.8003\n",
      "Epoch 715/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.8880\n",
      "Epoch 00715: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2603 - accuracy: 0.8881 - val_loss: 0.4008 - val_accuracy: 0.8169\n",
      "Epoch 716/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2631 - accuracy: 0.8867\n",
      "Epoch 00716: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2625 - accuracy: 0.8868 - val_loss: 0.4241 - val_accuracy: 0.8059\n",
      "Epoch 717/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.8859\n",
      "Epoch 00717: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2614 - accuracy: 0.8861 - val_loss: 0.4373 - val_accuracy: 0.7994\n",
      "Epoch 718/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2572 - accuracy: 0.8907\n",
      "Epoch 00718: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2567 - accuracy: 0.8908 - val_loss: 0.4350 - val_accuracy: 0.8025\n",
      "Epoch 719/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.8937\n",
      "Epoch 00719: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2497 - accuracy: 0.8939 - val_loss: 0.4288 - val_accuracy: 0.8052\n",
      "Epoch 720/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.8886\n",
      "Epoch 00720: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2607 - accuracy: 0.8885 - val_loss: 0.4056 - val_accuracy: 0.8134\n",
      "Epoch 721/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8905\n",
      "Epoch 00721: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2608 - accuracy: 0.8907 - val_loss: 0.4315 - val_accuracy: 0.8065\n",
      "Epoch 722/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.8900\n",
      "Epoch 00722: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2535 - accuracy: 0.8901 - val_loss: 0.4221 - val_accuracy: 0.8065\n",
      "Epoch 723/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.8907\n",
      "Epoch 00723: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2547 - accuracy: 0.8908 - val_loss: 0.4204 - val_accuracy: 0.8080\n",
      "Epoch 724/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.8896\n",
      "Epoch 00724: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2581 - accuracy: 0.8898 - val_loss: 0.3977 - val_accuracy: 0.8203\n",
      "Epoch 725/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.8901\n",
      "Epoch 00725: val_loss did not improve from 0.38265\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2543 - accuracy: 0.8904 - val_loss: 0.4632 - val_accuracy: 0.7872\n",
      "Epoch 00725: early stopping\n",
      "Epoch 1/10000\n",
      "    146/Unknown - 8s 57ms/step - loss: 0.6931 - accuracy: 0.5082\n",
      "Epoch 00001: val_loss improved from inf to 0.69241, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 63ms/step - loss: 0.6931 - accuracy: 0.5082 - val_loss: 0.6924 - val_accuracy: 0.5129\n",
      "Epoch 2/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5357\n",
      "Epoch 00002: val_loss improved from 0.69241 to 0.69215, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6907 - accuracy: 0.5354 - val_loss: 0.6922 - val_accuracy: 0.4933\n",
      "Epoch 3/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5620\n",
      "Epoch 00003: val_loss improved from 0.69215 to 0.69210, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6877 - accuracy: 0.5616 - val_loss: 0.6921 - val_accuracy: 0.4938\n",
      "Epoch 4/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5721\n",
      "Epoch 00004: val_loss improved from 0.69210 to 0.69179, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6842 - accuracy: 0.5722 - val_loss: 0.6918 - val_accuracy: 0.4966\n",
      "Epoch 5/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5816\n",
      "Epoch 00005: val_loss improved from 0.69179 to 0.68892, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6794 - accuracy: 0.5816 - val_loss: 0.6889 - val_accuracy: 0.5138\n",
      "Epoch 6/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6748 - accuracy: 0.5890\n",
      "Epoch 00006: val_loss did not improve from 0.68892\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6746 - accuracy: 0.5892 - val_loss: 0.6905 - val_accuracy: 0.5144\n",
      "Epoch 7/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.5935\n",
      "Epoch 00007: val_loss improved from 0.68892 to 0.68570, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6702 - accuracy: 0.5934 - val_loss: 0.6857 - val_accuracy: 0.5350\n",
      "Epoch 8/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6673 - accuracy: 0.5947\n",
      "Epoch 00008: val_loss improved from 0.68570 to 0.68381, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6670 - accuracy: 0.5953 - val_loss: 0.6838 - val_accuracy: 0.5505\n",
      "Epoch 9/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6642 - accuracy: 0.5980\n",
      "Epoch 00009: val_loss improved from 0.68381 to 0.67930, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6641 - accuracy: 0.5985 - val_loss: 0.6793 - val_accuracy: 0.5632\n",
      "Epoch 10/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6615 - accuracy: 0.6031\n",
      "Epoch 00010: val_loss improved from 0.67930 to 0.66669, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6612 - accuracy: 0.6031 - val_loss: 0.6667 - val_accuracy: 0.5907\n",
      "Epoch 11/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.6037\n",
      "Epoch 00011: val_loss did not improve from 0.66669\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6596 - accuracy: 0.6037 - val_loss: 0.6684 - val_accuracy: 0.5856\n",
      "Epoch 12/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.6053\n",
      "Epoch 00012: val_loss improved from 0.66669 to 0.66180, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6561 - accuracy: 0.6059 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 13/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6527 - accuracy: 0.6124\n",
      "Epoch 00013: val_loss did not improve from 0.66180\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6527 - accuracy: 0.6124 - val_loss: 0.6667 - val_accuracy: 0.5838\n",
      "Epoch 14/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6505 - accuracy: 0.6134\n",
      "Epoch 00014: val_loss improved from 0.66180 to 0.65237, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6505 - accuracy: 0.6133 - val_loss: 0.6524 - val_accuracy: 0.6101\n",
      "Epoch 15/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6496 - accuracy: 0.6145\n",
      "Epoch 00015: val_loss improved from 0.65237 to 0.64977, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6493 - accuracy: 0.6150 - val_loss: 0.6498 - val_accuracy: 0.6113\n",
      "Epoch 16/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6478 - accuracy: 0.6187\n",
      "Epoch 00016: val_loss improved from 0.64977 to 0.64694, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6474 - accuracy: 0.6189 - val_loss: 0.6469 - val_accuracy: 0.6154\n",
      "Epoch 17/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.6192\n",
      "Epoch 00017: val_loss improved from 0.64694 to 0.64596, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6443 - accuracy: 0.6198 - val_loss: 0.6460 - val_accuracy: 0.6156\n",
      "Epoch 18/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6428 - accuracy: 0.6226\n",
      "Epoch 00018: val_loss improved from 0.64596 to 0.64130, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6428 - accuracy: 0.6225 - val_loss: 0.6413 - val_accuracy: 0.6273\n",
      "Epoch 19/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6392 - accuracy: 0.6269\n",
      "Epoch 00019: val_loss improved from 0.64130 to 0.63338, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6392 - accuracy: 0.6268 - val_loss: 0.6334 - val_accuracy: 0.6402\n",
      "Epoch 20/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.6273\n",
      "Epoch 00020: val_loss did not improve from 0.63338\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6404 - accuracy: 0.6271 - val_loss: 0.6367 - val_accuracy: 0.6301\n",
      "Epoch 21/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6365 - accuracy: 0.6279\n",
      "Epoch 00021: val_loss improved from 0.63338 to 0.63028, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6361 - accuracy: 0.6285 - val_loss: 0.6303 - val_accuracy: 0.6397\n",
      "Epoch 22/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6351 - accuracy: 0.6308\n",
      "Epoch 00022: val_loss did not improve from 0.63028\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6351 - accuracy: 0.6306 - val_loss: 0.6315 - val_accuracy: 0.6365\n",
      "Epoch 23/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.6300\n",
      "Epoch 00023: val_loss improved from 0.63028 to 0.63027, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6341 - accuracy: 0.6297 - val_loss: 0.6303 - val_accuracy: 0.6356\n",
      "Epoch 24/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.6333\n",
      "Epoch 00024: val_loss improved from 0.63027 to 0.62084, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6318 - accuracy: 0.6332 - val_loss: 0.6208 - val_accuracy: 0.6481\n",
      "Epoch 25/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6311 - accuracy: 0.6357\n",
      "Epoch 00025: val_loss improved from 0.62084 to 0.61903, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6309 - accuracy: 0.6360 - val_loss: 0.6190 - val_accuracy: 0.6496\n",
      "Epoch 26/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6279 - accuracy: 0.6408\n",
      "Epoch 00026: val_loss improved from 0.61903 to 0.61901, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6280 - accuracy: 0.6406 - val_loss: 0.6190 - val_accuracy: 0.6488\n",
      "Epoch 27/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.6439\n",
      "Epoch 00027: val_loss improved from 0.61901 to 0.61460, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6229 - accuracy: 0.6438 - val_loss: 0.6146 - val_accuracy: 0.6507\n",
      "Epoch 28/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6201 - accuracy: 0.6474\n",
      "Epoch 00028: val_loss improved from 0.61460 to 0.61004, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6201 - accuracy: 0.6477 - val_loss: 0.6100 - val_accuracy: 0.6591\n",
      "Epoch 29/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.6545\n",
      "Epoch 00029: val_loss improved from 0.61004 to 0.60536, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6159 - accuracy: 0.6542 - val_loss: 0.6054 - val_accuracy: 0.6617\n",
      "Epoch 30/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6111 - accuracy: 0.6586\n",
      "Epoch 00030: val_loss did not improve from 0.60536\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6114 - accuracy: 0.6582 - val_loss: 0.6064 - val_accuracy: 0.6589\n",
      "Epoch 31/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6046 - accuracy: 0.6689\n",
      "Epoch 00031: val_loss improved from 0.60536 to 0.60055, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6047 - accuracy: 0.6689 - val_loss: 0.6006 - val_accuracy: 0.6707\n",
      "Epoch 32/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5985 - accuracy: 0.6727\n",
      "Epoch 00032: val_loss improved from 0.60055 to 0.59830, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5989 - accuracy: 0.6725 - val_loss: 0.5983 - val_accuracy: 0.6687\n",
      "Epoch 33/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5965 - accuracy: 0.6730\n",
      "Epoch 00033: val_loss did not improve from 0.59830\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5972 - accuracy: 0.6726 - val_loss: 0.6042 - val_accuracy: 0.6584\n",
      "Epoch 34/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.6777\n",
      "Epoch 00034: val_loss improved from 0.59830 to 0.59332, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5914 - accuracy: 0.6774 - val_loss: 0.5933 - val_accuracy: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5856 - accuracy: 0.6874 ETA: 0s - los\n",
      "Epoch 00035: val_loss did not improve from 0.59332\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5865 - accuracy: 0.6869 - val_loss: 0.5940 - val_accuracy: 0.6668\n",
      "Epoch 36/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.6879\n",
      "Epoch 00036: val_loss improved from 0.59332 to 0.57283, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5814 - accuracy: 0.6875 - val_loss: 0.5728 - val_accuracy: 0.6984\n",
      "Epoch 37/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5725 - accuracy: 0.6971\n",
      "Epoch 00037: val_loss improved from 0.57283 to 0.56390, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5731 - accuracy: 0.6968 - val_loss: 0.5639 - val_accuracy: 0.7031\n",
      "Epoch 38/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5686 - accuracy: 0.6992\n",
      "Epoch 00038: val_loss improved from 0.56390 to 0.55776, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5690 - accuracy: 0.6988 - val_loss: 0.5578 - val_accuracy: 0.7085\n",
      "Epoch 39/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7037\n",
      "Epoch 00039: val_loss improved from 0.55776 to 0.54101, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5655 - accuracy: 0.7034 - val_loss: 0.5410 - val_accuracy: 0.7322\n",
      "Epoch 40/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7037\n",
      "Epoch 00040: val_loss did not improve from 0.54101\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5619 - accuracy: 0.7033 - val_loss: 0.5619 - val_accuracy: 0.7287\n",
      "Epoch 41/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5581 - accuracy: 0.7110\n",
      "Epoch 00041: val_loss improved from 0.54101 to 0.53994, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5593 - accuracy: 0.7103 - val_loss: 0.5399 - val_accuracy: 0.7236\n",
      "Epoch 42/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5528 - accuracy: 0.7126\n",
      "Epoch 00042: val_loss improved from 0.53994 to 0.52771, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5539 - accuracy: 0.7121 - val_loss: 0.5277 - val_accuracy: 0.7433\n",
      "Epoch 43/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7153\n",
      "Epoch 00043: val_loss did not improve from 0.52771\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5514 - accuracy: 0.7148 - val_loss: 0.5589 - val_accuracy: 0.6980\n",
      "Epoch 44/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7159\n",
      "Epoch 00044: val_loss did not improve from 0.52771\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5497 - accuracy: 0.7157 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 45/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7214\n",
      "Epoch 00045: val_loss did not improve from 0.52771\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5438 - accuracy: 0.7208 - val_loss: 0.5409 - val_accuracy: 0.7334\n",
      "Epoch 46/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5419 - accuracy: 0.7248\n",
      "Epoch 00046: val_loss improved from 0.52771 to 0.52742, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5428 - accuracy: 0.7245 - val_loss: 0.5274 - val_accuracy: 0.7560\n",
      "Epoch 47/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7218\n",
      "Epoch 00047: val_loss improved from 0.52742 to 0.52673, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5422 - accuracy: 0.7214 - val_loss: 0.5267 - val_accuracy: 0.7388\n",
      "Epoch 48/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7286\n",
      "Epoch 00048: val_loss did not improve from 0.52673\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5370 - accuracy: 0.7282 - val_loss: 0.5376 - val_accuracy: 0.7175\n",
      "Epoch 49/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7302\n",
      "Epoch 00049: val_loss did not improve from 0.52673\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5334 - accuracy: 0.7300 - val_loss: 0.5379 - val_accuracy: 0.7193\n",
      "Epoch 50/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5288 - accuracy: 0.7324\n",
      "Epoch 00050: val_loss improved from 0.52673 to 0.52317, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5301 - accuracy: 0.7318 - val_loss: 0.5232 - val_accuracy: 0.7519\n",
      "Epoch 51/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5294 - accuracy: 0.7331\n",
      "Epoch 00051: val_loss improved from 0.52317 to 0.51814, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5303 - accuracy: 0.7323 - val_loss: 0.5181 - val_accuracy: 0.7489\n",
      "Epoch 52/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.7341\n",
      "Epoch 00052: val_loss improved from 0.51814 to 0.51526, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5280 - accuracy: 0.7336 - val_loss: 0.5153 - val_accuracy: 0.7515\n",
      "Epoch 53/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5265 - accuracy: 0.7341 ETA: 0s - loss: 0.5248 - accu\n",
      "Epoch 00053: val_loss did not improve from 0.51526\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5270 - accuracy: 0.7339 - val_loss: 0.5235 - val_accuracy: 0.7517\n",
      "Epoch 54/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7358\n",
      "Epoch 00054: val_loss improved from 0.51526 to 0.50533, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5227 - accuracy: 0.7355 - val_loss: 0.5053 - val_accuracy: 0.7623\n",
      "Epoch 55/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5209 - accuracy: 0.7380\n",
      "Epoch 00055: val_loss improved from 0.50533 to 0.49509, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5215 - accuracy: 0.7378 - val_loss: 0.4951 - val_accuracy: 0.7629\n",
      "Epoch 56/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7426\n",
      "Epoch 00056: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5210 - accuracy: 0.7421 - val_loss: 0.4971 - val_accuracy: 0.7653\n",
      "Epoch 57/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5188 - accuracy: 0.7426\n",
      "Epoch 00057: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5196 - accuracy: 0.7422 - val_loss: 0.5055 - val_accuracy: 0.7655\n",
      "Epoch 58/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5147 - accuracy: 0.7443\n",
      "Epoch 00058: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5154 - accuracy: 0.7440 - val_loss: 0.5016 - val_accuracy: 0.7582\n",
      "Epoch 59/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5158 - accuracy: 0.7443\n",
      "Epoch 00059: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5166 - accuracy: 0.7440 - val_loss: 0.5070 - val_accuracy: 0.7696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5108 - accuracy: 0.7436\n",
      "Epoch 00060: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5115 - accuracy: 0.7433 - val_loss: 0.5271 - val_accuracy: 0.7246\n",
      "Epoch 61/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5089 - accuracy: 0.7466\n",
      "Epoch 00061: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5099 - accuracy: 0.7463 - val_loss: 0.5066 - val_accuracy: 0.7455\n",
      "Epoch 62/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5099 - accuracy: 0.7475\n",
      "Epoch 00062: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5110 - accuracy: 0.7471 - val_loss: 0.5055 - val_accuracy: 0.7558\n",
      "Epoch 63/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.7517\n",
      "Epoch 00063: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5066 - accuracy: 0.7512 - val_loss: 0.5086 - val_accuracy: 0.7487\n",
      "Epoch 64/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5022 - accuracy: 0.7545\n",
      "Epoch 00064: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5026 - accuracy: 0.7543 - val_loss: 0.5223 - val_accuracy: 0.7339\n",
      "Epoch 65/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7455\n",
      "Epoch 00065: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5049 - accuracy: 0.7451 - val_loss: 0.5249 - val_accuracy: 0.7294\n",
      "Epoch 66/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5041 - accuracy: 0.7500\n",
      "Epoch 00066: val_loss did not improve from 0.49509\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5048 - accuracy: 0.7497 - val_loss: 0.5016 - val_accuracy: 0.7670\n",
      "Epoch 67/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5036 - accuracy: 0.7536\n",
      "Epoch 00067: val_loss improved from 0.49509 to 0.49454, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5044 - accuracy: 0.7537 - val_loss: 0.4945 - val_accuracy: 0.7599\n",
      "Epoch 68/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.7538\n",
      "Epoch 00068: val_loss did not improve from 0.49454\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4992 - accuracy: 0.7537 - val_loss: 0.5082 - val_accuracy: 0.7393\n",
      "Epoch 69/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5016 - accuracy: 0.7544\n",
      "Epoch 00069: val_loss improved from 0.49454 to 0.48864, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5025 - accuracy: 0.7542 - val_loss: 0.4886 - val_accuracy: 0.7620\n",
      "Epoch 70/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4966 - accuracy: 0.7569\n",
      "Epoch 00070: val_loss did not improve from 0.48864\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4975 - accuracy: 0.7566 - val_loss: 0.4980 - val_accuracy: 0.7543\n",
      "Epoch 71/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4940 - accuracy: 0.7614\n",
      "Epoch 00071: val_loss did not improve from 0.48864\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4948 - accuracy: 0.7615 - val_loss: 0.5221 - val_accuracy: 0.7311\n",
      "Epoch 72/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.7585\n",
      "Epoch 00072: val_loss did not improve from 0.48864\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4949 - accuracy: 0.7582 - val_loss: 0.4897 - val_accuracy: 0.7586\n",
      "Epoch 73/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.7546\n",
      "Epoch 00073: val_loss improved from 0.48864 to 0.48082, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4977 - accuracy: 0.7542 - val_loss: 0.4808 - val_accuracy: 0.7700\n",
      "Epoch 74/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4931 - accuracy: 0.7578\n",
      "Epoch 00074: val_loss did not improve from 0.48082\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4943 - accuracy: 0.7573 - val_loss: 0.5728 - val_accuracy: 0.7021\n",
      "Epoch 75/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4910 - accuracy: 0.7617\n",
      "Epoch 00075: val_loss did not improve from 0.48082\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4917 - accuracy: 0.7613 - val_loss: 0.4885 - val_accuracy: 0.7674\n",
      "Epoch 76/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4882 - accuracy: 0.7616\n",
      "Epoch 00076: val_loss improved from 0.48082 to 0.46310, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4898 - accuracy: 0.7611 - val_loss: 0.4631 - val_accuracy: 0.7812\n",
      "Epoch 77/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7610\n",
      "Epoch 00077: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4905 - accuracy: 0.7607 - val_loss: 0.4778 - val_accuracy: 0.7691\n",
      "Epoch 78/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.7624\n",
      "Epoch 00078: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4870 - accuracy: 0.7621 - val_loss: 0.5000 - val_accuracy: 0.7481\n",
      "Epoch 79/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4837 - accuracy: 0.7663\n",
      "Epoch 00079: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4844 - accuracy: 0.7661 - val_loss: 0.4872 - val_accuracy: 0.7575\n",
      "Epoch 80/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4828 - accuracy: 0.7662\n",
      "Epoch 00080: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4836 - accuracy: 0.7660 - val_loss: 0.4776 - val_accuracy: 0.7663\n",
      "Epoch 81/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.7631\n",
      "Epoch 00081: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4863 - accuracy: 0.7629 - val_loss: 0.4782 - val_accuracy: 0.7670\n",
      "Epoch 82/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7647\n",
      "Epoch 00082: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4850 - accuracy: 0.7644 - val_loss: 0.4714 - val_accuracy: 0.7732\n",
      "Epoch 83/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4851 - accuracy: 0.7662\n",
      "Epoch 00083: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4860 - accuracy: 0.7658 - val_loss: 0.4950 - val_accuracy: 0.7487\n",
      "Epoch 84/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4808 - accuracy: 0.7651\n",
      "Epoch 00084: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4817 - accuracy: 0.7649 - val_loss: 0.4718 - val_accuracy: 0.7689\n",
      "Epoch 85/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.7665\n",
      "Epoch 00085: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4807 - accuracy: 0.7663 - val_loss: 0.4846 - val_accuracy: 0.7584\n",
      "Epoch 86/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7673\n",
      "Epoch 00086: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4780 - accuracy: 0.7672 - val_loss: 0.4806 - val_accuracy: 0.7623\n",
      "Epoch 87/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.7699\n",
      "Epoch 00087: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4780 - accuracy: 0.7697 - val_loss: 0.5308 - val_accuracy: 0.7231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4812 - accuracy: 0.7630\n",
      "Epoch 00088: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4816 - accuracy: 0.7631 - val_loss: 0.4695 - val_accuracy: 0.7721\n",
      "Epoch 89/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4769 - accuracy: 0.7680\n",
      "Epoch 00089: val_loss did not improve from 0.46310\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4777 - accuracy: 0.7677 - val_loss: 0.4968 - val_accuracy: 0.7485\n",
      "Epoch 90/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4740 - accuracy: 0.7711\n",
      "Epoch 00090: val_loss improved from 0.46310 to 0.46080, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4749 - accuracy: 0.7709 - val_loss: 0.4608 - val_accuracy: 0.7792\n",
      "Epoch 91/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7726\n",
      "Epoch 00091: val_loss did not improve from 0.46080\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4761 - accuracy: 0.7724 - val_loss: 0.4633 - val_accuracy: 0.7775\n",
      "Epoch 92/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7677\n",
      "Epoch 00092: val_loss improved from 0.46080 to 0.45276, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4764 - accuracy: 0.7674 - val_loss: 0.4528 - val_accuracy: 0.7850\n",
      "Epoch 93/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.7724\n",
      "Epoch 00093: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4726 - accuracy: 0.7722 - val_loss: 0.4740 - val_accuracy: 0.7678\n",
      "Epoch 94/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.7737\n",
      "Epoch 00094: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4732 - accuracy: 0.7735 - val_loss: 0.4533 - val_accuracy: 0.7857\n",
      "Epoch 95/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4706 - accuracy: 0.7743\n",
      "Epoch 00095: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4710 - accuracy: 0.7741 - val_loss: 0.4781 - val_accuracy: 0.7640\n",
      "Epoch 96/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.7692\n",
      "Epoch 00096: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4731 - accuracy: 0.7693 - val_loss: 0.5093 - val_accuracy: 0.7362\n",
      "Epoch 97/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4685 - accuracy: 0.7758\n",
      "Epoch 00097: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4686 - accuracy: 0.7760 - val_loss: 0.4844 - val_accuracy: 0.7614\n",
      "Epoch 98/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4671 - accuracy: 0.7788 ETA: 0s - loss: 0.4655 - accu\n",
      "Epoch 00098: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4675 - accuracy: 0.7790 - val_loss: 0.4969 - val_accuracy: 0.7448\n",
      "Epoch 99/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7756\n",
      "Epoch 00099: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4666 - accuracy: 0.7756 - val_loss: 0.4659 - val_accuracy: 0.7713\n",
      "Epoch 100/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4657 - accuracy: 0.7740\n",
      "Epoch 00100: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4663 - accuracy: 0.7739 - val_loss: 0.4834 - val_accuracy: 0.7590\n",
      "Epoch 101/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4637 - accuracy: 0.7793\n",
      "Epoch 00101: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4647 - accuracy: 0.7790 - val_loss: 0.4538 - val_accuracy: 0.7855\n",
      "Epoch 102/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.7763\n",
      "Epoch 00102: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4660 - accuracy: 0.7760 - val_loss: 0.4541 - val_accuracy: 0.7842\n",
      "Epoch 103/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4622 - accuracy: 0.7759\n",
      "Epoch 00103: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4630 - accuracy: 0.7758 - val_loss: 0.4721 - val_accuracy: 0.7642\n",
      "Epoch 104/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.7774\n",
      "Epoch 00104: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4616 - accuracy: 0.7771 - val_loss: 0.5022 - val_accuracy: 0.7446\n",
      "Epoch 105/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4620 - accuracy: 0.7790\n",
      "Epoch 00105: val_loss did not improve from 0.45276\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4626 - accuracy: 0.7791 - val_loss: 0.4547 - val_accuracy: 0.7810\n",
      "Epoch 106/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.7776\n",
      "Epoch 00106: val_loss improved from 0.45276 to 0.44730, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4632 - accuracy: 0.7775 - val_loss: 0.4473 - val_accuracy: 0.7883\n",
      "Epoch 107/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.7815\n",
      "Epoch 00107: val_loss did not improve from 0.44730\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4576 - accuracy: 0.7812 - val_loss: 0.4709 - val_accuracy: 0.7670\n",
      "Epoch 108/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.7791\n",
      "Epoch 00108: val_loss did not improve from 0.44730\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4579 - accuracy: 0.7792 - val_loss: 0.4855 - val_accuracy: 0.7539\n",
      "Epoch 109/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.7828\n",
      "Epoch 00109: val_loss did not improve from 0.44730\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4565 - accuracy: 0.7825 - val_loss: 0.4761 - val_accuracy: 0.7620\n",
      "Epoch 110/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4554 - accuracy: 0.7848\n",
      "Epoch 00110: val_loss did not improve from 0.44730\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4557 - accuracy: 0.7848 - val_loss: 0.5073 - val_accuracy: 0.7390\n",
      "Epoch 111/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7832\n",
      "Epoch 00111: val_loss did not improve from 0.44730\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4530 - accuracy: 0.7831 - val_loss: 0.4981 - val_accuracy: 0.7444\n",
      "Epoch 112/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7822\n",
      "Epoch 00112: val_loss improved from 0.44730 to 0.43798, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4566 - accuracy: 0.7821 - val_loss: 0.4380 - val_accuracy: 0.7988\n",
      "Epoch 113/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7866\n",
      "Epoch 00113: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4509 - accuracy: 0.7862 - val_loss: 0.4675 - val_accuracy: 0.7668\n",
      "Epoch 114/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.7858\n",
      "Epoch 00114: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4511 - accuracy: 0.7858 - val_loss: 0.4606 - val_accuracy: 0.7743\n",
      "Epoch 115/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.7860\n",
      "Epoch 00115: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4470 - accuracy: 0.7860 - val_loss: 0.4711 - val_accuracy: 0.7666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.7823\n",
      "Epoch 00116: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4540 - accuracy: 0.7820 - val_loss: 0.4714 - val_accuracy: 0.7661\n",
      "Epoch 117/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7857\n",
      "Epoch 00117: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4510 - accuracy: 0.7854 - val_loss: 0.4549 - val_accuracy: 0.7812\n",
      "Epoch 118/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.7883\n",
      "Epoch 00118: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4497 - accuracy: 0.7882 - val_loss: 0.4983 - val_accuracy: 0.7455\n",
      "Epoch 119/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4453 - accuracy: 0.7872\n",
      "Epoch 00119: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4460 - accuracy: 0.7870 - val_loss: 0.4721 - val_accuracy: 0.7668\n",
      "Epoch 120/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4424 - accuracy: 0.7886\n",
      "Epoch 00120: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4428 - accuracy: 0.7886 - val_loss: 0.4458 - val_accuracy: 0.7872\n",
      "Epoch 121/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4459 - accuracy: 0.7888\n",
      "Epoch 00121: val_loss did not improve from 0.43798\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4465 - accuracy: 0.7887 - val_loss: 0.4701 - val_accuracy: 0.7653\n",
      "Epoch 122/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4375 - accuracy: 0.7921\n",
      "Epoch 00122: val_loss improved from 0.43798 to 0.43618, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4384 - accuracy: 0.7918 - val_loss: 0.4362 - val_accuracy: 0.7919\n",
      "Epoch 123/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4502 - accuracy: 0.7844\n",
      "Epoch 00123: val_loss did not improve from 0.43618\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4502 - accuracy: 0.7844 - val_loss: 0.4768 - val_accuracy: 0.7590\n",
      "Epoch 124/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.7925\n",
      "Epoch 00124: val_loss did not improve from 0.43618\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4402 - accuracy: 0.7925 - val_loss: 0.4575 - val_accuracy: 0.7758\n",
      "Epoch 125/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.7907\n",
      "Epoch 00125: val_loss did not improve from 0.43618\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4402 - accuracy: 0.7905 - val_loss: 0.4600 - val_accuracy: 0.7700\n",
      "Epoch 126/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.7941\n",
      "Epoch 00126: val_loss did not improve from 0.43618\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4403 - accuracy: 0.7938 - val_loss: 0.4446 - val_accuracy: 0.7850\n",
      "Epoch 127/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4378 - accuracy: 0.7931\n",
      "Epoch 00127: val_loss improved from 0.43618 to 0.42567, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4384 - accuracy: 0.7929 - val_loss: 0.4257 - val_accuracy: 0.7986\n",
      "Epoch 128/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4383 - accuracy: 0.7915\n",
      "Epoch 00128: val_loss did not improve from 0.42567\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4389 - accuracy: 0.7913 - val_loss: 0.4516 - val_accuracy: 0.7756\n",
      "Epoch 129/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.7922\n",
      "Epoch 00129: val_loss improved from 0.42567 to 0.42545, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 0.4394 - accuracy: 0.7922 - val_loss: 0.4255 - val_accuracy: 0.8005\n",
      "Epoch 130/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4383 - accuracy: 0.7906\n",
      "Epoch 00130: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4388 - accuracy: 0.7905 - val_loss: 0.4435 - val_accuracy: 0.7872\n",
      "Epoch 131/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.7958\n",
      "Epoch 00131: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4366 - accuracy: 0.7956 - val_loss: 0.4469 - val_accuracy: 0.7814\n",
      "Epoch 132/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.7898\n",
      "Epoch 00132: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4362 - accuracy: 0.7897 - val_loss: 0.4642 - val_accuracy: 0.7734\n",
      "Epoch 133/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4388 - accuracy: 0.7945\n",
      "Epoch 00133: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4397 - accuracy: 0.7940 - val_loss: 0.4480 - val_accuracy: 0.7827\n",
      "Epoch 134/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4394 - accuracy: 0.7916\n",
      "Epoch 00134: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4397 - accuracy: 0.7916 - val_loss: 0.4325 - val_accuracy: 0.7928\n",
      "Epoch 135/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4320 - accuracy: 0.7952\n",
      "Epoch 00135: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4327 - accuracy: 0.7951 - val_loss: 0.4755 - val_accuracy: 0.7582\n",
      "Epoch 136/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.7954\n",
      "Epoch 00136: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4307 - accuracy: 0.7953 - val_loss: 0.4468 - val_accuracy: 0.7797\n",
      "Epoch 137/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.7949\n",
      "Epoch 00137: val_loss did not improve from 0.42545\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4340 - accuracy: 0.7947 - val_loss: 0.4679 - val_accuracy: 0.7644\n",
      "Epoch 138/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.7941\n",
      "Epoch 00138: val_loss improved from 0.42545 to 0.42252, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4330 - accuracy: 0.7939 - val_loss: 0.4225 - val_accuracy: 0.8027\n",
      "Epoch 139/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4285 - accuracy: 0.7990\n",
      "Epoch 00139: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4290 - accuracy: 0.7988 - val_loss: 0.4245 - val_accuracy: 0.7966\n",
      "Epoch 140/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4292 - accuracy: 0.7978\n",
      "Epoch 00140: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4296 - accuracy: 0.7978 - val_loss: 0.4501 - val_accuracy: 0.7752\n",
      "Epoch 141/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.7964\n",
      "Epoch 00141: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4264 - accuracy: 0.7965 - val_loss: 0.4465 - val_accuracy: 0.7825\n",
      "Epoch 142/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4308 - accuracy: 0.7968\n",
      "Epoch 00142: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4310 - accuracy: 0.7968 - val_loss: 0.4381 - val_accuracy: 0.7891\n",
      "Epoch 143/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.7976\n",
      "Epoch 00143: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4295 - accuracy: 0.7973 - val_loss: 0.4278 - val_accuracy: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.7984\n",
      "Epoch 00144: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4277 - accuracy: 0.7983 - val_loss: 0.4832 - val_accuracy: 0.7549\n",
      "Epoch 145/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.7986\n",
      "Epoch 00145: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4270 - accuracy: 0.7982 - val_loss: 0.4900 - val_accuracy: 0.7543\n",
      "Epoch 146/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4254 - accuracy: 0.8011\n",
      "Epoch 00146: val_loss did not improve from 0.42252\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4260 - accuracy: 0.8010 - val_loss: 0.4236 - val_accuracy: 0.8025\n",
      "Epoch 147/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8009\n",
      "Epoch 00147: val_loss improved from 0.42252 to 0.41742, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4218 - accuracy: 0.8008 - val_loss: 0.4174 - val_accuracy: 0.8048\n",
      "Epoch 148/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.7998\n",
      "Epoch 00148: val_loss improved from 0.41742 to 0.41637, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4242 - accuracy: 0.7995 - val_loss: 0.4164 - val_accuracy: 0.8102\n",
      "Epoch 149/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8034\n",
      "Epoch 00149: val_loss did not improve from 0.41637\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4213 - accuracy: 0.8033 - val_loss: 0.4439 - val_accuracy: 0.7835\n",
      "Epoch 150/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8028\n",
      "Epoch 00150: val_loss did not improve from 0.41637\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4239 - accuracy: 0.8027 - val_loss: 0.4836 - val_accuracy: 0.7534\n",
      "Epoch 151/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8045 ETA: 0s - loss: 0.4189 \n",
      "Epoch 00151: val_loss did not improve from 0.41637\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4203 - accuracy: 0.8043 - val_loss: 0.4487 - val_accuracy: 0.7788\n",
      "Epoch 152/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4187 - accuracy: 0.8021\n",
      "Epoch 00152: val_loss did not improve from 0.41637\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4189 - accuracy: 0.8020 - val_loss: 0.4329 - val_accuracy: 0.7943\n",
      "Epoch 153/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8006\n",
      "Epoch 00153: val_loss did not improve from 0.41637\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4207 - accuracy: 0.8006 - val_loss: 0.4389 - val_accuracy: 0.7878\n",
      "Epoch 154/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8066\n",
      "Epoch 00154: val_loss improved from 0.41637 to 0.41343, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4177 - accuracy: 0.8064 - val_loss: 0.4134 - val_accuracy: 0.8102\n",
      "Epoch 155/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8031\n",
      "Epoch 00155: val_loss did not improve from 0.41343\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4207 - accuracy: 0.8028 - val_loss: 0.4550 - val_accuracy: 0.7752\n",
      "Epoch 156/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8041\n",
      "Epoch 00156: val_loss did not improve from 0.41343\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4176 - accuracy: 0.8040 - val_loss: 0.4414 - val_accuracy: 0.7863\n",
      "Epoch 157/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8041\n",
      "Epoch 00157: val_loss did not improve from 0.41343\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4166 - accuracy: 0.8041 - val_loss: 0.4499 - val_accuracy: 0.7773\n",
      "Epoch 158/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8063\n",
      "Epoch 00158: val_loss did not improve from 0.41343\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4114 - accuracy: 0.8062 - val_loss: 0.4265 - val_accuracy: 0.8009\n",
      "Epoch 159/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8043\n",
      "Epoch 00159: val_loss did not improve from 0.41343\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4133 - accuracy: 0.8043 - val_loss: 0.4783 - val_accuracy: 0.7616\n",
      "Epoch 160/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8031\n",
      "Epoch 00160: val_loss improved from 0.41343 to 0.40980, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4179 - accuracy: 0.8030 - val_loss: 0.4098 - val_accuracy: 0.8156\n",
      "Epoch 161/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.8092\n",
      "Epoch 00161: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4099 - accuracy: 0.8096 - val_loss: 0.4186 - val_accuracy: 0.8012\n",
      "Epoch 162/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4130 - accuracy: 0.8044\n",
      "Epoch 00162: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4134 - accuracy: 0.8044 - val_loss: 0.4279 - val_accuracy: 0.7982\n",
      "Epoch 163/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8058\n",
      "Epoch 00163: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4144 - accuracy: 0.8059 - val_loss: 0.4174 - val_accuracy: 0.8074\n",
      "Epoch 164/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8091\n",
      "Epoch 00164: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4097 - accuracy: 0.8089 - val_loss: 0.4710 - val_accuracy: 0.7657\n",
      "Epoch 165/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8098\n",
      "Epoch 00165: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4076 - accuracy: 0.8099 - val_loss: 0.4324 - val_accuracy: 0.7889\n",
      "Epoch 166/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4131 - accuracy: 0.8058\n",
      "Epoch 00166: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4133 - accuracy: 0.8057 - val_loss: 0.4389 - val_accuracy: 0.7893\n",
      "Epoch 167/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4088 - accuracy: 0.8108\n",
      "Epoch 00167: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4090 - accuracy: 0.8107 - val_loss: 0.4252 - val_accuracy: 0.7986\n",
      "Epoch 168/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8117\n",
      "Epoch 00168: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4062 - accuracy: 0.8117 - val_loss: 0.4114 - val_accuracy: 0.8104\n",
      "Epoch 169/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8080\n",
      "Epoch 00169: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4096 - accuracy: 0.8080 - val_loss: 0.4138 - val_accuracy: 0.8095\n",
      "Epoch 170/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8093\n",
      "Epoch 00170: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4077 - accuracy: 0.8092 - val_loss: 0.4288 - val_accuracy: 0.7977\n",
      "Epoch 171/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4094 - accuracy: 0.8103\n",
      "Epoch 00171: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4094 - accuracy: 0.8102 - val_loss: 0.4206 - val_accuracy: 0.8067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8081\n",
      "Epoch 00172: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4090 - accuracy: 0.8081 - val_loss: 0.4212 - val_accuracy: 0.8035\n",
      "Epoch 173/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8109\n",
      "Epoch 00173: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4058 - accuracy: 0.8107 - val_loss: 0.4152 - val_accuracy: 0.8076\n",
      "Epoch 174/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4037 - accuracy: 0.8096\n",
      "Epoch 00174: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4037 - accuracy: 0.8097 - val_loss: 0.4341 - val_accuracy: 0.7923\n",
      "Epoch 175/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3990 - accuracy: 0.8143\n",
      "Epoch 00175: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3994 - accuracy: 0.8141 - val_loss: 0.4307 - val_accuracy: 0.7960\n",
      "Epoch 176/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.8090\n",
      "Epoch 00176: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4016 - accuracy: 0.8088 - val_loss: 0.4423 - val_accuracy: 0.7893\n",
      "Epoch 177/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8152\n",
      "Epoch 00177: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3972 - accuracy: 0.8152 - val_loss: 0.4128 - val_accuracy: 0.8080\n",
      "Epoch 178/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3997 - accuracy: 0.8135\n",
      "Epoch 00178: val_loss did not improve from 0.40980\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4000 - accuracy: 0.8135 - val_loss: 0.4335 - val_accuracy: 0.7932\n",
      "Epoch 179/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8144\n",
      "Epoch 00179: val_loss improved from 0.40980 to 0.40902, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4012 - accuracy: 0.8141 - val_loss: 0.4090 - val_accuracy: 0.8117\n",
      "Epoch 180/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8150\n",
      "Epoch 00180: val_loss did not improve from 0.40902\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3986 - accuracy: 0.8150 - val_loss: 0.4139 - val_accuracy: 0.8091\n",
      "Epoch 181/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8171\n",
      "Epoch 00181: val_loss improved from 0.40902 to 0.40770, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3972 - accuracy: 0.8170 - val_loss: 0.4077 - val_accuracy: 0.8132\n",
      "Epoch 182/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8167\n",
      "Epoch 00182: val_loss did not improve from 0.40770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3975 - accuracy: 0.8168 - val_loss: 0.4150 - val_accuracy: 0.8037\n",
      "Epoch 183/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8140\n",
      "Epoch 00183: val_loss did not improve from 0.40770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3985 - accuracy: 0.8139 - val_loss: 0.4093 - val_accuracy: 0.8126\n",
      "Epoch 184/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8126\n",
      "Epoch 00184: val_loss did not improve from 0.40770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3992 - accuracy: 0.8125 - val_loss: 0.4079 - val_accuracy: 0.8179\n",
      "Epoch 185/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3948 - accuracy: 0.8147\n",
      "Epoch 00185: val_loss improved from 0.40770 to 0.40662, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3953 - accuracy: 0.8146 - val_loss: 0.4066 - val_accuracy: 0.8145\n",
      "Epoch 186/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8162\n",
      "Epoch 00186: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3958 - accuracy: 0.8164 - val_loss: 0.4390 - val_accuracy: 0.7893\n",
      "Epoch 187/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8190\n",
      "Epoch 00187: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3964 - accuracy: 0.8188 - val_loss: 0.4458 - val_accuracy: 0.7816\n",
      "Epoch 188/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8177\n",
      "Epoch 00188: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3911 - accuracy: 0.8177 - val_loss: 0.4144 - val_accuracy: 0.8106\n",
      "Epoch 189/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8206\n",
      "Epoch 00189: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3934 - accuracy: 0.8204 - val_loss: 0.4445 - val_accuracy: 0.7855\n",
      "Epoch 190/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8211\n",
      "Epoch 00190: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3897 - accuracy: 0.8210 - val_loss: 0.4232 - val_accuracy: 0.8027\n",
      "Epoch 191/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8242\n",
      "Epoch 00191: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3818 - accuracy: 0.8240 - val_loss: 0.4196 - val_accuracy: 0.8025\n",
      "Epoch 192/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8205\n",
      "Epoch 00192: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3892 - accuracy: 0.8207 - val_loss: 0.4236 - val_accuracy: 0.8050\n",
      "Epoch 193/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8193\n",
      "Epoch 00193: val_loss did not improve from 0.40662\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3890 - accuracy: 0.8194 - val_loss: 0.4093 - val_accuracy: 0.8123\n",
      "Epoch 194/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.8224\n",
      "Epoch 00194: val_loss improved from 0.40662 to 0.40399, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3852 - accuracy: 0.8225 - val_loss: 0.4040 - val_accuracy: 0.8177\n",
      "Epoch 195/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8198\n",
      "Epoch 00195: val_loss did not improve from 0.40399\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3894 - accuracy: 0.8197 - val_loss: 0.4093 - val_accuracy: 0.8138\n",
      "Epoch 196/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.8255\n",
      "Epoch 00196: val_loss did not improve from 0.40399\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3840 - accuracy: 0.8251 - val_loss: 0.4099 - val_accuracy: 0.8070\n",
      "Epoch 197/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3832 - accuracy: 0.8232\n",
      "Epoch 00197: val_loss did not improve from 0.40399\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3833 - accuracy: 0.8233 - val_loss: 0.4138 - val_accuracy: 0.8115\n",
      "Epoch 198/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.8217\n",
      "Epoch 00198: val_loss did not improve from 0.40399\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3839 - accuracy: 0.8218 - val_loss: 0.4176 - val_accuracy: 0.8095\n",
      "Epoch 199/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8248\n",
      "Epoch 00199: val_loss did not improve from 0.40399\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3809 - accuracy: 0.8248 - val_loss: 0.4198 - val_accuracy: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8270\n",
      "Epoch 00200: val_loss improved from 0.40399 to 0.39685, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3833 - accuracy: 0.8266 - val_loss: 0.3968 - val_accuracy: 0.8220\n",
      "Epoch 201/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8235 ETA: 0s - loss: 0.3877 - accuracy: 0.82\n",
      "Epoch 00201: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3882 - accuracy: 0.8233 - val_loss: 0.4045 - val_accuracy: 0.8132\n",
      "Epoch 202/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8235\n",
      "Epoch 00202: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3793 - accuracy: 0.8236 - val_loss: 0.4332 - val_accuracy: 0.7941\n",
      "Epoch 203/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8239\n",
      "Epoch 00203: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3817 - accuracy: 0.8239 - val_loss: 0.4194 - val_accuracy: 0.8055\n",
      "Epoch 204/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.8237\n",
      "Epoch 00204: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3848 - accuracy: 0.8236 - val_loss: 0.4140 - val_accuracy: 0.8080\n",
      "Epoch 205/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3804 - accuracy: 0.8254\n",
      "Epoch 00205: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3807 - accuracy: 0.8254 - val_loss: 0.4154 - val_accuracy: 0.8100\n",
      "Epoch 206/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3798 - accuracy: 0.8276\n",
      "Epoch 00206: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3803 - accuracy: 0.8275 - val_loss: 0.4094 - val_accuracy: 0.8126\n",
      "Epoch 207/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8259\n",
      "Epoch 00207: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3800 - accuracy: 0.8261 - val_loss: 0.4021 - val_accuracy: 0.8153\n",
      "Epoch 208/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8262\n",
      "Epoch 00208: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3772 - accuracy: 0.8263 - val_loss: 0.4094 - val_accuracy: 0.8076\n",
      "Epoch 209/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8316\n",
      "Epoch 00209: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3725 - accuracy: 0.8314 - val_loss: 0.4010 - val_accuracy: 0.8156\n",
      "Epoch 210/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8287\n",
      "Epoch 00210: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3747 - accuracy: 0.8288 - val_loss: 0.4240 - val_accuracy: 0.7964\n",
      "Epoch 211/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8313\n",
      "Epoch 00211: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3712 - accuracy: 0.8313 - val_loss: 0.4164 - val_accuracy: 0.8046\n",
      "Epoch 212/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8300\n",
      "Epoch 00212: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3715 - accuracy: 0.8300 - val_loss: 0.4014 - val_accuracy: 0.8113\n",
      "Epoch 213/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8286\n",
      "Epoch 00213: val_loss did not improve from 0.39685\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3728 - accuracy: 0.8285 - val_loss: 0.3972 - val_accuracy: 0.8199\n",
      "Epoch 214/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3711 - accuracy: 0.8291\n",
      "Epoch 00214: val_loss improved from 0.39685 to 0.39620, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3712 - accuracy: 0.8291 - val_loss: 0.3962 - val_accuracy: 0.8162\n",
      "Epoch 215/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3707 - accuracy: 0.8322\n",
      "Epoch 00215: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3706 - accuracy: 0.8321 - val_loss: 0.4146 - val_accuracy: 0.8048\n",
      "Epoch 216/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3709 - accuracy: 0.8295\n",
      "Epoch 00216: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3712 - accuracy: 0.8294 - val_loss: 0.4415 - val_accuracy: 0.7859\n",
      "Epoch 217/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8301\n",
      "Epoch 00217: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3693 - accuracy: 0.8300 - val_loss: 0.4145 - val_accuracy: 0.8040\n",
      "Epoch 218/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8297\n",
      "Epoch 00218: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3690 - accuracy: 0.8296 - val_loss: 0.3963 - val_accuracy: 0.8175\n",
      "Epoch 219/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8334\n",
      "Epoch 00219: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3670 - accuracy: 0.8333 - val_loss: 0.3980 - val_accuracy: 0.8151\n",
      "Epoch 220/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8333\n",
      "Epoch 00220: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3629 - accuracy: 0.8332 - val_loss: 0.4379 - val_accuracy: 0.7911\n",
      "Epoch 221/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8322\n",
      "Epoch 00221: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3696 - accuracy: 0.8320 - val_loss: 0.4111 - val_accuracy: 0.8031\n",
      "Epoch 222/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8276\n",
      "Epoch 00222: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3674 - accuracy: 0.8278 - val_loss: 0.4532 - val_accuracy: 0.7775\n",
      "Epoch 223/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8324\n",
      "Epoch 00223: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3631 - accuracy: 0.8324 - val_loss: 0.4276 - val_accuracy: 0.7988\n",
      "Epoch 224/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8346\n",
      "Epoch 00224: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3628 - accuracy: 0.8346 - val_loss: 0.4056 - val_accuracy: 0.8093\n",
      "Epoch 225/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8330\n",
      "Epoch 00225: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3635 - accuracy: 0.8329 - val_loss: 0.4086 - val_accuracy: 0.8083\n",
      "Epoch 226/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8364\n",
      "Epoch 00226: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3561 - accuracy: 0.8364 - val_loss: 0.4090 - val_accuracy: 0.8119\n",
      "Epoch 227/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8349\n",
      "Epoch 00227: val_loss improved from 0.39620 to 0.39034, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3619 - accuracy: 0.8347 - val_loss: 0.3903 - val_accuracy: 0.8196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8354 ETA: 0s - loss: 0.3573 - accura\n",
      "Epoch 00228: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3581 - accuracy: 0.8353 - val_loss: 0.3974 - val_accuracy: 0.8169\n",
      "Epoch 229/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8363\n",
      "Epoch 00229: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3585 - accuracy: 0.8365 - val_loss: 0.3910 - val_accuracy: 0.8181\n",
      "Epoch 230/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8379\n",
      "Epoch 00230: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3592 - accuracy: 0.8380 - val_loss: 0.3909 - val_accuracy: 0.8186\n",
      "Epoch 231/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8408\n",
      "Epoch 00231: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3542 - accuracy: 0.8407 - val_loss: 0.3922 - val_accuracy: 0.8222\n",
      "Epoch 232/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3603 - accuracy: 0.8344\n",
      "Epoch 00232: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3601 - accuracy: 0.8344 - val_loss: 0.3916 - val_accuracy: 0.8233\n",
      "Epoch 233/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8369\n",
      "Epoch 00233: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3594 - accuracy: 0.8369 - val_loss: 0.4120 - val_accuracy: 0.8044\n",
      "Epoch 234/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8401\n",
      "Epoch 00234: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3535 - accuracy: 0.8404 - val_loss: 0.4177 - val_accuracy: 0.8009\n",
      "Epoch 235/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8360\n",
      "Epoch 00235: val_loss did not improve from 0.39034\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3577 - accuracy: 0.8360 - val_loss: 0.4252 - val_accuracy: 0.7975\n",
      "Epoch 236/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3589 - accuracy: 0.8377\n",
      "Epoch 00236: val_loss improved from 0.39034 to 0.39020, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3595 - accuracy: 0.8376 - val_loss: 0.3902 - val_accuracy: 0.8169\n",
      "Epoch 237/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.8394\n",
      "Epoch 00237: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3513 - accuracy: 0.8394 - val_loss: 0.3946 - val_accuracy: 0.8143\n",
      "Epoch 238/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8391\n",
      "Epoch 00238: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3547 - accuracy: 0.8392 - val_loss: 0.4116 - val_accuracy: 0.8046\n",
      "Epoch 239/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3469 - accuracy: 0.8449\n",
      "Epoch 00239: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3474 - accuracy: 0.8448 - val_loss: 0.4262 - val_accuracy: 0.7949\n",
      "Epoch 240/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8409\n",
      "Epoch 00240: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3526 - accuracy: 0.8408 - val_loss: 0.4039 - val_accuracy: 0.8078\n",
      "Epoch 241/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.8399\n",
      "Epoch 00241: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3542 - accuracy: 0.8400 - val_loss: 0.4404 - val_accuracy: 0.7870\n",
      "Epoch 242/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8453\n",
      "Epoch 00242: val_loss did not improve from 0.39020\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3490 - accuracy: 0.8455 - val_loss: 0.3970 - val_accuracy: 0.8121\n",
      "Epoch 243/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8412\n",
      "Epoch 00243: val_loss improved from 0.39020 to 0.38764, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3493 - accuracy: 0.8411 - val_loss: 0.3876 - val_accuracy: 0.8229\n",
      "Epoch 244/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8405\n",
      "Epoch 00244: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3450 - accuracy: 0.8406 - val_loss: 0.3878 - val_accuracy: 0.8214\n",
      "Epoch 245/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8426\n",
      "Epoch 00245: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3504 - accuracy: 0.8427 - val_loss: 0.4262 - val_accuracy: 0.7986\n",
      "Epoch 246/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8421\n",
      "Epoch 00246: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3437 - accuracy: 0.8422 - val_loss: 0.4025 - val_accuracy: 0.8104\n",
      "Epoch 247/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8459\n",
      "Epoch 00247: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3442 - accuracy: 0.8459 - val_loss: 0.4492 - val_accuracy: 0.7822\n",
      "Epoch 248/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8443\n",
      "Epoch 00248: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3494 - accuracy: 0.8445 - val_loss: 0.4346 - val_accuracy: 0.7889\n",
      "Epoch 249/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8469\n",
      "Epoch 00249: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3410 - accuracy: 0.8470 - val_loss: 0.4059 - val_accuracy: 0.8093\n",
      "Epoch 250/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.8449\n",
      "Epoch 00250: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3410 - accuracy: 0.8449 - val_loss: 0.4205 - val_accuracy: 0.7984\n",
      "Epoch 251/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8474\n",
      "Epoch 00251: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3408 - accuracy: 0.8474 - val_loss: 0.4051 - val_accuracy: 0.8078\n",
      "Epoch 252/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8443\n",
      "Epoch 00252: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3433 - accuracy: 0.8442 - val_loss: 0.4085 - val_accuracy: 0.8061\n",
      "Epoch 253/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8434\n",
      "Epoch 00253: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3480 - accuracy: 0.8436 - val_loss: 0.3981 - val_accuracy: 0.8061\n",
      "Epoch 254/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8472\n",
      "Epoch 00254: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3405 - accuracy: 0.8472 - val_loss: 0.4173 - val_accuracy: 0.7992\n",
      "Epoch 255/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.8430\n",
      "Epoch 00255: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3422 - accuracy: 0.8433 - val_loss: 0.3889 - val_accuracy: 0.8186\n",
      "Epoch 256/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3389 - accuracy: 0.8467\n",
      "Epoch 00256: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3384 - accuracy: 0.8469 - val_loss: 0.4016 - val_accuracy: 0.8104\n",
      "Epoch 257/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.8494\n",
      "Epoch 00257: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3353 - accuracy: 0.8493 - val_loss: 0.3886 - val_accuracy: 0.8209\n",
      "Epoch 258/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.8486\n",
      "Epoch 00258: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3324 - accuracy: 0.8490 - val_loss: 0.3877 - val_accuracy: 0.8199\n",
      "Epoch 259/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8489\n",
      "Epoch 00259: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3374 - accuracy: 0.8490 - val_loss: 0.4051 - val_accuracy: 0.8061\n",
      "Epoch 260/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.8473\n",
      "Epoch 00260: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3358 - accuracy: 0.8475 - val_loss: 0.4028 - val_accuracy: 0.8085\n",
      "Epoch 261/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8445\n",
      "Epoch 00261: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3411 - accuracy: 0.8449 - val_loss: 0.3911 - val_accuracy: 0.8149\n",
      "Epoch 262/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8488\n",
      "Epoch 00262: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3360 - accuracy: 0.8487 - val_loss: 0.4070 - val_accuracy: 0.8078\n",
      "Epoch 263/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.8518\n",
      "Epoch 00263: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3323 - accuracy: 0.8519 - val_loss: 0.4006 - val_accuracy: 0.8143\n",
      "Epoch 264/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3383 - accuracy: 0.8483\n",
      "Epoch 00264: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3387 - accuracy: 0.8483 - val_loss: 0.4013 - val_accuracy: 0.8110\n",
      "Epoch 265/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8489\n",
      "Epoch 00265: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3320 - accuracy: 0.8489 - val_loss: 0.3941 - val_accuracy: 0.8136\n",
      "Epoch 266/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8525\n",
      "Epoch 00266: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3319 - accuracy: 0.8529 - val_loss: 0.4132 - val_accuracy: 0.8050\n",
      "Epoch 267/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8531\n",
      "Epoch 00267: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3273 - accuracy: 0.8535 - val_loss: 0.4059 - val_accuracy: 0.8085\n",
      "Epoch 268/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8539\n",
      "Epoch 00268: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3272 - accuracy: 0.8539 - val_loss: 0.4015 - val_accuracy: 0.8104\n",
      "Epoch 269/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8492\n",
      "Epoch 00269: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3340 - accuracy: 0.8493 - val_loss: 0.4383 - val_accuracy: 0.7878\n",
      "Epoch 270/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8558\n",
      "Epoch 00270: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3254 - accuracy: 0.8558 - val_loss: 0.3905 - val_accuracy: 0.8201\n",
      "Epoch 271/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8514\n",
      "Epoch 00271: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3308 - accuracy: 0.8513 - val_loss: 0.3934 - val_accuracy: 0.8158\n",
      "Epoch 272/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8538\n",
      "Epoch 00272: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3269 - accuracy: 0.8538 - val_loss: 0.3985 - val_accuracy: 0.8108\n",
      "Epoch 273/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.8546\n",
      "Epoch 00273: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3265 - accuracy: 0.8546 - val_loss: 0.3944 - val_accuracy: 0.8175\n",
      "Epoch 274/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.8541\n",
      "Epoch 00274: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3274 - accuracy: 0.8540 - val_loss: 0.4020 - val_accuracy: 0.8083\n",
      "Epoch 275/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8586\n",
      "Epoch 00275: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3190 - accuracy: 0.8586 - val_loss: 0.3989 - val_accuracy: 0.8119\n",
      "Epoch 276/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8570\n",
      "Epoch 00276: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3219 - accuracy: 0.8571 - val_loss: 0.3942 - val_accuracy: 0.8149\n",
      "Epoch 277/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8555\n",
      "Epoch 00277: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3195 - accuracy: 0.8556 - val_loss: 0.4524 - val_accuracy: 0.7868\n",
      "Epoch 278/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8571\n",
      "Epoch 00278: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 0.3182 - accuracy: 0.8573 - val_loss: 0.4364 - val_accuracy: 0.7902\n",
      "Epoch 279/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8567\n",
      "Epoch 00279: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3229 - accuracy: 0.8566 - val_loss: 0.4218 - val_accuracy: 0.7960\n",
      "Epoch 280/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8566\n",
      "Epoch 00280: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3218 - accuracy: 0.8569 - val_loss: 0.3946 - val_accuracy: 0.8162\n",
      "Epoch 281/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8579\n",
      "Epoch 00281: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3204 - accuracy: 0.8580 - val_loss: 0.3933 - val_accuracy: 0.8153\n",
      "Epoch 282/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8578\n",
      "Epoch 00282: val_loss did not improve from 0.38764\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3179 - accuracy: 0.8579 - val_loss: 0.3977 - val_accuracy: 0.8117\n",
      "Epoch 283/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8565\n",
      "Epoch 00283: val_loss improved from 0.38764 to 0.38244, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3176 - accuracy: 0.8567 - val_loss: 0.3824 - val_accuracy: 0.8259\n",
      "Epoch 284/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8602\n",
      "Epoch 00284: val_loss did not improve from 0.38244\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3118 - accuracy: 0.8606 - val_loss: 0.4020 - val_accuracy: 0.8121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8598\n",
      "Epoch 00285: val_loss did not improve from 0.38244\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3131 - accuracy: 0.8598 - val_loss: 0.3941 - val_accuracy: 0.8147\n",
      "Epoch 286/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8597\n",
      "Epoch 00286: val_loss did not improve from 0.38244\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3152 - accuracy: 0.8598 - val_loss: 0.3909 - val_accuracy: 0.8166\n",
      "Epoch 287/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.8622\n",
      "Epoch 00287: val_loss did not improve from 0.38244\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3156 - accuracy: 0.8622 - val_loss: 0.3923 - val_accuracy: 0.8147\n",
      "Epoch 288/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8572\n",
      "Epoch 00288: val_loss did not improve from 0.38244\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3143 - accuracy: 0.8573 - val_loss: 0.3832 - val_accuracy: 0.8255\n",
      "Epoch 289/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8621\n",
      "Epoch 00289: val_loss improved from 0.38244 to 0.38185, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3135 - accuracy: 0.8621 - val_loss: 0.3819 - val_accuracy: 0.8257\n",
      "Epoch 290/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8646\n",
      "Epoch 00290: val_loss improved from 0.38185 to 0.38170, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3086 - accuracy: 0.8648 - val_loss: 0.3817 - val_accuracy: 0.8248\n",
      "Epoch 291/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8628\n",
      "Epoch 00291: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3064 - accuracy: 0.8629 - val_loss: 0.3913 - val_accuracy: 0.8166\n",
      "Epoch 292/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8619\n",
      "Epoch 00292: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3098 - accuracy: 0.8621 - val_loss: 0.4188 - val_accuracy: 0.8007\n",
      "Epoch 293/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.8616\n",
      "Epoch 00293: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3106 - accuracy: 0.8616 - val_loss: 0.4196 - val_accuracy: 0.8020\n",
      "Epoch 294/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8647\n",
      "Epoch 00294: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3075 - accuracy: 0.8647 - val_loss: 0.3933 - val_accuracy: 0.8151\n",
      "Epoch 295/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8619\n",
      "Epoch 00295: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3078 - accuracy: 0.8622 - val_loss: 0.3931 - val_accuracy: 0.8173\n",
      "Epoch 296/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8654\n",
      "Epoch 00296: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3036 - accuracy: 0.8654 - val_loss: 0.4533 - val_accuracy: 0.7850\n",
      "Epoch 297/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8609\n",
      "Epoch 00297: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3143 - accuracy: 0.8612 - val_loss: 0.4135 - val_accuracy: 0.8029\n",
      "Epoch 298/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8651\n",
      "Epoch 00298: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3025 - accuracy: 0.8657 - val_loss: 0.4258 - val_accuracy: 0.7984\n",
      "Epoch 299/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8638\n",
      "Epoch 00299: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3050 - accuracy: 0.8638 - val_loss: 0.4492 - val_accuracy: 0.7876\n",
      "Epoch 300/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8624\n",
      "Epoch 00300: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3099 - accuracy: 0.8629 - val_loss: 0.3826 - val_accuracy: 0.8257\n",
      "Epoch 301/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8645\n",
      "Epoch 00301: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3026 - accuracy: 0.8645 - val_loss: 0.3951 - val_accuracy: 0.8147\n",
      "Epoch 302/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8645\n",
      "Epoch 00302: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3041 - accuracy: 0.8646 - val_loss: 0.3929 - val_accuracy: 0.8181\n",
      "Epoch 303/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8695\n",
      "Epoch 00303: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2995 - accuracy: 0.8696 - val_loss: 0.4051 - val_accuracy: 0.8080\n",
      "Epoch 304/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8671\n",
      "Epoch 00304: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2999 - accuracy: 0.8673 - val_loss: 0.3902 - val_accuracy: 0.8166\n",
      "Epoch 305/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8665\n",
      "Epoch 00305: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3031 - accuracy: 0.8667 - val_loss: 0.4069 - val_accuracy: 0.8087\n",
      "Epoch 306/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8685\n",
      "Epoch 00306: val_loss did not improve from 0.38170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2977 - accuracy: 0.8689 - val_loss: 0.3980 - val_accuracy: 0.8160\n",
      "Epoch 307/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8669\n",
      "Epoch 00307: val_loss improved from 0.38170 to 0.37574, saving model to pickled_objects/batch_size_128_lr_0.02_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3058 - accuracy: 0.8668 - val_loss: 0.3757 - val_accuracy: 0.8302\n",
      "Epoch 308/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8684\n",
      "Epoch 00308: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2964 - accuracy: 0.8685 - val_loss: 0.3816 - val_accuracy: 0.8242\n",
      "Epoch 309/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8728\n",
      "Epoch 00309: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2922 - accuracy: 0.8730 - val_loss: 0.4795 - val_accuracy: 0.7760\n",
      "Epoch 310/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8700\n",
      "Epoch 00310: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2942 - accuracy: 0.8703 - val_loss: 0.3949 - val_accuracy: 0.8169\n",
      "Epoch 311/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8707\n",
      "Epoch 00311: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2943 - accuracy: 0.8709 - val_loss: 0.3916 - val_accuracy: 0.8201\n",
      "Epoch 312/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8697\n",
      "Epoch 00312: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2992 - accuracy: 0.8697 - val_loss: 0.3971 - val_accuracy: 0.8162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.8709\n",
      "Epoch 00313: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2918 - accuracy: 0.8709 - val_loss: 0.4003 - val_accuracy: 0.8123\n",
      "Epoch 314/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8710\n",
      "Epoch 00314: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2933 - accuracy: 0.8710 - val_loss: 0.4090 - val_accuracy: 0.8095\n",
      "Epoch 315/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8691\n",
      "Epoch 00315: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2935 - accuracy: 0.8693 - val_loss: 0.3961 - val_accuracy: 0.8162\n",
      "Epoch 316/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8692\n",
      "Epoch 00316: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2934 - accuracy: 0.8694 - val_loss: 0.4124 - val_accuracy: 0.8057\n",
      "Epoch 317/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8698\n",
      "Epoch 00317: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2975 - accuracy: 0.8701 - val_loss: 0.3913 - val_accuracy: 0.8169\n",
      "Epoch 318/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8742\n",
      "Epoch 00318: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2906 - accuracy: 0.8743 - val_loss: 0.3903 - val_accuracy: 0.8153\n",
      "Epoch 319/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8724\n",
      "Epoch 00319: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2901 - accuracy: 0.8728 - val_loss: 0.3899 - val_accuracy: 0.8218\n",
      "Epoch 320/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8733\n",
      "Epoch 00320: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2867 - accuracy: 0.8732 - val_loss: 0.4215 - val_accuracy: 0.8014\n",
      "Epoch 321/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.8732\n",
      "Epoch 00321: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2871 - accuracy: 0.8735 - val_loss: 0.4389 - val_accuracy: 0.7932\n",
      "Epoch 322/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8746\n",
      "Epoch 00322: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2874 - accuracy: 0.8745 - val_loss: 0.3838 - val_accuracy: 0.8257\n",
      "Epoch 323/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8753\n",
      "Epoch 00323: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2863 - accuracy: 0.8753 - val_loss: 0.4203 - val_accuracy: 0.8050\n",
      "Epoch 324/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.8733 ETA: 0s - loss: 0.2841 - ac\n",
      "Epoch 00324: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2859 - accuracy: 0.8733 - val_loss: 0.3884 - val_accuracy: 0.8212\n",
      "Epoch 325/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8770\n",
      "Epoch 00325: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2807 - accuracy: 0.8772 - val_loss: 0.4050 - val_accuracy: 0.8113\n",
      "Epoch 326/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8755 ETA: 0s - los\n",
      "Epoch 00326: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2815 - accuracy: 0.8759 - val_loss: 0.3917 - val_accuracy: 0.8227\n",
      "Epoch 327/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8739\n",
      "Epoch 00327: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2847 - accuracy: 0.8740 - val_loss: 0.3872 - val_accuracy: 0.8233\n",
      "Epoch 328/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.8748\n",
      "Epoch 00328: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2838 - accuracy: 0.8750 - val_loss: 0.4209 - val_accuracy: 0.8009\n",
      "Epoch 329/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.8749\n",
      "Epoch 00329: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2809 - accuracy: 0.8752 - val_loss: 0.3907 - val_accuracy: 0.8209\n",
      "Epoch 330/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8740\n",
      "Epoch 00330: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2829 - accuracy: 0.8742 - val_loss: 0.3992 - val_accuracy: 0.8164\n",
      "Epoch 331/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8793 ETA: 0s - loss: 0.2773 - accu\n",
      "Epoch 00331: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2781 - accuracy: 0.8794 - val_loss: 0.3850 - val_accuracy: 0.8267\n",
      "Epoch 332/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8811\n",
      "Epoch 00332: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2789 - accuracy: 0.8815 - val_loss: 0.4053 - val_accuracy: 0.8171\n",
      "Epoch 333/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8814\n",
      "Epoch 00333: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2789 - accuracy: 0.8814 - val_loss: 0.4059 - val_accuracy: 0.8117\n",
      "Epoch 334/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8798\n",
      "Epoch 00334: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2813 - accuracy: 0.8798 - val_loss: 0.3876 - val_accuracy: 0.8261\n",
      "Epoch 335/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8775\n",
      "Epoch 00335: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2774 - accuracy: 0.8776 - val_loss: 0.4221 - val_accuracy: 0.8080\n",
      "Epoch 336/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.8823\n",
      "Epoch 00336: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2749 - accuracy: 0.8824 - val_loss: 0.4294 - val_accuracy: 0.8031\n",
      "Epoch 337/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.8797 ETA: 0s - loss: 0\n",
      "Epoch 00337: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2765 - accuracy: 0.8794 - val_loss: 0.3966 - val_accuracy: 0.8203\n",
      "Epoch 338/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8808\n",
      "Epoch 00338: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2761 - accuracy: 0.8810 - val_loss: 0.4001 - val_accuracy: 0.8203\n",
      "Epoch 339/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.8796\n",
      "Epoch 00339: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2741 - accuracy: 0.8796 - val_loss: 0.3930 - val_accuracy: 0.8199\n",
      "Epoch 340/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8850\n",
      "Epoch 00340: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2706 - accuracy: 0.8848 - val_loss: 0.4229 - val_accuracy: 0.8012\n",
      "Epoch 341/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8814\n",
      "Epoch 00341: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2712 - accuracy: 0.8818 - val_loss: 0.3922 - val_accuracy: 0.8252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8840\n",
      "Epoch 00342: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2688 - accuracy: 0.8843 - val_loss: 0.3906 - val_accuracy: 0.8259\n",
      "Epoch 343/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.8823\n",
      "Epoch 00343: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2738 - accuracy: 0.8824 - val_loss: 0.3944 - val_accuracy: 0.8203\n",
      "Epoch 344/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.8807\n",
      "Epoch 00344: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2709 - accuracy: 0.8809 - val_loss: 0.4214 - val_accuracy: 0.8061\n",
      "Epoch 345/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8841\n",
      "Epoch 00345: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2633 - accuracy: 0.8843 - val_loss: 0.4042 - val_accuracy: 0.8194\n",
      "Epoch 346/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.8852\n",
      "Epoch 00346: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2662 - accuracy: 0.8853 - val_loss: 0.4716 - val_accuracy: 0.7855\n",
      "Epoch 347/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8845\n",
      "Epoch 00347: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2672 - accuracy: 0.8848 - val_loss: 0.4588 - val_accuracy: 0.7917\n",
      "Epoch 348/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.8811\n",
      "Epoch 00348: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2680 - accuracy: 0.8812 - val_loss: 0.4195 - val_accuracy: 0.8035\n",
      "Epoch 349/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.8851\n",
      "Epoch 00349: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2687 - accuracy: 0.8846 - val_loss: 0.4034 - val_accuracy: 0.8175\n",
      "Epoch 350/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8893\n",
      "Epoch 00350: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2616 - accuracy: 0.8895 - val_loss: 0.4194 - val_accuracy: 0.8063\n",
      "Epoch 351/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.8845\n",
      "Epoch 00351: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2676 - accuracy: 0.8848 - val_loss: 0.3971 - val_accuracy: 0.8175\n",
      "Epoch 352/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.8881\n",
      "Epoch 00352: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2599 - accuracy: 0.8886 - val_loss: 0.4063 - val_accuracy: 0.8151\n",
      "Epoch 353/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8837\n",
      "Epoch 00353: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2663 - accuracy: 0.8838 - val_loss: 0.4121 - val_accuracy: 0.8113\n",
      "Epoch 354/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.8831\n",
      "Epoch 00354: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2636 - accuracy: 0.8833 - val_loss: 0.3994 - val_accuracy: 0.8209\n",
      "Epoch 355/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2635 - accuracy: 0.8854\n",
      "Epoch 00355: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2630 - accuracy: 0.8855 - val_loss: 0.4027 - val_accuracy: 0.8196\n",
      "Epoch 356/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.8890\n",
      "Epoch 00356: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2593 - accuracy: 0.8892 - val_loss: 0.4024 - val_accuracy: 0.8192\n",
      "Epoch 357/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8873\n",
      "Epoch 00357: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2627 - accuracy: 0.8875 - val_loss: 0.4089 - val_accuracy: 0.8145\n",
      "Epoch 358/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8837\n",
      "Epoch 00358: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2617 - accuracy: 0.8838 - val_loss: 0.4399 - val_accuracy: 0.7971\n",
      "Epoch 359/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8862\n",
      "Epoch 00359: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2596 - accuracy: 0.8864 - val_loss: 0.3965 - val_accuracy: 0.8239\n",
      "Epoch 360/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.8948\n",
      "Epoch 00360: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2523 - accuracy: 0.8947 - val_loss: 0.3944 - val_accuracy: 0.8227\n",
      "Epoch 361/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8892\n",
      "Epoch 00361: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2590 - accuracy: 0.8893 - val_loss: 0.4380 - val_accuracy: 0.7992\n",
      "Epoch 362/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8893\n",
      "Epoch 00362: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2573 - accuracy: 0.8895 - val_loss: 0.4251 - val_accuracy: 0.8052\n",
      "Epoch 363/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.8904\n",
      "Epoch 00363: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2573 - accuracy: 0.8904 - val_loss: 0.4086 - val_accuracy: 0.8173\n",
      "Epoch 364/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8916\n",
      "Epoch 00364: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2522 - accuracy: 0.8917 - val_loss: 0.3955 - val_accuracy: 0.8231\n",
      "Epoch 365/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8880\n",
      "Epoch 00365: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2609 - accuracy: 0.8881 - val_loss: 0.4355 - val_accuracy: 0.8018\n",
      "Epoch 366/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.8905\n",
      "Epoch 00366: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2567 - accuracy: 0.8909 - val_loss: 0.3961 - val_accuracy: 0.8218\n",
      "Epoch 367/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.8880\n",
      "Epoch 00367: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2585 - accuracy: 0.8881 - val_loss: 0.3926 - val_accuracy: 0.8246\n",
      "Epoch 368/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.8907\n",
      "Epoch 00368: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2537 - accuracy: 0.8909 - val_loss: 0.4547 - val_accuracy: 0.7889\n",
      "Epoch 369/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2560 - accuracy: 0.8879\n",
      "Epoch 00369: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2560 - accuracy: 0.8879 - val_loss: 0.4195 - val_accuracy: 0.8057\n",
      "Epoch 370/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.8924\n",
      "Epoch 00370: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2496 - accuracy: 0.8925 - val_loss: 0.3864 - val_accuracy: 0.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.8938\n",
      "Epoch 00371: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2469 - accuracy: 0.8937 - val_loss: 0.4134 - val_accuracy: 0.8087\n",
      "Epoch 372/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.8908\n",
      "Epoch 00372: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2535 - accuracy: 0.8911 - val_loss: 0.3970 - val_accuracy: 0.8227\n",
      "Epoch 373/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2468 - accuracy: 0.8943\n",
      "Epoch 00373: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2460 - accuracy: 0.8945 - val_loss: 0.4226 - val_accuracy: 0.8080\n",
      "Epoch 374/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.8925\n",
      "Epoch 00374: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2509 - accuracy: 0.8927 - val_loss: 0.4163 - val_accuracy: 0.8121\n",
      "Epoch 375/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8937\n",
      "Epoch 00375: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2493 - accuracy: 0.8940 - val_loss: 0.3920 - val_accuracy: 0.8244\n",
      "Epoch 376/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.8950\n",
      "Epoch 00376: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2467 - accuracy: 0.8952 - val_loss: 0.4170 - val_accuracy: 0.8087\n",
      "Epoch 377/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.8946\n",
      "Epoch 00377: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2443 - accuracy: 0.8948 - val_loss: 0.4192 - val_accuracy: 0.8102\n",
      "Epoch 378/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.8966\n",
      "Epoch 00378: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2455 - accuracy: 0.8967 - val_loss: 0.4679 - val_accuracy: 0.7891\n",
      "Epoch 379/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2483 - accuracy: 0.8965\n",
      "Epoch 00379: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2479 - accuracy: 0.8966 - val_loss: 0.4012 - val_accuracy: 0.8214\n",
      "Epoch 380/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.8961\n",
      "Epoch 00380: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2398 - accuracy: 0.8962 - val_loss: 0.4693 - val_accuracy: 0.7902\n",
      "Epoch 381/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8944\n",
      "Epoch 00381: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2438 - accuracy: 0.8945 - val_loss: 0.4516 - val_accuracy: 0.8001\n",
      "Epoch 382/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.8940\n",
      "Epoch 00382: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2424 - accuracy: 0.8943 - val_loss: 0.4366 - val_accuracy: 0.8025\n",
      "Epoch 383/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.8983\n",
      "Epoch 00383: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2416 - accuracy: 0.8985 - val_loss: 0.4202 - val_accuracy: 0.8121\n",
      "Epoch 384/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.8985\n",
      "Epoch 00384: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2407 - accuracy: 0.8985 - val_loss: 0.4542 - val_accuracy: 0.7990\n",
      "Epoch 385/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.8992\n",
      "Epoch 00385: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2391 - accuracy: 0.8994 - val_loss: 0.4218 - val_accuracy: 0.8134\n",
      "Epoch 386/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.8973\n",
      "Epoch 00386: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2447 - accuracy: 0.8974 - val_loss: 0.3885 - val_accuracy: 0.8233\n",
      "Epoch 387/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.8976\n",
      "Epoch 00387: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2359 - accuracy: 0.8979 - val_loss: 0.4392 - val_accuracy: 0.8001\n",
      "Epoch 388/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.8983\n",
      "Epoch 00388: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2367 - accuracy: 0.8984 - val_loss: 0.4398 - val_accuracy: 0.7986\n",
      "Epoch 389/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.8983\n",
      "Epoch 00389: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2385 - accuracy: 0.8981 - val_loss: 0.4315 - val_accuracy: 0.8091\n",
      "Epoch 390/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9001\n",
      "Epoch 00390: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2356 - accuracy: 0.9003 - val_loss: 0.4304 - val_accuracy: 0.8087\n",
      "Epoch 391/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.8969\n",
      "Epoch 00391: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2403 - accuracy: 0.8970 - val_loss: 0.4272 - val_accuracy: 0.7994\n",
      "Epoch 392/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.8980\n",
      "Epoch 00392: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2383 - accuracy: 0.8982 - val_loss: 0.4253 - val_accuracy: 0.8067\n",
      "Epoch 393/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2321 - accuracy: 0.9030\n",
      "Epoch 00393: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2316 - accuracy: 0.9031 - val_loss: 0.4178 - val_accuracy: 0.8108\n",
      "Epoch 394/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.8997\n",
      "Epoch 00394: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2354 - accuracy: 0.8998 - val_loss: 0.3965 - val_accuracy: 0.8224\n",
      "Epoch 395/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2349 - accuracy: 0.9016\n",
      "Epoch 00395: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2343 - accuracy: 0.9018 - val_loss: 0.4076 - val_accuracy: 0.8192\n",
      "Epoch 396/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.8975\n",
      "Epoch 00396: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2376 - accuracy: 0.8975 - val_loss: 0.3937 - val_accuracy: 0.8255\n",
      "Epoch 397/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9005\n",
      "Epoch 00397: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2334 - accuracy: 0.9006 - val_loss: 0.4309 - val_accuracy: 0.8100\n",
      "Epoch 398/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9003\n",
      "Epoch 00398: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2343 - accuracy: 0.9004 - val_loss: 0.3907 - val_accuracy: 0.8276\n",
      "Epoch 399/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9052\n",
      "Epoch 00399: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2235 - accuracy: 0.9053 - val_loss: 0.4293 - val_accuracy: 0.8126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9027\n",
      "Epoch 00400: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2304 - accuracy: 0.9028 - val_loss: 0.4178 - val_accuracy: 0.8156\n",
      "Epoch 401/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9054\n",
      "Epoch 00401: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2277 - accuracy: 0.9054 - val_loss: 0.4334 - val_accuracy: 0.8083\n",
      "Epoch 402/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9015\n",
      "Epoch 00402: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2274 - accuracy: 0.9016 - val_loss: 0.4217 - val_accuracy: 0.8138\n",
      "Epoch 403/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2217 - accuracy: 0.9048\n",
      "Epoch 00403: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2209 - accuracy: 0.9051 - val_loss: 0.4226 - val_accuracy: 0.8160\n",
      "Epoch 404/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9047\n",
      "Epoch 00404: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2245 - accuracy: 0.9051 - val_loss: 0.4273 - val_accuracy: 0.8136\n",
      "Epoch 405/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9032\n",
      "Epoch 00405: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2259 - accuracy: 0.9033 - val_loss: 0.4527 - val_accuracy: 0.8018\n",
      "Epoch 406/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9009\n",
      "Epoch 00406: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2312 - accuracy: 0.9010 - val_loss: 0.4397 - val_accuracy: 0.8042\n",
      "Epoch 407/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.9073\n",
      "Epoch 00407: val_loss did not improve from 0.37574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2226 - accuracy: 0.9074 - val_loss: 0.4053 - val_accuracy: 0.8179\n",
      "Epoch 00407: early stopping\n",
      "Epoch 1/10000\n",
      "    146/Unknown - 8s 58ms/step - loss: 0.6920 - accuracy: 0.5204\n",
      "Epoch 00001: val_loss improved from inf to 0.69130, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 64ms/step - loss: 0.6920 - accuracy: 0.5204 - val_loss: 0.6913 - val_accuracy: 0.5028\n",
      "Epoch 2/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5647\n",
      "Epoch 00002: val_loss improved from 0.69130 to 0.68758, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6860 - accuracy: 0.5648 - val_loss: 0.6876 - val_accuracy: 0.5234\n",
      "Epoch 3/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5770\n",
      "Epoch 00003: val_loss improved from 0.68758 to 0.68575, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6791 - accuracy: 0.5768 - val_loss: 0.6858 - val_accuracy: 0.5312\n",
      "Epoch 4/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6747 - accuracy: 0.5831\n",
      "Epoch 00004: val_loss improved from 0.68575 to 0.67530, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6746 - accuracy: 0.5831 - val_loss: 0.6753 - val_accuracy: 0.5755\n",
      "Epoch 5/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6706 - accuracy: 0.5907\n",
      "Epoch 00005: val_loss improved from 0.67530 to 0.66215, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6702 - accuracy: 0.5911 - val_loss: 0.6622 - val_accuracy: 0.6223\n",
      "Epoch 6/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.5921\n",
      "Epoch 00006: val_loss improved from 0.66215 to 0.66112, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6658 - accuracy: 0.5920 - val_loss: 0.6611 - val_accuracy: 0.6081\n",
      "Epoch 7/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6608 - accuracy: 0.6020\n",
      "Epoch 00007: val_loss improved from 0.66112 to 0.65167, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6607 - accuracy: 0.6019 - val_loss: 0.6517 - val_accuracy: 0.6281\n",
      "Epoch 8/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.6062\n",
      "Epoch 00008: val_loss did not improve from 0.65167\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6564 - accuracy: 0.6062 - val_loss: 0.6543 - val_accuracy: 0.6068\n",
      "Epoch 9/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6522 - accuracy: 0.6105\n",
      "Epoch 00009: val_loss improved from 0.65167 to 0.64922, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6522 - accuracy: 0.6102 - val_loss: 0.6492 - val_accuracy: 0.6133\n",
      "Epoch 10/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6478 - accuracy: 0.6200\n",
      "Epoch 00010: val_loss improved from 0.64922 to 0.63777, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6475 - accuracy: 0.6200 - val_loss: 0.6378 - val_accuracy: 0.6359\n",
      "Epoch 11/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.6229\n",
      "Epoch 00011: val_loss improved from 0.63777 to 0.63175, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6442 - accuracy: 0.6228 - val_loss: 0.6317 - val_accuracy: 0.6468\n",
      "Epoch 12/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6392 - accuracy: 0.6277\n",
      "Epoch 00012: val_loss improved from 0.63175 to 0.62692, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6393 - accuracy: 0.6276 - val_loss: 0.6269 - val_accuracy: 0.6498\n",
      "Epoch 13/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6327 - accuracy: 0.6359\n",
      "Epoch 00013: val_loss did not improve from 0.62692\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6330 - accuracy: 0.6355 - val_loss: 0.6305 - val_accuracy: 0.6371\n",
      "Epoch 14/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6265 - accuracy: 0.6453\n",
      "Epoch 00014: val_loss improved from 0.62692 to 0.61169, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6264 - accuracy: 0.6451 - val_loss: 0.6117 - val_accuracy: 0.6662\n",
      "Epoch 15/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.6556\n",
      "Epoch 00015: val_loss improved from 0.61169 to 0.60350, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6154 - accuracy: 0.6553 - val_loss: 0.6035 - val_accuracy: 0.6741\n",
      "Epoch 16/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6067 - accuracy: 0.6655\n",
      "Epoch 00016: val_loss improved from 0.60350 to 0.60035, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6069 - accuracy: 0.6651 - val_loss: 0.6004 - val_accuracy: 0.6713\n",
      "Epoch 17/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.6735\n",
      "Epoch 00017: val_loss improved from 0.60035 to 0.58468, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5992 - accuracy: 0.6732 - val_loss: 0.5847 - val_accuracy: 0.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5901 - accuracy: 0.6835\n",
      "Epoch 00018: val_loss improved from 0.58468 to 0.56688, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5905 - accuracy: 0.6832 - val_loss: 0.5669 - val_accuracy: 0.7186\n",
      "Epoch 19/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.6890\n",
      "Epoch 00019: val_loss improved from 0.56688 to 0.55816, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5850 - accuracy: 0.6887 - val_loss: 0.5582 - val_accuracy: 0.7259\n",
      "Epoch 20/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5786 - accuracy: 0.6906\n",
      "Epoch 00020: val_loss improved from 0.55816 to 0.55669, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5797 - accuracy: 0.6900 - val_loss: 0.5567 - val_accuracy: 0.7319\n",
      "Epoch 21/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7018\n",
      "Epoch 00021: val_loss did not improve from 0.55669\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5697 - accuracy: 0.7015 - val_loss: 0.5622 - val_accuracy: 0.7061\n",
      "Epoch 22/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7090\n",
      "Epoch 00022: val_loss improved from 0.55669 to 0.53397, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5639 - accuracy: 0.7088 - val_loss: 0.5340 - val_accuracy: 0.7371\n",
      "Epoch 23/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5574 - accuracy: 0.7103\n",
      "Epoch 00023: val_loss improved from 0.53397 to 0.52992, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5582 - accuracy: 0.7100 - val_loss: 0.5299 - val_accuracy: 0.7418\n",
      "Epoch 24/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7192\n",
      "Epoch 00024: val_loss improved from 0.52992 to 0.51408, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5499 - accuracy: 0.7187 - val_loss: 0.5141 - val_accuracy: 0.7599\n",
      "Epoch 25/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7259\n",
      "Epoch 00025: val_loss did not improve from 0.51408\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5401 - accuracy: 0.7251 - val_loss: 0.5578 - val_accuracy: 0.7253\n",
      "Epoch 26/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7321\n",
      "Epoch 00026: val_loss improved from 0.51408 to 0.51368, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5325 - accuracy: 0.7319 - val_loss: 0.5137 - val_accuracy: 0.7573\n",
      "Epoch 27/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7341\n",
      "Epoch 00027: val_loss did not improve from 0.51368\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5254 - accuracy: 0.7340 - val_loss: 0.5248 - val_accuracy: 0.7470\n",
      "Epoch 28/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5204 - accuracy: 0.7426\n",
      "Epoch 00028: val_loss improved from 0.51368 to 0.49613, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5209 - accuracy: 0.7424 - val_loss: 0.4961 - val_accuracy: 0.7657\n",
      "Epoch 29/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7414\n",
      "Epoch 00029: val_loss did not improve from 0.49613\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5153 - accuracy: 0.7411 - val_loss: 0.5062 - val_accuracy: 0.7562\n",
      "Epoch 30/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5071 - accuracy: 0.7482\n",
      "Epoch 00030: val_loss did not improve from 0.49613\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5075 - accuracy: 0.7480 - val_loss: 0.5299 - val_accuracy: 0.7549\n",
      "Epoch 31/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.7523\n",
      "Epoch 00031: val_loss improved from 0.49613 to 0.47993, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5049 - accuracy: 0.7522 - val_loss: 0.4799 - val_accuracy: 0.7786\n",
      "Epoch 32/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4975 - accuracy: 0.7543\n",
      "Epoch 00032: val_loss improved from 0.47993 to 0.47529, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4980 - accuracy: 0.7539 - val_loss: 0.4753 - val_accuracy: 0.7741\n",
      "Epoch 33/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7595\n",
      "Epoch 00033: val_loss did not improve from 0.47529\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4957 - accuracy: 0.7591 - val_loss: 0.4922 - val_accuracy: 0.7717\n",
      "Epoch 34/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4902 - accuracy: 0.7575\n",
      "Epoch 00034: val_loss improved from 0.47529 to 0.47055, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4906 - accuracy: 0.7573 - val_loss: 0.4706 - val_accuracy: 0.7792\n",
      "Epoch 35/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.7621\n",
      "Epoch 00035: val_loss did not improve from 0.47055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4859 - accuracy: 0.7616 - val_loss: 0.4958 - val_accuracy: 0.7614\n",
      "Epoch 36/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4823 - accuracy: 0.7673\n",
      "Epoch 00036: val_loss did not improve from 0.47055\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4829 - accuracy: 0.7671 - val_loss: 0.4735 - val_accuracy: 0.7764\n",
      "Epoch 37/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7713\n",
      "Epoch 00037: val_loss improved from 0.47055 to 0.46019, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4766 - accuracy: 0.7711 - val_loss: 0.4602 - val_accuracy: 0.7825\n",
      "Epoch 38/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.7716\n",
      "Epoch 00038: val_loss improved from 0.46019 to 0.44987, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4737 - accuracy: 0.7714 - val_loss: 0.4499 - val_accuracy: 0.7921\n",
      "Epoch 39/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7730 ETA: 1s\n",
      "Epoch 00039: val_loss improved from 0.44987 to 0.44923, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4720 - accuracy: 0.7726 - val_loss: 0.4492 - val_accuracy: 0.7893\n",
      "Epoch 40/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7766\n",
      "Epoch 00040: val_loss did not improve from 0.44923\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4715 - accuracy: 0.7763 - val_loss: 0.4504 - val_accuracy: 0.7911\n",
      "Epoch 41/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.7777\n",
      "Epoch 00041: val_loss did not improve from 0.44923\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4661 - accuracy: 0.7776 - val_loss: 0.4605 - val_accuracy: 0.7810\n",
      "Epoch 42/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.7787\n",
      "Epoch 00042: val_loss did not improve from 0.44923\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4643 - accuracy: 0.7783 - val_loss: 0.4613 - val_accuracy: 0.7827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4607 - accuracy: 0.7807\n",
      "Epoch 00043: val_loss improved from 0.44923 to 0.44482, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4614 - accuracy: 0.7802 - val_loss: 0.4448 - val_accuracy: 0.7936\n",
      "Epoch 44/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.7822\n",
      "Epoch 00044: val_loss did not improve from 0.44482\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4604 - accuracy: 0.7819 - val_loss: 0.4472 - val_accuracy: 0.7887\n",
      "Epoch 45/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.7846\n",
      "Epoch 00045: val_loss did not improve from 0.44482\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4537 - accuracy: 0.7844 - val_loss: 0.4558 - val_accuracy: 0.7829\n",
      "Epoch 46/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.7880\n",
      "Epoch 00046: val_loss improved from 0.44482 to 0.44267, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4523 - accuracy: 0.7879 - val_loss: 0.4427 - val_accuracy: 0.7923\n",
      "Epoch 47/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4513 - accuracy: 0.7877\n",
      "Epoch 00047: val_loss improved from 0.44267 to 0.43560, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4518 - accuracy: 0.7874 - val_loss: 0.4356 - val_accuracy: 0.7977\n",
      "Epoch 48/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.7851\n",
      "Epoch 00048: val_loss did not improve from 0.43560\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4524 - accuracy: 0.7848 - val_loss: 0.4380 - val_accuracy: 0.7936\n",
      "Epoch 49/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7883\n",
      "Epoch 00049: val_loss improved from 0.43560 to 0.43172, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4481 - accuracy: 0.7884 - val_loss: 0.4317 - val_accuracy: 0.7990\n",
      "Epoch 50/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4454 - accuracy: 0.7901\n",
      "Epoch 00050: val_loss did not improve from 0.43172\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4458 - accuracy: 0.7901 - val_loss: 0.4318 - val_accuracy: 0.8044\n",
      "Epoch 51/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.7913\n",
      "Epoch 00051: val_loss did not improve from 0.43172\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4426 - accuracy: 0.7910 - val_loss: 0.4434 - val_accuracy: 0.8001\n",
      "Epoch 52/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4452 - accuracy: 0.7912\n",
      "Epoch 00052: val_loss did not improve from 0.43172\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4452 - accuracy: 0.7913 - val_loss: 0.4331 - val_accuracy: 0.8016\n",
      "Epoch 53/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.7925\n",
      "Epoch 00053: val_loss improved from 0.43172 to 0.42923, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4407 - accuracy: 0.7922 - val_loss: 0.4292 - val_accuracy: 0.8005\n",
      "Epoch 54/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7975\n",
      "Epoch 00054: val_loss improved from 0.42923 to 0.42158, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4350 - accuracy: 0.7972 - val_loss: 0.4216 - val_accuracy: 0.8052\n",
      "Epoch 55/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7968\n",
      "Epoch 00055: val_loss did not improve from 0.42158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4334 - accuracy: 0.7967 - val_loss: 0.4292 - val_accuracy: 0.7990\n",
      "Epoch 56/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.7949\n",
      "Epoch 00056: val_loss did not improve from 0.42158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4350 - accuracy: 0.7947 - val_loss: 0.4254 - val_accuracy: 0.8072\n",
      "Epoch 57/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7984\n",
      "Epoch 00057: val_loss did not improve from 0.42158\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4335 - accuracy: 0.7982 - val_loss: 0.4233 - val_accuracy: 0.8025\n",
      "Epoch 58/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.7989\n",
      "Epoch 00058: val_loss improved from 0.42158 to 0.41574, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4288 - accuracy: 0.7987 - val_loss: 0.4157 - val_accuracy: 0.8098\n",
      "Epoch 59/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4296 - accuracy: 0.7996\n",
      "Epoch 00059: val_loss did not improve from 0.41574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4298 - accuracy: 0.7993 - val_loss: 0.4209 - val_accuracy: 0.8059\n",
      "Epoch 60/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8014\n",
      "Epoch 00060: val_loss did not improve from 0.41574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4262 - accuracy: 0.8011 - val_loss: 0.4244 - val_accuracy: 0.8022\n",
      "Epoch 61/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.7984\n",
      "Epoch 00061: val_loss did not improve from 0.41574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4257 - accuracy: 0.7983 - val_loss: 0.4176 - val_accuracy: 0.8100\n",
      "Epoch 62/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8046\n",
      "Epoch 00062: val_loss did not improve from 0.41574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4228 - accuracy: 0.8046 - val_loss: 0.4168 - val_accuracy: 0.8089\n",
      "Epoch 63/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.8065\n",
      "Epoch 00063: val_loss did not improve from 0.41574\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4188 - accuracy: 0.8063 - val_loss: 0.4240 - val_accuracy: 0.8029\n",
      "Epoch 64/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8083\n",
      "Epoch 00064: val_loss improved from 0.41574 to 0.41273, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4159 - accuracy: 0.8082 - val_loss: 0.4127 - val_accuracy: 0.8102\n",
      "Epoch 65/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4165 - accuracy: 0.8066\n",
      "Epoch 00065: val_loss did not improve from 0.41273\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4170 - accuracy: 0.8063 - val_loss: 0.4191 - val_accuracy: 0.8055\n",
      "Epoch 66/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4167 - accuracy: 0.8054\n",
      "Epoch 00066: val_loss did not improve from 0.41273\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4170 - accuracy: 0.8053 - val_loss: 0.4422 - val_accuracy: 0.7966\n",
      "Epoch 67/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8040\n",
      "Epoch 00067: val_loss improved from 0.41273 to 0.40599, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4162 - accuracy: 0.8040 - val_loss: 0.4060 - val_accuracy: 0.8149\n",
      "Epoch 68/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8073\n",
      "Epoch 00068: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4122 - accuracy: 0.8073 - val_loss: 0.4166 - val_accuracy: 0.8061\n",
      "Epoch 69/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.8091\n",
      "Epoch 00069: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4142 - accuracy: 0.8089 - val_loss: 0.4220 - val_accuracy: 0.8080\n",
      "Epoch 70/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8114\n",
      "Epoch 00070: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4065 - accuracy: 0.8112 - val_loss: 0.4090 - val_accuracy: 0.8119\n",
      "Epoch 71/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8145\n",
      "Epoch 00071: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4061 - accuracy: 0.8145 - val_loss: 0.4061 - val_accuracy: 0.8136\n",
      "Epoch 72/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4083 - accuracy: 0.8113\n",
      "Epoch 00072: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4090 - accuracy: 0.8110 - val_loss: 0.4153 - val_accuracy: 0.8057\n",
      "Epoch 73/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4073 - accuracy: 0.8122\n",
      "Epoch 00073: val_loss did not improve from 0.40599\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4078 - accuracy: 0.8120 - val_loss: 0.4084 - val_accuracy: 0.8156\n",
      "Epoch 74/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4027 - accuracy: 0.8133\n",
      "Epoch 00074: val_loss improved from 0.40599 to 0.40547, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4031 - accuracy: 0.8133 - val_loss: 0.4055 - val_accuracy: 0.8149\n",
      "Epoch 75/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4027 - accuracy: 0.8136 ETA: 0s - loss: 0.4028 - accuracy: 0.\n",
      "Epoch 00075: val_loss did not improve from 0.40547\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4027 - accuracy: 0.8136 - val_loss: 0.4111 - val_accuracy: 0.8089\n",
      "Epoch 76/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8167\n",
      "Epoch 00076: val_loss did not improve from 0.40547\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3997 - accuracy: 0.8166 - val_loss: 0.4065 - val_accuracy: 0.8171\n",
      "Epoch 77/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.8112\n",
      "Epoch 00077: val_loss did not improve from 0.40547\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3994 - accuracy: 0.8109 - val_loss: 0.4098 - val_accuracy: 0.8104\n",
      "Epoch 78/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3997 - accuracy: 0.8162\n",
      "Epoch 00078: val_loss improved from 0.40547 to 0.40309, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3995 - accuracy: 0.8162 - val_loss: 0.4031 - val_accuracy: 0.8130\n",
      "Epoch 79/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3966 - accuracy: 0.8185\n",
      "Epoch 00079: val_loss improved from 0.40309 to 0.39620, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3967 - accuracy: 0.8184 - val_loss: 0.3962 - val_accuracy: 0.8233\n",
      "Epoch 80/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8187\n",
      "Epoch 00080: val_loss did not improve from 0.39620\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3946 - accuracy: 0.8187 - val_loss: 0.4152 - val_accuracy: 0.8087\n",
      "Epoch 81/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.8197\n",
      "Epoch 00081: val_loss improved from 0.39620 to 0.39597, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3926 - accuracy: 0.8197 - val_loss: 0.3960 - val_accuracy: 0.8218\n",
      "Epoch 82/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8167\n",
      "Epoch 00082: val_loss did not improve from 0.39597\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3955 - accuracy: 0.8168 - val_loss: 0.4175 - val_accuracy: 0.8076\n",
      "Epoch 83/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8210\n",
      "Epoch 00083: val_loss did not improve from 0.39597\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3893 - accuracy: 0.8208 - val_loss: 0.4030 - val_accuracy: 0.8177\n",
      "Epoch 84/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3899 - accuracy: 0.8213\n",
      "Epoch 00084: val_loss did not improve from 0.39597\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3900 - accuracy: 0.8210 - val_loss: 0.4095 - val_accuracy: 0.8138\n",
      "Epoch 85/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8236\n",
      "Epoch 00085: val_loss did not improve from 0.39597\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3878 - accuracy: 0.8232 - val_loss: 0.3997 - val_accuracy: 0.8179\n",
      "Epoch 86/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3831 - accuracy: 0.8250\n",
      "Epoch 00086: val_loss did not improve from 0.39597\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3831 - accuracy: 0.8250 - val_loss: 0.4008 - val_accuracy: 0.8184\n",
      "Epoch 87/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8228\n",
      "Epoch 00087: val_loss improved from 0.39597 to 0.39454, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3835 - accuracy: 0.8226 - val_loss: 0.3945 - val_accuracy: 0.8244\n",
      "Epoch 88/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8235\n",
      "Epoch 00088: val_loss did not improve from 0.39454\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3876 - accuracy: 0.8234 - val_loss: 0.3948 - val_accuracy: 0.8233\n",
      "Epoch 89/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8247\n",
      "Epoch 00089: val_loss did not improve from 0.39454\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3831 - accuracy: 0.8247 - val_loss: 0.3999 - val_accuracy: 0.8141\n",
      "Epoch 90/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8272\n",
      "Epoch 00090: val_loss improved from 0.39454 to 0.39329, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3801 - accuracy: 0.8270 - val_loss: 0.3933 - val_accuracy: 0.8194\n",
      "Epoch 91/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3778 - accuracy: 0.8259\n",
      "Epoch 00091: val_loss did not improve from 0.39329\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3779 - accuracy: 0.8260 - val_loss: 0.4059 - val_accuracy: 0.8128\n",
      "Epoch 92/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8275\n",
      "Epoch 00092: val_loss did not improve from 0.39329\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3751 - accuracy: 0.8275 - val_loss: 0.3995 - val_accuracy: 0.8190\n",
      "Epoch 93/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3760 - accuracy: 0.8289\n",
      "Epoch 00093: val_loss did not improve from 0.39329\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3761 - accuracy: 0.8287 - val_loss: 0.4106 - val_accuracy: 0.8128\n",
      "Epoch 94/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3784 - accuracy: 0.8266\n",
      "Epoch 00094: val_loss did not improve from 0.39329\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3782 - accuracy: 0.8267 - val_loss: 0.3936 - val_accuracy: 0.8233\n",
      "Epoch 95/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8296\n",
      "Epoch 00095: val_loss improved from 0.39329 to 0.38634, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3752 - accuracy: 0.8297 - val_loss: 0.3863 - val_accuracy: 0.8257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8280\n",
      "Epoch 00096: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3768 - accuracy: 0.8279 - val_loss: 0.3995 - val_accuracy: 0.8143\n",
      "Epoch 97/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3668 - accuracy: 0.8344\n",
      "Epoch 00097: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3666 - accuracy: 0.8345 - val_loss: 0.3914 - val_accuracy: 0.8229\n",
      "Epoch 98/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8322 ETA: 1s -\n",
      "Epoch 00098: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3676 - accuracy: 0.8325 - val_loss: 0.3922 - val_accuracy: 0.8199\n",
      "Epoch 99/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8339\n",
      "Epoch 00099: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3668 - accuracy: 0.8335 - val_loss: 0.3879 - val_accuracy: 0.8267\n",
      "Epoch 100/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8332\n",
      "Epoch 00100: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3677 - accuracy: 0.8335 - val_loss: 0.3869 - val_accuracy: 0.8272\n",
      "Epoch 101/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3667 - accuracy: 0.8307\n",
      "Epoch 00101: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3667 - accuracy: 0.8309 - val_loss: 0.3899 - val_accuracy: 0.8244\n",
      "Epoch 102/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8312\n",
      "Epoch 00102: val_loss did not improve from 0.38634\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3644 - accuracy: 0.8311 - val_loss: 0.3868 - val_accuracy: 0.8289\n",
      "Epoch 103/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8363\n",
      "Epoch 00103: val_loss improved from 0.38634 to 0.38462, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3640 - accuracy: 0.8359 - val_loss: 0.3846 - val_accuracy: 0.8270\n",
      "Epoch 104/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8395\n",
      "Epoch 00104: val_loss did not improve from 0.38462\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3600 - accuracy: 0.8397 - val_loss: 0.3853 - val_accuracy: 0.8289\n",
      "Epoch 105/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8387\n",
      "Epoch 00105: val_loss did not improve from 0.38462\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3580 - accuracy: 0.8387 - val_loss: 0.3929 - val_accuracy: 0.8222\n",
      "Epoch 106/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8369\n",
      "Epoch 00106: val_loss improved from 0.38462 to 0.37885, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3629 - accuracy: 0.8369 - val_loss: 0.3789 - val_accuracy: 0.8321\n",
      "Epoch 107/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8385\n",
      "Epoch 00107: val_loss did not improve from 0.37885\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3543 - accuracy: 0.8383 - val_loss: 0.3985 - val_accuracy: 0.8194\n",
      "Epoch 108/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8391\n",
      "Epoch 00108: val_loss improved from 0.37885 to 0.37819, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3564 - accuracy: 0.8394 - val_loss: 0.3782 - val_accuracy: 0.8304\n",
      "Epoch 109/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8419\n",
      "Epoch 00109: val_loss did not improve from 0.37819\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3512 - accuracy: 0.8419 - val_loss: 0.3803 - val_accuracy: 0.8274\n",
      "Epoch 110/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8437\n",
      "Epoch 00110: val_loss did not improve from 0.37819\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3505 - accuracy: 0.8437 - val_loss: 0.3814 - val_accuracy: 0.8291\n",
      "Epoch 111/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8437\n",
      "Epoch 00111: val_loss improved from 0.37819 to 0.37811, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3488 - accuracy: 0.8438 - val_loss: 0.3781 - val_accuracy: 0.8306\n",
      "Epoch 112/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8405\n",
      "Epoch 00112: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3525 - accuracy: 0.8406 - val_loss: 0.3911 - val_accuracy: 0.8246\n",
      "Epoch 113/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8402\n",
      "Epoch 00113: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3517 - accuracy: 0.8400 - val_loss: 0.3893 - val_accuracy: 0.8272\n",
      "Epoch 114/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8488\n",
      "Epoch 00114: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3411 - accuracy: 0.8488 - val_loss: 0.3966 - val_accuracy: 0.8207\n",
      "Epoch 115/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3420 - accuracy: 0.8475\n",
      "Epoch 00115: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3414 - accuracy: 0.8479 - val_loss: 0.4025 - val_accuracy: 0.8186\n",
      "Epoch 116/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8428\n",
      "Epoch 00116: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3493 - accuracy: 0.8427 - val_loss: 0.3818 - val_accuracy: 0.8272\n",
      "Epoch 117/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8434\n",
      "Epoch 00117: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3427 - accuracy: 0.8434 - val_loss: 0.3949 - val_accuracy: 0.8235\n",
      "Epoch 118/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8467\n",
      "Epoch 00118: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3444 - accuracy: 0.8473 - val_loss: 0.3782 - val_accuracy: 0.8330\n",
      "Epoch 119/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3401 - accuracy: 0.8468\n",
      "Epoch 00119: val_loss did not improve from 0.37811\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3399 - accuracy: 0.8467 - val_loss: 0.3857 - val_accuracy: 0.8317\n",
      "Epoch 120/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8492\n",
      "Epoch 00120: val_loss improved from 0.37811 to 0.37417, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3417 - accuracy: 0.8491 - val_loss: 0.3742 - val_accuracy: 0.8340\n",
      "Epoch 121/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8493\n",
      "Epoch 00121: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3352 - accuracy: 0.8495 - val_loss: 0.3798 - val_accuracy: 0.8308\n",
      "Epoch 122/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8496\n",
      "Epoch 00122: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3379 - accuracy: 0.8493 - val_loss: 0.3827 - val_accuracy: 0.8282\n",
      "Epoch 123/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8502\n",
      "Epoch 00123: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3344 - accuracy: 0.8502 - val_loss: 0.3752 - val_accuracy: 0.8308\n",
      "Epoch 124/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8544\n",
      "Epoch 00124: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3320 - accuracy: 0.8544 - val_loss: 0.3748 - val_accuracy: 0.8313\n",
      "Epoch 125/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8537\n",
      "Epoch 00125: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3313 - accuracy: 0.8537 - val_loss: 0.3842 - val_accuracy: 0.8291\n",
      "Epoch 126/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8538\n",
      "Epoch 00126: val_loss did not improve from 0.37417\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3302 - accuracy: 0.8536 - val_loss: 0.3787 - val_accuracy: 0.8280\n",
      "Epoch 127/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8494\n",
      "Epoch 00127: val_loss improved from 0.37417 to 0.37259, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3315 - accuracy: 0.8494 - val_loss: 0.3726 - val_accuracy: 0.8321\n",
      "Epoch 128/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8570\n",
      "Epoch 00128: val_loss improved from 0.37259 to 0.37239, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3251 - accuracy: 0.8572 - val_loss: 0.3724 - val_accuracy: 0.8328\n",
      "Epoch 129/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8523\n",
      "Epoch 00129: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3289 - accuracy: 0.8523 - val_loss: 0.3764 - val_accuracy: 0.8345\n",
      "Epoch 130/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8535\n",
      "Epoch 00130: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3238 - accuracy: 0.8536 - val_loss: 0.3778 - val_accuracy: 0.8321\n",
      "Epoch 131/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8530\n",
      "Epoch 00131: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3271 - accuracy: 0.8528 - val_loss: 0.3833 - val_accuracy: 0.8287\n",
      "Epoch 132/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8555\n",
      "Epoch 00132: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3237 - accuracy: 0.8556 - val_loss: 0.3740 - val_accuracy: 0.8332\n",
      "Epoch 133/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8550\n",
      "Epoch 00133: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3275 - accuracy: 0.8551 - val_loss: 0.3888 - val_accuracy: 0.8231\n",
      "Epoch 134/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8586\n",
      "Epoch 00134: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3178 - accuracy: 0.8586 - val_loss: 0.3764 - val_accuracy: 0.8291\n",
      "Epoch 135/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8595\n",
      "Epoch 00135: val_loss did not improve from 0.37239\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3166 - accuracy: 0.8596 - val_loss: 0.3754 - val_accuracy: 0.8317\n",
      "Epoch 136/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8594\n",
      "Epoch 00136: val_loss improved from 0.37239 to 0.37117, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3215 - accuracy: 0.8595 - val_loss: 0.3712 - val_accuracy: 0.8362\n",
      "Epoch 137/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8579\n",
      "Epoch 00137: val_loss did not improve from 0.37117\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3198 - accuracy: 0.8581 - val_loss: 0.3785 - val_accuracy: 0.8328\n",
      "Epoch 138/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8597\n",
      "Epoch 00138: val_loss improved from 0.37117 to 0.36807, saving model to pickled_objects/batch_size_128_lr_0.04_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3157 - accuracy: 0.8599 - val_loss: 0.3681 - val_accuracy: 0.8371\n",
      "Epoch 139/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8629\n",
      "Epoch 00139: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3154 - accuracy: 0.8631 - val_loss: 0.3863 - val_accuracy: 0.8169\n",
      "Epoch 140/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8598\n",
      "Epoch 00140: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3191 - accuracy: 0.8598 - val_loss: 0.3796 - val_accuracy: 0.8310\n",
      "Epoch 141/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8630\n",
      "Epoch 00141: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3099 - accuracy: 0.8632 - val_loss: 0.3893 - val_accuracy: 0.8287\n",
      "Epoch 142/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8601\n",
      "Epoch 00142: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3140 - accuracy: 0.8602 - val_loss: 0.3793 - val_accuracy: 0.8212\n",
      "Epoch 143/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8648\n",
      "Epoch 00143: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3099 - accuracy: 0.8648 - val_loss: 0.3752 - val_accuracy: 0.8304\n",
      "Epoch 144/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8651\n",
      "Epoch 00144: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3085 - accuracy: 0.8651 - val_loss: 0.3817 - val_accuracy: 0.8267\n",
      "Epoch 145/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8643\n",
      "Epoch 00145: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3071 - accuracy: 0.8644 - val_loss: 0.3750 - val_accuracy: 0.8321\n",
      "Epoch 146/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8656\n",
      "Epoch 00146: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3059 - accuracy: 0.8658 - val_loss: 0.3884 - val_accuracy: 0.8196\n",
      "Epoch 147/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8680\n",
      "Epoch 00147: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3048 - accuracy: 0.8680 - val_loss: 0.3730 - val_accuracy: 0.8353\n",
      "Epoch 148/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8664\n",
      "Epoch 00148: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3053 - accuracy: 0.8666 - val_loss: 0.3702 - val_accuracy: 0.8336\n",
      "Epoch 149/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8689\n",
      "Epoch 00149: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3018 - accuracy: 0.8690 - val_loss: 0.3876 - val_accuracy: 0.8218\n",
      "Epoch 150/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8670\n",
      "Epoch 00150: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3010 - accuracy: 0.8672 - val_loss: 0.3766 - val_accuracy: 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8659\n",
      "Epoch 00151: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3003 - accuracy: 0.8660 - val_loss: 0.3823 - val_accuracy: 0.8319\n",
      "Epoch 152/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8708\n",
      "Epoch 00152: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2972 - accuracy: 0.8709 - val_loss: 0.3768 - val_accuracy: 0.8310\n",
      "Epoch 153/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8701\n",
      "Epoch 00153: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2957 - accuracy: 0.8702 - val_loss: 0.3727 - val_accuracy: 0.8308\n",
      "Epoch 154/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8740\n",
      "Epoch 00154: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2949 - accuracy: 0.8740 - val_loss: 0.3833 - val_accuracy: 0.8250\n",
      "Epoch 155/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8696\n",
      "Epoch 00155: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2971 - accuracy: 0.8695 - val_loss: 0.3800 - val_accuracy: 0.8310\n",
      "Epoch 156/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8724\n",
      "Epoch 00156: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2937 - accuracy: 0.8724 - val_loss: 0.3904 - val_accuracy: 0.8285\n",
      "Epoch 157/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8730\n",
      "Epoch 00157: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2901 - accuracy: 0.8731 - val_loss: 0.3831 - val_accuracy: 0.8218\n",
      "Epoch 158/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8735\n",
      "Epoch 00158: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2898 - accuracy: 0.8735 - val_loss: 0.3964 - val_accuracy: 0.8143\n",
      "Epoch 159/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8727\n",
      "Epoch 00159: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2937 - accuracy: 0.8732 - val_loss: 0.3809 - val_accuracy: 0.8274\n",
      "Epoch 160/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8740\n",
      "Epoch 00160: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2902 - accuracy: 0.8740 - val_loss: 0.3890 - val_accuracy: 0.8207\n",
      "Epoch 161/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8760 ETA: 0s - loss: 0.2850 - accura\n",
      "Epoch 00161: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2853 - accuracy: 0.8761 - val_loss: 0.3829 - val_accuracy: 0.8280\n",
      "Epoch 162/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.8756\n",
      "Epoch 00162: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2840 - accuracy: 0.8756 - val_loss: 0.3713 - val_accuracy: 0.8293\n",
      "Epoch 163/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8748\n",
      "Epoch 00163: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2895 - accuracy: 0.8749 - val_loss: 0.3766 - val_accuracy: 0.8306\n",
      "Epoch 164/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.8761\n",
      "Epoch 00164: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2819 - accuracy: 0.8762 - val_loss: 0.3805 - val_accuracy: 0.8263\n",
      "Epoch 165/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8776\n",
      "Epoch 00165: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2833 - accuracy: 0.8776 - val_loss: 0.3838 - val_accuracy: 0.8246\n",
      "Epoch 166/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.8717\n",
      "Epoch 00166: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2891 - accuracy: 0.8720 - val_loss: 0.3784 - val_accuracy: 0.8300\n",
      "Epoch 167/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8803\n",
      "Epoch 00167: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2788 - accuracy: 0.8803 - val_loss: 0.3760 - val_accuracy: 0.8304\n",
      "Epoch 168/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8804\n",
      "Epoch 00168: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2803 - accuracy: 0.8807 - val_loss: 0.3960 - val_accuracy: 0.8216\n",
      "Epoch 169/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.8785\n",
      "Epoch 00169: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2749 - accuracy: 0.8787 - val_loss: 0.4207 - val_accuracy: 0.8119\n",
      "Epoch 170/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8805\n",
      "Epoch 00170: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2787 - accuracy: 0.8805 - val_loss: 0.3808 - val_accuracy: 0.8308\n",
      "Epoch 171/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2741 - accuracy: 0.8807\n",
      "Epoch 00171: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2735 - accuracy: 0.8809 - val_loss: 0.3863 - val_accuracy: 0.8263\n",
      "Epoch 172/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2762 - accuracy: 0.8803\n",
      "Epoch 00172: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2759 - accuracy: 0.8804 - val_loss: 0.3789 - val_accuracy: 0.8308\n",
      "Epoch 173/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2763 - accuracy: 0.8821\n",
      "Epoch 00173: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2760 - accuracy: 0.8822 - val_loss: 0.3919 - val_accuracy: 0.8274\n",
      "Epoch 174/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8835\n",
      "Epoch 00174: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2693 - accuracy: 0.8837 - val_loss: 0.3833 - val_accuracy: 0.8293\n",
      "Epoch 175/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8838\n",
      "Epoch 00175: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2714 - accuracy: 0.8838 - val_loss: 0.3885 - val_accuracy: 0.8282\n",
      "Epoch 176/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.8833\n",
      "Epoch 00176: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2680 - accuracy: 0.8834 - val_loss: 0.3846 - val_accuracy: 0.8242\n",
      "Epoch 177/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8831\n",
      "Epoch 00177: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2669 - accuracy: 0.8832 - val_loss: 0.3798 - val_accuracy: 0.8276\n",
      "Epoch 178/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.8851\n",
      "Epoch 00178: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2666 - accuracy: 0.8851 - val_loss: 0.3763 - val_accuracy: 0.8298\n",
      "Epoch 179/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2651 - accuracy: 0.8862\n",
      "Epoch 00179: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2647 - accuracy: 0.8862 - val_loss: 0.3834 - val_accuracy: 0.8270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.8864\n",
      "Epoch 00180: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2639 - accuracy: 0.8863 - val_loss: 0.3751 - val_accuracy: 0.8325\n",
      "Epoch 181/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.8893\n",
      "Epoch 00181: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2663 - accuracy: 0.8894 - val_loss: 0.3932 - val_accuracy: 0.8233\n",
      "Epoch 182/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8862\n",
      "Epoch 00182: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2636 - accuracy: 0.8864 - val_loss: 0.3773 - val_accuracy: 0.8304\n",
      "Epoch 183/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.8854\n",
      "Epoch 00183: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2639 - accuracy: 0.8859 - val_loss: 0.3824 - val_accuracy: 0.8304\n",
      "Epoch 184/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.8867\n",
      "Epoch 00184: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2612 - accuracy: 0.8867 - val_loss: 0.3820 - val_accuracy: 0.8336\n",
      "Epoch 185/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.8880\n",
      "Epoch 00185: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2609 - accuracy: 0.8884 - val_loss: 0.3846 - val_accuracy: 0.8330\n",
      "Epoch 186/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.8900\n",
      "Epoch 00186: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2579 - accuracy: 0.8902 - val_loss: 0.3812 - val_accuracy: 0.8313\n",
      "Epoch 187/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8873\n",
      "Epoch 00187: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2598 - accuracy: 0.8875 - val_loss: 0.3959 - val_accuracy: 0.8291\n",
      "Epoch 188/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.8902\n",
      "Epoch 00188: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2541 - accuracy: 0.8904 - val_loss: 0.3868 - val_accuracy: 0.8295\n",
      "Epoch 189/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8919\n",
      "Epoch 00189: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2575 - accuracy: 0.8919 - val_loss: 0.3898 - val_accuracy: 0.8298\n",
      "Epoch 190/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.8926\n",
      "Epoch 00190: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2515 - accuracy: 0.8928 - val_loss: 0.3857 - val_accuracy: 0.8332\n",
      "Epoch 191/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.8951 ETA: 0s - loss: 0.2510 - \n",
      "Epoch 00191: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2505 - accuracy: 0.8954 - val_loss: 0.3983 - val_accuracy: 0.8212\n",
      "Epoch 192/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.8912\n",
      "Epoch 00192: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2524 - accuracy: 0.8913 - val_loss: 0.3852 - val_accuracy: 0.8330\n",
      "Epoch 193/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.8938\n",
      "Epoch 00193: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2519 - accuracy: 0.8939 - val_loss: 0.3895 - val_accuracy: 0.8274\n",
      "Epoch 194/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.8946\n",
      "Epoch 00194: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2461 - accuracy: 0.8946 - val_loss: 0.3874 - val_accuracy: 0.8300\n",
      "Epoch 195/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.8973\n",
      "Epoch 00195: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2463 - accuracy: 0.8973 - val_loss: 0.3967 - val_accuracy: 0.8212\n",
      "Epoch 196/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.8917\n",
      "Epoch 00196: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2515 - accuracy: 0.8917 - val_loss: 0.3860 - val_accuracy: 0.8270\n",
      "Epoch 197/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.8943\n",
      "Epoch 00197: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2485 - accuracy: 0.8946 - val_loss: 0.4015 - val_accuracy: 0.8220\n",
      "Epoch 198/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.8956\n",
      "Epoch 00198: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2416 - accuracy: 0.8959 - val_loss: 0.3859 - val_accuracy: 0.8313\n",
      "Epoch 199/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.8953\n",
      "Epoch 00199: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2428 - accuracy: 0.8953 - val_loss: 0.3787 - val_accuracy: 0.8336\n",
      "Epoch 200/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.8967\n",
      "Epoch 00200: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2407 - accuracy: 0.8968 - val_loss: 0.3972 - val_accuracy: 0.8255\n",
      "Epoch 201/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.8971\n",
      "Epoch 00201: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2447 - accuracy: 0.8973 - val_loss: 0.3857 - val_accuracy: 0.8332\n",
      "Epoch 202/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.8966\n",
      "Epoch 00202: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2440 - accuracy: 0.8966 - val_loss: 0.3843 - val_accuracy: 0.8315\n",
      "Epoch 203/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2378 - accuracy: 0.9007\n",
      "Epoch 00203: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2374 - accuracy: 0.9008 - val_loss: 0.3916 - val_accuracy: 0.8274\n",
      "Epoch 204/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2406 - accuracy: 0.8975\n",
      "Epoch 00204: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2404 - accuracy: 0.8975 - val_loss: 0.3973 - val_accuracy: 0.8267\n",
      "Epoch 205/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.8975\n",
      "Epoch 00205: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2375 - accuracy: 0.8977 - val_loss: 0.3933 - val_accuracy: 0.8289\n",
      "Epoch 206/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.8984\n",
      "Epoch 00206: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2367 - accuracy: 0.8986 - val_loss: 0.4091 - val_accuracy: 0.8173\n",
      "Epoch 207/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.8994\n",
      "Epoch 00207: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2363 - accuracy: 0.8997 - val_loss: 0.4037 - val_accuracy: 0.8242\n",
      "Epoch 208/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.8979\n",
      "Epoch 00208: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2374 - accuracy: 0.8981 - val_loss: 0.4016 - val_accuracy: 0.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9036\n",
      "Epoch 00209: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2290 - accuracy: 0.9036 - val_loss: 0.4078 - val_accuracy: 0.8242\n",
      "Epoch 210/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2310 - accuracy: 0.9023\n",
      "Epoch 00210: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2303 - accuracy: 0.9024 - val_loss: 0.3831 - val_accuracy: 0.8396\n",
      "Epoch 211/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9011\n",
      "Epoch 00211: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2335 - accuracy: 0.9014 - val_loss: 0.3923 - val_accuracy: 0.8255\n",
      "Epoch 212/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2265 - accuracy: 0.9046\n",
      "Epoch 00212: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2257 - accuracy: 0.9048 - val_loss: 0.3942 - val_accuracy: 0.8340\n",
      "Epoch 213/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9012\n",
      "Epoch 00213: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2330 - accuracy: 0.9016 - val_loss: 0.3939 - val_accuracy: 0.8259\n",
      "Epoch 214/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9052\n",
      "Epoch 00214: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2237 - accuracy: 0.9054 - val_loss: 0.4106 - val_accuracy: 0.8242\n",
      "Epoch 215/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9013\n",
      "Epoch 00215: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2323 - accuracy: 0.9013 - val_loss: 0.4045 - val_accuracy: 0.8259\n",
      "Epoch 216/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9018\n",
      "Epoch 00216: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2284 - accuracy: 0.9023 - val_loss: 0.4063 - val_accuracy: 0.8229\n",
      "Epoch 217/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9065\n",
      "Epoch 00217: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2224 - accuracy: 0.9068 - val_loss: 0.4048 - val_accuracy: 0.8252\n",
      "Epoch 218/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9062\n",
      "Epoch 00218: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2242 - accuracy: 0.9063 - val_loss: 0.4136 - val_accuracy: 0.8138\n",
      "Epoch 219/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9094\n",
      "Epoch 00219: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2177 - accuracy: 0.9096 - val_loss: 0.3914 - val_accuracy: 0.8319\n",
      "Epoch 220/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2215 - accuracy: 0.9080\n",
      "Epoch 00220: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2210 - accuracy: 0.9082 - val_loss: 0.3946 - val_accuracy: 0.8285\n",
      "Epoch 221/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9061\n",
      "Epoch 00221: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2235 - accuracy: 0.9063 - val_loss: 0.4209 - val_accuracy: 0.8169\n",
      "Epoch 222/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9086\n",
      "Epoch 00222: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2226 - accuracy: 0.9087 - val_loss: 0.3979 - val_accuracy: 0.8261\n",
      "Epoch 223/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2200 - accuracy: 0.9093\n",
      "Epoch 00223: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2191 - accuracy: 0.9095 - val_loss: 0.4054 - val_accuracy: 0.8259\n",
      "Epoch 224/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.9114\n",
      "Epoch 00224: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2119 - accuracy: 0.9116 - val_loss: 0.3906 - val_accuracy: 0.8317\n",
      "Epoch 225/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9091\n",
      "Epoch 00225: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2155 - accuracy: 0.9092 - val_loss: 0.3942 - val_accuracy: 0.8289\n",
      "Epoch 226/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9081\n",
      "Epoch 00226: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2170 - accuracy: 0.9083 - val_loss: 0.4166 - val_accuracy: 0.8222\n",
      "Epoch 227/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9117\n",
      "Epoch 00227: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2099 - accuracy: 0.9119 - val_loss: 0.4026 - val_accuracy: 0.8319\n",
      "Epoch 228/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9157\n",
      "Epoch 00228: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2061 - accuracy: 0.9159 - val_loss: 0.4349 - val_accuracy: 0.8128\n",
      "Epoch 229/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9089\n",
      "Epoch 00229: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2144 - accuracy: 0.9089 - val_loss: 0.4050 - val_accuracy: 0.8306\n",
      "Epoch 230/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9098\n",
      "Epoch 00230: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2120 - accuracy: 0.9099 - val_loss: 0.4051 - val_accuracy: 0.8300\n",
      "Epoch 231/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9131\n",
      "Epoch 00231: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2086 - accuracy: 0.9132 - val_loss: 0.4051 - val_accuracy: 0.8278\n",
      "Epoch 232/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9137\n",
      "Epoch 00232: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2049 - accuracy: 0.9139 - val_loss: 0.4021 - val_accuracy: 0.8289\n",
      "Epoch 233/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2064 - accuracy: 0.9125\n",
      "Epoch 00233: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2058 - accuracy: 0.9127 - val_loss: 0.4059 - val_accuracy: 0.8291\n",
      "Epoch 234/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9142\n",
      "Epoch 00234: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2044 - accuracy: 0.9144 - val_loss: 0.4072 - val_accuracy: 0.8298\n",
      "Epoch 235/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2097 - accuracy: 0.9140\n",
      "Epoch 00235: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2087 - accuracy: 0.9142 - val_loss: 0.4077 - val_accuracy: 0.8315\n",
      "Epoch 236/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9158\n",
      "Epoch 00236: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2041 - accuracy: 0.9160 - val_loss: 0.4077 - val_accuracy: 0.8261\n",
      "Epoch 237/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9116\n",
      "Epoch 00237: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2060 - accuracy: 0.9118 - val_loss: 0.3983 - val_accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9172\n",
      "Epoch 00238: val_loss did not improve from 0.36807\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2029 - accuracy: 0.9173 - val_loss: 0.4131 - val_accuracy: 0.8313\n",
      "Epoch 00238: early stopping\n",
      "Epoch 1/10000\n",
      "    146/Unknown - 9s 59ms/step - loss: 0.6897 - accuracy: 0.5386\n",
      "Epoch 00001: val_loss improved from inf to 0.68772, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 65ms/step - loss: 0.6897 - accuracy: 0.5386 - val_loss: 0.6877 - val_accuracy: 0.5312\n",
      "Epoch 2/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5680\n",
      "Epoch 00002: val_loss improved from 0.68772 to 0.67178, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6808 - accuracy: 0.5680 - val_loss: 0.6718 - val_accuracy: 0.6150\n",
      "Epoch 3/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6767 - accuracy: 0.5779\n",
      "Epoch 00003: val_loss improved from 0.67178 to 0.66711, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6766 - accuracy: 0.5781 - val_loss: 0.6671 - val_accuracy: 0.6167\n",
      "Epoch 4/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6696 - accuracy: 0.5903\n",
      "Epoch 00004: val_loss improved from 0.66711 to 0.65854, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6695 - accuracy: 0.5901 - val_loss: 0.6585 - val_accuracy: 0.6238\n",
      "Epoch 5/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6646 - accuracy: 0.5972\n",
      "Epoch 00005: val_loss improved from 0.65854 to 0.64743, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6643 - accuracy: 0.5972 - val_loss: 0.6474 - val_accuracy: 0.6445\n",
      "Epoch 6/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6553 - accuracy: 0.6087\n",
      "Epoch 00006: val_loss improved from 0.64743 to 0.64014, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6553 - accuracy: 0.6085 - val_loss: 0.6401 - val_accuracy: 0.6410\n",
      "Epoch 7/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6418 - accuracy: 0.6298\n",
      "Epoch 00007: val_loss improved from 0.64014 to 0.62570, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6416 - accuracy: 0.6300 - val_loss: 0.6257 - val_accuracy: 0.6586\n",
      "Epoch 8/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6257 - accuracy: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.62570\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6257 - accuracy: 0.6471 - val_loss: 0.6565 - val_accuracy: 0.5995\n",
      "Epoch 9/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.6508\n",
      "Epoch 00009: val_loss improved from 0.62570 to 0.61288, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6230 - accuracy: 0.6506 - val_loss: 0.6129 - val_accuracy: 0.6651\n",
      "Epoch 10/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.6763\n",
      "Epoch 00010: val_loss improved from 0.61288 to 0.58720, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6000 - accuracy: 0.6760 - val_loss: 0.5872 - val_accuracy: 0.6980\n",
      "Epoch 11/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5842 - accuracy: 0.6896\n",
      "Epoch 00011: val_loss improved from 0.58720 to 0.57743, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5848 - accuracy: 0.6893 - val_loss: 0.5774 - val_accuracy: 0.7070\n",
      "Epoch 12/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7001\n",
      "Epoch 00012: val_loss improved from 0.57743 to 0.55112, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5701 - accuracy: 0.7000 - val_loss: 0.5511 - val_accuracy: 0.7287\n",
      "Epoch 13/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7165\n",
      "Epoch 00013: val_loss improved from 0.55112 to 0.53618, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5515 - accuracy: 0.7161 - val_loss: 0.5362 - val_accuracy: 0.7483\n",
      "Epoch 14/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7247\n",
      "Epoch 00014: val_loss improved from 0.53618 to 0.51309, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5396 - accuracy: 0.7244 - val_loss: 0.5131 - val_accuracy: 0.7532\n",
      "Epoch 15/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7387\n",
      "Epoch 00015: val_loss improved from 0.51309 to 0.50717, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5263 - accuracy: 0.7386 - val_loss: 0.5072 - val_accuracy: 0.7582\n",
      "Epoch 16/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5163 - accuracy: 0.7398\n",
      "Epoch 00016: val_loss improved from 0.50717 to 0.49181, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5165 - accuracy: 0.7398 - val_loss: 0.4918 - val_accuracy: 0.7650\n",
      "Epoch 17/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7499\n",
      "Epoch 00017: val_loss improved from 0.49181 to 0.48768, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5090 - accuracy: 0.7497 - val_loss: 0.4877 - val_accuracy: 0.7674\n",
      "Epoch 18/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5005 - accuracy: 0.7538\n",
      "Epoch 00018: val_loss did not improve from 0.48768\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5016 - accuracy: 0.7534 - val_loss: 0.5046 - val_accuracy: 0.7552\n",
      "Epoch 19/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4983 - accuracy: 0.7551\n",
      "Epoch 00019: val_loss did not improve from 0.48768\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4988 - accuracy: 0.7548 - val_loss: 0.5072 - val_accuracy: 0.7515\n",
      "Epoch 20/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4919 - accuracy: 0.7600\n",
      "Epoch 00020: val_loss improved from 0.48768 to 0.47003, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4927 - accuracy: 0.7596 - val_loss: 0.4700 - val_accuracy: 0.7773\n",
      "Epoch 21/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4864 - accuracy: 0.7639\n",
      "Epoch 00021: val_loss did not improve from 0.47003\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4872 - accuracy: 0.7634 - val_loss: 0.4724 - val_accuracy: 0.7754\n",
      "Epoch 22/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.7684\n",
      "Epoch 00022: val_loss did not improve from 0.47003\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4832 - accuracy: 0.7681 - val_loss: 0.4846 - val_accuracy: 0.7642\n",
      "Epoch 23/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7691\n",
      "Epoch 00023: val_loss improved from 0.47003 to 0.46387, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4776 - accuracy: 0.7689 - val_loss: 0.4639 - val_accuracy: 0.7840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.7712\n",
      "Epoch 00024: val_loss improved from 0.46387 to 0.45196, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4726 - accuracy: 0.7710 - val_loss: 0.4520 - val_accuracy: 0.7857\n",
      "Epoch 25/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7770\n",
      "Epoch 00025: val_loss did not improve from 0.45196\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4670 - accuracy: 0.7765 - val_loss: 0.5047 - val_accuracy: 0.7526\n",
      "Epoch 26/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4647 - accuracy: 0.7771\n",
      "Epoch 00026: val_loss did not improve from 0.45196\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4651 - accuracy: 0.7771 - val_loss: 0.4682 - val_accuracy: 0.7698\n",
      "Epoch 27/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7818\n",
      "Epoch 00027: val_loss did not improve from 0.45196\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4578 - accuracy: 0.7818 - val_loss: 0.4765 - val_accuracy: 0.7713\n",
      "Epoch 28/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4562 - accuracy: 0.7845\n",
      "Epoch 00028: val_loss improved from 0.45196 to 0.44088, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4564 - accuracy: 0.7843 - val_loss: 0.4409 - val_accuracy: 0.7874\n",
      "Epoch 29/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4542 - accuracy: 0.7857\n",
      "Epoch 00029: val_loss improved from 0.44088 to 0.43004, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4545 - accuracy: 0.7857 - val_loss: 0.4300 - val_accuracy: 0.7999\n",
      "Epoch 30/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4476 - accuracy: 0.7857\n",
      "Epoch 00030: val_loss did not improve from 0.43004\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4480 - accuracy: 0.7853 - val_loss: 0.4536 - val_accuracy: 0.7853\n",
      "Epoch 31/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4460 - accuracy: 0.7890\n",
      "Epoch 00031: val_loss did not improve from 0.43004\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4459 - accuracy: 0.7890 - val_loss: 0.4375 - val_accuracy: 0.7988\n",
      "Epoch 32/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4408 - accuracy: 0.7915\n",
      "Epoch 00032: val_loss improved from 0.43004 to 0.42745, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4411 - accuracy: 0.7913 - val_loss: 0.4275 - val_accuracy: 0.8029\n",
      "Epoch 33/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4399 - accuracy: 0.7944\n",
      "Epoch 00033: val_loss improved from 0.42745 to 0.42440, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4401 - accuracy: 0.7938 - val_loss: 0.4244 - val_accuracy: 0.8037\n",
      "Epoch 34/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4327 - accuracy: 0.7948\n",
      "Epoch 00034: val_loss did not improve from 0.42440\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4328 - accuracy: 0.7948 - val_loss: 0.4338 - val_accuracy: 0.7915\n",
      "Epoch 35/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.7961\n",
      "Epoch 00035: val_loss improved from 0.42440 to 0.41770, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4325 - accuracy: 0.7955 - val_loss: 0.4177 - val_accuracy: 0.8061\n",
      "Epoch 36/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.7949\n",
      "Epoch 00036: val_loss did not improve from 0.41770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4309 - accuracy: 0.7948 - val_loss: 0.4324 - val_accuracy: 0.7956\n",
      "Epoch 37/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8027\n",
      "Epoch 00037: val_loss did not improve from 0.41770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4255 - accuracy: 0.8028 - val_loss: 0.4334 - val_accuracy: 0.7917\n",
      "Epoch 38/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8016\n",
      "Epoch 00038: val_loss did not improve from 0.41770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4232 - accuracy: 0.8016 - val_loss: 0.4316 - val_accuracy: 0.7971\n",
      "Epoch 39/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4180 - accuracy: 0.8030\n",
      "Epoch 00039: val_loss did not improve from 0.41770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4175 - accuracy: 0.8030 - val_loss: 0.4212 - val_accuracy: 0.8018\n",
      "Epoch 40/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8055\n",
      "Epoch 00040: val_loss did not improve from 0.41770\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4171 - accuracy: 0.8055 - val_loss: 0.4440 - val_accuracy: 0.7859\n",
      "Epoch 41/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8084\n",
      "Epoch 00041: val_loss improved from 0.41770 to 0.41138, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4122 - accuracy: 0.8084 - val_loss: 0.4114 - val_accuracy: 0.8078\n",
      "Epoch 42/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4090 - accuracy: 0.8070\n",
      "Epoch 00042: val_loss improved from 0.41138 to 0.40823, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4087 - accuracy: 0.8072 - val_loss: 0.4082 - val_accuracy: 0.8070\n",
      "Epoch 43/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4085 - accuracy: 0.8094\n",
      "Epoch 00043: val_loss did not improve from 0.40823\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4086 - accuracy: 0.8095 - val_loss: 0.4135 - val_accuracy: 0.8042\n",
      "Epoch 44/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8110\n",
      "Epoch 00044: val_loss did not improve from 0.40823\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4077 - accuracy: 0.8113 - val_loss: 0.4091 - val_accuracy: 0.8089\n",
      "Epoch 45/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3984 - accuracy: 0.8147\n",
      "Epoch 00045: val_loss improved from 0.40823 to 0.40455, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3985 - accuracy: 0.8147 - val_loss: 0.4045 - val_accuracy: 0.8126\n",
      "Epoch 46/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8201\n",
      "Epoch 00046: val_loss improved from 0.40455 to 0.40343, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3966 - accuracy: 0.8200 - val_loss: 0.4034 - val_accuracy: 0.8093\n",
      "Epoch 47/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8176\n",
      "Epoch 00047: val_loss improved from 0.40343 to 0.39313, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3974 - accuracy: 0.8177 - val_loss: 0.3931 - val_accuracy: 0.8158\n",
      "Epoch 48/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8206\n",
      "Epoch 00048: val_loss did not improve from 0.39313\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3933 - accuracy: 0.8206 - val_loss: 0.4010 - val_accuracy: 0.8098\n",
      "Epoch 49/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8212\n",
      "Epoch 00049: val_loss did not improve from 0.39313\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3916 - accuracy: 0.8215 - val_loss: 0.3932 - val_accuracy: 0.8132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3902 - accuracy: 0.8200\n",
      "Epoch 00050: val_loss improved from 0.39313 to 0.38845, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3903 - accuracy: 0.8200 - val_loss: 0.3884 - val_accuracy: 0.8220\n",
      "Epoch 51/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8246\n",
      "Epoch 00051: val_loss improved from 0.38845 to 0.38572, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3844 - accuracy: 0.8244 - val_loss: 0.3857 - val_accuracy: 0.8207\n",
      "Epoch 52/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8279\n",
      "Epoch 00052: val_loss did not improve from 0.38572\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3803 - accuracy: 0.8282 - val_loss: 0.3892 - val_accuracy: 0.8179\n",
      "Epoch 53/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8251\n",
      "Epoch 00053: val_loss improved from 0.38572 to 0.38552, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3824 - accuracy: 0.8251 - val_loss: 0.3855 - val_accuracy: 0.8212\n",
      "Epoch 54/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.8264\n",
      "Epoch 00054: val_loss did not improve from 0.38552\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3794 - accuracy: 0.8265 - val_loss: 0.4010 - val_accuracy: 0.8121\n",
      "Epoch 55/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8311\n",
      "Epoch 00055: val_loss improved from 0.38552 to 0.37990, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3743 - accuracy: 0.8312 - val_loss: 0.3799 - val_accuracy: 0.8239\n",
      "Epoch 56/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3743 - accuracy: 0.8289\n",
      "Epoch 00056: val_loss did not improve from 0.37990\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3736 - accuracy: 0.8291 - val_loss: 0.3844 - val_accuracy: 0.8201\n",
      "Epoch 57/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3711 - accuracy: 0.8294\n",
      "Epoch 00057: val_loss did not improve from 0.37990\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3711 - accuracy: 0.8293 - val_loss: 0.3924 - val_accuracy: 0.8214\n",
      "Epoch 58/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8333\n",
      "Epoch 00058: val_loss did not improve from 0.37990\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3663 - accuracy: 0.8333 - val_loss: 0.3809 - val_accuracy: 0.8274\n",
      "Epoch 59/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8320\n",
      "Epoch 00059: val_loss did not improve from 0.37990\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3677 - accuracy: 0.8322 - val_loss: 0.3865 - val_accuracy: 0.8156\n",
      "Epoch 60/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3620 - accuracy: 0.8332\n",
      "Epoch 00060: val_loss did not improve from 0.37990\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3616 - accuracy: 0.8334 - val_loss: 0.3820 - val_accuracy: 0.8222\n",
      "Epoch 61/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3618 - accuracy: 0.8336\n",
      "Epoch 00061: val_loss improved from 0.37990 to 0.37840, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3618 - accuracy: 0.8337 - val_loss: 0.3784 - val_accuracy: 0.8244\n",
      "Epoch 62/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8376\n",
      "Epoch 00062: val_loss improved from 0.37840 to 0.37579, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3596 - accuracy: 0.8377 - val_loss: 0.3758 - val_accuracy: 0.8267\n",
      "Epoch 63/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8406\n",
      "Epoch 00063: val_loss did not improve from 0.37579\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3537 - accuracy: 0.8407 - val_loss: 0.3891 - val_accuracy: 0.8216\n",
      "Epoch 64/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3517 - accuracy: 0.8428\n",
      "Epoch 00064: val_loss improved from 0.37579 to 0.36250, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3509 - accuracy: 0.8430 - val_loss: 0.3625 - val_accuracy: 0.8302\n",
      "Epoch 65/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8428\n",
      "Epoch 00065: val_loss did not improve from 0.36250\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3483 - accuracy: 0.8421 - val_loss: 0.3871 - val_accuracy: 0.8235\n",
      "Epoch 66/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8394\n",
      "Epoch 00066: val_loss did not improve from 0.36250\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3506 - accuracy: 0.8396 - val_loss: 0.3810 - val_accuracy: 0.8265\n",
      "Epoch 67/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8459\n",
      "Epoch 00067: val_loss did not improve from 0.36250\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3459 - accuracy: 0.8459 - val_loss: 0.3699 - val_accuracy: 0.8285\n",
      "Epoch 68/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3459 - accuracy: 0.8483\n",
      "Epoch 00068: val_loss did not improve from 0.36250\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3453 - accuracy: 0.8484 - val_loss: 0.3663 - val_accuracy: 0.8336\n",
      "Epoch 69/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.8436\n",
      "Epoch 00069: val_loss improved from 0.36250 to 0.36230, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3470 - accuracy: 0.8437 - val_loss: 0.3623 - val_accuracy: 0.8298\n",
      "Epoch 70/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8448\n",
      "Epoch 00070: val_loss did not improve from 0.36230\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3409 - accuracy: 0.8452 - val_loss: 0.3684 - val_accuracy: 0.8319\n",
      "Epoch 71/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.8518\n",
      "Epoch 00071: val_loss did not improve from 0.36230\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3367 - accuracy: 0.8517 - val_loss: 0.3626 - val_accuracy: 0.8276\n",
      "Epoch 72/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3389 - accuracy: 0.8482 E\n",
      "Epoch 00072: val_loss did not improve from 0.36230\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3389 - accuracy: 0.8481 - val_loss: 0.4036 - val_accuracy: 0.8110\n",
      "Epoch 73/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8485\n",
      "Epoch 00073: val_loss did not improve from 0.36230\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3370 - accuracy: 0.8484 - val_loss: 0.3983 - val_accuracy: 0.8138\n",
      "Epoch 74/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.8513\n",
      "Epoch 00074: val_loss improved from 0.36230 to 0.35950, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3314 - accuracy: 0.8515 - val_loss: 0.3595 - val_accuracy: 0.8358\n",
      "Epoch 75/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8525\n",
      "Epoch 00075: val_loss did not improve from 0.35950\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3307 - accuracy: 0.8528 - val_loss: 0.3787 - val_accuracy: 0.8257\n",
      "Epoch 76/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8564\n",
      "Epoch 00076: val_loss did not improve from 0.35950\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3246 - accuracy: 0.8565 - val_loss: 0.3713 - val_accuracy: 0.8321\n",
      "Epoch 77/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8549\n",
      "Epoch 00077: val_loss improved from 0.35950 to 0.35894, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3253 - accuracy: 0.8551 - val_loss: 0.3589 - val_accuracy: 0.8362\n",
      "Epoch 78/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8542\n",
      "Epoch 00078: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3254 - accuracy: 0.8542 - val_loss: 0.3729 - val_accuracy: 0.8287\n",
      "Epoch 79/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8546\n",
      "Epoch 00079: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3253 - accuracy: 0.8546 - val_loss: 0.3768 - val_accuracy: 0.8291\n",
      "Epoch 80/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8596\n",
      "Epoch 00080: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3183 - accuracy: 0.8596 - val_loss: 0.3835 - val_accuracy: 0.8207\n",
      "Epoch 81/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8605\n",
      "Epoch 00081: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3193 - accuracy: 0.8607 - val_loss: 0.3666 - val_accuracy: 0.8317\n",
      "Epoch 82/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8583\n",
      "Epoch 00082: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3194 - accuracy: 0.8584 - val_loss: 0.3842 - val_accuracy: 0.8272\n",
      "Epoch 83/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8616\n",
      "Epoch 00083: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3098 - accuracy: 0.8617 - val_loss: 0.3658 - val_accuracy: 0.8302\n",
      "Epoch 84/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8602\n",
      "Epoch 00084: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3134 - accuracy: 0.8603 - val_loss: 0.3814 - val_accuracy: 0.8265\n",
      "Epoch 85/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8617\n",
      "Epoch 00085: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3095 - accuracy: 0.8617 - val_loss: 0.3791 - val_accuracy: 0.8248\n",
      "Epoch 86/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8662\n",
      "Epoch 00086: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3049 - accuracy: 0.8664 - val_loss: 0.3641 - val_accuracy: 0.8343\n",
      "Epoch 87/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8664\n",
      "Epoch 00087: val_loss did not improve from 0.35894\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3015 - accuracy: 0.8667 - val_loss: 0.3709 - val_accuracy: 0.8340\n",
      "Epoch 88/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8655\n",
      "Epoch 00088: val_loss improved from 0.35894 to 0.35754, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3074 - accuracy: 0.8659 - val_loss: 0.3575 - val_accuracy: 0.8411\n",
      "Epoch 89/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8692\n",
      "Epoch 00089: val_loss did not improve from 0.35754\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3005 - accuracy: 0.8694 - val_loss: 0.3933 - val_accuracy: 0.8237\n",
      "Epoch 90/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8696\n",
      "Epoch 00090: val_loss did not improve from 0.35754\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3001 - accuracy: 0.8700 - val_loss: 0.3639 - val_accuracy: 0.8349\n",
      "Epoch 91/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8671\n",
      "Epoch 00091: val_loss did not improve from 0.35754\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2999 - accuracy: 0.8671 - val_loss: 0.4128 - val_accuracy: 0.8186\n",
      "Epoch 92/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8694\n",
      "Epoch 00092: val_loss did not improve from 0.35754\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2951 - accuracy: 0.8698 - val_loss: 0.3793 - val_accuracy: 0.8287\n",
      "Epoch 93/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8713\n",
      "Epoch 00093: val_loss improved from 0.35754 to 0.35528, saving model to pickled_objects/batch_size_128_lr_0.08_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2916 - accuracy: 0.8715 - val_loss: 0.3553 - val_accuracy: 0.8411\n",
      "Epoch 94/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8744\n",
      "Epoch 00094: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2898 - accuracy: 0.8744 - val_loss: 0.3554 - val_accuracy: 0.8401\n",
      "Epoch 95/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.8755\n",
      "Epoch 00095: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2834 - accuracy: 0.8758 - val_loss: 0.3574 - val_accuracy: 0.8414\n",
      "Epoch 96/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8732\n",
      "Epoch 00096: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2870 - accuracy: 0.8735 - val_loss: 0.3968 - val_accuracy: 0.8209\n",
      "Epoch 97/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8818\n",
      "Epoch 00097: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2801 - accuracy: 0.8819 - val_loss: 0.3715 - val_accuracy: 0.8313\n",
      "Epoch 98/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.8736\n",
      "Epoch 00098: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2842 - accuracy: 0.8738 - val_loss: 0.3669 - val_accuracy: 0.8317\n",
      "Epoch 99/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.8771\n",
      "Epoch 00099: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2815 - accuracy: 0.8773 - val_loss: 0.3728 - val_accuracy: 0.8298\n",
      "Epoch 100/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.8793\n",
      "Epoch 00100: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2805 - accuracy: 0.8795 - val_loss: 0.3726 - val_accuracy: 0.8336\n",
      "Epoch 101/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8762\n",
      "Epoch 00101: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2779 - accuracy: 0.8765 - val_loss: 0.3654 - val_accuracy: 0.8381\n",
      "Epoch 102/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.8831\n",
      "Epoch 00102: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2735 - accuracy: 0.8831 - val_loss: 0.3702 - val_accuracy: 0.8317\n",
      "Epoch 103/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8838\n",
      "Epoch 00103: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2694 - accuracy: 0.8838 - val_loss: 0.3794 - val_accuracy: 0.8310\n",
      "Epoch 104/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/146 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.8835\n",
      "Epoch 00104: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2723 - accuracy: 0.8837 - val_loss: 0.3600 - val_accuracy: 0.8448\n",
      "Epoch 105/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8835\n",
      "Epoch 00105: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2720 - accuracy: 0.8837 - val_loss: 0.3743 - val_accuracy: 0.8319\n",
      "Epoch 106/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.8840\n",
      "Epoch 00106: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2669 - accuracy: 0.8844 - val_loss: 0.3827 - val_accuracy: 0.8302\n",
      "Epoch 107/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.8841\n",
      "Epoch 00107: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2684 - accuracy: 0.8843 - val_loss: 0.3761 - val_accuracy: 0.8321\n",
      "Epoch 108/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.8851\n",
      "Epoch 00108: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2603 - accuracy: 0.8854 - val_loss: 0.3726 - val_accuracy: 0.8285\n",
      "Epoch 109/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8877\n",
      "Epoch 00109: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2614 - accuracy: 0.8880 - val_loss: 0.3775 - val_accuracy: 0.8347\n",
      "Epoch 110/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.8868\n",
      "Epoch 00110: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2610 - accuracy: 0.8868 - val_loss: 0.3782 - val_accuracy: 0.8310\n",
      "Epoch 111/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8869\n",
      "Epoch 00111: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2652 - accuracy: 0.8869 - val_loss: 0.3609 - val_accuracy: 0.8409\n",
      "Epoch 112/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8904\n",
      "Epoch 00112: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2580 - accuracy: 0.8907 - val_loss: 0.3797 - val_accuracy: 0.8306\n",
      "Epoch 113/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.8914\n",
      "Epoch 00113: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2552 - accuracy: 0.8916 - val_loss: 0.3837 - val_accuracy: 0.8276\n",
      "Epoch 114/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.8928\n",
      "Epoch 00114: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2525 - accuracy: 0.8930 - val_loss: 0.3763 - val_accuracy: 0.8362\n",
      "Epoch 115/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.8915\n",
      "Epoch 00115: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2538 - accuracy: 0.8916 - val_loss: 0.3828 - val_accuracy: 0.8308\n",
      "Epoch 116/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.8949\n",
      "Epoch 00116: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2511 - accuracy: 0.8952 - val_loss: 0.4409 - val_accuracy: 0.8005\n",
      "Epoch 117/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.8946\n",
      "Epoch 00117: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2449 - accuracy: 0.8949 - val_loss: 0.3899 - val_accuracy: 0.8308\n",
      "Epoch 118/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.8932\n",
      "Epoch 00118: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.2475 - accuracy: 0.8933 - val_loss: 0.3853 - val_accuracy: 0.8386\n",
      "Epoch 119/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.8988\n",
      "Epoch 00119: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2390 - accuracy: 0.8989 - val_loss: 0.4117 - val_accuracy: 0.8147\n",
      "Epoch 120/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.8968\n",
      "Epoch 00120: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2417 - accuracy: 0.8970 - val_loss: 0.4011 - val_accuracy: 0.8239\n",
      "Epoch 121/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9010\n",
      "Epoch 00121: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2353 - accuracy: 0.9010 - val_loss: 0.3887 - val_accuracy: 0.8343\n",
      "Epoch 122/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.8999\n",
      "Epoch 00122: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2402 - accuracy: 0.8998 - val_loss: 0.3985 - val_accuracy: 0.8289\n",
      "Epoch 123/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9009\n",
      "Epoch 00123: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2350 - accuracy: 0.9010 - val_loss: 0.4014 - val_accuracy: 0.8233\n",
      "Epoch 124/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.9002\n",
      "Epoch 00124: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2339 - accuracy: 0.9005 - val_loss: 0.3796 - val_accuracy: 0.8388\n",
      "Epoch 125/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.8990\n",
      "Epoch 00125: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2386 - accuracy: 0.8991 - val_loss: 0.3728 - val_accuracy: 0.8323\n",
      "Epoch 126/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9020\n",
      "Epoch 00126: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2290 - accuracy: 0.9022 - val_loss: 0.3856 - val_accuracy: 0.8340\n",
      "Epoch 127/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9012\n",
      "Epoch 00127: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2306 - accuracy: 0.9014 - val_loss: 0.3877 - val_accuracy: 0.8300\n",
      "Epoch 128/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9046\n",
      "Epoch 00128: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2257 - accuracy: 0.9046 - val_loss: 0.4000 - val_accuracy: 0.8291\n",
      "Epoch 129/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9044\n",
      "Epoch 00129: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2264 - accuracy: 0.9045 - val_loss: 0.3879 - val_accuracy: 0.8291\n",
      "Epoch 130/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9054\n",
      "Epoch 00130: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2252 - accuracy: 0.9054 - val_loss: 0.4339 - val_accuracy: 0.8151\n",
      "Epoch 131/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9019\n",
      "Epoch 00131: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2281 - accuracy: 0.9021 - val_loss: 0.4034 - val_accuracy: 0.8323\n",
      "Epoch 132/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9045\n",
      "Epoch 00132: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2282 - accuracy: 0.9047 - val_loss: 0.3937 - val_accuracy: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2220 - accuracy: 0.9077\n",
      "Epoch 00133: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2214 - accuracy: 0.9078 - val_loss: 0.3979 - val_accuracy: 0.8293\n",
      "Epoch 134/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.91 - ETA: 0s - loss: 0.2160 - accuracy: 0.9101\n",
      "Epoch 00134: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2151 - accuracy: 0.9103 - val_loss: 0.3992 - val_accuracy: 0.8261\n",
      "Epoch 135/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2146 - accuracy: 0.9110\n",
      "Epoch 00135: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2137 - accuracy: 0.9113 - val_loss: 0.3889 - val_accuracy: 0.8300\n",
      "Epoch 136/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9099\n",
      "Epoch 00136: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2148 - accuracy: 0.9100 - val_loss: 0.3984 - val_accuracy: 0.8287\n",
      "Epoch 137/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9103\n",
      "Epoch 00137: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2145 - accuracy: 0.9105 - val_loss: 0.3893 - val_accuracy: 0.8336\n",
      "Epoch 138/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9118\n",
      "Epoch 00138: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2103 - accuracy: 0.9122 - val_loss: 0.4024 - val_accuracy: 0.8272\n",
      "Epoch 139/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9112\n",
      "Epoch 00139: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2121 - accuracy: 0.9112 - val_loss: 0.4178 - val_accuracy: 0.8237\n",
      "Epoch 140/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9115\n",
      "Epoch 00140: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2121 - accuracy: 0.9116 - val_loss: 0.4085 - val_accuracy: 0.8285\n",
      "Epoch 141/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9127\n",
      "Epoch 00141: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2073 - accuracy: 0.9129 - val_loss: 0.4189 - val_accuracy: 0.8212\n",
      "Epoch 142/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9136\n",
      "Epoch 00142: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2043 - accuracy: 0.9138 - val_loss: 0.4054 - val_accuracy: 0.8310\n",
      "Epoch 143/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9155\n",
      "Epoch 00143: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2021 - accuracy: 0.9157 - val_loss: 0.4057 - val_accuracy: 0.8319\n",
      "Epoch 144/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9124\n",
      "Epoch 00144: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2085 - accuracy: 0.9126 - val_loss: 0.4268 - val_accuracy: 0.8203\n",
      "Epoch 145/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9156\n",
      "Epoch 00145: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2032 - accuracy: 0.9157 - val_loss: 0.4166 - val_accuracy: 0.8257\n",
      "Epoch 146/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9170\n",
      "Epoch 00146: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2000 - accuracy: 0.9171 - val_loss: 0.4121 - val_accuracy: 0.8351\n",
      "Epoch 147/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9160\n",
      "Epoch 00147: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2021 - accuracy: 0.9161 - val_loss: 0.4240 - val_accuracy: 0.8255\n",
      "Epoch 148/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9173\n",
      "Epoch 00148: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2001 - accuracy: 0.9172 - val_loss: 0.3987 - val_accuracy: 0.8272\n",
      "Epoch 149/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9197\n",
      "Epoch 00149: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1922 - accuracy: 0.9198 - val_loss: 0.4169 - val_accuracy: 0.8280\n",
      "Epoch 150/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9141\n",
      "Epoch 00150: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2020 - accuracy: 0.9140 - val_loss: 0.4365 - val_accuracy: 0.8212\n",
      "Epoch 151/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1938 - accuracy: 0.9206\n",
      "Epoch 00151: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1934 - accuracy: 0.9206 - val_loss: 0.4311 - val_accuracy: 0.8267\n",
      "Epoch 152/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9206\n",
      "Epoch 00152: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1956 - accuracy: 0.9207 - val_loss: 0.4247 - val_accuracy: 0.8274\n",
      "Epoch 153/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9187\n",
      "Epoch 00153: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1977 - accuracy: 0.9189 - val_loss: 0.4094 - val_accuracy: 0.8321\n",
      "Epoch 154/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9196\n",
      "Epoch 00154: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1913 - accuracy: 0.9196 - val_loss: 0.4118 - val_accuracy: 0.8282\n",
      "Epoch 155/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1852 - accuracy: 0.9239\n",
      "Epoch 00155: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1843 - accuracy: 0.9241 - val_loss: 0.4170 - val_accuracy: 0.8287\n",
      "Epoch 156/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9261\n",
      "Epoch 00156: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.1815 - accuracy: 0.9262 - val_loss: 0.4367 - val_accuracy: 0.8278\n",
      "Epoch 157/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9236\n",
      "Epoch 00157: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1863 - accuracy: 0.9238 - val_loss: 0.4349 - val_accuracy: 0.8282\n",
      "Epoch 158/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9274\n",
      "Epoch 00158: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1785 - accuracy: 0.9275 - val_loss: 0.4361 - val_accuracy: 0.8237\n",
      "Epoch 159/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9255\n",
      "Epoch 00159: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1819 - accuracy: 0.9257 - val_loss: 0.4464 - val_accuracy: 0.8212\n",
      "Epoch 160/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9248\n",
      "Epoch 00160: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1835 - accuracy: 0.9250 - val_loss: 0.4211 - val_accuracy: 0.8282\n",
      "Epoch 161/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9249\n",
      "Epoch 00161: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1836 - accuracy: 0.9253 - val_loss: 0.4288 - val_accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.9258\n",
      "Epoch 00162: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1818 - accuracy: 0.9259 - val_loss: 0.4388 - val_accuracy: 0.8151\n",
      "Epoch 163/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9276\n",
      "Epoch 00163: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1811 - accuracy: 0.9276 - val_loss: 0.4106 - val_accuracy: 0.8298\n",
      "Epoch 164/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9269\n",
      "Epoch 00164: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1774 - accuracy: 0.9269 - val_loss: 0.4255 - val_accuracy: 0.8280\n",
      "Epoch 165/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9278\n",
      "Epoch 00165: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1769 - accuracy: 0.9277 - val_loss: 0.4116 - val_accuracy: 0.8285\n",
      "Epoch 166/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9301\n",
      "Epoch 00166: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1721 - accuracy: 0.9303 - val_loss: 0.4103 - val_accuracy: 0.8405\n",
      "Epoch 167/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9286\n",
      "Epoch 00167: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1768 - accuracy: 0.9287 - val_loss: 0.4273 - val_accuracy: 0.8308\n",
      "Epoch 168/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9293\n",
      "Epoch 00168: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1721 - accuracy: 0.9293 - val_loss: 0.4294 - val_accuracy: 0.8244\n",
      "Epoch 169/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1681 - accuracy: 0.9314\n",
      "Epoch 00169: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1677 - accuracy: 0.9314 - val_loss: 0.4184 - val_accuracy: 0.8302\n",
      "Epoch 170/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9322\n",
      "Epoch 00170: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1664 - accuracy: 0.9323 - val_loss: 0.4423 - val_accuracy: 0.8192\n",
      "Epoch 171/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9269\n",
      "Epoch 00171: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1783 - accuracy: 0.9270 - val_loss: 0.4174 - val_accuracy: 0.8298\n",
      "Epoch 172/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9310\n",
      "Epoch 00172: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1664 - accuracy: 0.9311 - val_loss: 0.4577 - val_accuracy: 0.8263\n",
      "Epoch 173/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9283\n",
      "Epoch 00173: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1733 - accuracy: 0.9282 - val_loss: 0.4623 - val_accuracy: 0.8250\n",
      "Epoch 174/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1610 - accuracy: 0.9320\n",
      "Epoch 00174: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1603 - accuracy: 0.9322 - val_loss: 0.4358 - val_accuracy: 0.8310\n",
      "Epoch 175/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9330\n",
      "Epoch 00175: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1666 - accuracy: 0.9331 - val_loss: 0.4780 - val_accuracy: 0.8085\n",
      "Epoch 176/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9335\n",
      "Epoch 00176: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1603 - accuracy: 0.9337 - val_loss: 0.4424 - val_accuracy: 0.8248\n",
      "Epoch 177/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9322\n",
      "Epoch 00177: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1694 - accuracy: 0.9326 - val_loss: 0.4426 - val_accuracy: 0.8255\n",
      "Epoch 178/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9329\n",
      "Epoch 00178: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1669 - accuracy: 0.9330 - val_loss: 0.4644 - val_accuracy: 0.8205\n",
      "Epoch 179/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9350\n",
      "Epoch 00179: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1613 - accuracy: 0.9350 - val_loss: 0.4489 - val_accuracy: 0.8304\n",
      "Epoch 180/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1632 - accuracy: 0.9335\n",
      "Epoch 00180: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1633 - accuracy: 0.9334 - val_loss: 0.4559 - val_accuracy: 0.8248\n",
      "Epoch 181/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9313\n",
      "Epoch 00181: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1649 - accuracy: 0.9313 - val_loss: 0.5194 - val_accuracy: 0.8100\n",
      "Epoch 182/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9354\n",
      "Epoch 00182: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1615 - accuracy: 0.9355 - val_loss: 0.4827 - val_accuracy: 0.8220\n",
      "Epoch 183/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9378\n",
      "Epoch 00183: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1537 - accuracy: 0.9379 - val_loss: 0.4473 - val_accuracy: 0.8285\n",
      "Epoch 184/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9383\n",
      "Epoch 00184: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1536 - accuracy: 0.9385 - val_loss: 0.4802 - val_accuracy: 0.8235\n",
      "Epoch 185/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1537 - accuracy: 0.9398\n",
      "Epoch 00185: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1531 - accuracy: 0.9399 - val_loss: 0.4429 - val_accuracy: 0.8298\n",
      "Epoch 186/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.9377\n",
      "Epoch 00186: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1518 - accuracy: 0.9378 - val_loss: 0.4394 - val_accuracy: 0.8390\n",
      "Epoch 187/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9370\n",
      "Epoch 00187: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1550 - accuracy: 0.9371 - val_loss: 0.4659 - val_accuracy: 0.8252\n",
      "Epoch 188/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9358\n",
      "Epoch 00188: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1576 - accuracy: 0.9358 - val_loss: 0.4589 - val_accuracy: 0.8265\n",
      "Epoch 189/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1582 - accuracy: 0.9375\n",
      "Epoch 00189: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1581 - accuracy: 0.9372 - val_loss: 0.4474 - val_accuracy: 0.8246\n",
      "Epoch 190/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9409\n",
      "Epoch 00190: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1476 - accuracy: 0.9409 - val_loss: 0.4431 - val_accuracy: 0.8319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9406\n",
      "Epoch 00191: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1525 - accuracy: 0.9407 - val_loss: 0.4380 - val_accuracy: 0.8315\n",
      "Epoch 192/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9395\n",
      "Epoch 00192: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1487 - accuracy: 0.9397 - val_loss: 0.4441 - val_accuracy: 0.8278\n",
      "Epoch 193/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9412\n",
      "Epoch 00193: val_loss did not improve from 0.35528\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.1464 - accuracy: 0.9413 - val_loss: 0.4554 - val_accuracy: 0.8252\n",
      "Epoch 00193: early stopping\n",
      "Epoch 1/10000\n",
      "    146/Unknown - 8s 57ms/step - loss: 0.6889 - accuracy: 0.5422\n",
      "Epoch 00001: val_loss improved from inf to 0.69299, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 9s 63ms/step - loss: 0.6889 - accuracy: 0.5422 - val_loss: 0.6930 - val_accuracy: 0.4944\n",
      "Epoch 2/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5610\n",
      "Epoch 00002: val_loss improved from 0.69299 to 0.67440, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6825 - accuracy: 0.5613 - val_loss: 0.6744 - val_accuracy: 0.6212\n",
      "Epoch 3/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6751 - accuracy: 0.5838 ETA: 0s - loss: 0.6746 - accu\n",
      "Epoch 00003: val_loss improved from 0.67440 to 0.66094, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6750 - accuracy: 0.5837 - val_loss: 0.6609 - val_accuracy: 0.6350\n",
      "Epoch 4/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6596 - accuracy: 0.6040\n",
      "Epoch 00004: val_loss improved from 0.66094 to 0.64659, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6598 - accuracy: 0.6037 - val_loss: 0.6466 - val_accuracy: 0.6515\n",
      "Epoch 5/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.6325\n",
      "Epoch 00005: val_loss did not improve from 0.64659\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6418 - accuracy: 0.6318 - val_loss: 0.6503 - val_accuracy: 0.6189\n",
      "Epoch 6/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.6610\n",
      "Epoch 00006: val_loss improved from 0.64659 to 0.58633, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6187 - accuracy: 0.6608 - val_loss: 0.5863 - val_accuracy: 0.7064\n",
      "Epoch 7/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5934 - accuracy: 0.6824\n",
      "Epoch 00007: val_loss improved from 0.58633 to 0.57426, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5940 - accuracy: 0.6823 - val_loss: 0.5743 - val_accuracy: 0.7104\n",
      "Epoch 8/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.6980\n",
      "Epoch 00008: val_loss improved from 0.57426 to 0.53948, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5796 - accuracy: 0.6981 - val_loss: 0.5395 - val_accuracy: 0.7343\n",
      "Epoch 9/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7124\n",
      "Epoch 00009: val_loss improved from 0.53948 to 0.52095, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5568 - accuracy: 0.7123 - val_loss: 0.5210 - val_accuracy: 0.7528\n",
      "Epoch 10/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7232\n",
      "Epoch 00010: val_loss improved from 0.52095 to 0.51958, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5420 - accuracy: 0.7231 - val_loss: 0.5196 - val_accuracy: 0.7436\n",
      "Epoch 11/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5337 - accuracy: 0.7312\n",
      "Epoch 00011: val_loss did not improve from 0.51958\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5338 - accuracy: 0.7308 - val_loss: 0.5536 - val_accuracy: 0.6984\n",
      "Epoch 12/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7393\n",
      "Epoch 00012: val_loss did not improve from 0.51958\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5222 - accuracy: 0.7392 - val_loss: 0.5609 - val_accuracy: 0.7008\n",
      "Epoch 13/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5093 - accuracy: 0.7457\n",
      "Epoch 00013: val_loss did not improve from 0.51958\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5103 - accuracy: 0.7450 - val_loss: 0.5429 - val_accuracy: 0.7143\n",
      "Epoch 14/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.7565\n",
      "Epoch 00014: val_loss improved from 0.51958 to 0.48845, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4993 - accuracy: 0.7565 - val_loss: 0.4884 - val_accuracy: 0.7717\n",
      "Epoch 15/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.7631\n",
      "Epoch 00015: val_loss did not improve from 0.48845\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4933 - accuracy: 0.7627 - val_loss: 0.4897 - val_accuracy: 0.7588\n",
      "Epoch 16/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7628 ETA: 0s - loss: 0.4860 - accuracy: 0.76\n",
      "Epoch 00016: val_loss improved from 0.48845 to 0.45936, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4869 - accuracy: 0.7624 - val_loss: 0.4594 - val_accuracy: 0.7820\n",
      "Epoch 17/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.7673\n",
      "Epoch 00017: val_loss did not improve from 0.45936\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4799 - accuracy: 0.7669 - val_loss: 0.4751 - val_accuracy: 0.7752\n",
      "Epoch 18/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4710 - accuracy: 0.7742\n",
      "Epoch 00018: val_loss improved from 0.45936 to 0.45544, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4720 - accuracy: 0.7739 - val_loss: 0.4554 - val_accuracy: 0.7870\n",
      "Epoch 19/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.7809\n",
      "Epoch 00019: val_loss did not improve from 0.45544\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4618 - accuracy: 0.7805 - val_loss: 0.4747 - val_accuracy: 0.7693\n",
      "Epoch 20/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7775\n",
      "Epoch 00020: val_loss did not improve from 0.45544\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4617 - accuracy: 0.7772 - val_loss: 0.4750 - val_accuracy: 0.7569\n",
      "Epoch 21/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4560 - accuracy: 0.7813\n",
      "Epoch 00021: val_loss improved from 0.45544 to 0.43917, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4559 - accuracy: 0.7810 - val_loss: 0.4392 - val_accuracy: 0.7945\n",
      "Epoch 22/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4494 - accuracy: 0.7859\n",
      "Epoch 00022: val_loss did not improve from 0.43917\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4500 - accuracy: 0.7856 - val_loss: 0.4663 - val_accuracy: 0.7706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7905\n",
      "Epoch 00023: val_loss improved from 0.43917 to 0.42364, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4424 - accuracy: 0.7903 - val_loss: 0.4236 - val_accuracy: 0.8083\n",
      "Epoch 24/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4362 - accuracy: 0.7922\n",
      "Epoch 00024: val_loss improved from 0.42364 to 0.41052, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4358 - accuracy: 0.7924 - val_loss: 0.4105 - val_accuracy: 0.8126\n",
      "Epoch 25/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4332 - accuracy: 0.7951\n",
      "Epoch 00025: val_loss did not improve from 0.41052\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4339 - accuracy: 0.7948 - val_loss: 0.4739 - val_accuracy: 0.7709\n",
      "Epoch 26/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.7994\n",
      "Epoch 00026: val_loss did not improve from 0.41052\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4258 - accuracy: 0.7991 - val_loss: 0.4334 - val_accuracy: 0.7949\n",
      "Epoch 27/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.8011\n",
      "Epoch 00027: val_loss improved from 0.41052 to 0.40672, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4294 - accuracy: 0.8011 - val_loss: 0.4067 - val_accuracy: 0.8126\n",
      "Epoch 28/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8033\n",
      "Epoch 00028: val_loss did not improve from 0.40672\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4167 - accuracy: 0.8034 - val_loss: 0.4512 - val_accuracy: 0.7902\n",
      "Epoch 29/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4146 - accuracy: 0.8073\n",
      "Epoch 00029: val_loss did not improve from 0.40672\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4146 - accuracy: 0.8073 - val_loss: 0.4488 - val_accuracy: 0.7876\n",
      "Epoch 30/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8085\n",
      "Epoch 00030: val_loss improved from 0.40672 to 0.40397, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4120 - accuracy: 0.8083 - val_loss: 0.4040 - val_accuracy: 0.8117\n",
      "Epoch 31/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4033 - accuracy: 0.8124\n",
      "Epoch 00031: val_loss did not improve from 0.40397\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4028 - accuracy: 0.8127 - val_loss: 0.4047 - val_accuracy: 0.8091\n",
      "Epoch 32/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8137\n",
      "Epoch 00032: val_loss did not improve from 0.40397\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4023 - accuracy: 0.8139 - val_loss: 0.4049 - val_accuracy: 0.8059\n",
      "Epoch 33/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8138\n",
      "Epoch 00033: val_loss improved from 0.40397 to 0.39508, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4015 - accuracy: 0.8139 - val_loss: 0.3951 - val_accuracy: 0.8177\n",
      "Epoch 34/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8193\n",
      "Epoch 00034: val_loss did not improve from 0.39508\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3952 - accuracy: 0.8193 - val_loss: 0.4078 - val_accuracy: 0.8141\n",
      "Epoch 35/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3931 - accuracy: 0.8186\n",
      "Epoch 00035: val_loss did not improve from 0.39508\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3928 - accuracy: 0.8189 - val_loss: 0.3957 - val_accuracy: 0.8171\n",
      "Epoch 36/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8164\n",
      "Epoch 00036: val_loss did not improve from 0.39508\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3924 - accuracy: 0.8168 - val_loss: 0.3994 - val_accuracy: 0.8169\n",
      "Epoch 37/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8256\n",
      "Epoch 00037: val_loss improved from 0.39508 to 0.38458, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3796 - accuracy: 0.8258 - val_loss: 0.3846 - val_accuracy: 0.8237\n",
      "Epoch 38/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8263\n",
      "Epoch 00038: val_loss did not improve from 0.38458\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3785 - accuracy: 0.8263 - val_loss: 0.3934 - val_accuracy: 0.8205\n",
      "Epoch 39/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8342\n",
      "Epoch 00039: val_loss did not improve from 0.38458\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3706 - accuracy: 0.8342 - val_loss: 0.3994 - val_accuracy: 0.8149\n",
      "Epoch 40/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3711 - accuracy: 0.8301\n",
      "Epoch 00040: val_loss did not improve from 0.38458\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3710 - accuracy: 0.8300 - val_loss: 0.3859 - val_accuracy: 0.8239\n",
      "Epoch 41/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8315\n",
      "Epoch 00041: val_loss improved from 0.38458 to 0.38181, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3717 - accuracy: 0.8315 - val_loss: 0.3818 - val_accuracy: 0.8242\n",
      "Epoch 42/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3659 - accuracy: 0.8350\n",
      "Epoch 00042: val_loss did not improve from 0.38181\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3661 - accuracy: 0.8349 - val_loss: 0.3886 - val_accuracy: 0.8207\n",
      "Epoch 43/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8317\n",
      "Epoch 00043: val_loss improved from 0.38181 to 0.38117, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3640 - accuracy: 0.8320 - val_loss: 0.3812 - val_accuracy: 0.8196\n",
      "Epoch 44/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8359\n",
      "Epoch 00044: val_loss did not improve from 0.38117\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3576 - accuracy: 0.8362 - val_loss: 0.3843 - val_accuracy: 0.8233\n",
      "Epoch 45/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8411\n",
      "Epoch 00045: val_loss did not improve from 0.38117\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3549 - accuracy: 0.8411 - val_loss: 0.3913 - val_accuracy: 0.8177\n",
      "Epoch 46/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8453\n",
      "Epoch 00046: val_loss did not improve from 0.38117\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3481 - accuracy: 0.8453 - val_loss: 0.4662 - val_accuracy: 0.7825\n",
      "Epoch 47/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3511 - accuracy: 0.8418\n",
      "Epoch 00047: val_loss did not improve from 0.38117\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3516 - accuracy: 0.8418 - val_loss: 0.4149 - val_accuracy: 0.8119\n",
      "Epoch 48/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8456\n",
      "Epoch 00048: val_loss did not improve from 0.38117\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3481 - accuracy: 0.8458 - val_loss: 0.3910 - val_accuracy: 0.8188\n",
      "Epoch 49/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.8466\n",
      "Epoch 00049: val_loss improved from 0.38117 to 0.38044, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3435 - accuracy: 0.8468 - val_loss: 0.3804 - val_accuracy: 0.8242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8464\n",
      "Epoch 00050: val_loss did not improve from 0.38044\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3387 - accuracy: 0.8466 - val_loss: 0.3846 - val_accuracy: 0.8212\n",
      "Epoch 51/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8496 ETA: 0s - loss: 0.3378 \n",
      "Epoch 00051: val_loss did not improve from 0.38044\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3367 - accuracy: 0.8499 - val_loss: 0.3851 - val_accuracy: 0.8192\n",
      "Epoch 52/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8515\n",
      "Epoch 00052: val_loss improved from 0.38044 to 0.37161, saving model to pickled_objects/batch_size_128_lr_0.16_best_weights_trial_4.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3307 - accuracy: 0.8517 - val_loss: 0.3716 - val_accuracy: 0.8295\n",
      "Epoch 53/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8499\n",
      "Epoch 00053: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3361 - accuracy: 0.8501 - val_loss: 0.3805 - val_accuracy: 0.8282\n",
      "Epoch 54/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3263 - accuracy: 0.8593\n",
      "Epoch 00054: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3254 - accuracy: 0.8594 - val_loss: 0.3895 - val_accuracy: 0.8186\n",
      "Epoch 55/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8544\n",
      "Epoch 00055: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3238 - accuracy: 0.8550 - val_loss: 0.3867 - val_accuracy: 0.8255\n",
      "Epoch 56/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8576\n",
      "Epoch 00056: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3257 - accuracy: 0.8579 - val_loss: 0.3780 - val_accuracy: 0.8285\n",
      "Epoch 57/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8595\n",
      "Epoch 00057: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3209 - accuracy: 0.8596 - val_loss: 0.3989 - val_accuracy: 0.8138\n",
      "Epoch 58/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8595\n",
      "Epoch 00058: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3168 - accuracy: 0.8599 - val_loss: 0.3719 - val_accuracy: 0.8328\n",
      "Epoch 59/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8590\n",
      "Epoch 00059: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3172 - accuracy: 0.8590 - val_loss: 0.4009 - val_accuracy: 0.8145\n",
      "Epoch 60/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8645\n",
      "Epoch 00060: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3100 - accuracy: 0.8646 - val_loss: 0.3872 - val_accuracy: 0.8205\n",
      "Epoch 61/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8619\n",
      "Epoch 00061: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3098 - accuracy: 0.8618 - val_loss: 0.3757 - val_accuracy: 0.8302\n",
      "Epoch 62/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8641\n",
      "Epoch 00062: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3062 - accuracy: 0.8643 - val_loss: 0.3737 - val_accuracy: 0.8267\n",
      "Epoch 63/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8679\n",
      "Epoch 00063: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2976 - accuracy: 0.8680 - val_loss: 0.3978 - val_accuracy: 0.8173\n",
      "Epoch 64/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8675\n",
      "Epoch 00064: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2993 - accuracy: 0.8675 - val_loss: 0.3746 - val_accuracy: 0.8310\n",
      "Epoch 65/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8706\n",
      "Epoch 00065: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2998 - accuracy: 0.8707 - val_loss: 0.3794 - val_accuracy: 0.8278\n",
      "Epoch 66/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8694\n",
      "Epoch 00066: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2968 - accuracy: 0.8696 - val_loss: 0.3782 - val_accuracy: 0.8267\n",
      "Epoch 67/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8697\n",
      "Epoch 00067: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2903 - accuracy: 0.8700 - val_loss: 0.3946 - val_accuracy: 0.8194\n",
      "Epoch 68/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8720\n",
      "Epoch 00068: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2921 - accuracy: 0.8722 - val_loss: 0.3830 - val_accuracy: 0.8255\n",
      "Epoch 69/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8748\n",
      "Epoch 00069: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2879 - accuracy: 0.8754 - val_loss: 0.3719 - val_accuracy: 0.8302\n",
      "Epoch 70/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.8803\n",
      "Epoch 00070: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2798 - accuracy: 0.8804 - val_loss: 0.3955 - val_accuracy: 0.8205\n",
      "Epoch 71/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.8755\n",
      "Epoch 00071: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2837 - accuracy: 0.8757 - val_loss: 0.3856 - val_accuracy: 0.8263\n",
      "Epoch 72/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8787\n",
      "Epoch 00072: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2831 - accuracy: 0.8788 - val_loss: 0.3744 - val_accuracy: 0.8259\n",
      "Epoch 73/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.8779\n",
      "Epoch 00073: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2815 - accuracy: 0.8781 - val_loss: 0.3821 - val_accuracy: 0.8218\n",
      "Epoch 74/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.8780\n",
      "Epoch 00074: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2763 - accuracy: 0.8783 - val_loss: 0.3896 - val_accuracy: 0.8315\n",
      "Epoch 75/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.8834\n",
      "Epoch 00075: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2723 - accuracy: 0.8838 - val_loss: 0.3856 - val_accuracy: 0.8190\n",
      "Epoch 76/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2763 - accuracy: 0.8828\n",
      "Epoch 00076: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2757 - accuracy: 0.8829 - val_loss: 0.3917 - val_accuracy: 0.8224\n",
      "Epoch 77/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.8794\n",
      "Epoch 00077: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2750 - accuracy: 0.8795 - val_loss: 0.3932 - val_accuracy: 0.8227\n",
      "Epoch 78/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8822\n",
      "Epoch 00078: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2693 - accuracy: 0.8823 - val_loss: 0.3954 - val_accuracy: 0.8270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8813\n",
      "Epoch 00079: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2708 - accuracy: 0.8815 - val_loss: 0.3923 - val_accuracy: 0.8231\n",
      "Epoch 80/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.8886\n",
      "Epoch 00080: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2621 - accuracy: 0.8886 - val_loss: 0.4092 - val_accuracy: 0.8196\n",
      "Epoch 81/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.8876\n",
      "Epoch 00081: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2606 - accuracy: 0.8879 - val_loss: 0.3951 - val_accuracy: 0.8265\n",
      "Epoch 82/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.8879\n",
      "Epoch 00082: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2602 - accuracy: 0.8881 - val_loss: 0.3964 - val_accuracy: 0.8257\n",
      "Epoch 83/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.8888\n",
      "Epoch 00083: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2602 - accuracy: 0.8889 - val_loss: 0.3940 - val_accuracy: 0.8222\n",
      "Epoch 84/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.8877\n",
      "Epoch 00084: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2556 - accuracy: 0.8879 - val_loss: 0.3899 - val_accuracy: 0.8246\n",
      "Epoch 85/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.8882\n",
      "Epoch 00085: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2534 - accuracy: 0.8884 - val_loss: 0.4052 - val_accuracy: 0.8255\n",
      "Epoch 86/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.8926\n",
      "Epoch 00086: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2493 - accuracy: 0.8927 - val_loss: 0.4170 - val_accuracy: 0.8123\n",
      "Epoch 87/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.8971\n",
      "Epoch 00087: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2405 - accuracy: 0.8977 - val_loss: 0.4054 - val_accuracy: 0.8203\n",
      "Epoch 88/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.8957\n",
      "Epoch 00088: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2457 - accuracy: 0.8958 - val_loss: 0.4396 - val_accuracy: 0.8044\n",
      "Epoch 89/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.8981\n",
      "Epoch 00089: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2432 - accuracy: 0.8980 - val_loss: 0.3901 - val_accuracy: 0.8263\n",
      "Epoch 90/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8978\n",
      "Epoch 00090: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2431 - accuracy: 0.8979 - val_loss: 0.3866 - val_accuracy: 0.8274\n",
      "Epoch 91/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.8973\n",
      "Epoch 00091: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2402 - accuracy: 0.8975 - val_loss: 0.4144 - val_accuracy: 0.8224\n",
      "Epoch 92/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9030\n",
      "Epoch 00092: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2360 - accuracy: 0.9031 - val_loss: 0.4031 - val_accuracy: 0.8270\n",
      "Epoch 93/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9027\n",
      "Epoch 00093: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2329 - accuracy: 0.9027 - val_loss: 0.4498 - val_accuracy: 0.7921\n",
      "Epoch 94/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.8978\n",
      "Epoch 00094: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2387 - accuracy: 0.8980 - val_loss: 0.3918 - val_accuracy: 0.8250\n",
      "Epoch 95/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9033\n",
      "Epoch 00095: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2347 - accuracy: 0.9035 - val_loss: 0.3866 - val_accuracy: 0.8259\n",
      "Epoch 96/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9046\n",
      "Epoch 00096: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2304 - accuracy: 0.9048 - val_loss: 0.3938 - val_accuracy: 0.8291\n",
      "Epoch 97/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9075\n",
      "Epoch 00097: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2205 - accuracy: 0.9077 - val_loss: 0.3980 - val_accuracy: 0.8246\n",
      "Epoch 98/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9065\n",
      "Epoch 00098: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2235 - accuracy: 0.9067 - val_loss: 0.4144 - val_accuracy: 0.8233\n",
      "Epoch 99/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9047\n",
      "Epoch 00099: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2262 - accuracy: 0.9048 - val_loss: 0.4165 - val_accuracy: 0.8205\n",
      "Epoch 100/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9057\n",
      "Epoch 00100: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2253 - accuracy: 0.9055 - val_loss: 0.4023 - val_accuracy: 0.8199\n",
      "Epoch 101/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.9065\n",
      "Epoch 00101: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2203 - accuracy: 0.9066 - val_loss: 0.3966 - val_accuracy: 0.8229\n",
      "Epoch 102/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9080\n",
      "Epoch 00102: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2159 - accuracy: 0.9084 - val_loss: 0.4500 - val_accuracy: 0.8048\n",
      "Epoch 103/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9091\n",
      "Epoch 00103: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2173 - accuracy: 0.9093 - val_loss: 0.3949 - val_accuracy: 0.8239\n",
      "Epoch 104/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9112\n",
      "Epoch 00104: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2118 - accuracy: 0.9114 - val_loss: 0.4158 - val_accuracy: 0.8143\n",
      "Epoch 105/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9122\n",
      "Epoch 00105: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2104 - accuracy: 0.9121 - val_loss: 0.4027 - val_accuracy: 0.8287\n",
      "Epoch 106/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9129\n",
      "Epoch 00106: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2108 - accuracy: 0.9131 - val_loss: 0.4146 - val_accuracy: 0.8186\n",
      "Epoch 107/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9150\n",
      "Epoch 00107: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2065 - accuracy: 0.9151 - val_loss: 0.3994 - val_accuracy: 0.8280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9166\n",
      "Epoch 00108: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2023 - accuracy: 0.9165 - val_loss: 0.4310 - val_accuracy: 0.8173\n",
      "Epoch 109/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2081 - accuracy: 0.9126\n",
      "Epoch 00109: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2072 - accuracy: 0.9128 - val_loss: 0.3966 - val_accuracy: 0.8300\n",
      "Epoch 110/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9149\n",
      "Epoch 00110: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2012 - accuracy: 0.9143 - val_loss: 0.4119 - val_accuracy: 0.8181\n",
      "Epoch 111/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9167\n",
      "Epoch 00111: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2034 - accuracy: 0.9169 - val_loss: 0.4084 - val_accuracy: 0.8276\n",
      "Epoch 112/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9156\n",
      "Epoch 00112: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2047 - accuracy: 0.9157 - val_loss: 0.4131 - val_accuracy: 0.8282\n",
      "Epoch 113/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9176\n",
      "Epoch 00113: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2006 - accuracy: 0.9177 - val_loss: 0.4195 - val_accuracy: 0.8171\n",
      "Epoch 114/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9220\n",
      "Epoch 00114: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1925 - accuracy: 0.9221 - val_loss: 0.4364 - val_accuracy: 0.8136\n",
      "Epoch 115/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9190\n",
      "Epoch 00115: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1955 - accuracy: 0.9191 - val_loss: 0.4183 - val_accuracy: 0.8248\n",
      "Epoch 116/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9183\n",
      "Epoch 00116: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1970 - accuracy: 0.9182 - val_loss: 0.4290 - val_accuracy: 0.8220\n",
      "Epoch 117/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9198\n",
      "Epoch 00117: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1936 - accuracy: 0.9200 - val_loss: 0.4321 - val_accuracy: 0.8138\n",
      "Epoch 118/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9206\n",
      "Epoch 00118: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1885 - accuracy: 0.9207 - val_loss: 0.4119 - val_accuracy: 0.8287\n",
      "Epoch 119/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.92 - ETA: 0s - loss: 0.1892 - accuracy: 0.9228\n",
      "Epoch 00119: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1886 - accuracy: 0.9229 - val_loss: 0.4146 - val_accuracy: 0.8196\n",
      "Epoch 120/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9219\n",
      "Epoch 00120: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1888 - accuracy: 0.9222 - val_loss: 0.4529 - val_accuracy: 0.8141\n",
      "Epoch 121/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9244\n",
      "Epoch 00121: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1822 - accuracy: 0.9246 - val_loss: 0.4373 - val_accuracy: 0.8194\n",
      "Epoch 122/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9222\n",
      "Epoch 00122: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1915 - accuracy: 0.9222 - val_loss: 0.4150 - val_accuracy: 0.8255\n",
      "Epoch 123/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9248\n",
      "Epoch 00123: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1835 - accuracy: 0.9247 - val_loss: 0.4456 - val_accuracy: 0.8052\n",
      "Epoch 124/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1799 - accuracy: 0.9282\n",
      "Epoch 00124: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1792 - accuracy: 0.9284 - val_loss: 0.4424 - val_accuracy: 0.8143\n",
      "Epoch 125/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9259\n",
      "Epoch 00125: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1813 - accuracy: 0.9260 - val_loss: 0.4583 - val_accuracy: 0.8175\n",
      "Epoch 126/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9284\n",
      "Epoch 00126: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1780 - accuracy: 0.9286 - val_loss: 0.4510 - val_accuracy: 0.8095\n",
      "Epoch 127/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9245\n",
      "Epoch 00127: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1902 - accuracy: 0.9246 - val_loss: 0.4049 - val_accuracy: 0.8282\n",
      "Epoch 128/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9257\n",
      "Epoch 00128: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1768 - accuracy: 0.9259 - val_loss: 0.4277 - val_accuracy: 0.8272\n",
      "Epoch 129/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9317\n",
      "Epoch 00129: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1691 - accuracy: 0.9319 - val_loss: 0.4685 - val_accuracy: 0.8022\n",
      "Epoch 130/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9257\n",
      "Epoch 00130: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1848 - accuracy: 0.9258 - val_loss: 0.4271 - val_accuracy: 0.8212\n",
      "Epoch 131/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9265\n",
      "Epoch 00131: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1755 - accuracy: 0.9267 - val_loss: 0.4261 - val_accuracy: 0.8201\n",
      "Epoch 132/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9297\n",
      "Epoch 00132: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1726 - accuracy: 0.9298 - val_loss: 0.4428 - val_accuracy: 0.8095\n",
      "Epoch 133/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9277\n",
      "Epoch 00133: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1768 - accuracy: 0.9278 - val_loss: 0.4335 - val_accuracy: 0.8160\n",
      "Epoch 134/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9308\n",
      "Epoch 00134: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1680 - accuracy: 0.9310 - val_loss: 0.4265 - val_accuracy: 0.8274\n",
      "Epoch 135/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.9322\n",
      "Epoch 00135: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1699 - accuracy: 0.9324 - val_loss: 0.4799 - val_accuracy: 0.8067\n",
      "Epoch 136/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9289\n",
      "Epoch 00136: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1727 - accuracy: 0.9291 - val_loss: 0.4534 - val_accuracy: 0.8076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9289\n",
      "Epoch 00137: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1702 - accuracy: 0.9290 - val_loss: 0.4615 - val_accuracy: 0.8089\n",
      "Epoch 138/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9336\n",
      "Epoch 00138: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1645 - accuracy: 0.9338 - val_loss: 0.4431 - val_accuracy: 0.8138\n",
      "Epoch 139/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.9359\n",
      "Epoch 00139: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1578 - accuracy: 0.9362 - val_loss: 0.4519 - val_accuracy: 0.8153\n",
      "Epoch 140/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9293\n",
      "Epoch 00140: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1686 - accuracy: 0.9294 - val_loss: 0.4404 - val_accuracy: 0.8098\n",
      "Epoch 141/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9344\n",
      "Epoch 00141: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1625 - accuracy: 0.9344 - val_loss: 0.4671 - val_accuracy: 0.8061\n",
      "Epoch 142/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9359\n",
      "Epoch 00142: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1555 - accuracy: 0.9360 - val_loss: 0.4455 - val_accuracy: 0.8151\n",
      "Epoch 143/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1528 - accuracy: 0.9386\n",
      "Epoch 00143: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1531 - accuracy: 0.9385 - val_loss: 0.4427 - val_accuracy: 0.8147\n",
      "Epoch 144/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9329\n",
      "Epoch 00144: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1640 - accuracy: 0.9331 - val_loss: 0.4515 - val_accuracy: 0.8083\n",
      "Epoch 145/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9364\n",
      "Epoch 00145: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1585 - accuracy: 0.9366 - val_loss: 0.4538 - val_accuracy: 0.8095\n",
      "Epoch 146/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1594 - accuracy: 0.9355\n",
      "Epoch 00146: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1593 - accuracy: 0.9355 - val_loss: 0.4528 - val_accuracy: 0.8059\n",
      "Epoch 147/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9390\n",
      "Epoch 00147: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1517 - accuracy: 0.9391 - val_loss: 0.4548 - val_accuracy: 0.8164\n",
      "Epoch 148/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9418\n",
      "Epoch 00148: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1474 - accuracy: 0.9417 - val_loss: 0.4727 - val_accuracy: 0.8070\n",
      "Epoch 149/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9398\n",
      "Epoch 00149: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1534 - accuracy: 0.9399 - val_loss: 0.4458 - val_accuracy: 0.8153\n",
      "Epoch 150/10000\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9389\n",
      "Epoch 00150: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1521 - accuracy: 0.9391 - val_loss: 0.4469 - val_accuracy: 0.8151\n",
      "Epoch 151/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9365\n",
      "Epoch 00151: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.1572 - accuracy: 0.9367 - val_loss: 0.4397 - val_accuracy: 0.8171\n",
      "Epoch 152/10000\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9439\n",
      "Epoch 00152: val_loss did not improve from 0.37161\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1433 - accuracy: 0.9440 - val_loss: 0.4512 - val_accuracy: 0.8209\n",
      "Epoch 00152: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 24s 331ms/step - loss: 0.6937 - accuracy: 0.4992\n",
      "Epoch 00001: val_loss improved from inf to 0.69315, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 31s 419ms/step - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5138\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5066\n",
      "Epoch 00002: val_loss improved from 0.69315 to 0.69296, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6933 - accuracy: 0.5062 - val_loss: 0.6930 - val_accuracy: 0.5198\n",
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5069\n",
      "Epoch 00003: val_loss improved from 0.69296 to 0.69285, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6930 - accuracy: 0.5072 - val_loss: 0.6929 - val_accuracy: 0.5037\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5182\n",
      "Epoch 00004: val_loss improved from 0.69285 to 0.69278, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6923 - accuracy: 0.5184 - val_loss: 0.6928 - val_accuracy: 0.4936\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5177\n",
      "Epoch 00005: val_loss did not improve from 0.69278\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6923 - accuracy: 0.5174 - val_loss: 0.6929 - val_accuracy: 0.4908\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6915 - accuracy: 0.5270\n",
      "Epoch 00006: val_loss did not improve from 0.69278\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6915 - accuracy: 0.5270 - val_loss: 0.6929 - val_accuracy: 0.4912\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.5368\n",
      "Epoch 00007: val_loss did not improve from 0.69278\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6930 - val_accuracy: 0.4910\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6904 - accuracy: 0.5474\n",
      "Epoch 00008: val_loss did not improve from 0.69278\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6904 - accuracy: 0.5477 - val_loss: 0.6929 - val_accuracy: 0.4916\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6896 - accuracy: 0.5528\n",
      "Epoch 00009: val_loss did not improve from 0.69278\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6896 - accuracy: 0.5526 - val_loss: 0.6928 - val_accuracy: 0.4916\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5602\n",
      "Epoch 00010: val_loss improved from 0.69278 to 0.69268, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6886 - accuracy: 0.5601 - val_loss: 0.6927 - val_accuracy: 0.4914\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5691\n",
      "Epoch 00011: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6881 - accuracy: 0.5692 - val_loss: 0.6929 - val_accuracy: 0.4912\n",
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5768\n",
      "Epoch 00012: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6867 - accuracy: 0.5771 - val_loss: 0.6929 - val_accuracy: 0.4920\n",
      "Epoch 13/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5755\n",
      "Epoch 00013: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6864 - accuracy: 0.5751 - val_loss: 0.6937 - val_accuracy: 0.4910\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5790\n",
      "Epoch 00014: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6849 - accuracy: 0.5785 - val_loss: 0.6935 - val_accuracy: 0.4927\n",
      "Epoch 15/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5787\n",
      "Epoch 00015: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6840 - accuracy: 0.5786 - val_loss: 0.6940 - val_accuracy: 0.4931\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5803\n",
      "Epoch 00016: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6825 - accuracy: 0.5802 - val_loss: 0.6942 - val_accuracy: 0.4929\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5848\n",
      "Epoch 00017: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6803 - accuracy: 0.5843 - val_loss: 0.6940 - val_accuracy: 0.4953\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5826\n",
      "Epoch 00018: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6803 - accuracy: 0.5823 - val_loss: 0.6953 - val_accuracy: 0.4953\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5854\n",
      "Epoch 00019: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6780 - accuracy: 0.5854 - val_loss: 0.6959 - val_accuracy: 0.4957\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.5839\n",
      "Epoch 00020: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6775 - accuracy: 0.5838 - val_loss: 0.6971 - val_accuracy: 0.4959\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5882\n",
      "Epoch 00021: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6762 - accuracy: 0.5879 - val_loss: 0.6976 - val_accuracy: 0.4966\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6746 - accuracy: 0.5942\n",
      "Epoch 00022: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6745 - accuracy: 0.5942 - val_loss: 0.6987 - val_accuracy: 0.4970\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6727 - accuracy: 0.5958\n",
      "Epoch 00023: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6725 - accuracy: 0.5956 - val_loss: 0.6997 - val_accuracy: 0.4979\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6720 - accuracy: 0.5978\n",
      "Epoch 00024: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6717 - accuracy: 0.5979 - val_loss: 0.7016 - val_accuracy: 0.4989\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.5947\n",
      "Epoch 00025: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6701 - accuracy: 0.5953 - val_loss: 0.7033 - val_accuracy: 0.4987\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6685 - accuracy: 0.6010\n",
      "Epoch 00026: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6682 - accuracy: 0.6014 - val_loss: 0.7049 - val_accuracy: 0.4989\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6663 - accuracy: 0.6016\n",
      "Epoch 00027: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6660 - accuracy: 0.6018 - val_loss: 0.7052 - val_accuracy: 0.5019\n",
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6653 - accuracy: 0.6028\n",
      "Epoch 00028: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6651 - accuracy: 0.6032 - val_loss: 0.7080 - val_accuracy: 0.5013\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6632 - accuracy: 0.6042\n",
      "Epoch 00029: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6629 - accuracy: 0.6045 - val_loss: 0.7084 - val_accuracy: 0.5045\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6617 - accuracy: 0.6032\n",
      "Epoch 00030: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6613 - accuracy: 0.6038 - val_loss: 0.7090 - val_accuracy: 0.5064\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6615 - accuracy: 0.6028\n",
      "Epoch 00031: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6612 - accuracy: 0.6033 - val_loss: 0.7114 - val_accuracy: 0.5058\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6610 - accuracy: 0.6031\n",
      "Epoch 00032: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6608 - accuracy: 0.6036 - val_loss: 0.7063 - val_accuracy: 0.5116\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6606 - accuracy: 0.6062\n",
      "Epoch 00033: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6603 - accuracy: 0.6067 - val_loss: 0.7045 - val_accuracy: 0.5142\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.6043\n",
      "Epoch 00034: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6595 - accuracy: 0.6047 - val_loss: 0.7019 - val_accuracy: 0.5219\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6591 - accuracy: 0.6054\n",
      "Epoch 00035: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6586 - accuracy: 0.6060 - val_loss: 0.6990 - val_accuracy: 0.5286\n",
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6582 - accuracy: 0.6031\n",
      "Epoch 00036: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6579 - accuracy: 0.6035 - val_loss: 0.7031 - val_accuracy: 0.5228\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6571 - accuracy: 0.6089\n",
      "Epoch 00037: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6568 - accuracy: 0.6095 - val_loss: 0.7027 - val_accuracy: 0.5256\n",
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6564 - accuracy: 0.6108\n",
      "Epoch 00038: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6561 - accuracy: 0.6113 - val_loss: 0.6949 - val_accuracy: 0.5406\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6551 - accuracy: 0.6100\n",
      "Epoch 00039: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6548 - accuracy: 0.6107 - val_loss: 0.6977 - val_accuracy: 0.5383\n",
      "Epoch 40/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.6551 - accuracy: 0.6099\n",
      "Epoch 00040: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6548 - accuracy: 0.6105 - val_loss: 0.6937 - val_accuracy: 0.5421\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6540 - accuracy: 0.6125\n",
      "Epoch 00041: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6537 - accuracy: 0.6131 - val_loss: 0.6931 - val_accuracy: 0.5439\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6537 - accuracy: 0.6122\n",
      "Epoch 00042: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6533 - accuracy: 0.6127 - val_loss: 0.6951 - val_accuracy: 0.5417\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6527 - accuracy: 0.6122\n",
      "Epoch 00043: val_loss did not improve from 0.69268\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6524 - accuracy: 0.6126 - val_loss: 0.6928 - val_accuracy: 0.5441\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6520 - accuracy: 0.6152\n",
      "Epoch 00044: val_loss improved from 0.69268 to 0.69262, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6518 - accuracy: 0.6156 - val_loss: 0.6926 - val_accuracy: 0.5460\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.6132\n",
      "Epoch 00045: val_loss improved from 0.69262 to 0.68793, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6510 - accuracy: 0.6136 - val_loss: 0.6879 - val_accuracy: 0.5527\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.6123\n",
      "Epoch 00046: val_loss did not improve from 0.68793\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6510 - accuracy: 0.6126 - val_loss: 0.6908 - val_accuracy: 0.5475\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.6128\n",
      "Epoch 00047: val_loss improved from 0.68793 to 0.68657, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6511 - accuracy: 0.6134 - val_loss: 0.6866 - val_accuracy: 0.5559\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6500 - accuracy: 0.6167\n",
      "Epoch 00048: val_loss improved from 0.68657 to 0.68644, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6496 - accuracy: 0.6171 - val_loss: 0.6864 - val_accuracy: 0.5570\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6482 - accuracy: 0.6171\n",
      "Epoch 00049: val_loss improved from 0.68644 to 0.68303, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6480 - accuracy: 0.6178 - val_loss: 0.6830 - val_accuracy: 0.5628\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6489 - accuracy: 0.6110\n",
      "Epoch 00050: val_loss did not improve from 0.68303\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6485 - accuracy: 0.6116 - val_loss: 0.6832 - val_accuracy: 0.5636\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6483 - accuracy: 0.6152\n",
      "Epoch 00051: val_loss improved from 0.68303 to 0.68288, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6480 - accuracy: 0.6160 - val_loss: 0.6829 - val_accuracy: 0.5636\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6471 - accuracy: 0.6147\n",
      "Epoch 00052: val_loss improved from 0.68288 to 0.67525, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6467 - accuracy: 0.6154 - val_loss: 0.6753 - val_accuracy: 0.5744\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6469 - accuracy: 0.6163\n",
      "Epoch 00053: val_loss improved from 0.67525 to 0.67520, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6465 - accuracy: 0.6170 - val_loss: 0.6752 - val_accuracy: 0.5752\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6456 - accuracy: 0.6161\n",
      "Epoch 00054: val_loss did not improve from 0.67520\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6453 - accuracy: 0.6168 - val_loss: 0.6789 - val_accuracy: 0.5722\n",
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.6176\n",
      "Epoch 00055: val_loss improved from 0.67520 to 0.66774, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6460 - accuracy: 0.6182 - val_loss: 0.6677 - val_accuracy: 0.5858\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6452 - accuracy: 0.6202\n",
      "Epoch 00056: val_loss did not improve from 0.66774\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6449 - accuracy: 0.6206 - val_loss: 0.6735 - val_accuracy: 0.5776\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6443 - accuracy: 0.6214\n",
      "Epoch 00057: val_loss did not improve from 0.66774\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6441 - accuracy: 0.6219 - val_loss: 0.6718 - val_accuracy: 0.5806\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6444 - accuracy: 0.6212\n",
      "Epoch 00058: val_loss did not improve from 0.66774\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6441 - accuracy: 0.6214 - val_loss: 0.6724 - val_accuracy: 0.5791\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6428 - accuracy: 0.6183\n",
      "Epoch 00059: val_loss did not improve from 0.66774\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6424 - accuracy: 0.6191 - val_loss: 0.6679 - val_accuracy: 0.5860\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.6183\n",
      "Epoch 00060: val_loss improved from 0.66774 to 0.66663, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6426 - accuracy: 0.6189 - val_loss: 0.6666 - val_accuracy: 0.5871\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.6196\n",
      "Epoch 00061: val_loss improved from 0.66663 to 0.66396, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6425 - accuracy: 0.6203 - val_loss: 0.6640 - val_accuracy: 0.5905\n",
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6417 - accuracy: 0.6207\n",
      "Epoch 00062: val_loss improved from 0.66396 to 0.66276, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6413 - accuracy: 0.6213 - val_loss: 0.6628 - val_accuracy: 0.5922\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6401 - accuracy: 0.6247\n",
      "Epoch 00063: val_loss did not improve from 0.66276\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6398 - accuracy: 0.6250 - val_loss: 0.6634 - val_accuracy: 0.5920\n",
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6397 - accuracy: 0.6234\n",
      "Epoch 00064: val_loss improved from 0.66276 to 0.65835, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6395 - accuracy: 0.6239 - val_loss: 0.6583 - val_accuracy: 0.5991\n",
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.6220\n",
      "Epoch 00065: val_loss did not improve from 0.65835\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6401 - accuracy: 0.6225 - val_loss: 0.6651 - val_accuracy: 0.5896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.6274\n",
      "Epoch 00066: val_loss improved from 0.65835 to 0.65666, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6388 - accuracy: 0.6277 - val_loss: 0.6567 - val_accuracy: 0.6012\n",
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6392 - accuracy: 0.6229\n",
      "Epoch 00067: val_loss did not improve from 0.65666\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6388 - accuracy: 0.6235 - val_loss: 0.6598 - val_accuracy: 0.5946\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6378 - accuracy: 0.6270\n",
      "Epoch 00068: val_loss improved from 0.65666 to 0.65070, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6375 - accuracy: 0.6274 - val_loss: 0.6507 - val_accuracy: 0.6109\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6374 - accuracy: 0.6327\n",
      "Epoch 00069: val_loss did not improve from 0.65070\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6372 - accuracy: 0.6329 - val_loss: 0.6525 - val_accuracy: 0.6092\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6376 - accuracy: 0.6262\n",
      "Epoch 00070: val_loss did not improve from 0.65070\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6374 - accuracy: 0.6265 - val_loss: 0.6512 - val_accuracy: 0.6098\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6360 - accuracy: 0.6267\n",
      "Epoch 00071: val_loss improved from 0.65070 to 0.64777, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6356 - accuracy: 0.6274 - val_loss: 0.6478 - val_accuracy: 0.6129\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6345 - accuracy: 0.6342\n",
      "Epoch 00072: val_loss did not improve from 0.64777\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6342 - accuracy: 0.6345 - val_loss: 0.6489 - val_accuracy: 0.6111\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.6308\n",
      "Epoch 00073: val_loss did not improve from 0.64777\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6336 - accuracy: 0.6311 - val_loss: 0.6488 - val_accuracy: 0.6107\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.6326\n",
      "Epoch 00074: val_loss did not improve from 0.64777\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6337 - accuracy: 0.6327 - val_loss: 0.6515 - val_accuracy: 0.6077\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6332 - accuracy: 0.6287\n",
      "Epoch 00075: val_loss improved from 0.64777 to 0.64738, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6331 - accuracy: 0.6292 - val_loss: 0.6474 - val_accuracy: 0.6126\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.6293\n",
      "Epoch 00076: val_loss improved from 0.64738 to 0.64272, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6333 - accuracy: 0.6297 - val_loss: 0.6427 - val_accuracy: 0.6191\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6326 - accuracy: 0.6328\n",
      "Epoch 00077: val_loss did not improve from 0.64272\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6325 - accuracy: 0.6329 - val_loss: 0.6485 - val_accuracy: 0.6092\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6309 - accuracy: 0.6338\n",
      "Epoch 00078: val_loss did not improve from 0.64272\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6308 - accuracy: 0.6340 - val_loss: 0.6447 - val_accuracy: 0.6129\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6313 - accuracy: 0.6357\n",
      "Epoch 00079: val_loss improved from 0.64272 to 0.63914, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6310 - accuracy: 0.6361 - val_loss: 0.6391 - val_accuracy: 0.6247\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6305 - accuracy: 0.6369\n",
      "Epoch 00080: val_loss did not improve from 0.63914\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6303 - accuracy: 0.6370 - val_loss: 0.6398 - val_accuracy: 0.6225\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6306 - accuracy: 0.6373\n",
      "Epoch 00081: val_loss improved from 0.63914 to 0.63585, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6304 - accuracy: 0.6376 - val_loss: 0.6358 - val_accuracy: 0.6283\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6282 - accuracy: 0.6387\n",
      "Epoch 00082: val_loss did not improve from 0.63585\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6282 - accuracy: 0.6387 - val_loss: 0.6405 - val_accuracy: 0.6204\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6273 - accuracy: 0.6377\n",
      "Epoch 00083: val_loss did not improve from 0.63585\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6271 - accuracy: 0.6378 - val_loss: 0.6364 - val_accuracy: 0.6273\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6286 - accuracy: 0.6399\n",
      "Epoch 00084: val_loss improved from 0.63585 to 0.63583, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6284 - accuracy: 0.6402 - val_loss: 0.6358 - val_accuracy: 0.6270\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6270 - accuracy: 0.6385\n",
      "Epoch 00085: val_loss improved from 0.63583 to 0.63357, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6267 - accuracy: 0.6387 - val_loss: 0.6336 - val_accuracy: 0.6324\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6259 - accuracy: 0.6404\n",
      "Epoch 00086: val_loss did not improve from 0.63357\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6259 - accuracy: 0.6402 - val_loss: 0.6371 - val_accuracy: 0.6247\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6268 - accuracy: 0.6421\n",
      "Epoch 00087: val_loss did not improve from 0.63357\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6268 - accuracy: 0.6422 - val_loss: 0.6336 - val_accuracy: 0.6298\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.6421\n",
      "Epoch 00088: val_loss improved from 0.63357 to 0.63064, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6244 - accuracy: 0.6422 - val_loss: 0.6306 - val_accuracy: 0.6348\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6266 - accuracy: 0.6414\n",
      "Epoch 00089: val_loss did not improve from 0.63064\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6265 - accuracy: 0.6414 - val_loss: 0.6323 - val_accuracy: 0.6313\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6243 - accuracy: 0.6404\n",
      "Epoch 00090: val_loss improved from 0.63064 to 0.62358, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6239 - accuracy: 0.6407 - val_loss: 0.6236 - val_accuracy: 0.6421\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6251 - accuracy: 0.6438\n",
      "Epoch 00091: val_loss did not improve from 0.62358\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6249 - accuracy: 0.6438 - val_loss: 0.6277 - val_accuracy: 0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6235 - accuracy: 0.6441\n",
      "Epoch 00092: val_loss improved from 0.62358 to 0.62310, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6232 - accuracy: 0.6442 - val_loss: 0.6231 - val_accuracy: 0.6406\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6236 - accuracy: 0.6387\n",
      "Epoch 00093: val_loss did not improve from 0.62310\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6236 - accuracy: 0.6386 - val_loss: 0.6289 - val_accuracy: 0.6339\n",
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6227 - accuracy: 0.6426\n",
      "Epoch 00094: val_loss did not improve from 0.62310\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6225 - accuracy: 0.6429 - val_loss: 0.6300 - val_accuracy: 0.6337\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6219 - accuracy: 0.6434\n",
      "Epoch 00095: val_loss did not improve from 0.62310\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6218 - accuracy: 0.6435 - val_loss: 0.6277 - val_accuracy: 0.6350\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6197 - accuracy: 0.6516\n",
      "Epoch 00096: val_loss did not improve from 0.62310\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6196 - accuracy: 0.6516 - val_loss: 0.6244 - val_accuracy: 0.6395\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.6469\n",
      "Epoch 00097: val_loss improved from 0.62310 to 0.62143, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6196 - accuracy: 0.6474 - val_loss: 0.6214 - val_accuracy: 0.6406\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.6463\n",
      "Epoch 00098: val_loss did not improve from 0.62143\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6197 - accuracy: 0.6464 - val_loss: 0.6215 - val_accuracy: 0.6389\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6194 - accuracy: 0.6477\n",
      "Epoch 00099: val_loss improved from 0.62143 to 0.61816, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6192 - accuracy: 0.6478 - val_loss: 0.6182 - val_accuracy: 0.6451\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6191 - accuracy: 0.6486\n",
      "Epoch 00100: val_loss did not improve from 0.61816\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6189 - accuracy: 0.6490 - val_loss: 0.6194 - val_accuracy: 0.6414\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.6498\n",
      "Epoch 00101: val_loss improved from 0.61816 to 0.61692, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6164 - accuracy: 0.6500 - val_loss: 0.6169 - val_accuracy: 0.6440\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6163 - accuracy: 0.6513\n",
      "Epoch 00102: val_loss did not improve from 0.61692\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6164 - accuracy: 0.6510 - val_loss: 0.6248 - val_accuracy: 0.6363\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.6459\n",
      "Epoch 00103: val_loss did not improve from 0.61692\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6182 - accuracy: 0.6460 - val_loss: 0.6195 - val_accuracy: 0.6423\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.6500\n",
      "Epoch 00104: val_loss improved from 0.61692 to 0.61490, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6156 - accuracy: 0.6503 - val_loss: 0.6149 - val_accuracy: 0.6483\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6149 - accuracy: 0.6535\n",
      "Epoch 00105: val_loss did not improve from 0.61490\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6147 - accuracy: 0.6534 - val_loss: 0.6185 - val_accuracy: 0.6425\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6139 - accuracy: 0.6535\n",
      "Epoch 00106: val_loss did not improve from 0.61490\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6138 - accuracy: 0.6537 - val_loss: 0.6159 - val_accuracy: 0.6449\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.6541\n",
      "Epoch 00107: val_loss did not improve from 0.61490\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6137 - accuracy: 0.6539 - val_loss: 0.6156 - val_accuracy: 0.6432\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6121 - accuracy: 0.6573\n",
      "Epoch 00108: val_loss improved from 0.61490 to 0.61214, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6119 - accuracy: 0.6578 - val_loss: 0.6121 - val_accuracy: 0.6492\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6106 - accuracy: 0.6579\n",
      "Epoch 00109: val_loss did not improve from 0.61214\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6107 - accuracy: 0.6579 - val_loss: 0.6154 - val_accuracy: 0.6427\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6109 - accuracy: 0.6587\n",
      "Epoch 00110: val_loss did not improve from 0.61214\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6111 - accuracy: 0.6583 - val_loss: 0.6159 - val_accuracy: 0.6427\n",
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6104 - accuracy: 0.6558\n",
      "Epoch 00111: val_loss did not improve from 0.61214\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6105 - accuracy: 0.6559 - val_loss: 0.6138 - val_accuracy: 0.6438\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6091 - accuracy: 0.6564\n",
      "Epoch 00112: val_loss improved from 0.61214 to 0.61137, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6093 - accuracy: 0.6564 - val_loss: 0.6114 - val_accuracy: 0.6453\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6075 - accuracy: 0.6634\n",
      "Epoch 00113: val_loss did not improve from 0.61137\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6076 - accuracy: 0.6631 - val_loss: 0.6148 - val_accuracy: 0.6434\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6062 - accuracy: 0.6635\n",
      "Epoch 00114: val_loss improved from 0.61137 to 0.60979, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6061 - accuracy: 0.6636 - val_loss: 0.6098 - val_accuracy: 0.6483\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6052 - accuracy: 0.6627\n",
      "Epoch 00115: val_loss did not improve from 0.60979\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6053 - accuracy: 0.6625 - val_loss: 0.6131 - val_accuracy: 0.6445\n",
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6058 - accuracy: 0.6645\n",
      "Epoch 00116: val_loss did not improve from 0.60979\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6058 - accuracy: 0.6642 - val_loss: 0.6103 - val_accuracy: 0.6485\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.6661\n",
      "Epoch 00117: val_loss did not improve from 0.60979\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6023 - accuracy: 0.6656 - val_loss: 0.6104 - val_accuracy: 0.6472\n",
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.6648\n",
      "Epoch 00118: val_loss did not improve from 0.60979\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6025 - accuracy: 0.6649 - val_loss: 0.6127 - val_accuracy: 0.6421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6014 - accuracy: 0.6705\n",
      "Epoch 00119: val_loss did not improve from 0.60979\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6014 - accuracy: 0.6703 - val_loss: 0.6134 - val_accuracy: 0.6414\n",
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.6696\n",
      "Epoch 00120: val_loss improved from 0.60979 to 0.60913, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6003 - accuracy: 0.6692 - val_loss: 0.6091 - val_accuracy: 0.6500\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5982 - accuracy: 0.6733\n",
      "Epoch 00121: val_loss did not improve from 0.60913\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5985 - accuracy: 0.6729 - val_loss: 0.6165 - val_accuracy: 0.6387\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.6732\n",
      "Epoch 00122: val_loss improved from 0.60913 to 0.60834, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5975 - accuracy: 0.6727 - val_loss: 0.6083 - val_accuracy: 0.6494\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5953 - accuracy: 0.6707\n",
      "Epoch 00123: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5958 - accuracy: 0.6701 - val_loss: 0.6181 - val_accuracy: 0.6356\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.6765\n",
      "Epoch 00124: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5943 - accuracy: 0.6761 - val_loss: 0.6095 - val_accuracy: 0.6485\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.6790\n",
      "Epoch 00125: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5926 - accuracy: 0.6788 - val_loss: 0.6092 - val_accuracy: 0.6481\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5887 - accuracy: 0.6804\n",
      "Epoch 00126: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5892 - accuracy: 0.6800 - val_loss: 0.6161 - val_accuracy: 0.6404\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.6832\n",
      "Epoch 00127: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5882 - accuracy: 0.6824 - val_loss: 0.6135 - val_accuracy: 0.6425\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5861 - accuracy: 0.6798\n",
      "Epoch 00128: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5864 - accuracy: 0.6792 - val_loss: 0.6190 - val_accuracy: 0.6363\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.6835\n",
      "Epoch 00129: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5853 - accuracy: 0.6828 - val_loss: 0.6236 - val_accuracy: 0.6307\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.6854\n",
      "Epoch 00130: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5842 - accuracy: 0.6848 - val_loss: 0.6187 - val_accuracy: 0.6359\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5818 - accuracy: 0.6858\n",
      "Epoch 00131: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5825 - accuracy: 0.6853 - val_loss: 0.6224 - val_accuracy: 0.6344\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5807 - accuracy: 0.6889\n",
      "Epoch 00132: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5811 - accuracy: 0.6881 - val_loss: 0.6310 - val_accuracy: 0.6283\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5797 - accuracy: 0.6894\n",
      "Epoch 00133: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5800 - accuracy: 0.6887 - val_loss: 0.6331 - val_accuracy: 0.6258\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5750 - accuracy: 0.6921\n",
      "Epoch 00134: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5755 - accuracy: 0.6912 - val_loss: 0.6372 - val_accuracy: 0.6232\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.6922\n",
      "Epoch 00135: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5736 - accuracy: 0.6915 - val_loss: 0.6340 - val_accuracy: 0.6264\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.6958\n",
      "Epoch 00136: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5730 - accuracy: 0.6949 - val_loss: 0.6464 - val_accuracy: 0.6141\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.6984\n",
      "Epoch 00137: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5711 - accuracy: 0.6978 - val_loss: 0.6446 - val_accuracy: 0.6202\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5713 - accuracy: 0.6983\n",
      "Epoch 00138: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5715 - accuracy: 0.6981 - val_loss: 0.6479 - val_accuracy: 0.6187\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.6998\n",
      "Epoch 00139: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5683 - accuracy: 0.6994 - val_loss: 0.6496 - val_accuracy: 0.6163\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7022\n",
      "Epoch 00140: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5655 - accuracy: 0.7017 - val_loss: 0.6502 - val_accuracy: 0.6148\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7020\n",
      "Epoch 00141: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5666 - accuracy: 0.7019 - val_loss: 0.6574 - val_accuracy: 0.6077\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7046\n",
      "Epoch 00142: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5638 - accuracy: 0.7042 - val_loss: 0.6633 - val_accuracy: 0.6075\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7055\n",
      "Epoch 00143: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5636 - accuracy: 0.7049 - val_loss: 0.6515 - val_accuracy: 0.6131\n",
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7044\n",
      "Epoch 00144: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5633 - accuracy: 0.7039 - val_loss: 0.6557 - val_accuracy: 0.6079\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7032\n",
      "Epoch 00145: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5611 - accuracy: 0.7029 - val_loss: 0.6313 - val_accuracy: 0.6258\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7083\n",
      "Epoch 00146: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5579 - accuracy: 0.7076 - val_loss: 0.6579 - val_accuracy: 0.6083\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5574 - accuracy: 0.7106\n",
      "Epoch 00147: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5580 - accuracy: 0.7100 - val_loss: 0.6603 - val_accuracy: 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5561 - accuracy: 0.7137\n",
      "Epoch 00148: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5570 - accuracy: 0.7128 - val_loss: 0.6675 - val_accuracy: 0.6116\n",
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5551 - accuracy: 0.7091\n",
      "Epoch 00149: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5558 - accuracy: 0.7083 - val_loss: 0.6407 - val_accuracy: 0.6210\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5557 - accuracy: 0.7103\n",
      "Epoch 00150: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5562 - accuracy: 0.7102 - val_loss: 0.6576 - val_accuracy: 0.6197\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7169\n",
      "Epoch 00151: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5510 - accuracy: 0.7166 - val_loss: 0.6638 - val_accuracy: 0.6161\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7146\n",
      "Epoch 00152: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5511 - accuracy: 0.7144 - val_loss: 0.6493 - val_accuracy: 0.6260\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7132\n",
      "Epoch 00153: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5516 - accuracy: 0.7125 - val_loss: 0.6467 - val_accuracy: 0.6264\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5519 - accuracy: 0.7151\n",
      "Epoch 00154: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5522 - accuracy: 0.7147 - val_loss: 0.6543 - val_accuracy: 0.6212\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5480 - accuracy: 0.7192\n",
      "Epoch 00155: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5486 - accuracy: 0.7188 - val_loss: 0.6680 - val_accuracy: 0.6161\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7198\n",
      "Epoch 00156: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5503 - accuracy: 0.7195 - val_loss: 0.6303 - val_accuracy: 0.6350\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5484 - accuracy: 0.7173\n",
      "Epoch 00157: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5486 - accuracy: 0.7170 - val_loss: 0.6332 - val_accuracy: 0.6348\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7198\n",
      "Epoch 00158: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5469 - accuracy: 0.7188 - val_loss: 0.6306 - val_accuracy: 0.6352\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5456 - accuracy: 0.7237\n",
      "Epoch 00159: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5459 - accuracy: 0.7234 - val_loss: 0.6358 - val_accuracy: 0.6341\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5456 - accuracy: 0.7224\n",
      "Epoch 00160: val_loss did not improve from 0.60834\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5464 - accuracy: 0.7217 - val_loss: 0.6099 - val_accuracy: 0.6498\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.7207\n",
      "Epoch 00161: val_loss improved from 0.60834 to 0.60307, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5463 - accuracy: 0.7204 - val_loss: 0.6031 - val_accuracy: 0.6494\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5440 - accuracy: 0.7205\n",
      "Epoch 00162: val_loss did not improve from 0.60307\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5444 - accuracy: 0.7203 - val_loss: 0.6327 - val_accuracy: 0.6402\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7272\n",
      "Epoch 00163: val_loss improved from 0.60307 to 0.60194, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5403 - accuracy: 0.7266 - val_loss: 0.6019 - val_accuracy: 0.6531\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7222\n",
      "Epoch 00164: val_loss did not improve from 0.60194\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5426 - accuracy: 0.7218 - val_loss: 0.6202 - val_accuracy: 0.6488\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7266\n",
      "Epoch 00165: val_loss did not improve from 0.60194\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5403 - accuracy: 0.7260 - val_loss: 0.6524 - val_accuracy: 0.6258\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5406 - accuracy: 0.7247\n",
      "Epoch 00166: val_loss did not improve from 0.60194\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5410 - accuracy: 0.7242 - val_loss: 0.6351 - val_accuracy: 0.6402\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5378 - accuracy: 0.7260\n",
      "Epoch 00167: val_loss did not improve from 0.60194\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5384 - accuracy: 0.7257 - val_loss: 0.6331 - val_accuracy: 0.6425\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7304\n",
      "Epoch 00168: val_loss did not improve from 0.60194\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5386 - accuracy: 0.7299 - val_loss: 0.6093 - val_accuracy: 0.6604\n",
      "Epoch 169/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7275\n",
      "Epoch 00169: val_loss improved from 0.60194 to 0.58693, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5357 - accuracy: 0.7267 - val_loss: 0.5869 - val_accuracy: 0.6670\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7300\n",
      "Epoch 00170: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5358 - accuracy: 0.7293 - val_loss: 0.5929 - val_accuracy: 0.6621\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7292\n",
      "Epoch 00171: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5358 - accuracy: 0.7286 - val_loss: 0.5927 - val_accuracy: 0.6619\n",
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7296\n",
      "Epoch 00172: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5357 - accuracy: 0.7296 - val_loss: 0.6024 - val_accuracy: 0.6571\n",
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7280\n",
      "Epoch 00173: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5368 - accuracy: 0.7277 - val_loss: 0.5963 - val_accuracy: 0.6649\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7303\n",
      "Epoch 00174: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5357 - accuracy: 0.7297 - val_loss: 0.5949 - val_accuracy: 0.6625\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7311\n",
      "Epoch 00175: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5360 - accuracy: 0.7304 - val_loss: 0.5954 - val_accuracy: 0.6634\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5327 - accuracy: 0.7349\n",
      "Epoch 00176: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5335 - accuracy: 0.7340 - val_loss: 0.6223 - val_accuracy: 0.6470\n",
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7342\n",
      "Epoch 00177: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5302 - accuracy: 0.7332 - val_loss: 0.5989 - val_accuracy: 0.6644\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.7347\n",
      "Epoch 00178: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5279 - accuracy: 0.7344 - val_loss: 0.5887 - val_accuracy: 0.6668\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7313\n",
      "Epoch 00179: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5298 - accuracy: 0.7311 - val_loss: 0.5909 - val_accuracy: 0.6685\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5288 - accuracy: 0.7338\n",
      "Epoch 00180: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5293 - accuracy: 0.7337 - val_loss: 0.5905 - val_accuracy: 0.6687\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7359\n",
      "Epoch 00181: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5265 - accuracy: 0.7357 - val_loss: 0.6054 - val_accuracy: 0.6576\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7320\n",
      "Epoch 00182: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5297 - accuracy: 0.7319 - val_loss: 0.5963 - val_accuracy: 0.6668\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5264 - accuracy: 0.7352\n",
      "Epoch 00183: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5269 - accuracy: 0.7348 - val_loss: 0.6012 - val_accuracy: 0.6640\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7375\n",
      "Epoch 00184: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5281 - accuracy: 0.7372 - val_loss: 0.6061 - val_accuracy: 0.6580\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7359\n",
      "Epoch 00185: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5249 - accuracy: 0.7357 - val_loss: 0.6015 - val_accuracy: 0.6670\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7381\n",
      "Epoch 00186: val_loss did not improve from 0.58693\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5259 - accuracy: 0.7378 - val_loss: 0.6168 - val_accuracy: 0.6586\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5281 - accuracy: 0.7343\n",
      "Epoch 00187: val_loss improved from 0.58693 to 0.57031, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5291 - accuracy: 0.7335 - val_loss: 0.5703 - val_accuracy: 0.6840\n",
      "Epoch 188/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.7372\n",
      "Epoch 00188: val_loss did not improve from 0.57031\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5249 - accuracy: 0.7368 - val_loss: 0.5921 - val_accuracy: 0.6705\n",
      "Epoch 189/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7364\n",
      "Epoch 00189: val_loss did not improve from 0.57031\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5258 - accuracy: 0.7359 - val_loss: 0.5965 - val_accuracy: 0.6702\n",
      "Epoch 190/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.7340\n",
      "Epoch 00190: val_loss did not improve from 0.57031\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5278 - accuracy: 0.7336 - val_loss: 0.5884 - val_accuracy: 0.6741\n",
      "Epoch 191/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5216 - accuracy: 0.7417\n",
      "Epoch 00191: val_loss improved from 0.57031 to 0.55721, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5219 - accuracy: 0.7414 - val_loss: 0.5572 - val_accuracy: 0.6978\n",
      "Epoch 192/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7395\n",
      "Epoch 00192: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5241 - accuracy: 0.7390 - val_loss: 0.5646 - val_accuracy: 0.6885\n",
      "Epoch 193/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7397\n",
      "Epoch 00193: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5223 - accuracy: 0.7395 - val_loss: 0.5795 - val_accuracy: 0.6831\n",
      "Epoch 194/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5186 - accuracy: 0.7417\n",
      "Epoch 00194: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5188 - accuracy: 0.7418 - val_loss: 0.5766 - val_accuracy: 0.6806\n",
      "Epoch 195/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5214 - accuracy: 0.7413\n",
      "Epoch 00195: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5218 - accuracy: 0.7408 - val_loss: 0.5933 - val_accuracy: 0.6735\n",
      "Epoch 196/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.7418\n",
      "Epoch 00196: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5151 - accuracy: 0.7410 - val_loss: 0.5681 - val_accuracy: 0.6926\n",
      "Epoch 197/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5181 - accuracy: 0.7416\n",
      "Epoch 00197: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5188 - accuracy: 0.7412 - val_loss: 0.5865 - val_accuracy: 0.6806\n",
      "Epoch 198/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7401\n",
      "Epoch 00198: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5205 - accuracy: 0.7398 - val_loss: 0.5798 - val_accuracy: 0.6816\n",
      "Epoch 199/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5192 - accuracy: 0.7386\n",
      "Epoch 00199: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5199 - accuracy: 0.7383 - val_loss: 0.5636 - val_accuracy: 0.6915\n",
      "Epoch 200/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7394\n",
      "Epoch 00200: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5199 - accuracy: 0.7387 - val_loss: 0.6072 - val_accuracy: 0.6642\n",
      "Epoch 201/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5180 - accuracy: 0.7410\n",
      "Epoch 00201: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5187 - accuracy: 0.7408 - val_loss: 0.5716 - val_accuracy: 0.6857\n",
      "Epoch 202/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7463\n",
      "Epoch 00202: val_loss did not improve from 0.55721\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5157 - accuracy: 0.7459 - val_loss: 0.5835 - val_accuracy: 0.6810\n",
      "Epoch 203/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5186 - accuracy: 0.7412\n",
      "Epoch 00203: val_loss improved from 0.55721 to 0.55283, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5194 - accuracy: 0.7406 - val_loss: 0.5528 - val_accuracy: 0.7021\n",
      "Epoch 204/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7464\n",
      "Epoch 00204: val_loss improved from 0.55283 to 0.55237, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5149 - accuracy: 0.7455 - val_loss: 0.5524 - val_accuracy: 0.7016\n",
      "Epoch 205/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7459\n",
      "Epoch 00205: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5148 - accuracy: 0.7455 - val_loss: 0.5611 - val_accuracy: 0.6954\n",
      "Epoch 206/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7438\n",
      "Epoch 00206: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5154 - accuracy: 0.7434 - val_loss: 0.5673 - val_accuracy: 0.6926\n",
      "Epoch 207/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5128 - accuracy: 0.7479\n",
      "Epoch 00207: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5135 - accuracy: 0.7473 - val_loss: 0.5809 - val_accuracy: 0.6849\n",
      "Epoch 208/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7424\n",
      "Epoch 00208: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5148 - accuracy: 0.7423 - val_loss: 0.5661 - val_accuracy: 0.6933\n",
      "Epoch 209/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5127 - accuracy: 0.7440\n",
      "Epoch 00209: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5137 - accuracy: 0.7434 - val_loss: 0.5807 - val_accuracy: 0.6849\n",
      "Epoch 210/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7459\n",
      "Epoch 00210: val_loss did not improve from 0.55237\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5131 - accuracy: 0.7457 - val_loss: 0.5818 - val_accuracy: 0.6804\n",
      "Epoch 211/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5127 - accuracy: 0.7481\n",
      "Epoch 00211: val_loss improved from 0.55237 to 0.53140, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5133 - accuracy: 0.7476 - val_loss: 0.5314 - val_accuracy: 0.7244\n",
      "Epoch 212/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7483\n",
      "Epoch 00212: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5146 - accuracy: 0.7475 - val_loss: 0.5771 - val_accuracy: 0.6874\n",
      "Epoch 213/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5080 - accuracy: 0.7464\n",
      "Epoch 00213: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5086 - accuracy: 0.7462 - val_loss: 0.5898 - val_accuracy: 0.6834\n",
      "Epoch 214/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7477\n",
      "Epoch 00214: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5111 - accuracy: 0.7476 - val_loss: 0.5407 - val_accuracy: 0.7109\n",
      "Epoch 215/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5077 - accuracy: 0.7526\n",
      "Epoch 00215: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5079 - accuracy: 0.7526 - val_loss: 0.5399 - val_accuracy: 0.7115\n",
      "Epoch 216/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7465\n",
      "Epoch 00216: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5110 - accuracy: 0.7460 - val_loss: 0.5794 - val_accuracy: 0.6851\n",
      "Epoch 217/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5097 - accuracy: 0.7498\n",
      "Epoch 00217: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5097 - accuracy: 0.7497 - val_loss: 0.5959 - val_accuracy: 0.6773\n",
      "Epoch 218/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5083 - accuracy: 0.7482\n",
      "Epoch 00218: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5095 - accuracy: 0.7478 - val_loss: 0.5641 - val_accuracy: 0.6948\n",
      "Epoch 219/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5098 - accuracy: 0.7489\n",
      "Epoch 00219: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5107 - accuracy: 0.7481 - val_loss: 0.5798 - val_accuracy: 0.6829\n",
      "Epoch 220/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7479\n",
      "Epoch 00220: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5093 - accuracy: 0.7476 - val_loss: 0.5533 - val_accuracy: 0.7036\n",
      "Epoch 221/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7488\n",
      "Epoch 00221: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5094 - accuracy: 0.7483 - val_loss: 0.5330 - val_accuracy: 0.7195\n",
      "Epoch 222/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7489\n",
      "Epoch 00222: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5076 - accuracy: 0.7483 - val_loss: 0.5496 - val_accuracy: 0.7061\n",
      "Epoch 223/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.7527\n",
      "Epoch 00223: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5061 - accuracy: 0.7524 - val_loss: 0.5532 - val_accuracy: 0.7066\n",
      "Epoch 224/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5023 - accuracy: 0.7538\n",
      "Epoch 00224: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5032 - accuracy: 0.7532 - val_loss: 0.5545 - val_accuracy: 0.7042\n",
      "Epoch 225/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7529\n",
      "Epoch 00225: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5044 - accuracy: 0.7525 - val_loss: 0.5499 - val_accuracy: 0.7074\n",
      "Epoch 226/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5079 - accuracy: 0.7503\n",
      "Epoch 00226: val_loss did not improve from 0.53140\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5087 - accuracy: 0.7500 - val_loss: 0.5547 - val_accuracy: 0.7036\n",
      "Epoch 227/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.7505\n",
      "Epoch 00227: val_loss improved from 0.53140 to 0.51966, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5030 - accuracy: 0.7503 - val_loss: 0.5197 - val_accuracy: 0.7311\n",
      "Epoch 228/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.7542\n",
      "Epoch 00228: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5034 - accuracy: 0.7539 - val_loss: 0.5307 - val_accuracy: 0.7201\n",
      "Epoch 229/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.7497\n",
      "Epoch 00229: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5062 - accuracy: 0.7493 - val_loss: 0.5604 - val_accuracy: 0.7003\n",
      "Epoch 230/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7550\n",
      "Epoch 00230: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5064 - accuracy: 0.7548 - val_loss: 0.5819 - val_accuracy: 0.6870\n",
      "Epoch 231/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5016 - accuracy: 0.7544\n",
      "Epoch 00231: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5024 - accuracy: 0.7542 - val_loss: 0.5443 - val_accuracy: 0.7122\n",
      "Epoch 232/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.7486\n",
      "Epoch 00232: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5031 - accuracy: 0.7484 - val_loss: 0.5343 - val_accuracy: 0.7167\n",
      "Epoch 233/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4971 - accuracy: 0.7562\n",
      "Epoch 00233: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4974 - accuracy: 0.7560 - val_loss: 0.5438 - val_accuracy: 0.7132\n",
      "Epoch 234/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7514\n",
      "Epoch 00234: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5024 - accuracy: 0.7510 - val_loss: 0.5556 - val_accuracy: 0.7029\n",
      "Epoch 235/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5025 - accuracy: 0.7515\n",
      "Epoch 00235: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5032 - accuracy: 0.7507 - val_loss: 0.5533 - val_accuracy: 0.7064\n",
      "Epoch 236/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5012 - accuracy: 0.7524\n",
      "Epoch 00236: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5015 - accuracy: 0.7522 - val_loss: 0.5504 - val_accuracy: 0.7094\n",
      "Epoch 237/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.7509\n",
      "Epoch 00237: val_loss did not improve from 0.51966\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5037 - accuracy: 0.7509 - val_loss: 0.5556 - val_accuracy: 0.7046\n",
      "Epoch 238/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5006 - accuracy: 0.7544\n",
      "Epoch 00238: val_loss improved from 0.51966 to 0.51850, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5013 - accuracy: 0.7541 - val_loss: 0.5185 - val_accuracy: 0.7341\n",
      "Epoch 239/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.7558\n",
      "Epoch 00239: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4974 - accuracy: 0.7553 - val_loss: 0.5401 - val_accuracy: 0.7143\n",
      "Epoch 240/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5012 - accuracy: 0.7535\n",
      "Epoch 00240: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5015 - accuracy: 0.7534 - val_loss: 0.5395 - val_accuracy: 0.7152\n",
      "Epoch 241/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7543\n",
      "Epoch 00241: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5013 - accuracy: 0.7542 - val_loss: 0.5551 - val_accuracy: 0.7044\n",
      "Epoch 242/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7562\n",
      "Epoch 00242: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4985 - accuracy: 0.7561 - val_loss: 0.5488 - val_accuracy: 0.7104\n",
      "Epoch 243/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4995 - accuracy: 0.7547\n",
      "Epoch 00243: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4997 - accuracy: 0.7542 - val_loss: 0.5194 - val_accuracy: 0.7313\n",
      "Epoch 244/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4978 - accuracy: 0.7537\n",
      "Epoch 00244: val_loss did not improve from 0.51850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4980 - accuracy: 0.7535 - val_loss: 0.5477 - val_accuracy: 0.7126\n",
      "Epoch 245/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.7565\n",
      "Epoch 00245: val_loss improved from 0.51850 to 0.50923, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4973 - accuracy: 0.7561 - val_loss: 0.5092 - val_accuracy: 0.7412\n",
      "Epoch 246/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.7519\n",
      "Epoch 00246: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5028 - accuracy: 0.7519 - val_loss: 0.5623 - val_accuracy: 0.6999\n",
      "Epoch 247/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.7539\n",
      "Epoch 00247: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4989 - accuracy: 0.7533 - val_loss: 0.5518 - val_accuracy: 0.7111\n",
      "Epoch 248/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7553\n",
      "Epoch 00248: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5001 - accuracy: 0.7553 - val_loss: 0.5656 - val_accuracy: 0.6997\n",
      "Epoch 249/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4962 - accuracy: 0.7559\n",
      "Epoch 00249: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4969 - accuracy: 0.7555 - val_loss: 0.5216 - val_accuracy: 0.7313\n",
      "Epoch 250/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4972 - accuracy: 0.7580\n",
      "Epoch 00250: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4977 - accuracy: 0.7579 - val_loss: 0.5348 - val_accuracy: 0.7210\n",
      "Epoch 251/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4933 - accuracy: 0.7580\n",
      "Epoch 00251: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4940 - accuracy: 0.7577 - val_loss: 0.5253 - val_accuracy: 0.7272\n",
      "Epoch 252/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.7550\n",
      "Epoch 00252: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4985 - accuracy: 0.7550 - val_loss: 0.5833 - val_accuracy: 0.6874\n",
      "Epoch 253/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7579\n",
      "Epoch 00253: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4948 - accuracy: 0.7579 - val_loss: 0.5598 - val_accuracy: 0.7051\n",
      "Epoch 254/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4925 - accuracy: 0.7564\n",
      "Epoch 00254: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4931 - accuracy: 0.7564 - val_loss: 0.5404 - val_accuracy: 0.7171\n",
      "Epoch 255/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7573\n",
      "Epoch 00255: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4963 - accuracy: 0.7570 - val_loss: 0.5472 - val_accuracy: 0.7124\n",
      "Epoch 256/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7594\n",
      "Epoch 00256: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4911 - accuracy: 0.7592 - val_loss: 0.5406 - val_accuracy: 0.7175\n",
      "Epoch 257/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.7605\n",
      "Epoch 00257: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4942 - accuracy: 0.7601 - val_loss: 0.5425 - val_accuracy: 0.7150\n",
      "Epoch 258/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7582\n",
      "Epoch 00258: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4915 - accuracy: 0.7581 - val_loss: 0.5230 - val_accuracy: 0.7281\n",
      "Epoch 259/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4933 - accuracy: 0.7573\n",
      "Epoch 00259: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4941 - accuracy: 0.7572 - val_loss: 0.5362 - val_accuracy: 0.7203\n",
      "Epoch 260/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4940 - accuracy: 0.7585\n",
      "Epoch 00260: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4944 - accuracy: 0.7585 - val_loss: 0.5356 - val_accuracy: 0.7184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4917 - accuracy: 0.7599\n",
      "Epoch 00261: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4923 - accuracy: 0.7596 - val_loss: 0.5353 - val_accuracy: 0.7216\n",
      "Epoch 262/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.7594\n",
      "Epoch 00262: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4928 - accuracy: 0.7593 - val_loss: 0.5386 - val_accuracy: 0.7178\n",
      "Epoch 263/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4900 - accuracy: 0.7613\n",
      "Epoch 00263: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4905 - accuracy: 0.7610 - val_loss: 0.5297 - val_accuracy: 0.7259\n",
      "Epoch 264/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4921 - accuracy: 0.7593\n",
      "Epoch 00264: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4930 - accuracy: 0.7587 - val_loss: 0.5240 - val_accuracy: 0.7259\n",
      "Epoch 265/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4925 - accuracy: 0.7654\n",
      "Epoch 00265: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4928 - accuracy: 0.7651 - val_loss: 0.5339 - val_accuracy: 0.7248\n",
      "Epoch 266/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4926 - accuracy: 0.7580\n",
      "Epoch 00266: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4928 - accuracy: 0.7575 - val_loss: 0.5356 - val_accuracy: 0.7199\n",
      "Epoch 267/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4896 - accuracy: 0.7645\n",
      "Epoch 00267: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4899 - accuracy: 0.7644 - val_loss: 0.5669 - val_accuracy: 0.7006\n",
      "Epoch 268/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.7616\n",
      "Epoch 00268: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4911 - accuracy: 0.7613 - val_loss: 0.5197 - val_accuracy: 0.7300\n",
      "Epoch 269/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.7569\n",
      "Epoch 00269: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4918 - accuracy: 0.7568 - val_loss: 0.5464 - val_accuracy: 0.7143\n",
      "Epoch 270/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7630\n",
      "Epoch 00270: val_loss did not improve from 0.50923\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4848 - accuracy: 0.7625 - val_loss: 0.5285 - val_accuracy: 0.7251\n",
      "Epoch 271/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.7633\n",
      "Epoch 00271: val_loss improved from 0.50923 to 0.50365, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4894 - accuracy: 0.7629 - val_loss: 0.5036 - val_accuracy: 0.7436\n",
      "Epoch 272/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7573\n",
      "Epoch 00272: val_loss did not improve from 0.50365\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4912 - accuracy: 0.7573 - val_loss: 0.5182 - val_accuracy: 0.7350\n",
      "Epoch 273/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.7595\n",
      "Epoch 00273: val_loss did not improve from 0.50365\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4910 - accuracy: 0.7594 - val_loss: 0.5356 - val_accuracy: 0.7227\n",
      "Epoch 274/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7657\n",
      "Epoch 00274: val_loss improved from 0.50365 to 0.49493, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4872 - accuracy: 0.7651 - val_loss: 0.4949 - val_accuracy: 0.7483\n",
      "Epoch 275/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7641\n",
      "Epoch 00275: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4853 - accuracy: 0.7637 - val_loss: 0.5314 - val_accuracy: 0.7255\n",
      "Epoch 276/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7624\n",
      "Epoch 00276: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4877 - accuracy: 0.7623 - val_loss: 0.5323 - val_accuracy: 0.7257\n",
      "Epoch 277/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4869 - accuracy: 0.7639\n",
      "Epoch 00277: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4876 - accuracy: 0.7635 - val_loss: 0.5123 - val_accuracy: 0.7375\n",
      "Epoch 278/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.7601\n",
      "Epoch 00278: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4887 - accuracy: 0.7600 - val_loss: 0.5051 - val_accuracy: 0.7418\n",
      "Epoch 279/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4867 - accuracy: 0.7613\n",
      "Epoch 00279: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4874 - accuracy: 0.7610 - val_loss: 0.5179 - val_accuracy: 0.7334\n",
      "Epoch 280/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4897 - accuracy: 0.7630\n",
      "Epoch 00280: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4902 - accuracy: 0.7627 - val_loss: 0.5300 - val_accuracy: 0.7253\n",
      "Epoch 281/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7624\n",
      "Epoch 00281: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4881 - accuracy: 0.7622 - val_loss: 0.5112 - val_accuracy: 0.7369\n",
      "Epoch 282/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7642\n",
      "Epoch 00282: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4847 - accuracy: 0.7638 - val_loss: 0.5074 - val_accuracy: 0.7408\n",
      "Epoch 283/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7631\n",
      "Epoch 00283: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4851 - accuracy: 0.7628 - val_loss: 0.5267 - val_accuracy: 0.7287\n",
      "Epoch 284/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4833 - accuracy: 0.7616\n",
      "Epoch 00284: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4839 - accuracy: 0.7615 - val_loss: 0.5281 - val_accuracy: 0.7283\n",
      "Epoch 285/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.7612\n",
      "Epoch 00285: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4858 - accuracy: 0.7610 - val_loss: 0.5299 - val_accuracy: 0.7259\n",
      "Epoch 286/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7632\n",
      "Epoch 00286: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4848 - accuracy: 0.7628 - val_loss: 0.5004 - val_accuracy: 0.7457\n",
      "Epoch 287/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4818 - accuracy: 0.7650\n",
      "Epoch 00287: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4825 - accuracy: 0.7645 - val_loss: 0.5201 - val_accuracy: 0.7326\n",
      "Epoch 288/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4805 - accuracy: 0.7651\n",
      "Epoch 00288: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4814 - accuracy: 0.7648 - val_loss: 0.5244 - val_accuracy: 0.7302\n",
      "Epoch 289/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.7667\n",
      "Epoch 00289: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4817 - accuracy: 0.7666 - val_loss: 0.5338 - val_accuracy: 0.7236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4829 - accuracy: 0.7626\n",
      "Epoch 00290: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4840 - accuracy: 0.7619 - val_loss: 0.4999 - val_accuracy: 0.7463\n",
      "Epoch 291/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7639\n",
      "Epoch 00291: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4848 - accuracy: 0.7635 - val_loss: 0.5638 - val_accuracy: 0.7040\n",
      "Epoch 292/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.7652\n",
      "Epoch 00292: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4810 - accuracy: 0.7652 - val_loss: 0.5111 - val_accuracy: 0.7418\n",
      "Epoch 293/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4828 - accuracy: 0.7615\n",
      "Epoch 00293: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4831 - accuracy: 0.7613 - val_loss: 0.5153 - val_accuracy: 0.7371\n",
      "Epoch 294/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4810 - accuracy: 0.7649\n",
      "Epoch 00294: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4816 - accuracy: 0.7646 - val_loss: 0.5085 - val_accuracy: 0.7401\n",
      "Epoch 295/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4829 - accuracy: 0.7663\n",
      "Epoch 00295: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4833 - accuracy: 0.7660 - val_loss: 0.5248 - val_accuracy: 0.7311\n",
      "Epoch 296/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4823 - accuracy: 0.7633\n",
      "Epoch 00296: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4826 - accuracy: 0.7630 - val_loss: 0.5403 - val_accuracy: 0.7203\n",
      "Epoch 297/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4810 - accuracy: 0.7663\n",
      "Epoch 00297: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4812 - accuracy: 0.7661 - val_loss: 0.5199 - val_accuracy: 0.7332\n",
      "Epoch 298/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4816 - accuracy: 0.7668\n",
      "Epoch 00298: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4823 - accuracy: 0.7669 - val_loss: 0.5436 - val_accuracy: 0.7201\n",
      "Epoch 299/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.7666\n",
      "Epoch 00299: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4809 - accuracy: 0.7664 - val_loss: 0.4986 - val_accuracy: 0.7455\n",
      "Epoch 300/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.7618\n",
      "Epoch 00300: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4796 - accuracy: 0.7617 - val_loss: 0.5281 - val_accuracy: 0.7276\n",
      "Epoch 301/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.7638\n",
      "Epoch 00301: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4813 - accuracy: 0.7636 - val_loss: 0.5485 - val_accuracy: 0.7143\n",
      "Epoch 302/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4788 - accuracy: 0.7656\n",
      "Epoch 00302: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4792 - accuracy: 0.7655 - val_loss: 0.5280 - val_accuracy: 0.7289\n",
      "Epoch 303/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.7651\n",
      "Epoch 00303: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4810 - accuracy: 0.7650 - val_loss: 0.5122 - val_accuracy: 0.7384\n",
      "Epoch 304/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4795 - accuracy: 0.7646\n",
      "Epoch 00304: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4797 - accuracy: 0.7644 - val_loss: 0.5445 - val_accuracy: 0.7186\n",
      "Epoch 305/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7702\n",
      "Epoch 00305: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4781 - accuracy: 0.7700 - val_loss: 0.5197 - val_accuracy: 0.7356\n",
      "Epoch 306/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7681\n",
      "Epoch 00306: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4769 - accuracy: 0.7674 - val_loss: 0.5018 - val_accuracy: 0.7457\n",
      "Epoch 307/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.7662\n",
      "Epoch 00307: val_loss did not improve from 0.49493\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4813 - accuracy: 0.7658 - val_loss: 0.5095 - val_accuracy: 0.7405\n",
      "Epoch 308/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4741 - accuracy: 0.7708\n",
      "Epoch 00308: val_loss improved from 0.49493 to 0.49190, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4744 - accuracy: 0.7707 - val_loss: 0.4919 - val_accuracy: 0.7509\n",
      "Epoch 309/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4724 - accuracy: 0.7714\n",
      "Epoch 00309: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4728 - accuracy: 0.7711 - val_loss: 0.5303 - val_accuracy: 0.7261\n",
      "Epoch 310/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.7689\n",
      "Epoch 00310: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4765 - accuracy: 0.7688 - val_loss: 0.5335 - val_accuracy: 0.7261\n",
      "Epoch 311/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7696\n",
      "Epoch 00311: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4786 - accuracy: 0.7695 - val_loss: 0.5350 - val_accuracy: 0.7240\n",
      "Epoch 312/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7695\n",
      "Epoch 00312: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4753 - accuracy: 0.7692 - val_loss: 0.5065 - val_accuracy: 0.7427\n",
      "Epoch 313/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7660\n",
      "Epoch 00313: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4759 - accuracy: 0.7661 - val_loss: 0.5522 - val_accuracy: 0.7150\n",
      "Epoch 314/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.7694\n",
      "Epoch 00314: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4762 - accuracy: 0.7695 - val_loss: 0.5226 - val_accuracy: 0.7313\n",
      "Epoch 315/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4788 - accuracy: 0.7671\n",
      "Epoch 00315: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4795 - accuracy: 0.7670 - val_loss: 0.5284 - val_accuracy: 0.7274\n",
      "Epoch 316/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7724\n",
      "Epoch 00316: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4774 - accuracy: 0.7719 - val_loss: 0.5206 - val_accuracy: 0.7343\n",
      "Epoch 317/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7737\n",
      "Epoch 00317: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4769 - accuracy: 0.7736 - val_loss: 0.5383 - val_accuracy: 0.7210\n",
      "Epoch 318/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7677\n",
      "Epoch 00318: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4743 - accuracy: 0.7673 - val_loss: 0.5064 - val_accuracy: 0.7431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7703\n",
      "Epoch 00319: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4766 - accuracy: 0.7699 - val_loss: 0.5023 - val_accuracy: 0.7457\n",
      "Epoch 320/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.7706\n",
      "Epoch 00320: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4728 - accuracy: 0.7705 - val_loss: 0.5037 - val_accuracy: 0.7453\n",
      "Epoch 321/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7701\n",
      "Epoch 00321: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4739 - accuracy: 0.7701 - val_loss: 0.5225 - val_accuracy: 0.7319\n",
      "Epoch 322/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.7707\n",
      "Epoch 00322: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4743 - accuracy: 0.7708 - val_loss: 0.5225 - val_accuracy: 0.7313\n",
      "Epoch 323/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7686\n",
      "Epoch 00323: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4772 - accuracy: 0.7686 - val_loss: 0.5276 - val_accuracy: 0.7283\n",
      "Epoch 324/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7740\n",
      "Epoch 00324: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4717 - accuracy: 0.7743 - val_loss: 0.5303 - val_accuracy: 0.7261\n",
      "Epoch 325/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.7713\n",
      "Epoch 00325: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4744 - accuracy: 0.7715 - val_loss: 0.5433 - val_accuracy: 0.7186\n",
      "Epoch 326/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4710 - accuracy: 0.7747\n",
      "Epoch 00326: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4710 - accuracy: 0.7749 - val_loss: 0.5313 - val_accuracy: 0.7255\n",
      "Epoch 327/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.7701\n",
      "Epoch 00327: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4739 - accuracy: 0.7702 - val_loss: 0.5261 - val_accuracy: 0.7296\n",
      "Epoch 328/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.7687\n",
      "Epoch 00328: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4735 - accuracy: 0.7686 - val_loss: 0.4955 - val_accuracy: 0.7485\n",
      "Epoch 329/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4733 - accuracy: 0.7719\n",
      "Epoch 00329: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4739 - accuracy: 0.7718 - val_loss: 0.5165 - val_accuracy: 0.7354\n",
      "Epoch 330/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7716\n",
      "Epoch 00330: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4733 - accuracy: 0.7715 - val_loss: 0.5444 - val_accuracy: 0.7171\n",
      "Epoch 331/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4678 - accuracy: 0.7748\n",
      "Epoch 00331: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4679 - accuracy: 0.7748 - val_loss: 0.4979 - val_accuracy: 0.7509\n",
      "Epoch 332/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7715\n",
      "Epoch 00332: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4713 - accuracy: 0.7714 - val_loss: 0.5154 - val_accuracy: 0.7358\n",
      "Epoch 333/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.7734\n",
      "Epoch 00333: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4701 - accuracy: 0.7730 - val_loss: 0.4933 - val_accuracy: 0.7506\n",
      "Epoch 334/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7703\n",
      "Epoch 00334: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4694 - accuracy: 0.7703 - val_loss: 0.5380 - val_accuracy: 0.7214\n",
      "Epoch 335/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7722\n",
      "Epoch 00335: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4695 - accuracy: 0.7722 - val_loss: 0.5026 - val_accuracy: 0.7423\n",
      "Epoch 336/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.7727\n",
      "Epoch 00336: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4679 - accuracy: 0.7729 - val_loss: 0.5126 - val_accuracy: 0.7395\n",
      "Epoch 337/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4696 - accuracy: 0.7731\n",
      "Epoch 00337: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4700 - accuracy: 0.7730 - val_loss: 0.5201 - val_accuracy: 0.7337\n",
      "Epoch 338/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.7737\n",
      "Epoch 00338: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4704 - accuracy: 0.7737 - val_loss: 0.4995 - val_accuracy: 0.7463\n",
      "Epoch 339/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7725\n",
      "Epoch 00339: val_loss did not improve from 0.49190\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4713 - accuracy: 0.7725 - val_loss: 0.4986 - val_accuracy: 0.7500\n",
      "Epoch 340/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4695 - accuracy: 0.7726\n",
      "Epoch 00340: val_loss improved from 0.49190 to 0.49058, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4702 - accuracy: 0.7722 - val_loss: 0.4906 - val_accuracy: 0.7513\n",
      "Epoch 341/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.7731\n",
      "Epoch 00341: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4660 - accuracy: 0.7730 - val_loss: 0.5469 - val_accuracy: 0.7165\n",
      "Epoch 342/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4669 - accuracy: 0.7747\n",
      "Epoch 00342: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4671 - accuracy: 0.7747 - val_loss: 0.5276 - val_accuracy: 0.7302\n",
      "Epoch 343/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4668 - accuracy: 0.7746\n",
      "Epoch 00343: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4670 - accuracy: 0.7745 - val_loss: 0.5211 - val_accuracy: 0.7337\n",
      "Epoch 344/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4704 - accuracy: 0.7727\n",
      "Epoch 00344: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4713 - accuracy: 0.7725 - val_loss: 0.5181 - val_accuracy: 0.7330\n",
      "Epoch 345/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7718\n",
      "Epoch 00345: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4665 - accuracy: 0.7718 - val_loss: 0.5218 - val_accuracy: 0.7326\n",
      "Epoch 346/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4654 - accuracy: 0.7761\n",
      "Epoch 00346: val_loss did not improve from 0.49058\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4660 - accuracy: 0.7760 - val_loss: 0.5099 - val_accuracy: 0.7401\n",
      "Epoch 347/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.7734\n",
      "Epoch 00347: val_loss improved from 0.49058 to 0.48857, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4692 - accuracy: 0.7730 - val_loss: 0.4886 - val_accuracy: 0.7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7733\n",
      "Epoch 00348: val_loss did not improve from 0.48857\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4696 - accuracy: 0.7731 - val_loss: 0.5053 - val_accuracy: 0.7451\n",
      "Epoch 349/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.7765\n",
      "Epoch 00349: val_loss did not improve from 0.48857\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4660 - accuracy: 0.7762 - val_loss: 0.5150 - val_accuracy: 0.7373\n",
      "Epoch 350/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.7810\n",
      "Epoch 00350: val_loss did not improve from 0.48857\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4618 - accuracy: 0.7808 - val_loss: 0.4999 - val_accuracy: 0.7483\n",
      "Epoch 351/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4653 - accuracy: 0.7762\n",
      "Epoch 00351: val_loss improved from 0.48857 to 0.46469, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4660 - accuracy: 0.7758 - val_loss: 0.4647 - val_accuracy: 0.7687\n",
      "Epoch 352/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4668 - accuracy: 0.7750\n",
      "Epoch 00352: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4666 - accuracy: 0.7750 - val_loss: 0.5104 - val_accuracy: 0.7384\n",
      "Epoch 353/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.7731\n",
      "Epoch 00353: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4673 - accuracy: 0.7734 - val_loss: 0.5038 - val_accuracy: 0.7444\n",
      "Epoch 354/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7777\n",
      "Epoch 00354: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4656 - accuracy: 0.7775 - val_loss: 0.5074 - val_accuracy: 0.7416\n",
      "Epoch 355/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.7772\n",
      "Epoch 00355: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4661 - accuracy: 0.7772 - val_loss: 0.5194 - val_accuracy: 0.7347\n",
      "Epoch 356/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4672 - accuracy: 0.7777\n",
      "Epoch 00356: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4672 - accuracy: 0.7778 - val_loss: 0.5347 - val_accuracy: 0.7248\n",
      "Epoch 357/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7782\n",
      "Epoch 00357: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4655 - accuracy: 0.7779 - val_loss: 0.4965 - val_accuracy: 0.7506\n",
      "Epoch 358/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7762\n",
      "Epoch 00358: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4636 - accuracy: 0.7759 - val_loss: 0.4889 - val_accuracy: 0.7539\n",
      "Epoch 359/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4639 - accuracy: 0.7753\n",
      "Epoch 00359: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4647 - accuracy: 0.7749 - val_loss: 0.5235 - val_accuracy: 0.7324\n",
      "Epoch 360/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4646 - accuracy: 0.7770\n",
      "Epoch 00360: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4653 - accuracy: 0.7769 - val_loss: 0.4908 - val_accuracy: 0.7537\n",
      "Epoch 361/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4659 - accuracy: 0.7751\n",
      "Epoch 00361: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4663 - accuracy: 0.7751 - val_loss: 0.5045 - val_accuracy: 0.7433\n",
      "Epoch 362/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.7798\n",
      "Epoch 00362: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4652 - accuracy: 0.7797 - val_loss: 0.5038 - val_accuracy: 0.7446\n",
      "Epoch 363/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4653 - accuracy: 0.7780\n",
      "Epoch 00363: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4656 - accuracy: 0.7777 - val_loss: 0.4896 - val_accuracy: 0.7543\n",
      "Epoch 364/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.7771\n",
      "Epoch 00364: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4626 - accuracy: 0.7772 - val_loss: 0.5039 - val_accuracy: 0.7440\n",
      "Epoch 365/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4614 - accuracy: 0.7793\n",
      "Epoch 00365: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4619 - accuracy: 0.7796 - val_loss: 0.4894 - val_accuracy: 0.7545\n",
      "Epoch 366/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.7799\n",
      "Epoch 00366: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4633 - accuracy: 0.7798 - val_loss: 0.5101 - val_accuracy: 0.7384\n",
      "Epoch 367/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4608 - accuracy: 0.7777\n",
      "Epoch 00367: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4613 - accuracy: 0.7770 - val_loss: 0.5020 - val_accuracy: 0.7457\n",
      "Epoch 368/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4627 - accuracy: 0.7767\n",
      "Epoch 00368: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4630 - accuracy: 0.7767 - val_loss: 0.5004 - val_accuracy: 0.7457\n",
      "Epoch 369/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.7771\n",
      "Epoch 00369: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4635 - accuracy: 0.7770 - val_loss: 0.5306 - val_accuracy: 0.7279\n",
      "Epoch 370/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.7788\n",
      "Epoch 00370: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4612 - accuracy: 0.7789 - val_loss: 0.5142 - val_accuracy: 0.7373\n",
      "Epoch 371/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4627 - accuracy: 0.7762\n",
      "Epoch 00371: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4632 - accuracy: 0.7760 - val_loss: 0.5169 - val_accuracy: 0.7358\n",
      "Epoch 372/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7755\n",
      "Epoch 00372: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4663 - accuracy: 0.7754 - val_loss: 0.4996 - val_accuracy: 0.7453\n",
      "Epoch 373/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7776\n",
      "Epoch 00373: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4649 - accuracy: 0.7772 - val_loss: 0.4947 - val_accuracy: 0.7483\n",
      "Epoch 374/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.7751\n",
      "Epoch 00374: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4624 - accuracy: 0.7753 - val_loss: 0.4997 - val_accuracy: 0.7457\n",
      "Epoch 375/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7789\n",
      "Epoch 00375: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4595 - accuracy: 0.7787 - val_loss: 0.4849 - val_accuracy: 0.7526\n",
      "Epoch 376/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4607 - accuracy: 0.7794\n",
      "Epoch 00376: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4614 - accuracy: 0.7790 - val_loss: 0.4834 - val_accuracy: 0.7575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7770\n",
      "Epoch 00377: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4616 - accuracy: 0.7768 - val_loss: 0.4704 - val_accuracy: 0.7655\n",
      "Epoch 378/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.7784\n",
      "Epoch 00378: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4612 - accuracy: 0.7784 - val_loss: 0.5035 - val_accuracy: 0.7429\n",
      "Epoch 379/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7815\n",
      "Epoch 00379: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4594 - accuracy: 0.7814 - val_loss: 0.5088 - val_accuracy: 0.7410\n",
      "Epoch 380/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4568 - accuracy: 0.7818\n",
      "Epoch 00380: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4577 - accuracy: 0.7815 - val_loss: 0.5196 - val_accuracy: 0.7324\n",
      "Epoch 381/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4599 - accuracy: 0.7779\n",
      "Epoch 00381: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4602 - accuracy: 0.7781 - val_loss: 0.4813 - val_accuracy: 0.7577\n",
      "Epoch 382/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7831\n",
      "Epoch 00382: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4616 - accuracy: 0.7828 - val_loss: 0.4870 - val_accuracy: 0.7519\n",
      "Epoch 383/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4542 - accuracy: 0.7831\n",
      "Epoch 00383: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4550 - accuracy: 0.7829 - val_loss: 0.4933 - val_accuracy: 0.7504\n",
      "Epoch 384/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.7826\n",
      "Epoch 00384: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4579 - accuracy: 0.7825 - val_loss: 0.4968 - val_accuracy: 0.7489\n",
      "Epoch 385/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.7797\n",
      "Epoch 00385: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4580 - accuracy: 0.7796 - val_loss: 0.5183 - val_accuracy: 0.7356\n",
      "Epoch 386/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7831\n",
      "Epoch 00386: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4577 - accuracy: 0.7830 - val_loss: 0.4893 - val_accuracy: 0.7524\n",
      "Epoch 387/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4606 - accuracy: 0.7769\n",
      "Epoch 00387: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4610 - accuracy: 0.7767 - val_loss: 0.5174 - val_accuracy: 0.7369\n",
      "Epoch 388/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4590 - accuracy: 0.7783\n",
      "Epoch 00388: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4589 - accuracy: 0.7784 - val_loss: 0.4777 - val_accuracy: 0.7603\n",
      "Epoch 389/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4598 - accuracy: 0.7784\n",
      "Epoch 00389: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4603 - accuracy: 0.7783 - val_loss: 0.4865 - val_accuracy: 0.7569\n",
      "Epoch 390/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.7818\n",
      "Epoch 00390: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4557 - accuracy: 0.7816 - val_loss: 0.4763 - val_accuracy: 0.7635\n",
      "Epoch 391/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4587 - accuracy: 0.7779\n",
      "Epoch 00391: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4591 - accuracy: 0.7778 - val_loss: 0.4958 - val_accuracy: 0.7470\n",
      "Epoch 392/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.7817\n",
      "Epoch 00392: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4541 - accuracy: 0.7814 - val_loss: 0.4945 - val_accuracy: 0.7479\n",
      "Epoch 393/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4553 - accuracy: 0.7785\n",
      "Epoch 00393: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4559 - accuracy: 0.7785 - val_loss: 0.5187 - val_accuracy: 0.7345\n",
      "Epoch 394/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4535 - accuracy: 0.7825\n",
      "Epoch 00394: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4536 - accuracy: 0.7825 - val_loss: 0.5069 - val_accuracy: 0.7414\n",
      "Epoch 395/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7810\n",
      "Epoch 00395: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4558 - accuracy: 0.7808 - val_loss: 0.4728 - val_accuracy: 0.7659\n",
      "Epoch 396/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.7845\n",
      "Epoch 00396: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4542 - accuracy: 0.7844 - val_loss: 0.5170 - val_accuracy: 0.7367\n",
      "Epoch 397/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.7831\n",
      "Epoch 00397: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4580 - accuracy: 0.7831 - val_loss: 0.5268 - val_accuracy: 0.7317\n",
      "Epoch 398/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.7782\n",
      "Epoch 00398: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4576 - accuracy: 0.7782 - val_loss: 0.4863 - val_accuracy: 0.7549\n",
      "Epoch 399/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4519 - accuracy: 0.7859\n",
      "Epoch 00399: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4521 - accuracy: 0.7858 - val_loss: 0.5015 - val_accuracy: 0.7463\n",
      "Epoch 400/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.7845\n",
      "Epoch 00400: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4514 - accuracy: 0.7842 - val_loss: 0.4851 - val_accuracy: 0.7571\n",
      "Epoch 401/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.7840\n",
      "Epoch 00401: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4544 - accuracy: 0.7837 - val_loss: 0.4689 - val_accuracy: 0.7661\n",
      "Epoch 402/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7808\n",
      "Epoch 00402: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4574 - accuracy: 0.7804 - val_loss: 0.5205 - val_accuracy: 0.7326\n",
      "Epoch 403/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7805\n",
      "Epoch 00403: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4554 - accuracy: 0.7804 - val_loss: 0.4863 - val_accuracy: 0.7567\n",
      "Epoch 404/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4537 - accuracy: 0.7831\n",
      "Epoch 00404: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4541 - accuracy: 0.7830 - val_loss: 0.4900 - val_accuracy: 0.7545\n",
      "Epoch 405/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7834\n",
      "Epoch 00405: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4551 - accuracy: 0.7833 - val_loss: 0.5099 - val_accuracy: 0.7393\n",
      "Epoch 406/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.7829\n",
      "Epoch 00406: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4545 - accuracy: 0.7826 - val_loss: 0.4830 - val_accuracy: 0.7558\n",
      "Epoch 407/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.7835\n",
      "Epoch 00407: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4534 - accuracy: 0.7833 - val_loss: 0.4680 - val_accuracy: 0.7678\n",
      "Epoch 408/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7861\n",
      "Epoch 00408: val_loss did not improve from 0.46469\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4516 - accuracy: 0.7858 - val_loss: 0.4971 - val_accuracy: 0.7483\n",
      "Epoch 409/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4533 - accuracy: 0.7828\n",
      "Epoch 00409: val_loss improved from 0.46469 to 0.45754, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4544 - accuracy: 0.7826 - val_loss: 0.4575 - val_accuracy: 0.7788\n",
      "Epoch 410/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4547 - accuracy: 0.7831\n",
      "Epoch 00410: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4552 - accuracy: 0.7831 - val_loss: 0.4865 - val_accuracy: 0.7554\n",
      "Epoch 411/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.7825\n",
      "Epoch 00411: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4553 - accuracy: 0.7824 - val_loss: 0.5219 - val_accuracy: 0.7324\n",
      "Epoch 412/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4495 - accuracy: 0.7852\n",
      "Epoch 00412: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4500 - accuracy: 0.7851 - val_loss: 0.4973 - val_accuracy: 0.7487\n",
      "Epoch 413/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7858\n",
      "Epoch 00413: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4504 - accuracy: 0.7860 - val_loss: 0.5001 - val_accuracy: 0.7476\n",
      "Epoch 414/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.7846\n",
      "Epoch 00414: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4511 - accuracy: 0.7845 - val_loss: 0.5039 - val_accuracy: 0.7436\n",
      "Epoch 415/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4546 - accuracy: 0.7841\n",
      "Epoch 00415: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4553 - accuracy: 0.7838 - val_loss: 0.4981 - val_accuracy: 0.7509\n",
      "Epoch 416/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4483 - accuracy: 0.7856\n",
      "Epoch 00416: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4488 - accuracy: 0.7855 - val_loss: 0.4683 - val_accuracy: 0.7676\n",
      "Epoch 417/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.7874\n",
      "Epoch 00417: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4526 - accuracy: 0.7875 - val_loss: 0.4990 - val_accuracy: 0.7489\n",
      "Epoch 418/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7818\n",
      "Epoch 00418: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4568 - accuracy: 0.7816 - val_loss: 0.4689 - val_accuracy: 0.7661\n",
      "Epoch 419/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.7854\n",
      "Epoch 00419: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4522 - accuracy: 0.7853 - val_loss: 0.5001 - val_accuracy: 0.7474\n",
      "Epoch 420/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7861\n",
      "Epoch 00420: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4506 - accuracy: 0.7860 - val_loss: 0.4610 - val_accuracy: 0.7734\n",
      "Epoch 421/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.7867\n",
      "Epoch 00421: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4511 - accuracy: 0.7866 - val_loss: 0.4738 - val_accuracy: 0.7633\n",
      "Epoch 422/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4502 - accuracy: 0.7834\n",
      "Epoch 00422: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4506 - accuracy: 0.7832 - val_loss: 0.5209 - val_accuracy: 0.7341\n",
      "Epoch 423/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.7842\n",
      "Epoch 00423: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4559 - accuracy: 0.7840 - val_loss: 0.4863 - val_accuracy: 0.7541\n",
      "Epoch 424/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.7846\n",
      "Epoch 00424: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4513 - accuracy: 0.7846 - val_loss: 0.4734 - val_accuracy: 0.7640\n",
      "Epoch 425/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.7866\n",
      "Epoch 00425: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4481 - accuracy: 0.7865 - val_loss: 0.4717 - val_accuracy: 0.7683\n",
      "Epoch 426/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4465 - accuracy: 0.7871\n",
      "Epoch 00426: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4469 - accuracy: 0.7869 - val_loss: 0.4996 - val_accuracy: 0.7491\n",
      "Epoch 427/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.7877\n",
      "Epoch 00427: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4463 - accuracy: 0.7875 - val_loss: 0.4738 - val_accuracy: 0.7642\n",
      "Epoch 428/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4485 - accuracy: 0.7876\n",
      "Epoch 00428: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4490 - accuracy: 0.7876 - val_loss: 0.4744 - val_accuracy: 0.7642\n",
      "Epoch 429/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.7874\n",
      "Epoch 00429: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4453 - accuracy: 0.7872 - val_loss: 0.4649 - val_accuracy: 0.7687\n",
      "Epoch 430/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.7860\n",
      "Epoch 00430: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4491 - accuracy: 0.7860 - val_loss: 0.4966 - val_accuracy: 0.7502\n",
      "Epoch 431/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4495 - accuracy: 0.7837\n",
      "Epoch 00431: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4497 - accuracy: 0.7837 - val_loss: 0.4840 - val_accuracy: 0.7577\n",
      "Epoch 432/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7873\n",
      "Epoch 00432: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4482 - accuracy: 0.7869 - val_loss: 0.4807 - val_accuracy: 0.7595\n",
      "Epoch 433/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7865\n",
      "Epoch 00433: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4500 - accuracy: 0.7865 - val_loss: 0.4808 - val_accuracy: 0.7573\n",
      "Epoch 434/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4486 - accuracy: 0.7832\n",
      "Epoch 00434: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4490 - accuracy: 0.7832 - val_loss: 0.4771 - val_accuracy: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 435/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.7888\n",
      "Epoch 00435: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4443 - accuracy: 0.7884 - val_loss: 0.4719 - val_accuracy: 0.7640\n",
      "Epoch 436/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.7859\n",
      "Epoch 00436: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4483 - accuracy: 0.7855 - val_loss: 0.4799 - val_accuracy: 0.7580\n",
      "Epoch 437/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7864\n",
      "Epoch 00437: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4472 - accuracy: 0.7862 - val_loss: 0.4698 - val_accuracy: 0.7655\n",
      "Epoch 438/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4486 - accuracy: 0.7882\n",
      "Epoch 00438: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4485 - accuracy: 0.7882 - val_loss: 0.4798 - val_accuracy: 0.7580\n",
      "Epoch 439/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.7873\n",
      "Epoch 00439: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4484 - accuracy: 0.7872 - val_loss: 0.4620 - val_accuracy: 0.7730\n",
      "Epoch 440/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.7906\n",
      "Epoch 00440: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4438 - accuracy: 0.7905 - val_loss: 0.4866 - val_accuracy: 0.7549\n",
      "Epoch 441/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.7886\n",
      "Epoch 00441: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4474 - accuracy: 0.7883 - val_loss: 0.4863 - val_accuracy: 0.7558\n",
      "Epoch 442/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.7837\n",
      "Epoch 00442: val_loss did not improve from 0.45754\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4490 - accuracy: 0.7831 - val_loss: 0.4608 - val_accuracy: 0.7728\n",
      "Epoch 443/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7882\n",
      "Epoch 00443: val_loss improved from 0.45754 to 0.45481, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4444 - accuracy: 0.7883 - val_loss: 0.4548 - val_accuracy: 0.7760\n",
      "Epoch 444/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.7856\n",
      "Epoch 00444: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4493 - accuracy: 0.7853 - val_loss: 0.4874 - val_accuracy: 0.7573\n",
      "Epoch 445/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4468 - accuracy: 0.7880\n",
      "Epoch 00445: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4475 - accuracy: 0.7877 - val_loss: 0.4641 - val_accuracy: 0.7709\n",
      "Epoch 446/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4433 - accuracy: 0.7911\n",
      "Epoch 00446: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4439 - accuracy: 0.7910 - val_loss: 0.4724 - val_accuracy: 0.7653\n",
      "Epoch 447/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7895\n",
      "Epoch 00447: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4453 - accuracy: 0.7893 - val_loss: 0.4871 - val_accuracy: 0.7558\n",
      "Epoch 448/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.7869\n",
      "Epoch 00448: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4463 - accuracy: 0.7868 - val_loss: 0.4978 - val_accuracy: 0.7524\n",
      "Epoch 449/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7897\n",
      "Epoch 00449: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4460 - accuracy: 0.7894 - val_loss: 0.4970 - val_accuracy: 0.7479\n",
      "Epoch 450/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7903\n",
      "Epoch 00450: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4451 - accuracy: 0.7901 - val_loss: 0.4584 - val_accuracy: 0.7775\n",
      "Epoch 451/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.7906\n",
      "Epoch 00451: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4465 - accuracy: 0.7905 - val_loss: 0.5030 - val_accuracy: 0.7468\n",
      "Epoch 452/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.7882\n",
      "Epoch 00452: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4422 - accuracy: 0.7884 - val_loss: 0.5040 - val_accuracy: 0.7448\n",
      "Epoch 453/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4399 - accuracy: 0.7945\n",
      "Epoch 00453: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4401 - accuracy: 0.7943 - val_loss: 0.4916 - val_accuracy: 0.7539\n",
      "Epoch 454/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7881\n",
      "Epoch 00454: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4453 - accuracy: 0.7886 - val_loss: 0.4787 - val_accuracy: 0.7590\n",
      "Epoch 455/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4431 - accuracy: 0.7891\n",
      "Epoch 00455: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4436 - accuracy: 0.7888 - val_loss: 0.5045 - val_accuracy: 0.7466\n",
      "Epoch 456/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4440 - accuracy: 0.7904\n",
      "Epoch 00456: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4448 - accuracy: 0.7902 - val_loss: 0.4557 - val_accuracy: 0.7754\n",
      "Epoch 457/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7868\n",
      "Epoch 00457: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4425 - accuracy: 0.7868 - val_loss: 0.4553 - val_accuracy: 0.7752\n",
      "Epoch 458/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7904\n",
      "Epoch 00458: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4435 - accuracy: 0.7900 - val_loss: 0.4716 - val_accuracy: 0.7629\n",
      "Epoch 459/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4457 - accuracy: 0.7868\n",
      "Epoch 00459: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4462 - accuracy: 0.7868 - val_loss: 0.4752 - val_accuracy: 0.7607\n",
      "Epoch 460/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4411 - accuracy: 0.7928\n",
      "Epoch 00460: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4418 - accuracy: 0.7922 - val_loss: 0.4711 - val_accuracy: 0.7653\n",
      "Epoch 461/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4419 - accuracy: 0.7913\n",
      "Epoch 00461: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4428 - accuracy: 0.7908 - val_loss: 0.4797 - val_accuracy: 0.7595\n",
      "Epoch 462/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7904\n",
      "Epoch 00462: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4428 - accuracy: 0.7903 - val_loss: 0.4671 - val_accuracy: 0.7685\n",
      "Epoch 463/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4425 - accuracy: 0.7913\n",
      "Epoch 00463: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4429 - accuracy: 0.7913 - val_loss: 0.4692 - val_accuracy: 0.7655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7924\n",
      "Epoch 00464: val_loss did not improve from 0.45481\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4402 - accuracy: 0.7924 - val_loss: 0.4947 - val_accuracy: 0.7541\n",
      "Epoch 465/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4412 - accuracy: 0.7936\n",
      "Epoch 00465: val_loss improved from 0.45481 to 0.45244, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4418 - accuracy: 0.7935 - val_loss: 0.4524 - val_accuracy: 0.7820\n",
      "Epoch 466/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4376 - accuracy: 0.7915\n",
      "Epoch 00466: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4379 - accuracy: 0.7916 - val_loss: 0.4626 - val_accuracy: 0.7728\n",
      "Epoch 467/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.7919\n",
      "Epoch 00467: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4407 - accuracy: 0.7915 - val_loss: 0.4730 - val_accuracy: 0.7631\n",
      "Epoch 468/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.7922\n",
      "Epoch 00468: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4402 - accuracy: 0.7918 - val_loss: 0.4968 - val_accuracy: 0.7502\n",
      "Epoch 469/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7894\n",
      "Epoch 00469: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4402 - accuracy: 0.7895 - val_loss: 0.4913 - val_accuracy: 0.7534\n",
      "Epoch 470/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4406 - accuracy: 0.7889\n",
      "Epoch 00470: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4417 - accuracy: 0.7886 - val_loss: 0.4928 - val_accuracy: 0.7528\n",
      "Epoch 471/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7917\n",
      "Epoch 00471: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4390 - accuracy: 0.7916 - val_loss: 0.4748 - val_accuracy: 0.7633\n",
      "Epoch 472/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4378 - accuracy: 0.7918\n",
      "Epoch 00472: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4382 - accuracy: 0.7915 - val_loss: 0.4596 - val_accuracy: 0.7721\n",
      "Epoch 473/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4389 - accuracy: 0.7930\n",
      "Epoch 00473: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4393 - accuracy: 0.7926 - val_loss: 0.4636 - val_accuracy: 0.7700\n",
      "Epoch 474/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4424 - accuracy: 0.7921\n",
      "Epoch 00474: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4431 - accuracy: 0.7918 - val_loss: 0.4651 - val_accuracy: 0.7706\n",
      "Epoch 475/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.7901\n",
      "Epoch 00475: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4384 - accuracy: 0.7900 - val_loss: 0.4874 - val_accuracy: 0.7558\n",
      "Epoch 476/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7909\n",
      "Epoch 00476: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4427 - accuracy: 0.7908 - val_loss: 0.4841 - val_accuracy: 0.7562\n",
      "Epoch 477/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7909\n",
      "Epoch 00477: val_loss did not improve from 0.45244\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4434 - accuracy: 0.7905 - val_loss: 0.4703 - val_accuracy: 0.7633\n",
      "Epoch 478/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4341 - accuracy: 0.7971\n",
      "Epoch 00478: val_loss improved from 0.45244 to 0.43616, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4351 - accuracy: 0.7966 - val_loss: 0.4362 - val_accuracy: 0.7913\n",
      "Epoch 479/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.7950\n",
      "Epoch 00479: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4364 - accuracy: 0.7951 - val_loss: 0.4974 - val_accuracy: 0.7485\n",
      "Epoch 480/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.7936\n",
      "Epoch 00480: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4399 - accuracy: 0.7935 - val_loss: 0.5066 - val_accuracy: 0.7448\n",
      "Epoch 481/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4375 - accuracy: 0.7925\n",
      "Epoch 00481: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4379 - accuracy: 0.7924 - val_loss: 0.4456 - val_accuracy: 0.7842\n",
      "Epoch 482/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7932\n",
      "Epoch 00482: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4366 - accuracy: 0.7932 - val_loss: 0.4679 - val_accuracy: 0.7683\n",
      "Epoch 483/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.7914\n",
      "Epoch 00483: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4374 - accuracy: 0.7912 - val_loss: 0.4561 - val_accuracy: 0.7760\n",
      "Epoch 484/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4422 - accuracy: 0.7910\n",
      "Epoch 00484: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4427 - accuracy: 0.7909 - val_loss: 0.4715 - val_accuracy: 0.7661\n",
      "Epoch 485/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4410 - accuracy: 0.7904\n",
      "Epoch 00485: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4417 - accuracy: 0.7906 - val_loss: 0.4637 - val_accuracy: 0.7717\n",
      "Epoch 486/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.7929\n",
      "Epoch 00486: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4384 - accuracy: 0.7928 - val_loss: 0.4509 - val_accuracy: 0.7816\n",
      "Epoch 487/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.7950\n",
      "Epoch 00487: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4361 - accuracy: 0.7951 - val_loss: 0.4538 - val_accuracy: 0.7775\n",
      "Epoch 488/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7951\n",
      "Epoch 00488: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4341 - accuracy: 0.7952 - val_loss: 0.4811 - val_accuracy: 0.7625\n",
      "Epoch 489/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.7964\n",
      "Epoch 00489: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4363 - accuracy: 0.7963 - val_loss: 0.4560 - val_accuracy: 0.7758\n",
      "Epoch 490/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.7923\n",
      "Epoch 00490: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4353 - accuracy: 0.7922 - val_loss: 0.4746 - val_accuracy: 0.7648\n",
      "Epoch 491/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.7930\n",
      "Epoch 00491: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4394 - accuracy: 0.7927 - val_loss: 0.4862 - val_accuracy: 0.7571\n",
      "Epoch 492/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4365 - accuracy: 0.7927\n",
      "Epoch 00492: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4371 - accuracy: 0.7923 - val_loss: 0.4524 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4362 - accuracy: 0.7943\n",
      "Epoch 00493: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4365 - accuracy: 0.7944 - val_loss: 0.4852 - val_accuracy: 0.7582\n",
      "Epoch 494/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.7968\n",
      "Epoch 00494: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4353 - accuracy: 0.7966 - val_loss: 0.4647 - val_accuracy: 0.7717\n",
      "Epoch 495/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.7912\n",
      "Epoch 00495: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4394 - accuracy: 0.7905 - val_loss: 0.4772 - val_accuracy: 0.7638\n",
      "Epoch 496/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7909\n",
      "Epoch 00496: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4370 - accuracy: 0.7907 - val_loss: 0.4961 - val_accuracy: 0.7504\n",
      "Epoch 497/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7942\n",
      "Epoch 00497: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4352 - accuracy: 0.7944 - val_loss: 0.4595 - val_accuracy: 0.7721\n",
      "Epoch 498/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.7954\n",
      "Epoch 00498: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4342 - accuracy: 0.7952 - val_loss: 0.4619 - val_accuracy: 0.7726\n",
      "Epoch 499/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7949\n",
      "Epoch 00499: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4332 - accuracy: 0.7951 - val_loss: 0.4664 - val_accuracy: 0.7689\n",
      "Epoch 500/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.7964\n",
      "Epoch 00500: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4344 - accuracy: 0.7965 - val_loss: 0.4640 - val_accuracy: 0.7700\n",
      "Epoch 501/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4343 - accuracy: 0.7948\n",
      "Epoch 00501: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4345 - accuracy: 0.7948 - val_loss: 0.4961 - val_accuracy: 0.7534\n",
      "Epoch 502/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4343 - accuracy: 0.7959\n",
      "Epoch 00502: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4344 - accuracy: 0.7959 - val_loss: 0.4579 - val_accuracy: 0.7758\n",
      "Epoch 503/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4320 - accuracy: 0.7955\n",
      "Epoch 00503: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4328 - accuracy: 0.7955 - val_loss: 0.4472 - val_accuracy: 0.7844\n",
      "Epoch 504/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4375 - accuracy: 0.7948\n",
      "Epoch 00504: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4375 - accuracy: 0.7949 - val_loss: 0.4570 - val_accuracy: 0.7771\n",
      "Epoch 505/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.7968\n",
      "Epoch 00505: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4312 - accuracy: 0.7966 - val_loss: 0.4753 - val_accuracy: 0.7625\n",
      "Epoch 506/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.7930\n",
      "Epoch 00506: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4346 - accuracy: 0.7930 - val_loss: 0.4818 - val_accuracy: 0.7605\n",
      "Epoch 507/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.7948\n",
      "Epoch 00507: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4331 - accuracy: 0.7944 - val_loss: 0.4631 - val_accuracy: 0.7732\n",
      "Epoch 508/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.7949\n",
      "Epoch 00508: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4356 - accuracy: 0.7944 - val_loss: 0.4483 - val_accuracy: 0.7822\n",
      "Epoch 509/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.7937\n",
      "Epoch 00509: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4323 - accuracy: 0.7938 - val_loss: 0.4891 - val_accuracy: 0.7569\n",
      "Epoch 510/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7944\n",
      "Epoch 00510: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4356 - accuracy: 0.7943 - val_loss: 0.4796 - val_accuracy: 0.7627\n",
      "Epoch 511/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4300 - accuracy: 0.7968\n",
      "Epoch 00511: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4298 - accuracy: 0.7969 - val_loss: 0.4587 - val_accuracy: 0.7767\n",
      "Epoch 512/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4365 - accuracy: 0.7935\n",
      "Epoch 00512: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4370 - accuracy: 0.7934 - val_loss: 0.4821 - val_accuracy: 0.7599\n",
      "Epoch 513/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.7949\n",
      "Epoch 00513: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4330 - accuracy: 0.7948 - val_loss: 0.4738 - val_accuracy: 0.7646\n",
      "Epoch 514/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.7915\n",
      "Epoch 00514: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4356 - accuracy: 0.7913 - val_loss: 0.4636 - val_accuracy: 0.7706\n",
      "Epoch 515/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7926\n",
      "Epoch 00515: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4358 - accuracy: 0.7922 - val_loss: 0.4410 - val_accuracy: 0.7891\n",
      "Epoch 516/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.7973\n",
      "Epoch 00516: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4325 - accuracy: 0.7971 - val_loss: 0.4574 - val_accuracy: 0.7734\n",
      "Epoch 517/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7992\n",
      "Epoch 00517: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4301 - accuracy: 0.7990 - val_loss: 0.4597 - val_accuracy: 0.7719\n",
      "Epoch 518/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.7963\n",
      "Epoch 00518: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4311 - accuracy: 0.7962 - val_loss: 0.4672 - val_accuracy: 0.7689\n",
      "Epoch 519/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8001\n",
      "Epoch 00519: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4305 - accuracy: 0.8001 - val_loss: 0.4553 - val_accuracy: 0.7786\n",
      "Epoch 520/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4290 - accuracy: 0.7988\n",
      "Epoch 00520: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4294 - accuracy: 0.7987 - val_loss: 0.4625 - val_accuracy: 0.7717\n",
      "Epoch 521/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.7933\n",
      "Epoch 00521: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4327 - accuracy: 0.7933 - val_loss: 0.4918 - val_accuracy: 0.7537\n",
      "Epoch 522/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.7960\n",
      "Epoch 00522: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4327 - accuracy: 0.7960 - val_loss: 0.4604 - val_accuracy: 0.7726\n",
      "Epoch 523/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.7963\n",
      "Epoch 00523: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4307 - accuracy: 0.7965 - val_loss: 0.4947 - val_accuracy: 0.7517\n",
      "Epoch 524/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.7970\n",
      "Epoch 00524: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4312 - accuracy: 0.7968 - val_loss: 0.4489 - val_accuracy: 0.7805\n",
      "Epoch 525/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.7994\n",
      "Epoch 00525: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4295 - accuracy: 0.7995 - val_loss: 0.4651 - val_accuracy: 0.7691\n",
      "Epoch 526/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.7976\n",
      "Epoch 00526: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4309 - accuracy: 0.7979 - val_loss: 0.4637 - val_accuracy: 0.7728\n",
      "Epoch 527/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.7987\n",
      "Epoch 00527: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4299 - accuracy: 0.7984 - val_loss: 0.4794 - val_accuracy: 0.7618\n",
      "Epoch 528/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.7962\n",
      "Epoch 00528: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4311 - accuracy: 0.7956 - val_loss: 0.4517 - val_accuracy: 0.7805\n",
      "Epoch 529/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7926\n",
      "Epoch 00529: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4343 - accuracy: 0.7925 - val_loss: 0.4753 - val_accuracy: 0.7618\n",
      "Epoch 530/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4287 - accuracy: 0.7936\n",
      "Epoch 00530: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4293 - accuracy: 0.7934 - val_loss: 0.4666 - val_accuracy: 0.7700\n",
      "Epoch 531/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7973\n",
      "Epoch 00531: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4297 - accuracy: 0.7973 - val_loss: 0.4596 - val_accuracy: 0.7730\n",
      "Epoch 532/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7985\n",
      "Epoch 00532: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4298 - accuracy: 0.7985 - val_loss: 0.4494 - val_accuracy: 0.7814\n",
      "Epoch 533/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.7983\n",
      "Epoch 00533: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4284 - accuracy: 0.7981 - val_loss: 0.4684 - val_accuracy: 0.7657\n",
      "Epoch 534/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.7973\n",
      "Epoch 00534: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4307 - accuracy: 0.7974 - val_loss: 0.4594 - val_accuracy: 0.7732\n",
      "Epoch 535/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4279 - accuracy: 0.7987\n",
      "Epoch 00535: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4286 - accuracy: 0.7987 - val_loss: 0.4613 - val_accuracy: 0.7728\n",
      "Epoch 536/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.8041\n",
      "Epoch 00536: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4276 - accuracy: 0.8037 - val_loss: 0.4470 - val_accuracy: 0.7816\n",
      "Epoch 537/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4288 - accuracy: 0.8007\n",
      "Epoch 00537: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4295 - accuracy: 0.8002 - val_loss: 0.4500 - val_accuracy: 0.7805\n",
      "Epoch 538/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.7974\n",
      "Epoch 00538: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4297 - accuracy: 0.7973 - val_loss: 0.4595 - val_accuracy: 0.7739\n",
      "Epoch 539/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.7980\n",
      "Epoch 00539: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4280 - accuracy: 0.7975 - val_loss: 0.4438 - val_accuracy: 0.7868\n",
      "Epoch 540/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4266 - accuracy: 0.7998\n",
      "Epoch 00540: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4269 - accuracy: 0.7998 - val_loss: 0.4509 - val_accuracy: 0.7797\n",
      "Epoch 541/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4260 - accuracy: 0.7994\n",
      "Epoch 00541: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4263 - accuracy: 0.7996 - val_loss: 0.4598 - val_accuracy: 0.7721\n",
      "Epoch 542/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4285 - accuracy: 0.7977\n",
      "Epoch 00542: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4289 - accuracy: 0.7976 - val_loss: 0.4455 - val_accuracy: 0.7831\n",
      "Epoch 543/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.7986\n",
      "Epoch 00543: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4272 - accuracy: 0.7984 - val_loss: 0.4390 - val_accuracy: 0.7904\n",
      "Epoch 544/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4251 - accuracy: 0.7986\n",
      "Epoch 00544: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4257 - accuracy: 0.7982 - val_loss: 0.4442 - val_accuracy: 0.7872\n",
      "Epoch 545/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8026\n",
      "Epoch 00545: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4260 - accuracy: 0.8024 - val_loss: 0.4782 - val_accuracy: 0.7595\n",
      "Epoch 546/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7973\n",
      "Epoch 00546: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4297 - accuracy: 0.7972 - val_loss: 0.4936 - val_accuracy: 0.7521\n",
      "Epoch 547/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.8003\n",
      "Epoch 00547: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4260 - accuracy: 0.8001 - val_loss: 0.4383 - val_accuracy: 0.7913\n",
      "Epoch 548/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4287 - accuracy: 0.7983\n",
      "Epoch 00548: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4286 - accuracy: 0.7983 - val_loss: 0.4571 - val_accuracy: 0.7743\n",
      "Epoch 549/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.7959\n",
      "Epoch 00549: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4302 - accuracy: 0.7962 - val_loss: 0.4415 - val_accuracy: 0.7861\n",
      "Epoch 550/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.7991\n",
      "Epoch 00550: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4262 - accuracy: 0.7989 - val_loss: 0.4615 - val_accuracy: 0.7706\n",
      "Epoch 551/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.7996\n",
      "Epoch 00551: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4235 - accuracy: 0.7997 - val_loss: 0.4736 - val_accuracy: 0.7659\n",
      "Epoch 552/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4263 - accuracy: 0.8006\n",
      "Epoch 00552: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4269 - accuracy: 0.8002 - val_loss: 0.4810 - val_accuracy: 0.7590\n",
      "Epoch 553/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.7975\n",
      "Epoch 00553: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4256 - accuracy: 0.7969 - val_loss: 0.4596 - val_accuracy: 0.7726\n",
      "Epoch 554/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.7975\n",
      "Epoch 00554: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4264 - accuracy: 0.7975 - val_loss: 0.4564 - val_accuracy: 0.7749\n",
      "Epoch 555/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8000\n",
      "Epoch 00555: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4223 - accuracy: 0.8000 - val_loss: 0.4630 - val_accuracy: 0.7687\n",
      "Epoch 556/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4239 - accuracy: 0.8027\n",
      "Epoch 00556: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4240 - accuracy: 0.8027 - val_loss: 0.4614 - val_accuracy: 0.7713\n",
      "Epoch 557/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.7999\n",
      "Epoch 00557: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4260 - accuracy: 0.7997 - val_loss: 0.4507 - val_accuracy: 0.7782\n",
      "Epoch 558/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.7991\n",
      "Epoch 00558: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4231 - accuracy: 0.7991 - val_loss: 0.4611 - val_accuracy: 0.7709\n",
      "Epoch 559/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.7989\n",
      "Epoch 00559: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4256 - accuracy: 0.7987 - val_loss: 0.4389 - val_accuracy: 0.7904\n",
      "Epoch 560/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8014\n",
      "Epoch 00560: val_loss did not improve from 0.43616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4249 - accuracy: 0.8013 - val_loss: 0.4644 - val_accuracy: 0.7704\n",
      "Epoch 561/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.7983\n",
      "Epoch 00561: val_loss improved from 0.43616 to 0.43341, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4264 - accuracy: 0.7980 - val_loss: 0.4334 - val_accuracy: 0.7930\n",
      "Epoch 562/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8033\n",
      "Epoch 00562: val_loss improved from 0.43341 to 0.43174, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4228 - accuracy: 0.8028 - val_loss: 0.4317 - val_accuracy: 0.7930\n",
      "Epoch 563/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8015\n",
      "Epoch 00563: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4250 - accuracy: 0.8013 - val_loss: 0.4553 - val_accuracy: 0.7747\n",
      "Epoch 564/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8018\n",
      "Epoch 00564: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4228 - accuracy: 0.8017 - val_loss: 0.4503 - val_accuracy: 0.7818\n",
      "Epoch 565/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8022\n",
      "Epoch 00565: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4213 - accuracy: 0.8021 - val_loss: 0.4345 - val_accuracy: 0.7932\n",
      "Epoch 566/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8001\n",
      "Epoch 00566: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4250 - accuracy: 0.8001 - val_loss: 0.4572 - val_accuracy: 0.7736\n",
      "Epoch 567/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4239 - accuracy: 0.8023\n",
      "Epoch 00567: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4238 - accuracy: 0.8023 - val_loss: 0.4363 - val_accuracy: 0.7923\n",
      "Epoch 568/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8038\n",
      "Epoch 00568: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4188 - accuracy: 0.8036 - val_loss: 0.4840 - val_accuracy: 0.7597\n",
      "Epoch 569/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8038\n",
      "Epoch 00569: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4206 - accuracy: 0.8038 - val_loss: 0.4597 - val_accuracy: 0.7732\n",
      "Epoch 570/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8057\n",
      "Epoch 00570: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4171 - accuracy: 0.8057 - val_loss: 0.4429 - val_accuracy: 0.7861\n",
      "Epoch 571/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8031\n",
      "Epoch 00571: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4228 - accuracy: 0.8031 - val_loss: 0.4381 - val_accuracy: 0.7898\n",
      "Epoch 572/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.7965\n",
      "Epoch 00572: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4276 - accuracy: 0.7963 - val_loss: 0.4454 - val_accuracy: 0.7820\n",
      "Epoch 573/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8033\n",
      "Epoch 00573: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4212 - accuracy: 0.8031 - val_loss: 0.4516 - val_accuracy: 0.7775\n",
      "Epoch 574/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8017\n",
      "Epoch 00574: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4211 - accuracy: 0.8015 - val_loss: 0.4716 - val_accuracy: 0.7668\n",
      "Epoch 575/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.7999\n",
      "Epoch 00575: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4233 - accuracy: 0.8001 - val_loss: 0.4832 - val_accuracy: 0.7575\n",
      "Epoch 576/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.7967\n",
      "Epoch 00576: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4287 - accuracy: 0.7968 - val_loss: 0.4609 - val_accuracy: 0.7717\n",
      "Epoch 577/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8039\n",
      "Epoch 00577: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4182 - accuracy: 0.8037 - val_loss: 0.4665 - val_accuracy: 0.7698\n",
      "Epoch 578/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8049\n",
      "Epoch 00578: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4190 - accuracy: 0.8047 - val_loss: 0.4471 - val_accuracy: 0.7820\n",
      "Epoch 579/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.7998\n",
      "Epoch 00579: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4231 - accuracy: 0.7996 - val_loss: 0.4491 - val_accuracy: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8029\n",
      "Epoch 00580: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4214 - accuracy: 0.8028 - val_loss: 0.4788 - val_accuracy: 0.7635\n",
      "Epoch 581/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8046\n",
      "Epoch 00581: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4188 - accuracy: 0.8044 - val_loss: 0.4559 - val_accuracy: 0.7773\n",
      "Epoch 582/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8056\n",
      "Epoch 00582: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4190 - accuracy: 0.8057 - val_loss: 0.4547 - val_accuracy: 0.7758\n",
      "Epoch 583/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8025\n",
      "Epoch 00583: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4191 - accuracy: 0.8025 - val_loss: 0.4686 - val_accuracy: 0.7676\n",
      "Epoch 584/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8011\n",
      "Epoch 00584: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4220 - accuracy: 0.8011 - val_loss: 0.4378 - val_accuracy: 0.7908\n",
      "Epoch 585/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8040\n",
      "Epoch 00585: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4200 - accuracy: 0.8041 - val_loss: 0.4383 - val_accuracy: 0.7885\n",
      "Epoch 586/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8053\n",
      "Epoch 00586: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4171 - accuracy: 0.8054 - val_loss: 0.4594 - val_accuracy: 0.7730\n",
      "Epoch 587/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8036\n",
      "Epoch 00587: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4221 - accuracy: 0.8038 - val_loss: 0.4456 - val_accuracy: 0.7840\n",
      "Epoch 588/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8049\n",
      "Epoch 00588: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4171 - accuracy: 0.8048 - val_loss: 0.4446 - val_accuracy: 0.7831\n",
      "Epoch 589/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8019\n",
      "Epoch 00589: val_loss did not improve from 0.43174\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4207 - accuracy: 0.8020 - val_loss: 0.4357 - val_accuracy: 0.7930\n",
      "Epoch 590/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4153 - accuracy: 0.8056\n",
      "Epoch 00590: val_loss improved from 0.43174 to 0.42960, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4160 - accuracy: 0.8053 - val_loss: 0.4296 - val_accuracy: 0.7975\n",
      "Epoch 591/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4165 - accuracy: 0.8036\n",
      "Epoch 00591: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4169 - accuracy: 0.8031 - val_loss: 0.4642 - val_accuracy: 0.7706\n",
      "Epoch 592/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8030\n",
      "Epoch 00592: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4168 - accuracy: 0.8030 - val_loss: 0.4600 - val_accuracy: 0.7758\n",
      "Epoch 593/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8076\n",
      "Epoch 00593: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4155 - accuracy: 0.8075 - val_loss: 0.4357 - val_accuracy: 0.7923\n",
      "Epoch 594/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4165 - accuracy: 0.8069\n",
      "Epoch 00594: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4163 - accuracy: 0.8068 - val_loss: 0.4531 - val_accuracy: 0.7767\n",
      "Epoch 595/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8032\n",
      "Epoch 00595: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4194 - accuracy: 0.8030 - val_loss: 0.4407 - val_accuracy: 0.7870\n",
      "Epoch 596/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8041\n",
      "Epoch 00596: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4179 - accuracy: 0.8042 - val_loss: 0.4568 - val_accuracy: 0.7760\n",
      "Epoch 597/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8051\n",
      "Epoch 00597: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4184 - accuracy: 0.8055 - val_loss: 0.4613 - val_accuracy: 0.7715\n",
      "Epoch 598/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8029\n",
      "Epoch 00598: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4193 - accuracy: 0.8031 - val_loss: 0.4766 - val_accuracy: 0.7640\n",
      "Epoch 599/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8067\n",
      "Epoch 00599: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4160 - accuracy: 0.8064 - val_loss: 0.4460 - val_accuracy: 0.7805\n",
      "Epoch 600/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4136 - accuracy: 0.8076\n",
      "Epoch 00600: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4138 - accuracy: 0.8074 - val_loss: 0.4455 - val_accuracy: 0.7863\n",
      "Epoch 601/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8046\n",
      "Epoch 00601: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4167 - accuracy: 0.8044 - val_loss: 0.4695 - val_accuracy: 0.7672\n",
      "Epoch 602/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8068\n",
      "Epoch 00602: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4126 - accuracy: 0.8068 - val_loss: 0.4587 - val_accuracy: 0.7739\n",
      "Epoch 603/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8059\n",
      "Epoch 00603: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4132 - accuracy: 0.8058 - val_loss: 0.4399 - val_accuracy: 0.7853\n",
      "Epoch 604/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8044\n",
      "Epoch 00604: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4163 - accuracy: 0.8041 - val_loss: 0.4384 - val_accuracy: 0.7898\n",
      "Epoch 605/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4148 - accuracy: 0.8072\n",
      "Epoch 00605: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4156 - accuracy: 0.8071 - val_loss: 0.4517 - val_accuracy: 0.7771\n",
      "Epoch 606/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8057\n",
      "Epoch 00606: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4185 - accuracy: 0.8053 - val_loss: 0.4348 - val_accuracy: 0.7880\n",
      "Epoch 607/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8044\n",
      "Epoch 00607: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4160 - accuracy: 0.8047 - val_loss: 0.4459 - val_accuracy: 0.7818\n",
      "Epoch 608/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8052\n",
      "Epoch 00608: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4147 - accuracy: 0.8052 - val_loss: 0.4608 - val_accuracy: 0.7706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 609/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.8028\n",
      "Epoch 00609: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4183 - accuracy: 0.8025 - val_loss: 0.4482 - val_accuracy: 0.7818\n",
      "Epoch 610/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4147 - accuracy: 0.8057\n",
      "Epoch 00610: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4147 - accuracy: 0.8059 - val_loss: 0.4312 - val_accuracy: 0.7941\n",
      "Epoch 611/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8080\n",
      "Epoch 00611: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4144 - accuracy: 0.8080 - val_loss: 0.4396 - val_accuracy: 0.7874\n",
      "Epoch 612/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8078\n",
      "Epoch 00612: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4101 - accuracy: 0.8079 - val_loss: 0.4403 - val_accuracy: 0.7861\n",
      "Epoch 613/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8076\n",
      "Epoch 00613: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4161 - accuracy: 0.8075 - val_loss: 0.4627 - val_accuracy: 0.7706\n",
      "Epoch 614/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4164 - accuracy: 0.8044\n",
      "Epoch 00614: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4168 - accuracy: 0.8042 - val_loss: 0.4368 - val_accuracy: 0.7865\n",
      "Epoch 615/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8061\n",
      "Epoch 00615: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4139 - accuracy: 0.8060 - val_loss: 0.4503 - val_accuracy: 0.7810\n",
      "Epoch 616/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8075\n",
      "Epoch 00616: val_loss did not improve from 0.42960\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4131 - accuracy: 0.8075 - val_loss: 0.4497 - val_accuracy: 0.7799\n",
      "Epoch 617/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8079\n",
      "Epoch 00617: val_loss improved from 0.42960 to 0.42769, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4139 - accuracy: 0.8079 - val_loss: 0.4277 - val_accuracy: 0.7958\n",
      "Epoch 618/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8117\n",
      "Epoch 00618: val_loss did not improve from 0.42769\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4112 - accuracy: 0.8118 - val_loss: 0.4583 - val_accuracy: 0.7745\n",
      "Epoch 619/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8077\n",
      "Epoch 00619: val_loss improved from 0.42769 to 0.41955, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4129 - accuracy: 0.8074 - val_loss: 0.4195 - val_accuracy: 0.8005\n",
      "Epoch 620/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4132 - accuracy: 0.8065\n",
      "Epoch 00620: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4136 - accuracy: 0.8065 - val_loss: 0.4389 - val_accuracy: 0.7889\n",
      "Epoch 621/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8100\n",
      "Epoch 00621: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4133 - accuracy: 0.8097 - val_loss: 0.4346 - val_accuracy: 0.7902\n",
      "Epoch 622/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8114\n",
      "Epoch 00622: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4103 - accuracy: 0.8112 - val_loss: 0.4412 - val_accuracy: 0.7840\n",
      "Epoch 623/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8086\n",
      "Epoch 00623: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4102 - accuracy: 0.8089 - val_loss: 0.4695 - val_accuracy: 0.7650\n",
      "Epoch 624/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8066\n",
      "Epoch 00624: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4105 - accuracy: 0.8066 - val_loss: 0.4604 - val_accuracy: 0.7691\n",
      "Epoch 625/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8070\n",
      "Epoch 00625: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4111 - accuracy: 0.8069 - val_loss: 0.4434 - val_accuracy: 0.7833\n",
      "Epoch 626/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4122 - accuracy: 0.8087\n",
      "Epoch 00626: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4127 - accuracy: 0.8084 - val_loss: 0.4423 - val_accuracy: 0.7840\n",
      "Epoch 627/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8097\n",
      "Epoch 00627: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4120 - accuracy: 0.8092 - val_loss: 0.4334 - val_accuracy: 0.7917\n",
      "Epoch 628/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4125 - accuracy: 0.8062\n",
      "Epoch 00628: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4127 - accuracy: 0.8062 - val_loss: 0.4295 - val_accuracy: 0.7956\n",
      "Epoch 629/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4090 - accuracy: 0.8107\n",
      "Epoch 00629: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4099 - accuracy: 0.8103 - val_loss: 0.4721 - val_accuracy: 0.7642\n",
      "Epoch 630/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8084\n",
      "Epoch 00630: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4120 - accuracy: 0.8085 - val_loss: 0.4366 - val_accuracy: 0.7872\n",
      "Epoch 631/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4097 - accuracy: 0.8091\n",
      "Epoch 00631: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4098 - accuracy: 0.8090 - val_loss: 0.4484 - val_accuracy: 0.7784\n",
      "Epoch 632/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8076\n",
      "Epoch 00632: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4112 - accuracy: 0.8077 - val_loss: 0.4465 - val_accuracy: 0.7788\n",
      "Epoch 633/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4091 - accuracy: 0.8086\n",
      "Epoch 00633: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4097 - accuracy: 0.8084 - val_loss: 0.4272 - val_accuracy: 0.7979\n",
      "Epoch 634/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4147 - accuracy: 0.8066\n",
      "Epoch 00634: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4149 - accuracy: 0.8065 - val_loss: 0.4675 - val_accuracy: 0.7653\n",
      "Epoch 635/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4116 - accuracy: 0.8086\n",
      "Epoch 00635: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4121 - accuracy: 0.8087 - val_loss: 0.4375 - val_accuracy: 0.7896\n",
      "Epoch 636/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8116\n",
      "Epoch 00636: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4112 - accuracy: 0.8110 - val_loss: 0.4212 - val_accuracy: 0.7977\n",
      "Epoch 637/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8067\n",
      "Epoch 00637: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4120 - accuracy: 0.8067 - val_loss: 0.4542 - val_accuracy: 0.7769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 638/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8113\n",
      "Epoch 00638: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4074 - accuracy: 0.8111 - val_loss: 0.4341 - val_accuracy: 0.7921\n",
      "Epoch 639/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8064\n",
      "Epoch 00639: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4120 - accuracy: 0.8063 - val_loss: 0.4426 - val_accuracy: 0.7837\n",
      "Epoch 640/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8060\n",
      "Epoch 00640: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4112 - accuracy: 0.8063 - val_loss: 0.4544 - val_accuracy: 0.7743\n",
      "Epoch 641/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4091 - accuracy: 0.8091\n",
      "Epoch 00641: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4096 - accuracy: 0.8091 - val_loss: 0.4290 - val_accuracy: 0.7966\n",
      "Epoch 642/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8069\n",
      "Epoch 00642: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4123 - accuracy: 0.8068 - val_loss: 0.4665 - val_accuracy: 0.7655\n",
      "Epoch 643/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8094\n",
      "Epoch 00643: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4094 - accuracy: 0.8092 - val_loss: 0.4386 - val_accuracy: 0.7861\n",
      "Epoch 644/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8084\n",
      "Epoch 00644: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4104 - accuracy: 0.8081 - val_loss: 0.4555 - val_accuracy: 0.7741\n",
      "Epoch 645/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8086\n",
      "Epoch 00645: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4107 - accuracy: 0.8084 - val_loss: 0.4573 - val_accuracy: 0.7709\n",
      "Epoch 646/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8089\n",
      "Epoch 00646: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4096 - accuracy: 0.8088 - val_loss: 0.4596 - val_accuracy: 0.7709\n",
      "Epoch 647/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8078\n",
      "Epoch 00647: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4116 - accuracy: 0.8078 - val_loss: 0.4500 - val_accuracy: 0.7775\n",
      "Epoch 648/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8079\n",
      "Epoch 00648: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4077 - accuracy: 0.8077 - val_loss: 0.4364 - val_accuracy: 0.7898\n",
      "Epoch 649/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8087\n",
      "Epoch 00649: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4094 - accuracy: 0.8086 - val_loss: 0.4291 - val_accuracy: 0.7926\n",
      "Epoch 650/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4058 - accuracy: 0.8123\n",
      "Epoch 00650: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4064 - accuracy: 0.8124 - val_loss: 0.4413 - val_accuracy: 0.7846\n",
      "Epoch 651/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8115\n",
      "Epoch 00651: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4080 - accuracy: 0.8110 - val_loss: 0.4331 - val_accuracy: 0.7893\n",
      "Epoch 652/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4055 - accuracy: 0.8115\n",
      "Epoch 00652: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4057 - accuracy: 0.8116 - val_loss: 0.4590 - val_accuracy: 0.7709\n",
      "Epoch 653/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4058 - accuracy: 0.8117\n",
      "Epoch 00653: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4061 - accuracy: 0.8114 - val_loss: 0.4514 - val_accuracy: 0.7773\n",
      "Epoch 654/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8100\n",
      "Epoch 00654: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4081 - accuracy: 0.8097 - val_loss: 0.4771 - val_accuracy: 0.7614\n",
      "Epoch 655/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8115\n",
      "Epoch 00655: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4049 - accuracy: 0.8116 - val_loss: 0.4489 - val_accuracy: 0.7810\n",
      "Epoch 656/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8092\n",
      "Epoch 00656: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4071 - accuracy: 0.8093 - val_loss: 0.4451 - val_accuracy: 0.7803\n",
      "Epoch 657/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8127\n",
      "Epoch 00657: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4063 - accuracy: 0.8127 - val_loss: 0.4426 - val_accuracy: 0.7848\n",
      "Epoch 658/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4037 - accuracy: 0.8159\n",
      "Epoch 00658: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4041 - accuracy: 0.8157 - val_loss: 0.4382 - val_accuracy: 0.7883\n",
      "Epoch 659/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8129\n",
      "Epoch 00659: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4077 - accuracy: 0.8124 - val_loss: 0.4390 - val_accuracy: 0.7872\n",
      "Epoch 660/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4039 - accuracy: 0.8136\n",
      "Epoch 00660: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4042 - accuracy: 0.8137 - val_loss: 0.4279 - val_accuracy: 0.7945\n",
      "Epoch 661/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8118\n",
      "Epoch 00661: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4080 - accuracy: 0.8116 - val_loss: 0.4452 - val_accuracy: 0.7827\n",
      "Epoch 662/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8109\n",
      "Epoch 00662: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4102 - accuracy: 0.8105 - val_loss: 0.4281 - val_accuracy: 0.7973\n",
      "Epoch 663/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4057 - accuracy: 0.8114\n",
      "Epoch 00663: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4055 - accuracy: 0.8112 - val_loss: 0.4474 - val_accuracy: 0.7801\n",
      "Epoch 664/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8121\n",
      "Epoch 00664: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4081 - accuracy: 0.8119 - val_loss: 0.4630 - val_accuracy: 0.7689\n",
      "Epoch 665/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8073\n",
      "Epoch 00665: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4079 - accuracy: 0.8072 - val_loss: 0.4384 - val_accuracy: 0.7880\n",
      "Epoch 666/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8103\n",
      "Epoch 00666: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4084 - accuracy: 0.8102 - val_loss: 0.4470 - val_accuracy: 0.7812\n",
      "Epoch 667/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.8118\n",
      "Epoch 00667: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4055 - accuracy: 0.8117 - val_loss: 0.4544 - val_accuracy: 0.7758\n",
      "Epoch 668/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8139\n",
      "Epoch 00668: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4058 - accuracy: 0.8138 - val_loss: 0.4426 - val_accuracy: 0.7844\n",
      "Epoch 669/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8124\n",
      "Epoch 00669: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4065 - accuracy: 0.8121 - val_loss: 0.4463 - val_accuracy: 0.7818\n",
      "Epoch 670/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4014 - accuracy: 0.8134\n",
      "Epoch 00670: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4020 - accuracy: 0.8131 - val_loss: 0.4522 - val_accuracy: 0.7788\n",
      "Epoch 671/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4058 - accuracy: 0.8126\n",
      "Epoch 00671: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4060 - accuracy: 0.8125 - val_loss: 0.4349 - val_accuracy: 0.7928\n",
      "Epoch 672/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8120\n",
      "Epoch 00672: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4056 - accuracy: 0.8116 - val_loss: 0.4373 - val_accuracy: 0.7908\n",
      "Epoch 673/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8130\n",
      "Epoch 00673: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4053 - accuracy: 0.8129 - val_loss: 0.4441 - val_accuracy: 0.7827\n",
      "Epoch 674/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8149\n",
      "Epoch 00674: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4019 - accuracy: 0.8148 - val_loss: 0.4566 - val_accuracy: 0.7741\n",
      "Epoch 675/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8150\n",
      "Epoch 00675: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4020 - accuracy: 0.8150 - val_loss: 0.4319 - val_accuracy: 0.7951\n",
      "Epoch 676/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8130\n",
      "Epoch 00676: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4034 - accuracy: 0.8125 - val_loss: 0.4358 - val_accuracy: 0.7896\n",
      "Epoch 677/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4050 - accuracy: 0.8102\n",
      "Epoch 00677: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4054 - accuracy: 0.8102 - val_loss: 0.4473 - val_accuracy: 0.7771\n",
      "Epoch 678/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4020 - accuracy: 0.8128\n",
      "Epoch 00678: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4020 - accuracy: 0.8130 - val_loss: 0.4457 - val_accuracy: 0.7805\n",
      "Epoch 679/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4018 - accuracy: 0.8137\n",
      "Epoch 00679: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4021 - accuracy: 0.8138 - val_loss: 0.4466 - val_accuracy: 0.7801\n",
      "Epoch 680/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8151\n",
      "Epoch 00680: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4013 - accuracy: 0.8152 - val_loss: 0.4484 - val_accuracy: 0.7797\n",
      "Epoch 681/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4035 - accuracy: 0.8103\n",
      "Epoch 00681: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4035 - accuracy: 0.8100 - val_loss: 0.4537 - val_accuracy: 0.7758\n",
      "Epoch 682/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8136\n",
      "Epoch 00682: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4030 - accuracy: 0.8136 - val_loss: 0.4376 - val_accuracy: 0.7872\n",
      "Epoch 683/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8134\n",
      "Epoch 00683: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4050 - accuracy: 0.8132 - val_loss: 0.4409 - val_accuracy: 0.7844\n",
      "Epoch 684/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8137\n",
      "Epoch 00684: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4000 - accuracy: 0.8135 - val_loss: 0.4567 - val_accuracy: 0.7719\n",
      "Epoch 685/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.8132\n",
      "Epoch 00685: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4001 - accuracy: 0.8131 - val_loss: 0.4259 - val_accuracy: 0.7988\n",
      "Epoch 686/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8145\n",
      "Epoch 00686: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3997 - accuracy: 0.8142 - val_loss: 0.4488 - val_accuracy: 0.7810\n",
      "Epoch 687/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8158\n",
      "Epoch 00687: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4020 - accuracy: 0.8158 - val_loss: 0.4432 - val_accuracy: 0.7837\n",
      "Epoch 688/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8118\n",
      "Epoch 00688: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4053 - accuracy: 0.8118 - val_loss: 0.4251 - val_accuracy: 0.7994\n",
      "Epoch 689/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3997 - accuracy: 0.8169\n",
      "Epoch 00689: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4000 - accuracy: 0.8167 - val_loss: 0.4476 - val_accuracy: 0.7779\n",
      "Epoch 690/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4020 - accuracy: 0.8126\n",
      "Epoch 00690: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4026 - accuracy: 0.8125 - val_loss: 0.4379 - val_accuracy: 0.7876\n",
      "Epoch 691/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8117\n",
      "Epoch 00691: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4023 - accuracy: 0.8114 - val_loss: 0.4366 - val_accuracy: 0.7889\n",
      "Epoch 692/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8181\n",
      "Epoch 00692: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3974 - accuracy: 0.8180 - val_loss: 0.4344 - val_accuracy: 0.7926\n",
      "Epoch 693/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8146\n",
      "Epoch 00693: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3997 - accuracy: 0.8144 - val_loss: 0.4440 - val_accuracy: 0.7837\n",
      "Epoch 694/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8146\n",
      "Epoch 00694: val_loss did not improve from 0.41955\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3982 - accuracy: 0.8145 - val_loss: 0.4406 - val_accuracy: 0.7848\n",
      "Epoch 695/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8156\n",
      "Epoch 00695: val_loss improved from 0.41955 to 0.41730, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3984 - accuracy: 0.8149 - val_loss: 0.4173 - val_accuracy: 0.8061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 696/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8159\n",
      "Epoch 00696: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4007 - accuracy: 0.8156 - val_loss: 0.4435 - val_accuracy: 0.7837\n",
      "Epoch 697/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.8137\n",
      "Epoch 00697: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4013 - accuracy: 0.8138 - val_loss: 0.4451 - val_accuracy: 0.7842\n",
      "Epoch 698/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8141\n",
      "Epoch 00698: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3999 - accuracy: 0.8142 - val_loss: 0.4509 - val_accuracy: 0.7773\n",
      "Epoch 699/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4006 - accuracy: 0.8136\n",
      "Epoch 00699: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4007 - accuracy: 0.8138 - val_loss: 0.4381 - val_accuracy: 0.7872\n",
      "Epoch 700/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8129\n",
      "Epoch 00700: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4002 - accuracy: 0.8128 - val_loss: 0.4400 - val_accuracy: 0.7855\n",
      "Epoch 701/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8186\n",
      "Epoch 00701: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3953 - accuracy: 0.8186 - val_loss: 0.4317 - val_accuracy: 0.7943\n",
      "Epoch 702/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8165\n",
      "Epoch 00702: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4021 - accuracy: 0.8157 - val_loss: 0.4312 - val_accuracy: 0.7947\n",
      "Epoch 703/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3966 - accuracy: 0.8133\n",
      "Epoch 00703: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3972 - accuracy: 0.8134 - val_loss: 0.4553 - val_accuracy: 0.7739\n",
      "Epoch 704/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8141\n",
      "Epoch 00704: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4011 - accuracy: 0.8140 - val_loss: 0.4429 - val_accuracy: 0.7865\n",
      "Epoch 705/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8150\n",
      "Epoch 00705: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4006 - accuracy: 0.8150 - val_loss: 0.4327 - val_accuracy: 0.7926\n",
      "Epoch 706/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4003 - accuracy: 0.8132\n",
      "Epoch 00706: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4008 - accuracy: 0.8127 - val_loss: 0.4323 - val_accuracy: 0.7949\n",
      "Epoch 707/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8155\n",
      "Epoch 00707: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3956 - accuracy: 0.8156 - val_loss: 0.4390 - val_accuracy: 0.7874\n",
      "Epoch 708/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8124\n",
      "Epoch 00708: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3980 - accuracy: 0.8125 - val_loss: 0.4318 - val_accuracy: 0.7921\n",
      "Epoch 709/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8158\n",
      "Epoch 00709: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4008 - accuracy: 0.8157 - val_loss: 0.4306 - val_accuracy: 0.7911\n",
      "Epoch 710/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.8153\n",
      "Epoch 00710: val_loss did not improve from 0.41730\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3975 - accuracy: 0.8153 - val_loss: 0.4450 - val_accuracy: 0.7797\n",
      "Epoch 711/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8154\n",
      "Epoch 00711: val_loss improved from 0.41730 to 0.41206, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3982 - accuracy: 0.8151 - val_loss: 0.4121 - val_accuracy: 0.8076\n",
      "Epoch 712/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.8153\n",
      "Epoch 00712: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3964 - accuracy: 0.8153 - val_loss: 0.4344 - val_accuracy: 0.7928\n",
      "Epoch 713/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8161\n",
      "Epoch 00713: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3946 - accuracy: 0.8159 - val_loss: 0.4386 - val_accuracy: 0.7878\n",
      "Epoch 714/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3961 - accuracy: 0.8154\n",
      "Epoch 00714: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3956 - accuracy: 0.8155 - val_loss: 0.4695 - val_accuracy: 0.7629\n",
      "Epoch 715/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8143\n",
      "Epoch 00715: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3980 - accuracy: 0.8141 - val_loss: 0.4236 - val_accuracy: 0.8016\n",
      "Epoch 716/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.8148\n",
      "Epoch 00716: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3996 - accuracy: 0.8144 - val_loss: 0.4259 - val_accuracy: 0.7958\n",
      "Epoch 717/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3951 - accuracy: 0.8175\n",
      "Epoch 00717: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3955 - accuracy: 0.8175 - val_loss: 0.4265 - val_accuracy: 0.7962\n",
      "Epoch 718/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.8165\n",
      "Epoch 00718: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3962 - accuracy: 0.8164 - val_loss: 0.4422 - val_accuracy: 0.7848\n",
      "Epoch 719/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8194\n",
      "Epoch 00719: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3939 - accuracy: 0.8193 - val_loss: 0.4322 - val_accuracy: 0.7941\n",
      "Epoch 720/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8188\n",
      "Epoch 00720: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3959 - accuracy: 0.8185 - val_loss: 0.4284 - val_accuracy: 0.7958\n",
      "Epoch 721/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8156\n",
      "Epoch 00721: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3989 - accuracy: 0.8155 - val_loss: 0.4219 - val_accuracy: 0.8005\n",
      "Epoch 722/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8152\n",
      "Epoch 00722: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3981 - accuracy: 0.8150 - val_loss: 0.4195 - val_accuracy: 0.8018\n",
      "Epoch 723/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3950 - accuracy: 0.8155\n",
      "Epoch 00723: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3954 - accuracy: 0.8155 - val_loss: 0.4363 - val_accuracy: 0.7891\n",
      "Epoch 724/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8172\n",
      "Epoch 00724: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3975 - accuracy: 0.8173 - val_loss: 0.4276 - val_accuracy: 0.7960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 725/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8174\n",
      "Epoch 00725: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3930 - accuracy: 0.8174 - val_loss: 0.4370 - val_accuracy: 0.7880\n",
      "Epoch 726/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8170\n",
      "Epoch 00726: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3958 - accuracy: 0.8172 - val_loss: 0.4509 - val_accuracy: 0.7760\n",
      "Epoch 727/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8191\n",
      "Epoch 00727: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3936 - accuracy: 0.8190 - val_loss: 0.4266 - val_accuracy: 0.7951\n",
      "Epoch 728/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8162\n",
      "Epoch 00728: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3977 - accuracy: 0.8161 - val_loss: 0.4317 - val_accuracy: 0.7928\n",
      "Epoch 729/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8182\n",
      "Epoch 00729: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3913 - accuracy: 0.8181 - val_loss: 0.4195 - val_accuracy: 0.8009\n",
      "Epoch 730/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8181\n",
      "Epoch 00730: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3919 - accuracy: 0.8181 - val_loss: 0.4531 - val_accuracy: 0.7745\n",
      "Epoch 731/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3937 - accuracy: 0.8181\n",
      "Epoch 00731: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3935 - accuracy: 0.8181 - val_loss: 0.4474 - val_accuracy: 0.7825\n",
      "Epoch 732/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.8165\n",
      "Epoch 00732: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3962 - accuracy: 0.8164 - val_loss: 0.4225 - val_accuracy: 0.7990\n",
      "Epoch 733/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8154\n",
      "Epoch 00733: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3986 - accuracy: 0.8153 - val_loss: 0.4326 - val_accuracy: 0.7913\n",
      "Epoch 734/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8153\n",
      "Epoch 00734: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3958 - accuracy: 0.8155 - val_loss: 0.4279 - val_accuracy: 0.7984\n",
      "Epoch 735/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8231\n",
      "Epoch 00735: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3943 - accuracy: 0.8226 - val_loss: 0.4271 - val_accuracy: 0.7930\n",
      "Epoch 736/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8224\n",
      "Epoch 00736: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3889 - accuracy: 0.8220 - val_loss: 0.4318 - val_accuracy: 0.7913\n",
      "Epoch 737/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8217\n",
      "Epoch 00737: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3892 - accuracy: 0.8218 - val_loss: 0.4207 - val_accuracy: 0.8005\n",
      "Epoch 738/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8154\n",
      "Epoch 00738: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3944 - accuracy: 0.8153 - val_loss: 0.4202 - val_accuracy: 0.8033\n",
      "Epoch 739/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8193\n",
      "Epoch 00739: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3960 - accuracy: 0.8191 - val_loss: 0.4437 - val_accuracy: 0.7827\n",
      "Epoch 740/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8191\n",
      "Epoch 00740: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3895 - accuracy: 0.8193 - val_loss: 0.4468 - val_accuracy: 0.7807\n",
      "Epoch 741/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8183\n",
      "Epoch 00741: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3910 - accuracy: 0.8179 - val_loss: 0.4239 - val_accuracy: 0.7969\n",
      "Epoch 742/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3937 - accuracy: 0.8180\n",
      "Epoch 00742: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3942 - accuracy: 0.8179 - val_loss: 0.4307 - val_accuracy: 0.7932\n",
      "Epoch 743/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8183\n",
      "Epoch 00743: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3927 - accuracy: 0.8181 - val_loss: 0.4248 - val_accuracy: 0.7964\n",
      "Epoch 744/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8218\n",
      "Epoch 00744: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3898 - accuracy: 0.8221 - val_loss: 0.4245 - val_accuracy: 0.7964\n",
      "Epoch 745/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8205\n",
      "Epoch 00745: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3895 - accuracy: 0.8207 - val_loss: 0.4566 - val_accuracy: 0.7721\n",
      "Epoch 746/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8203\n",
      "Epoch 00746: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3894 - accuracy: 0.8204 - val_loss: 0.4249 - val_accuracy: 0.8001\n",
      "Epoch 747/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8199\n",
      "Epoch 00747: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3921 - accuracy: 0.8196 - val_loss: 0.4189 - val_accuracy: 0.8012\n",
      "Epoch 748/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8211\n",
      "Epoch 00748: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3917 - accuracy: 0.8213 - val_loss: 0.4563 - val_accuracy: 0.7721\n",
      "Epoch 749/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3910 - accuracy: 0.8199\n",
      "Epoch 00749: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3912 - accuracy: 0.8198 - val_loss: 0.4506 - val_accuracy: 0.7773\n",
      "Epoch 750/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8203\n",
      "Epoch 00750: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3891 - accuracy: 0.8202 - val_loss: 0.4574 - val_accuracy: 0.7728\n",
      "Epoch 751/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8190\n",
      "Epoch 00751: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3947 - accuracy: 0.8190 - val_loss: 0.4275 - val_accuracy: 0.7949\n",
      "Epoch 752/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8238\n",
      "Epoch 00752: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3847 - accuracy: 0.8232 - val_loss: 0.4371 - val_accuracy: 0.7861\n",
      "Epoch 753/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8217\n",
      "Epoch 00753: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3917 - accuracy: 0.8214 - val_loss: 0.4281 - val_accuracy: 0.7962\n",
      "Epoch 754/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8222\n",
      "Epoch 00754: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3865 - accuracy: 0.8220 - val_loss: 0.4395 - val_accuracy: 0.7859\n",
      "Epoch 755/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8170\n",
      "Epoch 00755: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3914 - accuracy: 0.8170 - val_loss: 0.4245 - val_accuracy: 0.7973\n",
      "Epoch 756/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3853 - accuracy: 0.8250\n",
      "Epoch 00756: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3859 - accuracy: 0.8246 - val_loss: 0.4256 - val_accuracy: 0.7964\n",
      "Epoch 757/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3882 - accuracy: 0.8199\n",
      "Epoch 00757: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3889 - accuracy: 0.8197 - val_loss: 0.4289 - val_accuracy: 0.7949\n",
      "Epoch 758/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3843 - accuracy: 0.8206\n",
      "Epoch 00758: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3845 - accuracy: 0.8206 - val_loss: 0.4353 - val_accuracy: 0.7906\n",
      "Epoch 759/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.8208\n",
      "Epoch 00759: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3879 - accuracy: 0.8207 - val_loss: 0.4253 - val_accuracy: 0.7975\n",
      "Epoch 760/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3856 - accuracy: 0.8236\n",
      "Epoch 00760: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3855 - accuracy: 0.8236 - val_loss: 0.4259 - val_accuracy: 0.7954\n",
      "Epoch 761/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3897 - accuracy: 0.8190\n",
      "Epoch 00761: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3900 - accuracy: 0.8189 - val_loss: 0.4245 - val_accuracy: 0.7986\n",
      "Epoch 762/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3885 - accuracy: 0.8198\n",
      "Epoch 00762: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3893 - accuracy: 0.8192 - val_loss: 0.4185 - val_accuracy: 0.7986\n",
      "Epoch 763/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8220\n",
      "Epoch 00763: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3869 - accuracy: 0.8220 - val_loss: 0.4478 - val_accuracy: 0.7784\n",
      "Epoch 764/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.8235\n",
      "Epoch 00764: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3841 - accuracy: 0.8235 - val_loss: 0.4430 - val_accuracy: 0.7844\n",
      "Epoch 765/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3885 - accuracy: 0.8190\n",
      "Epoch 00765: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3887 - accuracy: 0.8188 - val_loss: 0.4499 - val_accuracy: 0.7777\n",
      "Epoch 766/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8198\n",
      "Epoch 00766: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3910 - accuracy: 0.8199 - val_loss: 0.4335 - val_accuracy: 0.7911\n",
      "Epoch 767/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8183\n",
      "Epoch 00767: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3899 - accuracy: 0.8182 - val_loss: 0.4696 - val_accuracy: 0.7640\n",
      "Epoch 768/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3879 - accuracy: 0.8188\n",
      "Epoch 00768: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3888 - accuracy: 0.8184 - val_loss: 0.4172 - val_accuracy: 0.8033\n",
      "Epoch 769/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3905 - accuracy: 0.8186\n",
      "Epoch 00769: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3911 - accuracy: 0.8183 - val_loss: 0.4538 - val_accuracy: 0.7749\n",
      "Epoch 770/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3875 - accuracy: 0.8236\n",
      "Epoch 00770: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3881 - accuracy: 0.8235 - val_loss: 0.4412 - val_accuracy: 0.7820\n",
      "Epoch 771/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8237\n",
      "Epoch 00771: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3878 - accuracy: 0.8236 - val_loss: 0.4313 - val_accuracy: 0.7921\n",
      "Epoch 772/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3853 - accuracy: 0.8217\n",
      "Epoch 00772: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3854 - accuracy: 0.8217 - val_loss: 0.4372 - val_accuracy: 0.7872\n",
      "Epoch 773/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.8232\n",
      "Epoch 00773: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3852 - accuracy: 0.8232 - val_loss: 0.4309 - val_accuracy: 0.7917\n",
      "Epoch 774/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8250\n",
      "Epoch 00774: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3817 - accuracy: 0.8250 - val_loss: 0.4280 - val_accuracy: 0.7947\n",
      "Epoch 775/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8194\n",
      "Epoch 00775: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3851 - accuracy: 0.8195 - val_loss: 0.4481 - val_accuracy: 0.7790\n",
      "Epoch 776/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3839 - accuracy: 0.8256\n",
      "Epoch 00776: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3840 - accuracy: 0.8257 - val_loss: 0.4252 - val_accuracy: 0.7966\n",
      "Epoch 777/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8233\n",
      "Epoch 00777: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3871 - accuracy: 0.8234 - val_loss: 0.4293 - val_accuracy: 0.7904\n",
      "Epoch 778/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.8215\n",
      "Epoch 00778: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3878 - accuracy: 0.8214 - val_loss: 0.4181 - val_accuracy: 0.8022\n",
      "Epoch 779/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.8209\n",
      "Epoch 00779: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3852 - accuracy: 0.8210 - val_loss: 0.4560 - val_accuracy: 0.7726\n",
      "Epoch 780/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3831 - accuracy: 0.8249\n",
      "Epoch 00780: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3831 - accuracy: 0.8251 - val_loss: 0.4523 - val_accuracy: 0.7764\n",
      "Epoch 781/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8278\n",
      "Epoch 00781: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3794 - accuracy: 0.8278 - val_loss: 0.4554 - val_accuracy: 0.7754\n",
      "Epoch 782/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8234\n",
      "Epoch 00782: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3855 - accuracy: 0.8233 - val_loss: 0.4563 - val_accuracy: 0.7754\n",
      "Epoch 783/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.8223\n",
      "Epoch 00783: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3843 - accuracy: 0.8218 - val_loss: 0.4325 - val_accuracy: 0.7926\n",
      "Epoch 784/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8223\n",
      "Epoch 00784: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3820 - accuracy: 0.8222 - val_loss: 0.4392 - val_accuracy: 0.7870\n",
      "Epoch 785/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8247\n",
      "Epoch 00785: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3820 - accuracy: 0.8243 - val_loss: 0.4318 - val_accuracy: 0.7902\n",
      "Epoch 786/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8218\n",
      "Epoch 00786: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3852 - accuracy: 0.8219 - val_loss: 0.4238 - val_accuracy: 0.7936\n",
      "Epoch 787/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8222\n",
      "Epoch 00787: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3875 - accuracy: 0.8222 - val_loss: 0.4444 - val_accuracy: 0.7825\n",
      "Epoch 788/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8259\n",
      "Epoch 00788: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3791 - accuracy: 0.8260 - val_loss: 0.4537 - val_accuracy: 0.7782\n",
      "Epoch 789/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8248\n",
      "Epoch 00789: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3819 - accuracy: 0.8251 - val_loss: 0.4293 - val_accuracy: 0.7915\n",
      "Epoch 790/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3798 - accuracy: 0.8264\n",
      "Epoch 00790: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3798 - accuracy: 0.8262 - val_loss: 0.4281 - val_accuracy: 0.7975\n",
      "Epoch 791/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8245\n",
      "Epoch 00791: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3806 - accuracy: 0.8244 - val_loss: 0.4551 - val_accuracy: 0.7752\n",
      "Epoch 792/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8252\n",
      "Epoch 00792: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3861 - accuracy: 0.8248 - val_loss: 0.4342 - val_accuracy: 0.7896\n",
      "Epoch 793/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8220\n",
      "Epoch 00793: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3824 - accuracy: 0.8222 - val_loss: 0.4367 - val_accuracy: 0.7863\n",
      "Epoch 794/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8248\n",
      "Epoch 00794: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3828 - accuracy: 0.8245 - val_loss: 0.4139 - val_accuracy: 0.8048\n",
      "Epoch 795/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8252\n",
      "Epoch 00795: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3803 - accuracy: 0.8248 - val_loss: 0.4428 - val_accuracy: 0.7835\n",
      "Epoch 796/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8267\n",
      "Epoch 00796: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3824 - accuracy: 0.8265 - val_loss: 0.4411 - val_accuracy: 0.7820\n",
      "Epoch 797/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8245\n",
      "Epoch 00797: val_loss did not improve from 0.41206\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3814 - accuracy: 0.8242 - val_loss: 0.4532 - val_accuracy: 0.7764\n",
      "Epoch 798/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8293\n",
      "Epoch 00798: val_loss improved from 0.41206 to 0.40976, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3805 - accuracy: 0.8291 - val_loss: 0.4098 - val_accuracy: 0.8076\n",
      "Epoch 799/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3804 - accuracy: 0.8254\n",
      "Epoch 00799: val_loss did not improve from 0.40976\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3802 - accuracy: 0.8253 - val_loss: 0.4241 - val_accuracy: 0.7982\n",
      "Epoch 800/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3780 - accuracy: 0.8293\n",
      "Epoch 00800: val_loss did not improve from 0.40976\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3784 - accuracy: 0.8295 - val_loss: 0.4560 - val_accuracy: 0.7736\n",
      "Epoch 801/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8228\n",
      "Epoch 00801: val_loss improved from 0.40976 to 0.40459, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3854 - accuracy: 0.8227 - val_loss: 0.4046 - val_accuracy: 0.8098\n",
      "Epoch 802/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8234\n",
      "Epoch 00802: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3825 - accuracy: 0.8231 - val_loss: 0.4285 - val_accuracy: 0.7917\n",
      "Epoch 803/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.8296\n",
      "Epoch 00803: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3740 - accuracy: 0.8297 - val_loss: 0.4238 - val_accuracy: 0.7973\n",
      "Epoch 804/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8272\n",
      "Epoch 00804: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3804 - accuracy: 0.8270 - val_loss: 0.4266 - val_accuracy: 0.7962\n",
      "Epoch 805/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8252\n",
      "Epoch 00805: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3816 - accuracy: 0.8253 - val_loss: 0.4251 - val_accuracy: 0.7960\n",
      "Epoch 806/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8274\n",
      "Epoch 00806: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3827 - accuracy: 0.8271 - val_loss: 0.4569 - val_accuracy: 0.7743\n",
      "Epoch 807/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.8275\n",
      "Epoch 00807: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3782 - accuracy: 0.8275 - val_loss: 0.4463 - val_accuracy: 0.7792\n",
      "Epoch 808/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8224\n",
      "Epoch 00808: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3818 - accuracy: 0.8222 - val_loss: 0.4255 - val_accuracy: 0.7958\n",
      "Epoch 809/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8255\n",
      "Epoch 00809: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3803 - accuracy: 0.8255 - val_loss: 0.4282 - val_accuracy: 0.7921\n",
      "Epoch 810/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8295\n",
      "Epoch 00810: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3750 - accuracy: 0.8295 - val_loss: 0.4442 - val_accuracy: 0.7801\n",
      "Epoch 811/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8204\n",
      "Epoch 00811: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3873 - accuracy: 0.8201 - val_loss: 0.4162 - val_accuracy: 0.8031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8244\n",
      "Epoch 00812: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3840 - accuracy: 0.8243 - val_loss: 0.4259 - val_accuracy: 0.7954\n",
      "Epoch 813/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3781 - accuracy: 0.8238\n",
      "Epoch 00813: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3781 - accuracy: 0.8239 - val_loss: 0.4224 - val_accuracy: 0.7975\n",
      "Epoch 814/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8263\n",
      "Epoch 00814: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3784 - accuracy: 0.8262 - val_loss: 0.4343 - val_accuracy: 0.7855\n",
      "Epoch 815/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8285\n",
      "Epoch 00815: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3763 - accuracy: 0.8288 - val_loss: 0.4256 - val_accuracy: 0.7975\n",
      "Epoch 816/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8230\n",
      "Epoch 00816: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3815 - accuracy: 0.8229 - val_loss: 0.4285 - val_accuracy: 0.7923\n",
      "Epoch 817/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8278\n",
      "Epoch 00817: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3765 - accuracy: 0.8281 - val_loss: 0.4403 - val_accuracy: 0.7844\n",
      "Epoch 818/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8257\n",
      "Epoch 00818: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3771 - accuracy: 0.8257 - val_loss: 0.4284 - val_accuracy: 0.7943\n",
      "Epoch 819/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8255\n",
      "Epoch 00819: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3805 - accuracy: 0.8252 - val_loss: 0.4086 - val_accuracy: 0.8065\n",
      "Epoch 820/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8283\n",
      "Epoch 00820: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3749 - accuracy: 0.8282 - val_loss: 0.4508 - val_accuracy: 0.7749\n",
      "Epoch 821/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8305\n",
      "Epoch 00821: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3758 - accuracy: 0.8305 - val_loss: 0.4320 - val_accuracy: 0.7896\n",
      "Epoch 822/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3778 - accuracy: 0.8278\n",
      "Epoch 00822: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3782 - accuracy: 0.8276 - val_loss: 0.4315 - val_accuracy: 0.7880\n",
      "Epoch 823/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8299\n",
      "Epoch 00823: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3761 - accuracy: 0.8301 - val_loss: 0.4328 - val_accuracy: 0.7870\n",
      "Epoch 824/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3774 - accuracy: 0.8255\n",
      "Epoch 00824: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3773 - accuracy: 0.8256 - val_loss: 0.4185 - val_accuracy: 0.7992\n",
      "Epoch 825/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8282\n",
      "Epoch 00825: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3763 - accuracy: 0.8277 - val_loss: 0.4134 - val_accuracy: 0.8020\n",
      "Epoch 826/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.8269\n",
      "Epoch 00826: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3781 - accuracy: 0.8265 - val_loss: 0.4245 - val_accuracy: 0.7960\n",
      "Epoch 827/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8273\n",
      "Epoch 00827: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3807 - accuracy: 0.8272 - val_loss: 0.4211 - val_accuracy: 0.7999\n",
      "Epoch 828/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8259\n",
      "Epoch 00828: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3765 - accuracy: 0.8261 - val_loss: 0.4208 - val_accuracy: 0.7994\n",
      "Epoch 829/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8275\n",
      "Epoch 00829: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3753 - accuracy: 0.8276 - val_loss: 0.4350 - val_accuracy: 0.7865\n",
      "Epoch 830/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8288\n",
      "Epoch 00830: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3757 - accuracy: 0.8290 - val_loss: 0.4416 - val_accuracy: 0.7833\n",
      "Epoch 831/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8322\n",
      "Epoch 00831: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3740 - accuracy: 0.8320 - val_loss: 0.4127 - val_accuracy: 0.8063\n",
      "Epoch 832/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8233\n",
      "Epoch 00832: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3784 - accuracy: 0.8228 - val_loss: 0.4300 - val_accuracy: 0.7919\n",
      "Epoch 833/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8314\n",
      "Epoch 00833: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3756 - accuracy: 0.8313 - val_loss: 0.4177 - val_accuracy: 0.7984\n",
      "Epoch 834/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8267\n",
      "Epoch 00834: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3757 - accuracy: 0.8267 - val_loss: 0.4284 - val_accuracy: 0.7919\n",
      "Epoch 835/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.8288\n",
      "Epoch 00835: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3742 - accuracy: 0.8290 - val_loss: 0.4446 - val_accuracy: 0.7827\n",
      "Epoch 836/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8271\n",
      "Epoch 00836: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3753 - accuracy: 0.8271 - val_loss: 0.4432 - val_accuracy: 0.7846\n",
      "Epoch 837/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8267\n",
      "Epoch 00837: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3731 - accuracy: 0.8268 - val_loss: 0.4254 - val_accuracy: 0.7932\n",
      "Epoch 838/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8281\n",
      "Epoch 00838: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3735 - accuracy: 0.8279 - val_loss: 0.4745 - val_accuracy: 0.7648\n",
      "Epoch 839/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8270\n",
      "Epoch 00839: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3775 - accuracy: 0.8268 - val_loss: 0.4152 - val_accuracy: 0.7997\n",
      "Epoch 840/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3687 - accuracy: 0.8292\n",
      "Epoch 00840: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3694 - accuracy: 0.8290 - val_loss: 0.4250 - val_accuracy: 0.7945\n",
      "Epoch 841/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8272\n",
      "Epoch 00841: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3736 - accuracy: 0.8271 - val_loss: 0.4242 - val_accuracy: 0.7969\n",
      "Epoch 842/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8286\n",
      "Epoch 00842: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3756 - accuracy: 0.8283 - val_loss: 0.4379 - val_accuracy: 0.7865\n",
      "Epoch 843/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8334\n",
      "Epoch 00843: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3714 - accuracy: 0.8331 - val_loss: 0.4467 - val_accuracy: 0.7810\n",
      "Epoch 844/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8261\n",
      "Epoch 00844: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3737 - accuracy: 0.8260 - val_loss: 0.4277 - val_accuracy: 0.7934\n",
      "Epoch 845/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8268\n",
      "Epoch 00845: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3752 - accuracy: 0.8268 - val_loss: 0.4182 - val_accuracy: 0.8005\n",
      "Epoch 846/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8292\n",
      "Epoch 00846: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3720 - accuracy: 0.8289 - val_loss: 0.4206 - val_accuracy: 0.7988\n",
      "Epoch 847/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8266\n",
      "Epoch 00847: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3753 - accuracy: 0.8267 - val_loss: 0.4455 - val_accuracy: 0.7822\n",
      "Epoch 848/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8266\n",
      "Epoch 00848: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3769 - accuracy: 0.8263 - val_loss: 0.4478 - val_accuracy: 0.7805\n",
      "Epoch 849/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8293\n",
      "Epoch 00849: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3692 - accuracy: 0.8294 - val_loss: 0.4329 - val_accuracy: 0.7919\n",
      "Epoch 850/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8347\n",
      "Epoch 00850: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3681 - accuracy: 0.8346 - val_loss: 0.4269 - val_accuracy: 0.7939\n",
      "Epoch 851/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8327\n",
      "Epoch 00851: val_loss did not improve from 0.40459\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3684 - accuracy: 0.8325 - val_loss: 0.4230 - val_accuracy: 0.7969\n",
      "Epoch 852/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8295\n",
      "Epoch 00852: val_loss improved from 0.40459 to 0.40420, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3753 - accuracy: 0.8291 - val_loss: 0.4042 - val_accuracy: 0.8110\n",
      "Epoch 853/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.8252\n",
      "Epoch 00853: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3787 - accuracy: 0.8250 - val_loss: 0.4296 - val_accuracy: 0.7906\n",
      "Epoch 854/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8304\n",
      "Epoch 00854: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3716 - accuracy: 0.8304 - val_loss: 0.4108 - val_accuracy: 0.8055\n",
      "Epoch 855/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8336\n",
      "Epoch 00855: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3695 - accuracy: 0.8334 - val_loss: 0.4179 - val_accuracy: 0.8009\n",
      "Epoch 856/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8298\n",
      "Epoch 00856: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3738 - accuracy: 0.8298 - val_loss: 0.4444 - val_accuracy: 0.7812\n",
      "Epoch 857/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8325\n",
      "Epoch 00857: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3692 - accuracy: 0.8323 - val_loss: 0.4350 - val_accuracy: 0.7891\n",
      "Epoch 858/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3703 - accuracy: 0.8305\n",
      "Epoch 00858: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3705 - accuracy: 0.8300 - val_loss: 0.4407 - val_accuracy: 0.7853\n",
      "Epoch 859/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8328\n",
      "Epoch 00859: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3691 - accuracy: 0.8330 - val_loss: 0.4419 - val_accuracy: 0.7788\n",
      "Epoch 860/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8335\n",
      "Epoch 00860: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3686 - accuracy: 0.8333 - val_loss: 0.4617 - val_accuracy: 0.7741\n",
      "Epoch 861/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8346\n",
      "Epoch 00861: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3679 - accuracy: 0.8343 - val_loss: 0.4341 - val_accuracy: 0.7883\n",
      "Epoch 862/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8313\n",
      "Epoch 00862: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3705 - accuracy: 0.8311 - val_loss: 0.4488 - val_accuracy: 0.7784\n",
      "Epoch 863/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8282\n",
      "Epoch 00863: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3684 - accuracy: 0.8281 - val_loss: 0.4220 - val_accuracy: 0.7990\n",
      "Epoch 864/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3671 - accuracy: 0.8322\n",
      "Epoch 00864: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3670 - accuracy: 0.8322 - val_loss: 0.4227 - val_accuracy: 0.7984\n",
      "Epoch 865/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8309\n",
      "Epoch 00865: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3687 - accuracy: 0.8309 - val_loss: 0.4375 - val_accuracy: 0.7865\n",
      "Epoch 866/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3687 - accuracy: 0.8341\n",
      "Epoch 00866: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3687 - accuracy: 0.8341 - val_loss: 0.4191 - val_accuracy: 0.7984\n",
      "Epoch 867/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8333\n",
      "Epoch 00867: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3678 - accuracy: 0.8333 - val_loss: 0.4153 - val_accuracy: 0.8003\n",
      "Epoch 868/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8359\n",
      "Epoch 00868: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3651 - accuracy: 0.8359 - val_loss: 0.4432 - val_accuracy: 0.7842\n",
      "Epoch 869/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8350\n",
      "Epoch 00869: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3628 - accuracy: 0.8348 - val_loss: 0.4173 - val_accuracy: 0.7992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 870/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3680 - accuracy: 0.8324\n",
      "Epoch 00870: val_loss did not improve from 0.40420\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3678 - accuracy: 0.8325 - val_loss: 0.4179 - val_accuracy: 0.7984\n",
      "Epoch 871/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.8339\n",
      "Epoch 00871: val_loss improved from 0.40420 to 0.40177, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3700 - accuracy: 0.8339 - val_loss: 0.4018 - val_accuracy: 0.8098\n",
      "Epoch 872/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8280\n",
      "Epoch 00872: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3691 - accuracy: 0.8282 - val_loss: 0.4166 - val_accuracy: 0.8007\n",
      "Epoch 873/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8317\n",
      "Epoch 00873: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3658 - accuracy: 0.8315 - val_loss: 0.4271 - val_accuracy: 0.7934\n",
      "Epoch 874/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8309\n",
      "Epoch 00874: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3679 - accuracy: 0.8310 - val_loss: 0.4167 - val_accuracy: 0.7999\n",
      "Epoch 875/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8328\n",
      "Epoch 00875: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3666 - accuracy: 0.8327 - val_loss: 0.4103 - val_accuracy: 0.8052\n",
      "Epoch 876/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8305\n",
      "Epoch 00876: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3704 - accuracy: 0.8303 - val_loss: 0.4281 - val_accuracy: 0.7958\n",
      "Epoch 877/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8322\n",
      "Epoch 00877: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3664 - accuracy: 0.8326 - val_loss: 0.4646 - val_accuracy: 0.7704\n",
      "Epoch 878/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8308\n",
      "Epoch 00878: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3688 - accuracy: 0.8305 - val_loss: 0.4256 - val_accuracy: 0.7934\n",
      "Epoch 879/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8314\n",
      "Epoch 00879: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3682 - accuracy: 0.8315 - val_loss: 0.4420 - val_accuracy: 0.7831\n",
      "Epoch 880/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3667 - accuracy: 0.8340\n",
      "Epoch 00880: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3674 - accuracy: 0.8337 - val_loss: 0.4446 - val_accuracy: 0.7825\n",
      "Epoch 881/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8302\n",
      "Epoch 00881: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3699 - accuracy: 0.8303 - val_loss: 0.4304 - val_accuracy: 0.7898\n",
      "Epoch 882/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3634 - accuracy: 0.8348\n",
      "Epoch 00882: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3634 - accuracy: 0.8346 - val_loss: 0.4363 - val_accuracy: 0.7863\n",
      "Epoch 883/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3705 - accuracy: 0.8325\n",
      "Epoch 00883: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3711 - accuracy: 0.8324 - val_loss: 0.4189 - val_accuracy: 0.7973\n",
      "Epoch 884/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8343\n",
      "Epoch 00884: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3639 - accuracy: 0.8344 - val_loss: 0.4305 - val_accuracy: 0.7902\n",
      "Epoch 885/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8313\n",
      "Epoch 00885: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3647 - accuracy: 0.8314 - val_loss: 0.4067 - val_accuracy: 0.8080\n",
      "Epoch 886/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.8354\n",
      "Epoch 00886: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3642 - accuracy: 0.8356 - val_loss: 0.4212 - val_accuracy: 0.7969\n",
      "Epoch 887/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8330\n",
      "Epoch 00887: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3652 - accuracy: 0.8329 - val_loss: 0.4271 - val_accuracy: 0.7943\n",
      "Epoch 888/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8393\n",
      "Epoch 00888: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3569 - accuracy: 0.8395 - val_loss: 0.4250 - val_accuracy: 0.7939\n",
      "Epoch 889/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8343\n",
      "Epoch 00889: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3678 - accuracy: 0.8341 - val_loss: 0.4187 - val_accuracy: 0.7988\n",
      "Epoch 890/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8364\n",
      "Epoch 00890: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3616 - accuracy: 0.8363 - val_loss: 0.4441 - val_accuracy: 0.7833\n",
      "Epoch 891/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8348\n",
      "Epoch 00891: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3664 - accuracy: 0.8347 - val_loss: 0.4244 - val_accuracy: 0.7958\n",
      "Epoch 892/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8312\n",
      "Epoch 00892: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3676 - accuracy: 0.8312 - val_loss: 0.4082 - val_accuracy: 0.8078\n",
      "Epoch 893/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8357\n",
      "Epoch 00893: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3614 - accuracy: 0.8355 - val_loss: 0.4308 - val_accuracy: 0.7904\n",
      "Epoch 894/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8341\n",
      "Epoch 00894: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3637 - accuracy: 0.8340 - val_loss: 0.4275 - val_accuracy: 0.7941\n",
      "Epoch 895/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8339\n",
      "Epoch 00895: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3672 - accuracy: 0.8338 - val_loss: 0.4362 - val_accuracy: 0.7870\n",
      "Epoch 896/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8356\n",
      "Epoch 00896: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3600 - accuracy: 0.8357 - val_loss: 0.4433 - val_accuracy: 0.7853\n",
      "Epoch 897/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8383\n",
      "Epoch 00897: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3586 - accuracy: 0.8383 - val_loss: 0.4352 - val_accuracy: 0.7911\n",
      "Epoch 898/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8362\n",
      "Epoch 00898: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3613 - accuracy: 0.8361 - val_loss: 0.4255 - val_accuracy: 0.7962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8328\n",
      "Epoch 00899: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3643 - accuracy: 0.8331 - val_loss: 0.4187 - val_accuracy: 0.7988\n",
      "Epoch 900/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8309\n",
      "Epoch 00900: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3667 - accuracy: 0.8308 - val_loss: 0.4338 - val_accuracy: 0.7883\n",
      "Epoch 901/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8333\n",
      "Epoch 00901: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3656 - accuracy: 0.8335 - val_loss: 0.4206 - val_accuracy: 0.7979\n",
      "Epoch 902/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8335\n",
      "Epoch 00902: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3629 - accuracy: 0.8334 - val_loss: 0.4446 - val_accuracy: 0.7837\n",
      "Epoch 903/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8325\n",
      "Epoch 00903: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3650 - accuracy: 0.8325 - val_loss: 0.4260 - val_accuracy: 0.7928\n",
      "Epoch 904/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8333\n",
      "Epoch 00904: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3648 - accuracy: 0.8335 - val_loss: 0.4359 - val_accuracy: 0.7863\n",
      "Epoch 905/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8332\n",
      "Epoch 00905: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3620 - accuracy: 0.8336 - val_loss: 0.4165 - val_accuracy: 0.8009\n",
      "Epoch 906/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8348\n",
      "Epoch 00906: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3636 - accuracy: 0.8346 - val_loss: 0.4336 - val_accuracy: 0.7893\n",
      "Epoch 907/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8340\n",
      "Epoch 00907: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3630 - accuracy: 0.8343 - val_loss: 0.4327 - val_accuracy: 0.7887\n",
      "Epoch 908/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8361\n",
      "Epoch 00908: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3578 - accuracy: 0.8356 - val_loss: 0.4181 - val_accuracy: 0.7997\n",
      "Epoch 909/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8324\n",
      "Epoch 00909: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3647 - accuracy: 0.8321 - val_loss: 0.4219 - val_accuracy: 0.7977\n",
      "Epoch 910/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8334\n",
      "Epoch 00910: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3630 - accuracy: 0.8333 - val_loss: 0.4226 - val_accuracy: 0.7966\n",
      "Epoch 911/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8369\n",
      "Epoch 00911: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3578 - accuracy: 0.8374 - val_loss: 0.4276 - val_accuracy: 0.7926\n",
      "Epoch 912/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8330\n",
      "Epoch 00912: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3621 - accuracy: 0.8333 - val_loss: 0.4298 - val_accuracy: 0.7913\n",
      "Epoch 913/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8346\n",
      "Epoch 00913: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3588 - accuracy: 0.8346 - val_loss: 0.4115 - val_accuracy: 0.8007\n",
      "Epoch 914/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8393\n",
      "Epoch 00914: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3583 - accuracy: 0.8391 - val_loss: 0.4522 - val_accuracy: 0.7790\n",
      "Epoch 915/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8356\n",
      "Epoch 00915: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3628 - accuracy: 0.8358 - val_loss: 0.4085 - val_accuracy: 0.8044\n",
      "Epoch 916/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8368\n",
      "Epoch 00916: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3576 - accuracy: 0.8366 - val_loss: 0.4257 - val_accuracy: 0.7932\n",
      "Epoch 917/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8391\n",
      "Epoch 00917: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3548 - accuracy: 0.8393 - val_loss: 0.4150 - val_accuracy: 0.7992\n",
      "Epoch 918/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8343\n",
      "Epoch 00918: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3620 - accuracy: 0.8341 - val_loss: 0.4044 - val_accuracy: 0.8057\n",
      "Epoch 919/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8379\n",
      "Epoch 00919: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3594 - accuracy: 0.8374 - val_loss: 0.4276 - val_accuracy: 0.7919\n",
      "Epoch 920/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8421\n",
      "Epoch 00920: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3598 - accuracy: 0.8421 - val_loss: 0.4208 - val_accuracy: 0.7977\n",
      "Epoch 921/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8376\n",
      "Epoch 00921: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3556 - accuracy: 0.8376 - val_loss: 0.4215 - val_accuracy: 0.7977\n",
      "Epoch 922/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8367\n",
      "Epoch 00922: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3595 - accuracy: 0.8368 - val_loss: 0.4296 - val_accuracy: 0.7898\n",
      "Epoch 923/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8411\n",
      "Epoch 00923: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3534 - accuracy: 0.8408 - val_loss: 0.4316 - val_accuracy: 0.7919\n",
      "Epoch 924/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8360\n",
      "Epoch 00924: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3585 - accuracy: 0.8356 - val_loss: 0.4290 - val_accuracy: 0.7926\n",
      "Epoch 925/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8389\n",
      "Epoch 00925: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3541 - accuracy: 0.8393 - val_loss: 0.4289 - val_accuracy: 0.7936\n",
      "Epoch 926/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.8396\n",
      "Epoch 00926: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3575 - accuracy: 0.8394 - val_loss: 0.4166 - val_accuracy: 0.7999\n",
      "Epoch 927/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3578 - accuracy: 0.8359\n",
      "Epoch 00927: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3585 - accuracy: 0.8357 - val_loss: 0.4278 - val_accuracy: 0.7932\n",
      "Epoch 928/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3584 - accuracy: 0.8367\n",
      "Epoch 00928: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3582 - accuracy: 0.8368 - val_loss: 0.4166 - val_accuracy: 0.7992\n",
      "Epoch 929/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.8357\n",
      "Epoch 00929: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3616 - accuracy: 0.8358 - val_loss: 0.4322 - val_accuracy: 0.7921\n",
      "Epoch 930/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8392\n",
      "Epoch 00930: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3577 - accuracy: 0.8394 - val_loss: 0.4097 - val_accuracy: 0.8035\n",
      "Epoch 931/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.8375\n",
      "Epoch 00931: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3533 - accuracy: 0.8375 - val_loss: 0.4169 - val_accuracy: 0.8016\n",
      "Epoch 932/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8375\n",
      "Epoch 00932: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3560 - accuracy: 0.8375 - val_loss: 0.4299 - val_accuracy: 0.7934\n",
      "Epoch 933/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 00933: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3544 - accuracy: 0.8399 - val_loss: 0.4202 - val_accuracy: 0.8012\n",
      "Epoch 934/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8396\n",
      "Epoch 00934: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3530 - accuracy: 0.8395 - val_loss: 0.4326 - val_accuracy: 0.7919\n",
      "Epoch 935/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8408\n",
      "Epoch 00935: val_loss did not improve from 0.40177\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3523 - accuracy: 0.8407 - val_loss: 0.4040 - val_accuracy: 0.8067\n",
      "Epoch 936/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8363\n",
      "Epoch 00936: val_loss improved from 0.40177 to 0.39513, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3583 - accuracy: 0.8361 - val_loss: 0.3951 - val_accuracy: 0.8138\n",
      "Epoch 937/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8422\n",
      "Epoch 00937: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3552 - accuracy: 0.8421 - val_loss: 0.4247 - val_accuracy: 0.7966\n",
      "Epoch 938/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8407\n",
      "Epoch 00938: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3557 - accuracy: 0.8404 - val_loss: 0.4299 - val_accuracy: 0.7936\n",
      "Epoch 939/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3558 - accuracy: 0.8369\n",
      "Epoch 00939: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3559 - accuracy: 0.8367 - val_loss: 0.4225 - val_accuracy: 0.7992\n",
      "Epoch 940/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.8396\n",
      "Epoch 00940: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3536 - accuracy: 0.8395 - val_loss: 0.4166 - val_accuracy: 0.8014\n",
      "Epoch 941/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8400\n",
      "Epoch 00941: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3541 - accuracy: 0.8398 - val_loss: 0.4581 - val_accuracy: 0.7764\n",
      "Epoch 942/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8410\n",
      "Epoch 00942: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3519 - accuracy: 0.8410 - val_loss: 0.4150 - val_accuracy: 0.8020\n",
      "Epoch 943/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8413\n",
      "Epoch 00943: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3539 - accuracy: 0.8413 - val_loss: 0.4494 - val_accuracy: 0.7810\n",
      "Epoch 944/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8395\n",
      "Epoch 00944: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3536 - accuracy: 0.8398 - val_loss: 0.4444 - val_accuracy: 0.7844\n",
      "Epoch 945/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8385\n",
      "Epoch 00945: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3541 - accuracy: 0.8386 - val_loss: 0.4269 - val_accuracy: 0.7971\n",
      "Epoch 946/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8392\n",
      "Epoch 00946: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3577 - accuracy: 0.8391 - val_loss: 0.4460 - val_accuracy: 0.7837\n",
      "Epoch 947/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8415\n",
      "Epoch 00947: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3506 - accuracy: 0.8413 - val_loss: 0.4055 - val_accuracy: 0.8065\n",
      "Epoch 948/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8400\n",
      "Epoch 00948: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3549 - accuracy: 0.8400 - val_loss: 0.4131 - val_accuracy: 0.8031\n",
      "Epoch 949/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8404\n",
      "Epoch 00949: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3513 - accuracy: 0.8404 - val_loss: 0.4015 - val_accuracy: 0.8100\n",
      "Epoch 950/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8368\n",
      "Epoch 00950: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3551 - accuracy: 0.8370 - val_loss: 0.4396 - val_accuracy: 0.7861\n",
      "Epoch 951/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.8396\n",
      "Epoch 00951: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3492 - accuracy: 0.8399 - val_loss: 0.4326 - val_accuracy: 0.7891\n",
      "Epoch 952/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8421\n",
      "Epoch 00952: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3522 - accuracy: 0.8422 - val_loss: 0.4150 - val_accuracy: 0.8001\n",
      "Epoch 953/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8413\n",
      "Epoch 00953: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3491 - accuracy: 0.8415 - val_loss: 0.4326 - val_accuracy: 0.7902\n",
      "Epoch 954/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8377\n",
      "Epoch 00954: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3526 - accuracy: 0.8372 - val_loss: 0.4198 - val_accuracy: 0.8003\n",
      "Epoch 955/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.8400\n",
      "Epoch 00955: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3527 - accuracy: 0.8402 - val_loss: 0.4391 - val_accuracy: 0.7855\n",
      "Epoch 956/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8411\n",
      "Epoch 00956: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3514 - accuracy: 0.8410 - val_loss: 0.4042 - val_accuracy: 0.8070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 957/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8415\n",
      "Epoch 00957: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3500 - accuracy: 0.8418 - val_loss: 0.4398 - val_accuracy: 0.7870\n",
      "Epoch 958/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.8401\n",
      "Epoch 00958: val_loss did not improve from 0.39513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3520 - accuracy: 0.8397 - val_loss: 0.4163 - val_accuracy: 0.7990\n",
      "Epoch 959/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8435\n",
      "Epoch 00959: val_loss improved from 0.39513 to 0.39496, saving model to pickled_objects/batch_size_256_lr_0.01_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3515 - accuracy: 0.8436 - val_loss: 0.3950 - val_accuracy: 0.8143\n",
      "Epoch 960/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.8395\n",
      "Epoch 00960: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3531 - accuracy: 0.8395 - val_loss: 0.4302 - val_accuracy: 0.7919\n",
      "Epoch 961/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 00961: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3544 - accuracy: 0.8396 - val_loss: 0.4228 - val_accuracy: 0.7966\n",
      "Epoch 962/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8409\n",
      "Epoch 00962: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3515 - accuracy: 0.8407 - val_loss: 0.4334 - val_accuracy: 0.7906\n",
      "Epoch 963/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8412\n",
      "Epoch 00963: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3507 - accuracy: 0.8414 - val_loss: 0.4303 - val_accuracy: 0.7934\n",
      "Epoch 964/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.8413\n",
      "Epoch 00964: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3535 - accuracy: 0.8411 - val_loss: 0.4275 - val_accuracy: 0.7906\n",
      "Epoch 965/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8397\n",
      "Epoch 00965: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3514 - accuracy: 0.8395 - val_loss: 0.4151 - val_accuracy: 0.8018\n",
      "Epoch 966/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8429\n",
      "Epoch 00966: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3466 - accuracy: 0.8429 - val_loss: 0.4126 - val_accuracy: 0.8007\n",
      "Epoch 967/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8426\n",
      "Epoch 00967: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3490 - accuracy: 0.8426 - val_loss: 0.4275 - val_accuracy: 0.7943\n",
      "Epoch 968/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8468\n",
      "Epoch 00968: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3476 - accuracy: 0.8468 - val_loss: 0.4129 - val_accuracy: 0.8027\n",
      "Epoch 969/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8413\n",
      "Epoch 00969: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3487 - accuracy: 0.8412 - val_loss: 0.4182 - val_accuracy: 0.7979\n",
      "Epoch 970/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3517 - accuracy: 0.8404\n",
      "Epoch 00970: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3513 - accuracy: 0.8406 - val_loss: 0.4240 - val_accuracy: 0.7934\n",
      "Epoch 971/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8383\n",
      "Epoch 00971: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3525 - accuracy: 0.8385 - val_loss: 0.4271 - val_accuracy: 0.7934\n",
      "Epoch 972/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3483 - accuracy: 0.8395\n",
      "Epoch 00972: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3482 - accuracy: 0.8397 - val_loss: 0.4276 - val_accuracy: 0.7928\n",
      "Epoch 973/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8433\n",
      "Epoch 00973: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3491 - accuracy: 0.8433 - val_loss: 0.4389 - val_accuracy: 0.7840\n",
      "Epoch 974/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.8491\n",
      "Epoch 00974: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3450 - accuracy: 0.8492 - val_loss: 0.4183 - val_accuracy: 0.7984\n",
      "Epoch 975/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8391\n",
      "Epoch 00975: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3530 - accuracy: 0.8392 - val_loss: 0.4108 - val_accuracy: 0.8014\n",
      "Epoch 976/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8407\n",
      "Epoch 00976: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3496 - accuracy: 0.8406 - val_loss: 0.4092 - val_accuracy: 0.8048\n",
      "Epoch 977/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8388\n",
      "Epoch 00977: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3504 - accuracy: 0.8385 - val_loss: 0.3997 - val_accuracy: 0.8108\n",
      "Epoch 978/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8441\n",
      "Epoch 00978: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3489 - accuracy: 0.8438 - val_loss: 0.4115 - val_accuracy: 0.8033\n",
      "Epoch 979/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8424\n",
      "Epoch 00979: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3493 - accuracy: 0.8423 - val_loss: 0.4212 - val_accuracy: 0.7979\n",
      "Epoch 980/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3463 - accuracy: 0.8456\n",
      "Epoch 00980: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3470 - accuracy: 0.8455 - val_loss: 0.4064 - val_accuracy: 0.8046\n",
      "Epoch 981/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8416\n",
      "Epoch 00981: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3487 - accuracy: 0.8416 - val_loss: 0.4126 - val_accuracy: 0.8029\n",
      "Epoch 982/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8424\n",
      "Epoch 00982: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3528 - accuracy: 0.8423 - val_loss: 0.4126 - val_accuracy: 0.8050\n",
      "Epoch 983/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3469 - accuracy: 0.8425\n",
      "Epoch 00983: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3471 - accuracy: 0.8425 - val_loss: 0.4147 - val_accuracy: 0.8018\n",
      "Epoch 984/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8428\n",
      "Epoch 00984: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3489 - accuracy: 0.8426 - val_loss: 0.4125 - val_accuracy: 0.8020\n",
      "Epoch 985/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8415\n",
      "Epoch 00985: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3490 - accuracy: 0.8410 - val_loss: 0.4457 - val_accuracy: 0.7807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8458\n",
      "Epoch 00986: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3444 - accuracy: 0.8458 - val_loss: 0.4142 - val_accuracy: 0.8022\n",
      "Epoch 987/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8414\n",
      "Epoch 00987: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3496 - accuracy: 0.8414 - val_loss: 0.3976 - val_accuracy: 0.8110\n",
      "Epoch 988/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8423\n",
      "Epoch 00988: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3461 - accuracy: 0.8423 - val_loss: 0.4201 - val_accuracy: 0.7951\n",
      "Epoch 989/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.8463\n",
      "Epoch 00989: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3416 - accuracy: 0.8467 - val_loss: 0.4140 - val_accuracy: 0.8025\n",
      "Epoch 990/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8431\n",
      "Epoch 00990: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3466 - accuracy: 0.8437 - val_loss: 0.4397 - val_accuracy: 0.7857\n",
      "Epoch 991/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3477 - accuracy: 0.8447\n",
      "Epoch 00991: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3475 - accuracy: 0.8448 - val_loss: 0.4241 - val_accuracy: 0.7947\n",
      "Epoch 992/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.8425\n",
      "Epoch 00992: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3475 - accuracy: 0.8425 - val_loss: 0.4117 - val_accuracy: 0.8022\n",
      "Epoch 993/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8456\n",
      "Epoch 00993: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3408 - accuracy: 0.8454 - val_loss: 0.4051 - val_accuracy: 0.8065\n",
      "Epoch 994/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8453\n",
      "Epoch 00994: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3445 - accuracy: 0.8451 - val_loss: 0.4017 - val_accuracy: 0.8098\n",
      "Epoch 995/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8416\n",
      "Epoch 00995: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3469 - accuracy: 0.8419 - val_loss: 0.4368 - val_accuracy: 0.7900\n",
      "Epoch 996/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8447\n",
      "Epoch 00996: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3474 - accuracy: 0.8447 - val_loss: 0.4266 - val_accuracy: 0.7915\n",
      "Epoch 997/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8433\n",
      "Epoch 00997: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3455 - accuracy: 0.8430 - val_loss: 0.4187 - val_accuracy: 0.7975\n",
      "Epoch 998/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8442\n",
      "Epoch 00998: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3440 - accuracy: 0.8441 - val_loss: 0.4175 - val_accuracy: 0.7997\n",
      "Epoch 999/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.8493\n",
      "Epoch 00999: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3397 - accuracy: 0.8493 - val_loss: 0.4285 - val_accuracy: 0.7917\n",
      "Epoch 1000/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.8494\n",
      "Epoch 01000: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3417 - accuracy: 0.8492 - val_loss: 0.4079 - val_accuracy: 0.8042\n",
      "Epoch 1001/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8473\n",
      "Epoch 01001: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3424 - accuracy: 0.8473 - val_loss: 0.4180 - val_accuracy: 0.7990\n",
      "Epoch 1002/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8429\n",
      "Epoch 01002: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3449 - accuracy: 0.8428 - val_loss: 0.4148 - val_accuracy: 0.7986\n",
      "Epoch 1003/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8445\n",
      "Epoch 01003: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3426 - accuracy: 0.8449 - val_loss: 0.4228 - val_accuracy: 0.7956\n",
      "Epoch 1004/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8449\n",
      "Epoch 01004: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3420 - accuracy: 0.8451 - val_loss: 0.4235 - val_accuracy: 0.7964\n",
      "Epoch 1005/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8417\n",
      "Epoch 01005: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3477 - accuracy: 0.8417 - val_loss: 0.4518 - val_accuracy: 0.7792\n",
      "Epoch 1006/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.8443\n",
      "Epoch 01006: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3412 - accuracy: 0.8444 - val_loss: 0.4261 - val_accuracy: 0.7943\n",
      "Epoch 1007/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8462\n",
      "Epoch 01007: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3419 - accuracy: 0.8463 - val_loss: 0.4178 - val_accuracy: 0.7977\n",
      "Epoch 1008/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8479\n",
      "Epoch 01008: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3398 - accuracy: 0.8481 - val_loss: 0.4212 - val_accuracy: 0.7969\n",
      "Epoch 1009/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8429\n",
      "Epoch 01009: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3418 - accuracy: 0.8433 - val_loss: 0.4247 - val_accuracy: 0.7947\n",
      "Epoch 1010/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8451\n",
      "Epoch 01010: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3424 - accuracy: 0.8446 - val_loss: 0.4276 - val_accuracy: 0.7962\n",
      "Epoch 1011/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8429\n",
      "Epoch 01011: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3417 - accuracy: 0.8431 - val_loss: 0.4196 - val_accuracy: 0.7977\n",
      "Epoch 1012/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8430\n",
      "Epoch 01012: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3447 - accuracy: 0.8429 - val_loss: 0.4305 - val_accuracy: 0.7945\n",
      "Epoch 1013/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8480\n",
      "Epoch 01013: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3419 - accuracy: 0.8478 - val_loss: 0.4240 - val_accuracy: 0.7962\n",
      "Epoch 1014/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8454\n",
      "Epoch 01014: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3440 - accuracy: 0.8450 - val_loss: 0.4021 - val_accuracy: 0.8117\n",
      "Epoch 1015/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8465\n",
      "Epoch 01015: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3425 - accuracy: 0.8466 - val_loss: 0.4274 - val_accuracy: 0.7932\n",
      "Epoch 1016/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3436 - accuracy: 0.8440\n",
      "Epoch 01016: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3429 - accuracy: 0.8444 - val_loss: 0.4301 - val_accuracy: 0.7930\n",
      "Epoch 1017/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8493\n",
      "Epoch 01017: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3378 - accuracy: 0.8492 - val_loss: 0.4182 - val_accuracy: 0.8005\n",
      "Epoch 1018/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8504\n",
      "Epoch 01018: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3366 - accuracy: 0.8506 - val_loss: 0.4263 - val_accuracy: 0.7923\n",
      "Epoch 1019/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.8476\n",
      "Epoch 01019: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3394 - accuracy: 0.8477 - val_loss: 0.4353 - val_accuracy: 0.7900\n",
      "Epoch 1020/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8521\n",
      "Epoch 01020: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3335 - accuracy: 0.8523 - val_loss: 0.4292 - val_accuracy: 0.7932\n",
      "Epoch 1021/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8453\n",
      "Epoch 01021: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3407 - accuracy: 0.8452 - val_loss: 0.4209 - val_accuracy: 0.7971\n",
      "Epoch 1022/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8441\n",
      "Epoch 01022: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3409 - accuracy: 0.8440 - val_loss: 0.4081 - val_accuracy: 0.8061\n",
      "Epoch 1023/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8506\n",
      "Epoch 01023: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3372 - accuracy: 0.8505 - val_loss: 0.4469 - val_accuracy: 0.7865\n",
      "Epoch 1024/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8466\n",
      "Epoch 01024: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3420 - accuracy: 0.8469 - val_loss: 0.4317 - val_accuracy: 0.7904\n",
      "Epoch 1025/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8439\n",
      "Epoch 01025: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3435 - accuracy: 0.8439 - val_loss: 0.4227 - val_accuracy: 0.7945\n",
      "Epoch 1026/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8458\n",
      "Epoch 01026: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3414 - accuracy: 0.8455 - val_loss: 0.4232 - val_accuracy: 0.7960\n",
      "Epoch 1027/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8449\n",
      "Epoch 01027: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3399 - accuracy: 0.8451 - val_loss: 0.4286 - val_accuracy: 0.7930\n",
      "Epoch 1028/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8465\n",
      "Epoch 01028: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3420 - accuracy: 0.8466 - val_loss: 0.4271 - val_accuracy: 0.7958\n",
      "Epoch 1029/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8451\n",
      "Epoch 01029: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3408 - accuracy: 0.8455 - val_loss: 0.4353 - val_accuracy: 0.7887\n",
      "Epoch 1030/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8473\n",
      "Epoch 01030: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3398 - accuracy: 0.8472 - val_loss: 0.4370 - val_accuracy: 0.7865\n",
      "Epoch 1031/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8500\n",
      "Epoch 01031: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3355 - accuracy: 0.8498 - val_loss: 0.4073 - val_accuracy: 0.8046\n",
      "Epoch 1032/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8488\n",
      "Epoch 01032: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3350 - accuracy: 0.8487 - val_loss: 0.4112 - val_accuracy: 0.8009\n",
      "Epoch 1033/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8463\n",
      "Epoch 01033: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3408 - accuracy: 0.8458 - val_loss: 0.4204 - val_accuracy: 0.7977\n",
      "Epoch 1034/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.8485\n",
      "Epoch 01034: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3362 - accuracy: 0.8485 - val_loss: 0.4183 - val_accuracy: 0.7960\n",
      "Epoch 1035/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8505\n",
      "Epoch 01035: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3342 - accuracy: 0.8507 - val_loss: 0.4078 - val_accuracy: 0.8037\n",
      "Epoch 1036/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.8486\n",
      "Epoch 01036: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3347 - accuracy: 0.8484 - val_loss: 0.4125 - val_accuracy: 0.8009\n",
      "Epoch 1037/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3384 - accuracy: 0.8479\n",
      "Epoch 01037: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3380 - accuracy: 0.8481 - val_loss: 0.4193 - val_accuracy: 0.7977\n",
      "Epoch 1038/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8494\n",
      "Epoch 01038: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3383 - accuracy: 0.8495 - val_loss: 0.4179 - val_accuracy: 0.7999\n",
      "Epoch 1039/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8471\n",
      "Epoch 01039: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3377 - accuracy: 0.8469 - val_loss: 0.4160 - val_accuracy: 0.7971\n",
      "Epoch 1040/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8548\n",
      "Epoch 01040: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3309 - accuracy: 0.8548 - val_loss: 0.4287 - val_accuracy: 0.7921\n",
      "Epoch 1041/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8522\n",
      "Epoch 01041: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3337 - accuracy: 0.8521 - val_loss: 0.4133 - val_accuracy: 0.7990\n",
      "Epoch 1042/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8472\n",
      "Epoch 01042: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3399 - accuracy: 0.8471 - val_loss: 0.4216 - val_accuracy: 0.7947\n",
      "Epoch 1043/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.8500\n",
      "Epoch 01043: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3334 - accuracy: 0.8499 - val_loss: 0.4133 - val_accuracy: 0.8018\n",
      "Epoch 1044/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8519\n",
      "Epoch 01044: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3329 - accuracy: 0.8521 - val_loss: 0.4299 - val_accuracy: 0.7906\n",
      "Epoch 1045/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.8510\n",
      "Epoch 01045: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3350 - accuracy: 0.8509 - val_loss: 0.4313 - val_accuracy: 0.7908\n",
      "Epoch 1046/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.8508\n",
      "Epoch 01046: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3344 - accuracy: 0.8509 - val_loss: 0.4158 - val_accuracy: 0.8020\n",
      "Epoch 1047/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3348 - accuracy: 0.8514\n",
      "Epoch 01047: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3347 - accuracy: 0.8513 - val_loss: 0.4271 - val_accuracy: 0.7943\n",
      "Epoch 1048/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8500\n",
      "Epoch 01048: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3325 - accuracy: 0.8499 - val_loss: 0.4153 - val_accuracy: 0.7992\n",
      "Epoch 1049/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8505\n",
      "Epoch 01049: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3375 - accuracy: 0.8505 - val_loss: 0.4029 - val_accuracy: 0.8072\n",
      "Epoch 1050/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8510\n",
      "Epoch 01050: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3355 - accuracy: 0.8513 - val_loss: 0.4253 - val_accuracy: 0.7947\n",
      "Epoch 1051/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8495\n",
      "Epoch 01051: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3366 - accuracy: 0.8494 - val_loss: 0.4220 - val_accuracy: 0.7932\n",
      "Epoch 1052/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.8497\n",
      "Epoch 01052: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3322 - accuracy: 0.8498 - val_loss: 0.4105 - val_accuracy: 0.8059\n",
      "Epoch 1053/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8498\n",
      "Epoch 01053: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3320 - accuracy: 0.8499 - val_loss: 0.4243 - val_accuracy: 0.7947\n",
      "Epoch 1054/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8512\n",
      "Epoch 01054: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3301 - accuracy: 0.8511 - val_loss: 0.4096 - val_accuracy: 0.8027\n",
      "Epoch 1055/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3348 - accuracy: 0.8502\n",
      "Epoch 01055: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3347 - accuracy: 0.8501 - val_loss: 0.4168 - val_accuracy: 0.7986\n",
      "Epoch 1056/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3329 - accuracy: 0.8513\n",
      "Epoch 01056: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3324 - accuracy: 0.8514 - val_loss: 0.4328 - val_accuracy: 0.7932\n",
      "Epoch 1057/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8496\n",
      "Epoch 01057: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3338 - accuracy: 0.8493 - val_loss: 0.4052 - val_accuracy: 0.8078\n",
      "Epoch 1058/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8514\n",
      "Epoch 01058: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3328 - accuracy: 0.8515 - val_loss: 0.4220 - val_accuracy: 0.7984\n",
      "Epoch 1059/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8473\n",
      "Epoch 01059: val_loss did not improve from 0.39496\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3345 - accuracy: 0.8472 - val_loss: 0.4165 - val_accuracy: 0.7971\n",
      "Epoch 01059: early stopping\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 8s 106ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "Epoch 00001: val_loss improved from inf to 0.69310, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 0.6936 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5151\n",
      "Epoch 00002: val_loss improved from 0.69310 to 0.69276, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6928 - val_accuracy: 0.4951\n",
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5193\n",
      "Epoch 00003: val_loss improved from 0.69276 to 0.69275, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6920 - accuracy: 0.5196 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.5432\n",
      "Epoch 00004: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6906 - accuracy: 0.5430 - val_loss: 0.6928 - val_accuracy: 0.4914\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5567\n",
      "Epoch 00005: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6895 - accuracy: 0.5562 - val_loss: 0.6928 - val_accuracy: 0.4912\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.5695\n",
      "Epoch 00006: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6876 - accuracy: 0.5693 - val_loss: 0.6928 - val_accuracy: 0.4923\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5744\n",
      "Epoch 00007: val_loss did not improve from 0.69275\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6852 - accuracy: 0.5747 - val_loss: 0.6929 - val_accuracy: 0.4931\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5803\n",
      "Epoch 00008: val_loss improved from 0.69275 to 0.69264, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6834 - accuracy: 0.5805 - val_loss: 0.6926 - val_accuracy: 0.4946\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5832\n",
      "Epoch 00009: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6809 - accuracy: 0.5828 - val_loss: 0.6935 - val_accuracy: 0.4953\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5864\n",
      "Epoch 00010: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6780 - accuracy: 0.5865 - val_loss: 0.6939 - val_accuracy: 0.4981\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5869\n",
      "Epoch 00011: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6761 - accuracy: 0.5870 - val_loss: 0.6958 - val_accuracy: 0.4994\n",
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6729 - accuracy: 0.5921\n",
      "Epoch 00012: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6727 - accuracy: 0.5920 - val_loss: 0.6999 - val_accuracy: 0.4991\n",
      "Epoch 13/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.6718 - accuracy: 0.5926\n",
      "Epoch 00013: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6717 - accuracy: 0.5930 - val_loss: 0.6992 - val_accuracy: 0.5034\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6686 - accuracy: 0.5965\n",
      "Epoch 00014: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6685 - accuracy: 0.5967 - val_loss: 0.7036 - val_accuracy: 0.5037\n",
      "Epoch 15/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6672 - accuracy: 0.5995\n",
      "Epoch 00015: val_loss did not improve from 0.69264\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6670 - accuracy: 0.6000 - val_loss: 0.6963 - val_accuracy: 0.5157\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6642 - accuracy: 0.6045\n",
      "Epoch 00016: val_loss improved from 0.69264 to 0.69188, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6639 - accuracy: 0.6049 - val_loss: 0.6919 - val_accuracy: 0.5277\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.6030\n",
      "Epoch 00017: val_loss did not improve from 0.69188\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6615 - accuracy: 0.6031 - val_loss: 0.7072 - val_accuracy: 0.5107\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.5980\n",
      "Epoch 00018: val_loss improved from 0.69188 to 0.68787, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6632 - accuracy: 0.5984 - val_loss: 0.6879 - val_accuracy: 0.5406\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6601 - accuracy: 0.6050\n",
      "Epoch 00019: val_loss did not improve from 0.68787\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6597 - accuracy: 0.6056 - val_loss: 0.6881 - val_accuracy: 0.5432\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6592 - accuracy: 0.6059\n",
      "Epoch 00020: val_loss improved from 0.68787 to 0.68777, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6590 - accuracy: 0.6061 - val_loss: 0.6878 - val_accuracy: 0.5466\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6588 - accuracy: 0.6047\n",
      "Epoch 00021: val_loss improved from 0.68777 to 0.68541, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6587 - accuracy: 0.6051 - val_loss: 0.6854 - val_accuracy: 0.5497\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6562 - accuracy: 0.6080\n",
      "Epoch 00022: val_loss improved from 0.68541 to 0.67720, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6560 - accuracy: 0.6084 - val_loss: 0.6772 - val_accuracy: 0.5692\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6558 - accuracy: 0.6082\n",
      "Epoch 00023: val_loss improved from 0.67720 to 0.67496, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6556 - accuracy: 0.6088 - val_loss: 0.6750 - val_accuracy: 0.5739\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6546 - accuracy: 0.6094\n",
      "Epoch 00024: val_loss improved from 0.67496 to 0.67310, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6544 - accuracy: 0.6102 - val_loss: 0.6731 - val_accuracy: 0.5772\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6536 - accuracy: 0.6118\n",
      "Epoch 00025: val_loss did not improve from 0.67310\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6534 - accuracy: 0.6123 - val_loss: 0.6732 - val_accuracy: 0.5757\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6521 - accuracy: 0.6156\n",
      "Epoch 00026: val_loss improved from 0.67310 to 0.66970, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6517 - accuracy: 0.6161 - val_loss: 0.6697 - val_accuracy: 0.5808\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6499 - accuracy: 0.6125\n",
      "Epoch 00027: val_loss improved from 0.66970 to 0.66447, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6496 - accuracy: 0.6133 - val_loss: 0.6645 - val_accuracy: 0.5896\n",
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6135\n",
      "Epoch 00028: val_loss improved from 0.66447 to 0.66290, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6491 - accuracy: 0.6143 - val_loss: 0.6629 - val_accuracy: 0.5937\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6474 - accuracy: 0.6170\n",
      "Epoch 00029: val_loss did not improve from 0.66290\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6472 - accuracy: 0.6176 - val_loss: 0.6639 - val_accuracy: 0.5901\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6473 - accuracy: 0.6167\n",
      "Epoch 00030: val_loss improved from 0.66290 to 0.66075, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6468 - accuracy: 0.6175 - val_loss: 0.6608 - val_accuracy: 0.5976\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.6201\n",
      "Epoch 00031: val_loss did not improve from 0.66075\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6461 - accuracy: 0.6204 - val_loss: 0.6648 - val_accuracy: 0.5875\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6444 - accuracy: 0.6205\n",
      "Epoch 00032: val_loss improved from 0.66075 to 0.65843, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6442 - accuracy: 0.6206 - val_loss: 0.6584 - val_accuracy: 0.5982\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.6211\n",
      "Epoch 00033: val_loss improved from 0.65843 to 0.65058, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6443 - accuracy: 0.6216 - val_loss: 0.6506 - val_accuracy: 0.6135\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.6203\n",
      "Epoch 00034: val_loss improved from 0.65058 to 0.64900, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6432 - accuracy: 0.6207 - val_loss: 0.6490 - val_accuracy: 0.6139\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6420 - accuracy: 0.6263\n",
      "Epoch 00035: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6417 - accuracy: 0.6264 - val_loss: 0.6502 - val_accuracy: 0.6120\n",
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6411 - accuracy: 0.6246\n",
      "Epoch 00036: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6410 - accuracy: 0.6249 - val_loss: 0.6508 - val_accuracy: 0.6124\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.6228\n",
      "Epoch 00037: val_loss did not improve from 0.64900\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6401 - accuracy: 0.6229 - val_loss: 0.6505 - val_accuracy: 0.6098\n",
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.6283\n",
      "Epoch 00038: val_loss improved from 0.64900 to 0.63857, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6383 - accuracy: 0.6284 - val_loss: 0.6386 - val_accuracy: 0.6264\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.6312\n",
      "Epoch 00039: val_loss did not improve from 0.63857\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6371 - accuracy: 0.6314 - val_loss: 0.6449 - val_accuracy: 0.6176\n",
      "Epoch 40/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.6321\n",
      "Epoch 00040: val_loss did not improve from 0.63857\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6357 - accuracy: 0.6322 - val_loss: 0.6422 - val_accuracy: 0.6195\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6350 - accuracy: 0.6316\n",
      "Epoch 00041: val_loss improved from 0.63857 to 0.63716, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6349 - accuracy: 0.6318 - val_loss: 0.6372 - val_accuracy: 0.6262\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6329 - accuracy: 0.6382\n",
      "Epoch 00042: val_loss did not improve from 0.63716\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6327 - accuracy: 0.6384 - val_loss: 0.6390 - val_accuracy: 0.6227\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6324 - accuracy: 0.6319\n",
      "Epoch 00043: val_loss improved from 0.63716 to 0.63604, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6323 - accuracy: 0.6319 - val_loss: 0.6360 - val_accuracy: 0.6268\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6309 - accuracy: 0.6365\n",
      "Epoch 00044: val_loss did not improve from 0.63604\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6311 - accuracy: 0.6363 - val_loss: 0.6413 - val_accuracy: 0.6202\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6282 - accuracy: 0.6397\n",
      "Epoch 00045: val_loss improved from 0.63604 to 0.62894, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6282 - accuracy: 0.6397 - val_loss: 0.6289 - val_accuracy: 0.6391\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.6404\n",
      "Epoch 00046: val_loss did not improve from 0.62894\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6289 - accuracy: 0.6402 - val_loss: 0.6301 - val_accuracy: 0.6354\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6275 - accuracy: 0.6404\n",
      "Epoch 00047: val_loss did not improve from 0.62894\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6275 - accuracy: 0.6403 - val_loss: 0.6313 - val_accuracy: 0.6320\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.6486\n",
      "Epoch 00048: val_loss improved from 0.62894 to 0.62723, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6247 - accuracy: 0.6485 - val_loss: 0.6272 - val_accuracy: 0.6346\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6237 - accuracy: 0.6451\n",
      "Epoch 00049: val_loss did not improve from 0.62723\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6239 - accuracy: 0.6451 - val_loss: 0.6289 - val_accuracy: 0.6346\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.6450\n",
      "Epoch 00050: val_loss improved from 0.62723 to 0.62277, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6229 - accuracy: 0.6451 - val_loss: 0.6228 - val_accuracy: 0.6395\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6211 - accuracy: 0.6460\n",
      "Epoch 00051: val_loss did not improve from 0.62277\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6213 - accuracy: 0.6458 - val_loss: 0.6283 - val_accuracy: 0.6354\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.6513\n",
      "Epoch 00052: val_loss improved from 0.62277 to 0.61855, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6183 - accuracy: 0.6514 - val_loss: 0.6186 - val_accuracy: 0.6438\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6182 - accuracy: 0.6523\n",
      "Epoch 00053: val_loss did not improve from 0.61855\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6183 - accuracy: 0.6522 - val_loss: 0.6206 - val_accuracy: 0.6417\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.6564\n",
      "Epoch 00054: val_loss did not improve from 0.61855\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6140 - accuracy: 0.6560 - val_loss: 0.6257 - val_accuracy: 0.6376\n",
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.6608\n",
      "Epoch 00055: val_loss improved from 0.61855 to 0.61534, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6119 - accuracy: 0.6606 - val_loss: 0.6153 - val_accuracy: 0.6455\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6089 - accuracy: 0.6610\n",
      "Epoch 00056: val_loss did not improve from 0.61534\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6093 - accuracy: 0.6605 - val_loss: 0.6218 - val_accuracy: 0.6408\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.6654\n",
      "Epoch 00057: val_loss improved from 0.61534 to 0.61423, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6063 - accuracy: 0.6649 - val_loss: 0.6142 - val_accuracy: 0.6492\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.6659\n",
      "Epoch 00058: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6046 - accuracy: 0.6654 - val_loss: 0.6165 - val_accuracy: 0.6429\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.6698\n",
      "Epoch 00059: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5999 - accuracy: 0.6694 - val_loss: 0.6201 - val_accuracy: 0.6406\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5961 - accuracy: 0.6744\n",
      "Epoch 00060: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5966 - accuracy: 0.6738 - val_loss: 0.6181 - val_accuracy: 0.6436\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.6757\n",
      "Epoch 00061: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5950 - accuracy: 0.6752 - val_loss: 0.6188 - val_accuracy: 0.6434\n",
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.6797\n",
      "Epoch 00062: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5924 - accuracy: 0.6791 - val_loss: 0.6256 - val_accuracy: 0.6341\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5869 - accuracy: 0.6858\n",
      "Epoch 00063: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5874 - accuracy: 0.6850 - val_loss: 0.6273 - val_accuracy: 0.6318\n",
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.6852\n",
      "Epoch 00064: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5847 - accuracy: 0.6847 - val_loss: 0.6244 - val_accuracy: 0.6348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.6873\n",
      "Epoch 00065: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5819 - accuracy: 0.6863 - val_loss: 0.6206 - val_accuracy: 0.6361\n",
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.6898\n",
      "Epoch 00066: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5803 - accuracy: 0.6895 - val_loss: 0.6283 - val_accuracy: 0.6305\n",
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.6937\n",
      "Epoch 00067: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5778 - accuracy: 0.6931 - val_loss: 0.6296 - val_accuracy: 0.6279\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.6969\n",
      "Epoch 00068: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5741 - accuracy: 0.6964 - val_loss: 0.6387 - val_accuracy: 0.6249\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.6949\n",
      "Epoch 00069: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5745 - accuracy: 0.6941 - val_loss: 0.6308 - val_accuracy: 0.6303\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5716 - accuracy: 0.6934\n",
      "Epoch 00070: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5723 - accuracy: 0.6927 - val_loss: 0.6326 - val_accuracy: 0.6292\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7043\n",
      "Epoch 00071: val_loss did not improve from 0.61423\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5678 - accuracy: 0.7038 - val_loss: 0.6282 - val_accuracy: 0.6311\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7055\n",
      "Epoch 00072: val_loss improved from 0.61423 to 0.61340, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5634 - accuracy: 0.7048 - val_loss: 0.6134 - val_accuracy: 0.6457\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7050\n",
      "Epoch 00073: val_loss improved from 0.61340 to 0.59906, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5643 - accuracy: 0.7046 - val_loss: 0.5991 - val_accuracy: 0.6552\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7052\n",
      "Epoch 00074: val_loss did not improve from 0.59906\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5612 - accuracy: 0.7047 - val_loss: 0.6282 - val_accuracy: 0.6292\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7078\n",
      "Epoch 00075: val_loss improved from 0.59906 to 0.59271, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5627 - accuracy: 0.7074 - val_loss: 0.5927 - val_accuracy: 0.6608\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7079\n",
      "Epoch 00076: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5614 - accuracy: 0.7073 - val_loss: 0.6067 - val_accuracy: 0.6528\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5586 - accuracy: 0.7083\n",
      "Epoch 00077: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5592 - accuracy: 0.7080 - val_loss: 0.6116 - val_accuracy: 0.6477\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5580 - accuracy: 0.7063\n",
      "Epoch 00078: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5587 - accuracy: 0.7054 - val_loss: 0.5977 - val_accuracy: 0.6604\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7167\n",
      "Epoch 00079: val_loss did not improve from 0.59271\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5529 - accuracy: 0.7165 - val_loss: 0.6174 - val_accuracy: 0.6507\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7161\n",
      "Epoch 00080: val_loss improved from 0.59271 to 0.57793, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5521 - accuracy: 0.7154 - val_loss: 0.5779 - val_accuracy: 0.6733\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7134\n",
      "Epoch 00081: val_loss did not improve from 0.57793\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5521 - accuracy: 0.7130 - val_loss: 0.5883 - val_accuracy: 0.6625\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7197\n",
      "Epoch 00082: val_loss improved from 0.57793 to 0.56616, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5465 - accuracy: 0.7193 - val_loss: 0.5662 - val_accuracy: 0.6838\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7199\n",
      "Epoch 00083: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5455 - accuracy: 0.7192 - val_loss: 0.5971 - val_accuracy: 0.6634\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7215\n",
      "Epoch 00084: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5469 - accuracy: 0.7211 - val_loss: 0.5812 - val_accuracy: 0.6687\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7223\n",
      "Epoch 00085: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5444 - accuracy: 0.7217 - val_loss: 0.5770 - val_accuracy: 0.6786\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5422 - accuracy: 0.7202\n",
      "Epoch 00086: val_loss did not improve from 0.56616\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5430 - accuracy: 0.7193 - val_loss: 0.5938 - val_accuracy: 0.6636\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7252\n",
      "Epoch 00087: val_loss improved from 0.56616 to 0.56482, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5396 - accuracy: 0.7245 - val_loss: 0.5648 - val_accuracy: 0.6862\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5370 - accuracy: 0.7288\n",
      "Epoch 00088: val_loss improved from 0.56482 to 0.56343, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5377 - accuracy: 0.7285 - val_loss: 0.5634 - val_accuracy: 0.6885\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7255\n",
      "Epoch 00089: val_loss did not improve from 0.56343\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5391 - accuracy: 0.7248 - val_loss: 0.5691 - val_accuracy: 0.6791\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7273\n",
      "Epoch 00090: val_loss improved from 0.56343 to 0.55753, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5370 - accuracy: 0.7270 - val_loss: 0.5575 - val_accuracy: 0.6999\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7263\n",
      "Epoch 00091: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5383 - accuracy: 0.7257 - val_loss: 0.5752 - val_accuracy: 0.6847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7296\n",
      "Epoch 00092: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5352 - accuracy: 0.7291 - val_loss: 0.5666 - val_accuracy: 0.6853\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5317 - accuracy: 0.7310\n",
      "Epoch 00093: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5324 - accuracy: 0.7304 - val_loss: 0.5599 - val_accuracy: 0.6928\n",
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7333\n",
      "Epoch 00094: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5311 - accuracy: 0.7329 - val_loss: 0.5588 - val_accuracy: 0.6928\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5314 - accuracy: 0.7296\n",
      "Epoch 00095: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5321 - accuracy: 0.7292 - val_loss: 0.5617 - val_accuracy: 0.6933\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5312 - accuracy: 0.7309\n",
      "Epoch 00096: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5318 - accuracy: 0.7303 - val_loss: 0.5827 - val_accuracy: 0.6791\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7341\n",
      "Epoch 00097: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5285 - accuracy: 0.7337 - val_loss: 0.5577 - val_accuracy: 0.6978\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7337\n",
      "Epoch 00098: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5268 - accuracy: 0.7336 - val_loss: 0.5578 - val_accuracy: 0.6922\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7348\n",
      "Epoch 00099: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5297 - accuracy: 0.7343 - val_loss: 0.5667 - val_accuracy: 0.6868\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7333\n",
      "Epoch 00100: val_loss did not improve from 0.55753\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5286 - accuracy: 0.7328 - val_loss: 0.5579 - val_accuracy: 0.6935\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5182 - accuracy: 0.7403\n",
      "Epoch 00101: val_loss improved from 0.55753 to 0.55664, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5184 - accuracy: 0.7402 - val_loss: 0.5566 - val_accuracy: 0.7016\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5214 - accuracy: 0.7397\n",
      "Epoch 00102: val_loss improved from 0.55664 to 0.54574, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5225 - accuracy: 0.7390 - val_loss: 0.5457 - val_accuracy: 0.7061\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7356\n",
      "Epoch 00103: val_loss did not improve from 0.54574\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5256 - accuracy: 0.7352 - val_loss: 0.5561 - val_accuracy: 0.6930\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5210 - accuracy: 0.7389\n",
      "Epoch 00104: val_loss did not improve from 0.54574\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5216 - accuracy: 0.7384 - val_loss: 0.5506 - val_accuracy: 0.7012\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7391\n",
      "Epoch 00105: val_loss improved from 0.54574 to 0.52690, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5199 - accuracy: 0.7387 - val_loss: 0.5269 - val_accuracy: 0.7268\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7402\n",
      "Epoch 00106: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5208 - accuracy: 0.7397 - val_loss: 0.5566 - val_accuracy: 0.6999\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.7422\n",
      "Epoch 00107: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5195 - accuracy: 0.7420 - val_loss: 0.5721 - val_accuracy: 0.6924\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7434\n",
      "Epoch 00108: val_loss did not improve from 0.52690\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5179 - accuracy: 0.7429 - val_loss: 0.5318 - val_accuracy: 0.7184\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5178 - accuracy: 0.7413\n",
      "Epoch 00109: val_loss improved from 0.52690 to 0.51703, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5185 - accuracy: 0.7407 - val_loss: 0.5170 - val_accuracy: 0.7339\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5138 - accuracy: 0.7457\n",
      "Epoch 00110: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5145 - accuracy: 0.7450 - val_loss: 0.5182 - val_accuracy: 0.7343\n",
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5170 - accuracy: 0.7409\n",
      "Epoch 00111: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5178 - accuracy: 0.7403 - val_loss: 0.5421 - val_accuracy: 0.7077\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7461\n",
      "Epoch 00112: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5141 - accuracy: 0.7455 - val_loss: 0.5176 - val_accuracy: 0.7350\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7466\n",
      "Epoch 00113: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5128 - accuracy: 0.7460 - val_loss: 0.5734 - val_accuracy: 0.6885\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5126 - accuracy: 0.7453\n",
      "Epoch 00114: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5135 - accuracy: 0.7447 - val_loss: 0.5436 - val_accuracy: 0.7102\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7472\n",
      "Epoch 00115: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5121 - accuracy: 0.7473 - val_loss: 0.5941 - val_accuracy: 0.6754\n",
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7493\n",
      "Epoch 00116: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5101 - accuracy: 0.7493 - val_loss: 0.5421 - val_accuracy: 0.7096\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5114 - accuracy: 0.7463\n",
      "Epoch 00117: val_loss did not improve from 0.51703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5124 - accuracy: 0.7454 - val_loss: 0.5253 - val_accuracy: 0.7225\n",
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5068 - accuracy: 0.7491\n",
      "Epoch 00118: val_loss improved from 0.51703 to 0.51377, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5071 - accuracy: 0.7486 - val_loss: 0.5138 - val_accuracy: 0.7388\n",
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.7471\n",
      "Epoch 00119: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5089 - accuracy: 0.7463 - val_loss: 0.5286 - val_accuracy: 0.7216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7486\n",
      "Epoch 00120: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5044 - accuracy: 0.7486 - val_loss: 0.5705 - val_accuracy: 0.6917\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7511\n",
      "Epoch 00121: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5069 - accuracy: 0.7506 - val_loss: 0.5445 - val_accuracy: 0.7100\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.7516\n",
      "Epoch 00122: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5040 - accuracy: 0.7516 - val_loss: 0.5793 - val_accuracy: 0.6847\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7515\n",
      "Epoch 00123: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5050 - accuracy: 0.7509 - val_loss: 0.5152 - val_accuracy: 0.7326\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7507\n",
      "Epoch 00124: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5047 - accuracy: 0.7503 - val_loss: 0.5421 - val_accuracy: 0.7113\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4995 - accuracy: 0.7557\n",
      "Epoch 00125: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5000 - accuracy: 0.7552 - val_loss: 0.5207 - val_accuracy: 0.7272\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5017 - accuracy: 0.7540\n",
      "Epoch 00126: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5024 - accuracy: 0.7536 - val_loss: 0.5239 - val_accuracy: 0.7261\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5018 - accuracy: 0.7546\n",
      "Epoch 00127: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5022 - accuracy: 0.7542 - val_loss: 0.5356 - val_accuracy: 0.7173\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7563\n",
      "Epoch 00128: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5008 - accuracy: 0.7564 - val_loss: 0.5420 - val_accuracy: 0.7152\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.7564\n",
      "Epoch 00129: val_loss did not improve from 0.51377\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4994 - accuracy: 0.7560 - val_loss: 0.5300 - val_accuracy: 0.7229\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.7510\n",
      "Epoch 00130: val_loss improved from 0.51377 to 0.50360, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5021 - accuracy: 0.7505 - val_loss: 0.5036 - val_accuracy: 0.7423\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4976 - accuracy: 0.7540\n",
      "Epoch 00131: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4983 - accuracy: 0.7537 - val_loss: 0.5337 - val_accuracy: 0.7178\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7550\n",
      "Epoch 00132: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4979 - accuracy: 0.7545 - val_loss: 0.5401 - val_accuracy: 0.7128\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4990 - accuracy: 0.7535\n",
      "Epoch 00133: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4994 - accuracy: 0.7531 - val_loss: 0.5262 - val_accuracy: 0.7231\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.7569\n",
      "Epoch 00134: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4966 - accuracy: 0.7564 - val_loss: 0.5118 - val_accuracy: 0.7375\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.7562\n",
      "Epoch 00135: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4941 - accuracy: 0.7558 - val_loss: 0.5141 - val_accuracy: 0.7341\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4942 - accuracy: 0.7546\n",
      "Epoch 00136: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4948 - accuracy: 0.7540 - val_loss: 0.5272 - val_accuracy: 0.7279\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.7545\n",
      "Epoch 00137: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4968 - accuracy: 0.7542 - val_loss: 0.5046 - val_accuracy: 0.7423\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7576\n",
      "Epoch 00138: val_loss did not improve from 0.50360\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4957 - accuracy: 0.7575 - val_loss: 0.5138 - val_accuracy: 0.7337\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4932 - accuracy: 0.7592\n",
      "Epoch 00139: val_loss improved from 0.50360 to 0.50187, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4939 - accuracy: 0.7589 - val_loss: 0.5019 - val_accuracy: 0.7431\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4872 - accuracy: 0.7627\n",
      "Epoch 00140: val_loss did not improve from 0.50187\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4878 - accuracy: 0.7624 - val_loss: 0.5177 - val_accuracy: 0.7339\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7618\n",
      "Epoch 00141: val_loss improved from 0.50187 to 0.49735, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4915 - accuracy: 0.7615 - val_loss: 0.4973 - val_accuracy: 0.7498\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7629\n",
      "Epoch 00142: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4896 - accuracy: 0.7628 - val_loss: 0.5117 - val_accuracy: 0.7384\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4934 - accuracy: 0.7593\n",
      "Epoch 00143: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4944 - accuracy: 0.7589 - val_loss: 0.5033 - val_accuracy: 0.7448\n",
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.7626\n",
      "Epoch 00144: val_loss did not improve from 0.49735\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4897 - accuracy: 0.7624 - val_loss: 0.5409 - val_accuracy: 0.7139\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7613\n",
      "Epoch 00145: val_loss improved from 0.49735 to 0.48245, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4900 - accuracy: 0.7615 - val_loss: 0.4825 - val_accuracy: 0.7607\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.7611\n",
      "Epoch 00146: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4885 - accuracy: 0.7607 - val_loss: 0.5055 - val_accuracy: 0.7418\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7613\n",
      "Epoch 00147: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4881 - accuracy: 0.7607 - val_loss: 0.5089 - val_accuracy: 0.7393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.7617\n",
      "Epoch 00148: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4888 - accuracy: 0.7616 - val_loss: 0.5405 - val_accuracy: 0.7150\n",
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7632\n",
      "Epoch 00149: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4885 - accuracy: 0.7626 - val_loss: 0.5038 - val_accuracy: 0.7446\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4861 - accuracy: 0.7611\n",
      "Epoch 00150: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4868 - accuracy: 0.7608 - val_loss: 0.5185 - val_accuracy: 0.7289\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4830 - accuracy: 0.7660\n",
      "Epoch 00151: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4834 - accuracy: 0.7659 - val_loss: 0.5031 - val_accuracy: 0.7444\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4838 - accuracy: 0.7622\n",
      "Epoch 00152: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4840 - accuracy: 0.7621 - val_loss: 0.5078 - val_accuracy: 0.7397\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7649\n",
      "Epoch 00153: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4854 - accuracy: 0.7645 - val_loss: 0.5197 - val_accuracy: 0.7326\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4858 - accuracy: 0.7668\n",
      "Epoch 00154: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4859 - accuracy: 0.7665 - val_loss: 0.5060 - val_accuracy: 0.7401\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4837 - accuracy: 0.7664\n",
      "Epoch 00155: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4841 - accuracy: 0.7659 - val_loss: 0.4946 - val_accuracy: 0.7504\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7671\n",
      "Epoch 00156: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4829 - accuracy: 0.7669 - val_loss: 0.5039 - val_accuracy: 0.7420\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.7647\n",
      "Epoch 00157: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4858 - accuracy: 0.7645 - val_loss: 0.5611 - val_accuracy: 0.7055\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.7675\n",
      "Epoch 00158: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4805 - accuracy: 0.7674 - val_loss: 0.5525 - val_accuracy: 0.7096\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4812 - accuracy: 0.7678\n",
      "Epoch 00159: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4815 - accuracy: 0.7679 - val_loss: 0.5141 - val_accuracy: 0.7365\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.7669\n",
      "Epoch 00160: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4826 - accuracy: 0.7665 - val_loss: 0.5471 - val_accuracy: 0.7126\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.7693\n",
      "Epoch 00161: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4810 - accuracy: 0.7690 - val_loss: 0.4900 - val_accuracy: 0.7502\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.7648\n",
      "Epoch 00162: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4814 - accuracy: 0.7644 - val_loss: 0.4934 - val_accuracy: 0.7498\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7701\n",
      "Epoch 00163: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4786 - accuracy: 0.7697 - val_loss: 0.5104 - val_accuracy: 0.7390\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.7649\n",
      "Epoch 00164: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4802 - accuracy: 0.7646 - val_loss: 0.4962 - val_accuracy: 0.7494\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4774 - accuracy: 0.7664\n",
      "Epoch 00165: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4781 - accuracy: 0.7665 - val_loss: 0.5666 - val_accuracy: 0.7003\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7706\n",
      "Epoch 00166: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4761 - accuracy: 0.7704 - val_loss: 0.5225 - val_accuracy: 0.7283\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7673\n",
      "Epoch 00167: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4773 - accuracy: 0.7668 - val_loss: 0.4868 - val_accuracy: 0.7547\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7695\n",
      "Epoch 00168: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4766 - accuracy: 0.7694 - val_loss: 0.4885 - val_accuracy: 0.7515\n",
      "Epoch 169/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.7713\n",
      "Epoch 00169: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4738 - accuracy: 0.7709 - val_loss: 0.4951 - val_accuracy: 0.7481\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7702\n",
      "Epoch 00170: val_loss did not improve from 0.48245\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4761 - accuracy: 0.7699 - val_loss: 0.4834 - val_accuracy: 0.7537\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.7658\n",
      "Epoch 00171: val_loss improved from 0.48245 to 0.47492, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4759 - accuracy: 0.7656 - val_loss: 0.4749 - val_accuracy: 0.7588\n",
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7695\n",
      "Epoch 00172: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4751 - accuracy: 0.7694 - val_loss: 0.4995 - val_accuracy: 0.7457\n",
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7712\n",
      "Epoch 00173: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4756 - accuracy: 0.7709 - val_loss: 0.4813 - val_accuracy: 0.7586\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.7719\n",
      "Epoch 00174: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4730 - accuracy: 0.7717 - val_loss: 0.4996 - val_accuracy: 0.7453\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.7729\n",
      "Epoch 00175: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4749 - accuracy: 0.7724 - val_loss: 0.4823 - val_accuracy: 0.7580\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7713\n",
      "Epoch 00176: val_loss did not improve from 0.47492\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4720 - accuracy: 0.7708 - val_loss: 0.5139 - val_accuracy: 0.7345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4679 - accuracy: 0.7744\n",
      "Epoch 00177: val_loss improved from 0.47492 to 0.46389, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4688 - accuracy: 0.7737 - val_loss: 0.4639 - val_accuracy: 0.7696\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4713 - accuracy: 0.7706\n",
      "Epoch 00178: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4719 - accuracy: 0.7704 - val_loss: 0.4859 - val_accuracy: 0.7549\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7742\n",
      "Epoch 00179: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4718 - accuracy: 0.7742 - val_loss: 0.5128 - val_accuracy: 0.7354\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7710\n",
      "Epoch 00180: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4736 - accuracy: 0.7709 - val_loss: 0.4658 - val_accuracy: 0.7674\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7741\n",
      "Epoch 00181: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4668 - accuracy: 0.7736 - val_loss: 0.4849 - val_accuracy: 0.7558\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7738\n",
      "Epoch 00182: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4697 - accuracy: 0.7737 - val_loss: 0.5138 - val_accuracy: 0.7330\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.7747\n",
      "Epoch 00183: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4683 - accuracy: 0.7745 - val_loss: 0.4831 - val_accuracy: 0.7560\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7722\n",
      "Epoch 00184: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4695 - accuracy: 0.7719 - val_loss: 0.4708 - val_accuracy: 0.7642\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4680 - accuracy: 0.7715\n",
      "Epoch 00185: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4686 - accuracy: 0.7713 - val_loss: 0.4923 - val_accuracy: 0.7494\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.7765\n",
      "Epoch 00186: val_loss did not improve from 0.46389\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4662 - accuracy: 0.7767 - val_loss: 0.4945 - val_accuracy: 0.7479\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7759\n",
      "Epoch 00187: val_loss improved from 0.46389 to 0.45718, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4670 - accuracy: 0.7759 - val_loss: 0.4572 - val_accuracy: 0.7814\n",
      "Epoch 188/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4666 - accuracy: 0.7751\n",
      "Epoch 00188: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4673 - accuracy: 0.7750 - val_loss: 0.4635 - val_accuracy: 0.7687\n",
      "Epoch 189/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7754\n",
      "Epoch 00189: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4650 - accuracy: 0.7754 - val_loss: 0.4823 - val_accuracy: 0.7562\n",
      "Epoch 190/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.7754\n",
      "Epoch 00190: val_loss did not improve from 0.45718\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4694 - accuracy: 0.7752 - val_loss: 0.4729 - val_accuracy: 0.7616\n",
      "Epoch 191/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7794\n",
      "Epoch 00191: val_loss improved from 0.45718 to 0.45063, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4651 - accuracy: 0.7790 - val_loss: 0.4506 - val_accuracy: 0.7818\n",
      "Epoch 192/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7767\n",
      "Epoch 00192: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4660 - accuracy: 0.7761 - val_loss: 0.4760 - val_accuracy: 0.7590\n",
      "Epoch 193/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7760\n",
      "Epoch 00193: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4643 - accuracy: 0.7760 - val_loss: 0.4661 - val_accuracy: 0.7670\n",
      "Epoch 194/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7802\n",
      "Epoch 00194: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4607 - accuracy: 0.7802 - val_loss: 0.5064 - val_accuracy: 0.7408\n",
      "Epoch 195/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.7779\n",
      "Epoch 00195: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4637 - accuracy: 0.7775 - val_loss: 0.4702 - val_accuracy: 0.7638\n",
      "Epoch 196/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7807\n",
      "Epoch 00196: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4569 - accuracy: 0.7805 - val_loss: 0.4861 - val_accuracy: 0.7521\n",
      "Epoch 197/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7782\n",
      "Epoch 00197: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4599 - accuracy: 0.7782 - val_loss: 0.4818 - val_accuracy: 0.7562\n",
      "Epoch 198/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7772\n",
      "Epoch 00198: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4612 - accuracy: 0.7773 - val_loss: 0.4693 - val_accuracy: 0.7646\n",
      "Epoch 199/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4594 - accuracy: 0.7788\n",
      "Epoch 00199: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4604 - accuracy: 0.7785 - val_loss: 0.4684 - val_accuracy: 0.7661\n",
      "Epoch 200/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4603 - accuracy: 0.7790\n",
      "Epoch 00200: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4606 - accuracy: 0.7788 - val_loss: 0.4992 - val_accuracy: 0.7440\n",
      "Epoch 201/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.7824\n",
      "Epoch 00201: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4582 - accuracy: 0.7822 - val_loss: 0.5122 - val_accuracy: 0.7375\n",
      "Epoch 202/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.7800\n",
      "Epoch 00202: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4593 - accuracy: 0.7796 - val_loss: 0.5008 - val_accuracy: 0.7427\n",
      "Epoch 203/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7808\n",
      "Epoch 00203: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4589 - accuracy: 0.7804 - val_loss: 0.4889 - val_accuracy: 0.7541\n",
      "Epoch 204/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4570 - accuracy: 0.7824\n",
      "Epoch 00204: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4579 - accuracy: 0.7819 - val_loss: 0.4696 - val_accuracy: 0.7644\n",
      "Epoch 205/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7798\n",
      "Epoch 00205: val_loss did not improve from 0.45063\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4576 - accuracy: 0.7800 - val_loss: 0.4823 - val_accuracy: 0.7582\n",
      "Epoch 206/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7817\n",
      "Epoch 00206: val_loss improved from 0.45063 to 0.45034, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4534 - accuracy: 0.7813 - val_loss: 0.4503 - val_accuracy: 0.7771\n",
      "Epoch 207/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7829\n",
      "Epoch 00207: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4566 - accuracy: 0.7829 - val_loss: 0.5213 - val_accuracy: 0.7311\n",
      "Epoch 208/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4568 - accuracy: 0.7819\n",
      "Epoch 00208: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4568 - accuracy: 0.7821 - val_loss: 0.4979 - val_accuracy: 0.7453\n",
      "Epoch 209/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7843\n",
      "Epoch 00209: val_loss did not improve from 0.45034\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4538 - accuracy: 0.7841 - val_loss: 0.4745 - val_accuracy: 0.7603\n",
      "Epoch 210/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.7829\n",
      "Epoch 00210: val_loss improved from 0.45034 to 0.44169, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4555 - accuracy: 0.7825 - val_loss: 0.4417 - val_accuracy: 0.7880\n",
      "Epoch 211/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.7832\n",
      "Epoch 00211: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4526 - accuracy: 0.7830 - val_loss: 0.4580 - val_accuracy: 0.7745\n",
      "Epoch 212/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7836\n",
      "Epoch 00212: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4559 - accuracy: 0.7831 - val_loss: 0.4828 - val_accuracy: 0.7552\n",
      "Epoch 213/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4505 - accuracy: 0.7853\n",
      "Epoch 00213: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4509 - accuracy: 0.7852 - val_loss: 0.4651 - val_accuracy: 0.7698\n",
      "Epoch 214/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.7868\n",
      "Epoch 00214: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4514 - accuracy: 0.7869 - val_loss: 0.5067 - val_accuracy: 0.7388\n",
      "Epoch 215/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4505 - accuracy: 0.7860\n",
      "Epoch 00215: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4505 - accuracy: 0.7864 - val_loss: 0.4614 - val_accuracy: 0.7730\n",
      "Epoch 216/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.7846\n",
      "Epoch 00216: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4522 - accuracy: 0.7844 - val_loss: 0.4769 - val_accuracy: 0.7612\n",
      "Epoch 217/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7855\n",
      "Epoch 00217: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4514 - accuracy: 0.7853 - val_loss: 0.5418 - val_accuracy: 0.7214\n",
      "Epoch 218/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7863\n",
      "Epoch 00218: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4509 - accuracy: 0.7862 - val_loss: 0.4688 - val_accuracy: 0.7681\n",
      "Epoch 219/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.7889\n",
      "Epoch 00219: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4498 - accuracy: 0.7884 - val_loss: 0.4931 - val_accuracy: 0.7479\n",
      "Epoch 220/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4499 - accuracy: 0.7858\n",
      "Epoch 00220: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4503 - accuracy: 0.7856 - val_loss: 0.4485 - val_accuracy: 0.7812\n",
      "Epoch 221/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4479 - accuracy: 0.7898\n",
      "Epoch 00221: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4488 - accuracy: 0.7894 - val_loss: 0.5292 - val_accuracy: 0.7285\n",
      "Epoch 222/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.7791\n",
      "Epoch 00222: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4530 - accuracy: 0.7791 - val_loss: 0.4892 - val_accuracy: 0.7504\n",
      "Epoch 223/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4459 - accuracy: 0.7867\n",
      "Epoch 00223: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4460 - accuracy: 0.7866 - val_loss: 0.4484 - val_accuracy: 0.7767\n",
      "Epoch 224/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.7935\n",
      "Epoch 00224: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4427 - accuracy: 0.7931 - val_loss: 0.4509 - val_accuracy: 0.7777\n",
      "Epoch 225/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.7885\n",
      "Epoch 00225: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4451 - accuracy: 0.7882 - val_loss: 0.4561 - val_accuracy: 0.7754\n",
      "Epoch 226/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7860\n",
      "Epoch 00226: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4489 - accuracy: 0.7859 - val_loss: 0.4836 - val_accuracy: 0.7552\n",
      "Epoch 227/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7855\n",
      "Epoch 00227: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4482 - accuracy: 0.7854 - val_loss: 0.4438 - val_accuracy: 0.7857\n",
      "Epoch 228/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4468 - accuracy: 0.7880\n",
      "Epoch 00228: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4475 - accuracy: 0.7880 - val_loss: 0.4607 - val_accuracy: 0.7717\n",
      "Epoch 229/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4463 - accuracy: 0.7837\n",
      "Epoch 00229: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4467 - accuracy: 0.7838 - val_loss: 0.4597 - val_accuracy: 0.7717\n",
      "Epoch 230/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.7858\n",
      "Epoch 00230: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4504 - accuracy: 0.7857 - val_loss: 0.4890 - val_accuracy: 0.7511\n",
      "Epoch 231/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.7893\n",
      "Epoch 00231: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4436 - accuracy: 0.7891 - val_loss: 0.4505 - val_accuracy: 0.7801\n",
      "Epoch 232/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7871\n",
      "Epoch 00232: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4447 - accuracy: 0.7873 - val_loss: 0.4724 - val_accuracy: 0.7640\n",
      "Epoch 233/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7935\n",
      "Epoch 00233: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4400 - accuracy: 0.7936 - val_loss: 0.4647 - val_accuracy: 0.7678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4454 - accuracy: 0.7869\n",
      "Epoch 00234: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4454 - accuracy: 0.7872 - val_loss: 0.4659 - val_accuracy: 0.7661\n",
      "Epoch 235/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7860\n",
      "Epoch 00235: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4447 - accuracy: 0.7856 - val_loss: 0.4479 - val_accuracy: 0.7820\n",
      "Epoch 236/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.7923\n",
      "Epoch 00236: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4432 - accuracy: 0.7922 - val_loss: 0.4549 - val_accuracy: 0.7756\n",
      "Epoch 237/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4438 - accuracy: 0.7896\n",
      "Epoch 00237: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4440 - accuracy: 0.7894 - val_loss: 0.4625 - val_accuracy: 0.7676\n",
      "Epoch 238/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.7892\n",
      "Epoch 00238: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4440 - accuracy: 0.7890 - val_loss: 0.4439 - val_accuracy: 0.7822\n",
      "Epoch 239/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.7938\n",
      "Epoch 00239: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4377 - accuracy: 0.7938 - val_loss: 0.4469 - val_accuracy: 0.7810\n",
      "Epoch 240/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.7914\n",
      "Epoch 00240: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4416 - accuracy: 0.7916 - val_loss: 0.4707 - val_accuracy: 0.7638\n",
      "Epoch 241/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.7914\n",
      "Epoch 00241: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4409 - accuracy: 0.7913 - val_loss: 0.4511 - val_accuracy: 0.7769\n",
      "Epoch 242/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.7918\n",
      "Epoch 00242: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4386 - accuracy: 0.7918 - val_loss: 0.4502 - val_accuracy: 0.7773\n",
      "Epoch 243/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.7919\n",
      "Epoch 00243: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4409 - accuracy: 0.7916 - val_loss: 0.4607 - val_accuracy: 0.7702\n",
      "Epoch 244/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4359 - accuracy: 0.7928\n",
      "Epoch 00244: val_loss did not improve from 0.44169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4359 - accuracy: 0.7927 - val_loss: 0.4759 - val_accuracy: 0.7648\n",
      "Epoch 245/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7944\n",
      "Epoch 00245: val_loss improved from 0.44169 to 0.43419, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4372 - accuracy: 0.7940 - val_loss: 0.4342 - val_accuracy: 0.7917\n",
      "Epoch 246/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.7877\n",
      "Epoch 00246: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4448 - accuracy: 0.7875 - val_loss: 0.4525 - val_accuracy: 0.7782\n",
      "Epoch 247/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.7911\n",
      "Epoch 00247: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4375 - accuracy: 0.7909 - val_loss: 0.4513 - val_accuracy: 0.7801\n",
      "Epoch 248/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7891\n",
      "Epoch 00248: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4407 - accuracy: 0.7891 - val_loss: 0.4684 - val_accuracy: 0.7650\n",
      "Epoch 249/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.7889\n",
      "Epoch 00249: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4410 - accuracy: 0.7890 - val_loss: 0.4358 - val_accuracy: 0.7896\n",
      "Epoch 250/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.7932\n",
      "Epoch 00250: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4367 - accuracy: 0.7933 - val_loss: 0.4736 - val_accuracy: 0.7631\n",
      "Epoch 251/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4353 - accuracy: 0.7954\n",
      "Epoch 00251: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4358 - accuracy: 0.7951 - val_loss: 0.4606 - val_accuracy: 0.7734\n",
      "Epoch 252/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7911\n",
      "Epoch 00252: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4397 - accuracy: 0.7908 - val_loss: 0.4502 - val_accuracy: 0.7805\n",
      "Epoch 253/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7938\n",
      "Epoch 00253: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4338 - accuracy: 0.7935 - val_loss: 0.4839 - val_accuracy: 0.7547\n",
      "Epoch 254/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7955\n",
      "Epoch 00254: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4346 - accuracy: 0.7956 - val_loss: 0.4523 - val_accuracy: 0.7767\n",
      "Epoch 255/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.7900\n",
      "Epoch 00255: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4388 - accuracy: 0.7902 - val_loss: 0.4541 - val_accuracy: 0.7728\n",
      "Epoch 256/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.7979\n",
      "Epoch 00256: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4319 - accuracy: 0.7977 - val_loss: 0.4397 - val_accuracy: 0.7865\n",
      "Epoch 257/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.7966\n",
      "Epoch 00257: val_loss did not improve from 0.43419\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4361 - accuracy: 0.7965 - val_loss: 0.4473 - val_accuracy: 0.7788\n",
      "Epoch 258/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.7951\n",
      "Epoch 00258: val_loss improved from 0.43419 to 0.42990, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4313 - accuracy: 0.7947 - val_loss: 0.4299 - val_accuracy: 0.7945\n",
      "Epoch 259/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.7968\n",
      "Epoch 00259: val_loss did not improve from 0.42990\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4317 - accuracy: 0.7962 - val_loss: 0.4343 - val_accuracy: 0.7919\n",
      "Epoch 260/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.7955\n",
      "Epoch 00260: val_loss did not improve from 0.42990\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4333 - accuracy: 0.7951 - val_loss: 0.4464 - val_accuracy: 0.7840\n",
      "Epoch 261/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy: 0.7962\n",
      "Epoch 00261: val_loss improved from 0.42990 to 0.42889, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4331 - accuracy: 0.7959 - val_loss: 0.4289 - val_accuracy: 0.7921\n",
      "Epoch 262/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7973\n",
      "Epoch 00262: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4294 - accuracy: 0.7975 - val_loss: 0.4555 - val_accuracy: 0.7734\n",
      "Epoch 263/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.7986\n",
      "Epoch 00263: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4289 - accuracy: 0.7988 - val_loss: 0.4521 - val_accuracy: 0.7758\n",
      "Epoch 264/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.7949\n",
      "Epoch 00264: val_loss did not improve from 0.42889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4328 - accuracy: 0.7949 - val_loss: 0.4650 - val_accuracy: 0.7661\n",
      "Epoch 265/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4316 - accuracy: 0.7961\n",
      "Epoch 00265: val_loss improved from 0.42889 to 0.42342, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4319 - accuracy: 0.7960 - val_loss: 0.4234 - val_accuracy: 0.7977\n",
      "Epoch 266/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.7952\n",
      "Epoch 00266: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4326 - accuracy: 0.7952 - val_loss: 0.4485 - val_accuracy: 0.7779\n",
      "Epoch 267/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.7977\n",
      "Epoch 00267: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4288 - accuracy: 0.7979 - val_loss: 0.4477 - val_accuracy: 0.7805\n",
      "Epoch 268/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.7956\n",
      "Epoch 00268: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4312 - accuracy: 0.7958 - val_loss: 0.4366 - val_accuracy: 0.7891\n",
      "Epoch 269/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4297 - accuracy: 0.7960\n",
      "Epoch 00269: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4303 - accuracy: 0.7957 - val_loss: 0.4395 - val_accuracy: 0.7868\n",
      "Epoch 270/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8030\n",
      "Epoch 00270: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4237 - accuracy: 0.8026 - val_loss: 0.4324 - val_accuracy: 0.7900\n",
      "Epoch 271/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4300 - accuracy: 0.8009\n",
      "Epoch 00271: val_loss did not improve from 0.42342\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4309 - accuracy: 0.8005 - val_loss: 0.4269 - val_accuracy: 0.7958\n",
      "Epoch 272/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.7995\n",
      "Epoch 00272: val_loss improved from 0.42342 to 0.42275, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4265 - accuracy: 0.7998 - val_loss: 0.4227 - val_accuracy: 0.7988\n",
      "Epoch 273/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.7951\n",
      "Epoch 00273: val_loss did not improve from 0.42275\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4317 - accuracy: 0.7952 - val_loss: 0.4513 - val_accuracy: 0.7767\n",
      "Epoch 274/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.8014\n",
      "Epoch 00274: val_loss improved from 0.42275 to 0.41889, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4267 - accuracy: 0.8008 - val_loss: 0.4189 - val_accuracy: 0.8044\n",
      "Epoch 275/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8014\n",
      "Epoch 00275: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4254 - accuracy: 0.8016 - val_loss: 0.4289 - val_accuracy: 0.7956\n",
      "Epoch 276/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.8004\n",
      "Epoch 00276: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4239 - accuracy: 0.8003 - val_loss: 0.4225 - val_accuracy: 0.7994\n",
      "Epoch 277/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.7985\n",
      "Epoch 00277: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4285 - accuracy: 0.7982 - val_loss: 0.4246 - val_accuracy: 0.8016\n",
      "Epoch 278/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.7990\n",
      "Epoch 00278: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4257 - accuracy: 0.7991 - val_loss: 0.4853 - val_accuracy: 0.7539\n",
      "Epoch 279/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.7991\n",
      "Epoch 00279: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4266 - accuracy: 0.7988 - val_loss: 0.4202 - val_accuracy: 0.8087\n",
      "Epoch 280/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.7983\n",
      "Epoch 00280: val_loss did not improve from 0.41889\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4279 - accuracy: 0.7981 - val_loss: 0.4431 - val_accuracy: 0.7833\n",
      "Epoch 281/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4251 - accuracy: 0.8002\n",
      "Epoch 00281: val_loss improved from 0.41889 to 0.41883, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4260 - accuracy: 0.8001 - val_loss: 0.4188 - val_accuracy: 0.8061\n",
      "Epoch 282/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.7987\n",
      "Epoch 00282: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4231 - accuracy: 0.7983 - val_loss: 0.4297 - val_accuracy: 0.7945\n",
      "Epoch 283/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8011\n",
      "Epoch 00283: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4225 - accuracy: 0.8009 - val_loss: 0.4499 - val_accuracy: 0.7764\n",
      "Epoch 284/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.7991\n",
      "Epoch 00284: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4237 - accuracy: 0.7987 - val_loss: 0.4329 - val_accuracy: 0.7902\n",
      "Epoch 285/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8030\n",
      "Epoch 00285: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4202 - accuracy: 0.8028 - val_loss: 0.4380 - val_accuracy: 0.7861\n",
      "Epoch 286/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8011\n",
      "Epoch 00286: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4230 - accuracy: 0.8011 - val_loss: 0.4358 - val_accuracy: 0.7900\n",
      "Epoch 287/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8009\n",
      "Epoch 00287: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4237 - accuracy: 0.8005 - val_loss: 0.4263 - val_accuracy: 0.7960\n",
      "Epoch 288/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8059\n",
      "Epoch 00288: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4148 - accuracy: 0.8056 - val_loss: 0.4962 - val_accuracy: 0.7491\n",
      "Epoch 289/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8028\n",
      "Epoch 00289: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4189 - accuracy: 0.8030 - val_loss: 0.4533 - val_accuracy: 0.7767\n",
      "Epoch 290/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8043\n",
      "Epoch 00290: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4220 - accuracy: 0.8040 - val_loss: 0.4352 - val_accuracy: 0.7908\n",
      "Epoch 291/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8018\n",
      "Epoch 00291: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4195 - accuracy: 0.8017 - val_loss: 0.4744 - val_accuracy: 0.7623\n",
      "Epoch 292/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8042\n",
      "Epoch 00292: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4191 - accuracy: 0.8044 - val_loss: 0.4248 - val_accuracy: 0.7975\n",
      "Epoch 293/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8059\n",
      "Epoch 00293: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4187 - accuracy: 0.8061 - val_loss: 0.4220 - val_accuracy: 0.7982\n",
      "Epoch 294/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8039\n",
      "Epoch 00294: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4180 - accuracy: 0.8041 - val_loss: 0.4295 - val_accuracy: 0.7919\n",
      "Epoch 295/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8035\n",
      "Epoch 00295: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4191 - accuracy: 0.8037 - val_loss: 0.4357 - val_accuracy: 0.7885\n",
      "Epoch 296/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.7990\n",
      "Epoch 00296: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4204 - accuracy: 0.7988 - val_loss: 0.4192 - val_accuracy: 0.8025\n",
      "Epoch 297/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8062\n",
      "Epoch 00297: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4194 - accuracy: 0.8061 - val_loss: 0.4322 - val_accuracy: 0.7904\n",
      "Epoch 298/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.8062\n",
      "Epoch 00298: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4167 - accuracy: 0.8062 - val_loss: 0.4376 - val_accuracy: 0.7900\n",
      "Epoch 299/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8040\n",
      "Epoch 00299: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4155 - accuracy: 0.8040 - val_loss: 0.4275 - val_accuracy: 0.7939\n",
      "Epoch 300/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4148 - accuracy: 0.8045\n",
      "Epoch 00300: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4148 - accuracy: 0.8045 - val_loss: 0.4394 - val_accuracy: 0.7870\n",
      "Epoch 301/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8037\n",
      "Epoch 00301: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4170 - accuracy: 0.8035 - val_loss: 0.4388 - val_accuracy: 0.7870\n",
      "Epoch 302/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4154 - accuracy: 0.8051\n",
      "Epoch 00302: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4153 - accuracy: 0.8053 - val_loss: 0.4420 - val_accuracy: 0.7874\n",
      "Epoch 303/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8040\n",
      "Epoch 00303: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4161 - accuracy: 0.8041 - val_loss: 0.4397 - val_accuracy: 0.7870\n",
      "Epoch 304/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8079\n",
      "Epoch 00304: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4129 - accuracy: 0.8080 - val_loss: 0.4345 - val_accuracy: 0.7908\n",
      "Epoch 305/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8085\n",
      "Epoch 00305: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4137 - accuracy: 0.8084 - val_loss: 0.4199 - val_accuracy: 0.8009\n",
      "Epoch 306/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8060\n",
      "Epoch 00306: val_loss improved from 0.41883 to 0.41646, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4107 - accuracy: 0.8057 - val_loss: 0.4165 - val_accuracy: 0.8022\n",
      "Epoch 307/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4164 - accuracy: 0.8059\n",
      "Epoch 00307: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4166 - accuracy: 0.8057 - val_loss: 0.4361 - val_accuracy: 0.7874\n",
      "Epoch 308/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8142\n",
      "Epoch 00308: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4091 - accuracy: 0.8140 - val_loss: 0.4309 - val_accuracy: 0.7919\n",
      "Epoch 309/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8132\n",
      "Epoch 00309: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4052 - accuracy: 0.8132 - val_loss: 0.4389 - val_accuracy: 0.7868\n",
      "Epoch 310/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.8073\n",
      "Epoch 00310: val_loss did not improve from 0.41646\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4123 - accuracy: 0.8071 - val_loss: 0.4232 - val_accuracy: 0.7999\n",
      "Epoch 311/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4100 - accuracy: 0.8110\n",
      "Epoch 00311: val_loss improved from 0.41646 to 0.41469, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4108 - accuracy: 0.8107 - val_loss: 0.4147 - val_accuracy: 0.8080\n",
      "Epoch 312/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8082\n",
      "Epoch 00312: val_loss did not improve from 0.41469\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4112 - accuracy: 0.8078 - val_loss: 0.4162 - val_accuracy: 0.8044\n",
      "Epoch 313/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8059\n",
      "Epoch 00313: val_loss improved from 0.41469 to 0.40701, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4110 - accuracy: 0.8061 - val_loss: 0.4070 - val_accuracy: 0.8158\n",
      "Epoch 314/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8110\n",
      "Epoch 00314: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4103 - accuracy: 0.8110 - val_loss: 0.4284 - val_accuracy: 0.7947\n",
      "Epoch 315/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8111\n",
      "Epoch 00315: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4083 - accuracy: 0.8111 - val_loss: 0.4296 - val_accuracy: 0.7921\n",
      "Epoch 316/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8099\n",
      "Epoch 00316: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4077 - accuracy: 0.8098 - val_loss: 0.4232 - val_accuracy: 0.7969\n",
      "Epoch 317/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8070\n",
      "Epoch 00317: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4131 - accuracy: 0.8069 - val_loss: 0.4230 - val_accuracy: 0.7960\n",
      "Epoch 318/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8103\n",
      "Epoch 00318: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4067 - accuracy: 0.8103 - val_loss: 0.4310 - val_accuracy: 0.7902\n",
      "Epoch 319/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8083\n",
      "Epoch 00319: val_loss did not improve from 0.40701\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4083 - accuracy: 0.8082 - val_loss: 0.4470 - val_accuracy: 0.7816\n",
      "Epoch 320/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8096\n",
      "Epoch 00320: val_loss improved from 0.40701 to 0.40501, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4066 - accuracy: 0.8093 - val_loss: 0.4050 - val_accuracy: 0.8106\n",
      "Epoch 321/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8123\n",
      "Epoch 00321: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4075 - accuracy: 0.8122 - val_loss: 0.4575 - val_accuracy: 0.7728\n",
      "Epoch 322/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8112\n",
      "Epoch 00322: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4071 - accuracy: 0.8111 - val_loss: 0.4228 - val_accuracy: 0.7984\n",
      "Epoch 323/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8116\n",
      "Epoch 00323: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4092 - accuracy: 0.8118 - val_loss: 0.4090 - val_accuracy: 0.8106\n",
      "Epoch 324/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8106\n",
      "Epoch 00324: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4080 - accuracy: 0.8105 - val_loss: 0.4357 - val_accuracy: 0.7868\n",
      "Epoch 325/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4062 - accuracy: 0.8101\n",
      "Epoch 00325: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4058 - accuracy: 0.8102 - val_loss: 0.4349 - val_accuracy: 0.7898\n",
      "Epoch 326/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8132\n",
      "Epoch 00326: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4026 - accuracy: 0.8133 - val_loss: 0.4248 - val_accuracy: 0.7943\n",
      "Epoch 327/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8114\n",
      "Epoch 00327: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4054 - accuracy: 0.8117 - val_loss: 0.4355 - val_accuracy: 0.7896\n",
      "Epoch 328/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8117\n",
      "Epoch 00328: val_loss did not improve from 0.40501\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4055 - accuracy: 0.8116 - val_loss: 0.4145 - val_accuracy: 0.8050\n",
      "Epoch 329/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8141\n",
      "Epoch 00329: val_loss improved from 0.40501 to 0.40432, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4029 - accuracy: 0.8142 - val_loss: 0.4043 - val_accuracy: 0.8106\n",
      "Epoch 330/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8117\n",
      "Epoch 00330: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4055 - accuracy: 0.8118 - val_loss: 0.4397 - val_accuracy: 0.7859\n",
      "Epoch 331/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8135\n",
      "Epoch 00331: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4030 - accuracy: 0.8134 - val_loss: 0.4099 - val_accuracy: 0.8078\n",
      "Epoch 332/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8164\n",
      "Epoch 00332: val_loss did not improve from 0.40432\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3991 - accuracy: 0.8163 - val_loss: 0.4124 - val_accuracy: 0.8044\n",
      "Epoch 333/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4007 - accuracy: 0.8127\n",
      "Epoch 00333: val_loss improved from 0.40432 to 0.40157, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4013 - accuracy: 0.8121 - val_loss: 0.4016 - val_accuracy: 0.8138\n",
      "Epoch 334/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.8147\n",
      "Epoch 00334: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3993 - accuracy: 0.8149 - val_loss: 0.4233 - val_accuracy: 0.7943\n",
      "Epoch 335/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8117\n",
      "Epoch 00335: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4034 - accuracy: 0.8114 - val_loss: 0.4019 - val_accuracy: 0.8158\n",
      "Epoch 336/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8161\n",
      "Epoch 00336: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3989 - accuracy: 0.8160 - val_loss: 0.4186 - val_accuracy: 0.7984\n",
      "Epoch 337/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3997 - accuracy: 0.8142\n",
      "Epoch 00337: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4002 - accuracy: 0.8141 - val_loss: 0.4026 - val_accuracy: 0.8160\n",
      "Epoch 338/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8110\n",
      "Epoch 00338: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4021 - accuracy: 0.8107 - val_loss: 0.4077 - val_accuracy: 0.8095\n",
      "Epoch 339/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4015 - accuracy: 0.8104\n",
      "Epoch 00339: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4015 - accuracy: 0.8106 - val_loss: 0.4178 - val_accuracy: 0.8020\n",
      "Epoch 340/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8113\n",
      "Epoch 00340: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4035 - accuracy: 0.8110 - val_loss: 0.4077 - val_accuracy: 0.8104\n",
      "Epoch 341/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8145\n",
      "Epoch 00341: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3974 - accuracy: 0.8145 - val_loss: 0.4258 - val_accuracy: 0.7973\n",
      "Epoch 342/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.8137\n",
      "Epoch 00342: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4009 - accuracy: 0.8137 - val_loss: 0.4286 - val_accuracy: 0.7919\n",
      "Epoch 343/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8173\n",
      "Epoch 00343: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3959 - accuracy: 0.8173 - val_loss: 0.4151 - val_accuracy: 0.8031\n",
      "Epoch 344/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8123\n",
      "Epoch 00344: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4030 - accuracy: 0.8119 - val_loss: 0.4064 - val_accuracy: 0.8132\n",
      "Epoch 345/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8155\n",
      "Epoch 00345: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3931 - accuracy: 0.8153 - val_loss: 0.4068 - val_accuracy: 0.8091\n",
      "Epoch 346/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.3930 - accuracy: 0.8193\n",
      "Epoch 00346: val_loss did not improve from 0.40157\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3938 - accuracy: 0.8189 - val_loss: 0.4023 - val_accuracy: 0.8147\n",
      "Epoch 347/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8134\n",
      "Epoch 00347: val_loss improved from 0.40157 to 0.39875, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3969 - accuracy: 0.8132 - val_loss: 0.3988 - val_accuracy: 0.8194\n",
      "Epoch 348/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8161\n",
      "Epoch 00348: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3956 - accuracy: 0.8159 - val_loss: 0.4121 - val_accuracy: 0.8035\n",
      "Epoch 349/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8173\n",
      "Epoch 00349: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3947 - accuracy: 0.8173 - val_loss: 0.3996 - val_accuracy: 0.8179\n",
      "Epoch 350/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.8192\n",
      "Epoch 00350: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3929 - accuracy: 0.8193 - val_loss: 0.4141 - val_accuracy: 0.8050\n",
      "Epoch 351/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8181\n",
      "Epoch 00351: val_loss did not improve from 0.39875\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3936 - accuracy: 0.8180 - val_loss: 0.4141 - val_accuracy: 0.8035\n",
      "Epoch 352/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3927 - accuracy: 0.8167\n",
      "Epoch 00352: val_loss improved from 0.39875 to 0.39657, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3929 - accuracy: 0.8166 - val_loss: 0.3966 - val_accuracy: 0.8220\n",
      "Epoch 353/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8167\n",
      "Epoch 00353: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3946 - accuracy: 0.8166 - val_loss: 0.4142 - val_accuracy: 0.8025\n",
      "Epoch 354/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8150\n",
      "Epoch 00354: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3940 - accuracy: 0.8148 - val_loss: 0.4277 - val_accuracy: 0.7923\n",
      "Epoch 355/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8143\n",
      "Epoch 00355: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3958 - accuracy: 0.8143 - val_loss: 0.4124 - val_accuracy: 0.8042\n",
      "Epoch 356/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8193\n",
      "Epoch 00356: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3901 - accuracy: 0.8193 - val_loss: 0.4122 - val_accuracy: 0.8063\n",
      "Epoch 357/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8183\n",
      "Epoch 00357: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3956 - accuracy: 0.8180 - val_loss: 0.4142 - val_accuracy: 0.8050\n",
      "Epoch 358/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8184\n",
      "Epoch 00358: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3911 - accuracy: 0.8183 - val_loss: 0.4127 - val_accuracy: 0.8031\n",
      "Epoch 359/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8184\n",
      "Epoch 00359: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3916 - accuracy: 0.8181 - val_loss: 0.4034 - val_accuracy: 0.8141\n",
      "Epoch 360/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8150\n",
      "Epoch 00360: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3947 - accuracy: 0.8149 - val_loss: 0.4085 - val_accuracy: 0.8102\n",
      "Epoch 361/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8154\n",
      "Epoch 00361: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3925 - accuracy: 0.8156 - val_loss: 0.4206 - val_accuracy: 0.7982\n",
      "Epoch 362/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8223\n",
      "Epoch 00362: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3890 - accuracy: 0.8226 - val_loss: 0.4090 - val_accuracy: 0.8080\n",
      "Epoch 363/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8217\n",
      "Epoch 00363: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3896 - accuracy: 0.8214 - val_loss: 0.4080 - val_accuracy: 0.8115\n",
      "Epoch 364/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8217\n",
      "Epoch 00364: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3847 - accuracy: 0.8218 - val_loss: 0.4154 - val_accuracy: 0.8048\n",
      "Epoch 365/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8210\n",
      "Epoch 00365: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3890 - accuracy: 0.8213 - val_loss: 0.4011 - val_accuracy: 0.8141\n",
      "Epoch 366/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8217\n",
      "Epoch 00366: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3885 - accuracy: 0.8215 - val_loss: 0.4183 - val_accuracy: 0.8018\n",
      "Epoch 367/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8210\n",
      "Epoch 00367: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3868 - accuracy: 0.8206 - val_loss: 0.4199 - val_accuracy: 0.8020\n",
      "Epoch 368/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8181\n",
      "Epoch 00368: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3902 - accuracy: 0.8179 - val_loss: 0.4108 - val_accuracy: 0.8072\n",
      "Epoch 369/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8191\n",
      "Epoch 00369: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3896 - accuracy: 0.8190 - val_loss: 0.4056 - val_accuracy: 0.8113\n",
      "Epoch 370/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8202\n",
      "Epoch 00370: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3896 - accuracy: 0.8205 - val_loss: 0.3989 - val_accuracy: 0.8184\n",
      "Epoch 371/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8216\n",
      "Epoch 00371: val_loss did not improve from 0.39657\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3871 - accuracy: 0.8211 - val_loss: 0.4037 - val_accuracy: 0.8093\n",
      "Epoch 372/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8204\n",
      "Epoch 00372: val_loss improved from 0.39657 to 0.39076, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3887 - accuracy: 0.8202 - val_loss: 0.3908 - val_accuracy: 0.8188\n",
      "Epoch 373/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8208\n",
      "Epoch 00373: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3844 - accuracy: 0.8207 - val_loss: 0.4138 - val_accuracy: 0.8085\n",
      "Epoch 374/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8206\n",
      "Epoch 00374: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3873 - accuracy: 0.8205 - val_loss: 0.3972 - val_accuracy: 0.8186\n",
      "Epoch 375/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8257\n",
      "Epoch 00375: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3823 - accuracy: 0.8256 - val_loss: 0.3979 - val_accuracy: 0.8151\n",
      "Epoch 376/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8200\n",
      "Epoch 00376: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3880 - accuracy: 0.8197 - val_loss: 0.4130 - val_accuracy: 0.8050\n",
      "Epoch 377/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.8225\n",
      "Epoch 00377: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3841 - accuracy: 0.8220 - val_loss: 0.3919 - val_accuracy: 0.8214\n",
      "Epoch 378/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8238\n",
      "Epoch 00378: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3836 - accuracy: 0.8237 - val_loss: 0.3974 - val_accuracy: 0.8171\n",
      "Epoch 379/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8220\n",
      "Epoch 00379: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3873 - accuracy: 0.8220 - val_loss: 0.3974 - val_accuracy: 0.8194\n",
      "Epoch 380/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8242\n",
      "Epoch 00380: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3817 - accuracy: 0.8238 - val_loss: 0.4124 - val_accuracy: 0.8031\n",
      "Epoch 381/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.8225\n",
      "Epoch 00381: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3856 - accuracy: 0.8223 - val_loss: 0.4186 - val_accuracy: 0.7977\n",
      "Epoch 382/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8246\n",
      "Epoch 00382: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3845 - accuracy: 0.8240 - val_loss: 0.4095 - val_accuracy: 0.8095\n",
      "Epoch 383/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3827 - accuracy: 0.8252\n",
      "Epoch 00383: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3836 - accuracy: 0.8248 - val_loss: 0.4014 - val_accuracy: 0.8153\n",
      "Epoch 384/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8234\n",
      "Epoch 00384: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3821 - accuracy: 0.8230 - val_loss: 0.4097 - val_accuracy: 0.8074\n",
      "Epoch 385/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8272\n",
      "Epoch 00385: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3755 - accuracy: 0.8270 - val_loss: 0.4005 - val_accuracy: 0.8141\n",
      "Epoch 386/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3860 - accuracy: 0.8208\n",
      "Epoch 00386: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3864 - accuracy: 0.8209 - val_loss: 0.4091 - val_accuracy: 0.8104\n",
      "Epoch 387/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.8275\n",
      "Epoch 00387: val_loss did not improve from 0.39076\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3785 - accuracy: 0.8274 - val_loss: 0.4126 - val_accuracy: 0.8065\n",
      "Epoch 388/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8228\n",
      "Epoch 00388: val_loss improved from 0.39076 to 0.39055, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3839 - accuracy: 0.8229 - val_loss: 0.3905 - val_accuracy: 0.8233\n",
      "Epoch 389/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8213\n",
      "Epoch 00389: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3813 - accuracy: 0.8212 - val_loss: 0.4185 - val_accuracy: 0.8033\n",
      "Epoch 390/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8289\n",
      "Epoch 00390: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3764 - accuracy: 0.8289 - val_loss: 0.4059 - val_accuracy: 0.8089\n",
      "Epoch 391/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.8254\n",
      "Epoch 00391: val_loss did not improve from 0.39055\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3803 - accuracy: 0.8251 - val_loss: 0.4021 - val_accuracy: 0.8136\n",
      "Epoch 392/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.8269\n",
      "Epoch 00392: val_loss improved from 0.39055 to 0.38848, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3758 - accuracy: 0.8264 - val_loss: 0.3885 - val_accuracy: 0.8263\n",
      "Epoch 393/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8269\n",
      "Epoch 00393: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3757 - accuracy: 0.8266 - val_loss: 0.4074 - val_accuracy: 0.8110\n",
      "Epoch 394/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8279\n",
      "Epoch 00394: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3758 - accuracy: 0.8276 - val_loss: 0.4007 - val_accuracy: 0.8151\n",
      "Epoch 395/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8277\n",
      "Epoch 00395: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3745 - accuracy: 0.8271 - val_loss: 0.3962 - val_accuracy: 0.8196\n",
      "Epoch 396/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.8280\n",
      "Epoch 00396: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3770 - accuracy: 0.8276 - val_loss: 0.3899 - val_accuracy: 0.8237\n",
      "Epoch 397/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8258\n",
      "Epoch 00397: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3801 - accuracy: 0.8258 - val_loss: 0.4262 - val_accuracy: 0.7936\n",
      "Epoch 398/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8265\n",
      "Epoch 00398: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3770 - accuracy: 0.8265 - val_loss: 0.3904 - val_accuracy: 0.8220\n",
      "Epoch 399/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8318\n",
      "Epoch 00399: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3707 - accuracy: 0.8318 - val_loss: 0.4044 - val_accuracy: 0.8113\n",
      "Epoch 400/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8293\n",
      "Epoch 00400: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3754 - accuracy: 0.8293 - val_loss: 0.4006 - val_accuracy: 0.8151\n",
      "Epoch 401/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8324\n",
      "Epoch 00401: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3729 - accuracy: 0.8320 - val_loss: 0.3890 - val_accuracy: 0.8239\n",
      "Epoch 402/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8290\n",
      "Epoch 00402: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3729 - accuracy: 0.8289 - val_loss: 0.4020 - val_accuracy: 0.8149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3730 - accuracy: 0.8281\n",
      "Epoch 00403: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3731 - accuracy: 0.8279 - val_loss: 0.4045 - val_accuracy: 0.8110\n",
      "Epoch 404/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8308\n",
      "Epoch 00404: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3697 - accuracy: 0.8305 - val_loss: 0.3928 - val_accuracy: 0.8190\n",
      "Epoch 405/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8268\n",
      "Epoch 00405: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3730 - accuracy: 0.8271 - val_loss: 0.4076 - val_accuracy: 0.8080\n",
      "Epoch 406/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8308\n",
      "Epoch 00406: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3735 - accuracy: 0.8305 - val_loss: 0.4030 - val_accuracy: 0.8134\n",
      "Epoch 407/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8294\n",
      "Epoch 00407: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3700 - accuracy: 0.8294 - val_loss: 0.4008 - val_accuracy: 0.8156\n",
      "Epoch 408/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8286\n",
      "Epoch 00408: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3729 - accuracy: 0.8288 - val_loss: 0.3966 - val_accuracy: 0.8177\n",
      "Epoch 409/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8315\n",
      "Epoch 00409: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3715 - accuracy: 0.8310 - val_loss: 0.3903 - val_accuracy: 0.8203\n",
      "Epoch 410/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8289\n",
      "Epoch 00410: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3754 - accuracy: 0.8290 - val_loss: 0.3928 - val_accuracy: 0.8203\n",
      "Epoch 411/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8328\n",
      "Epoch 00411: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3682 - accuracy: 0.8329 - val_loss: 0.4126 - val_accuracy: 0.8080\n",
      "Epoch 412/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8324\n",
      "Epoch 00412: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3675 - accuracy: 0.8321 - val_loss: 0.4010 - val_accuracy: 0.8147\n",
      "Epoch 413/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8337\n",
      "Epoch 00413: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3655 - accuracy: 0.8333 - val_loss: 0.4016 - val_accuracy: 0.8132\n",
      "Epoch 414/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8357\n",
      "Epoch 00414: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3654 - accuracy: 0.8356 - val_loss: 0.4097 - val_accuracy: 0.8070\n",
      "Epoch 415/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8290\n",
      "Epoch 00415: val_loss did not improve from 0.38848\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3708 - accuracy: 0.8292 - val_loss: 0.4194 - val_accuracy: 0.7990\n",
      "Epoch 416/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8352\n",
      "Epoch 00416: val_loss improved from 0.38848 to 0.38803, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3656 - accuracy: 0.8351 - val_loss: 0.3880 - val_accuracy: 0.8229\n",
      "Epoch 417/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8283\n",
      "Epoch 00417: val_loss did not improve from 0.38803\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3700 - accuracy: 0.8285 - val_loss: 0.3926 - val_accuracy: 0.8205\n",
      "Epoch 418/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8269\n",
      "Epoch 00418: val_loss improved from 0.38803 to 0.38572, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3742 - accuracy: 0.8267 - val_loss: 0.3857 - val_accuracy: 0.8237\n",
      "Epoch 419/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8325\n",
      "Epoch 00419: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3664 - accuracy: 0.8323 - val_loss: 0.3892 - val_accuracy: 0.8231\n",
      "Epoch 420/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8339\n",
      "Epoch 00420: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3654 - accuracy: 0.8336 - val_loss: 0.3893 - val_accuracy: 0.8205\n",
      "Epoch 421/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8318\n",
      "Epoch 00421: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3669 - accuracy: 0.8319 - val_loss: 0.3944 - val_accuracy: 0.8207\n",
      "Epoch 422/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8298\n",
      "Epoch 00422: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3668 - accuracy: 0.8297 - val_loss: 0.4106 - val_accuracy: 0.8070\n",
      "Epoch 423/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8337\n",
      "Epoch 00423: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3695 - accuracy: 0.8337 - val_loss: 0.4000 - val_accuracy: 0.8153\n",
      "Epoch 424/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8319\n",
      "Epoch 00424: val_loss did not improve from 0.38572\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3709 - accuracy: 0.8316 - val_loss: 0.3923 - val_accuracy: 0.8216\n",
      "Epoch 425/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8355\n",
      "Epoch 00425: val_loss improved from 0.38572 to 0.38433, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3603 - accuracy: 0.8356 - val_loss: 0.3843 - val_accuracy: 0.8257\n",
      "Epoch 426/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8314\n",
      "Epoch 00426: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3647 - accuracy: 0.8315 - val_loss: 0.4079 - val_accuracy: 0.8070\n",
      "Epoch 427/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8339\n",
      "Epoch 00427: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3621 - accuracy: 0.8341 - val_loss: 0.4044 - val_accuracy: 0.8098\n",
      "Epoch 428/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8355\n",
      "Epoch 00428: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3603 - accuracy: 0.8355 - val_loss: 0.3869 - val_accuracy: 0.8237\n",
      "Epoch 429/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8354\n",
      "Epoch 00429: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3612 - accuracy: 0.8356 - val_loss: 0.3982 - val_accuracy: 0.8175\n",
      "Epoch 430/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8364\n",
      "Epoch 00430: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3609 - accuracy: 0.8363 - val_loss: 0.3991 - val_accuracy: 0.8164\n",
      "Epoch 431/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8314\n",
      "Epoch 00431: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3637 - accuracy: 0.8314 - val_loss: 0.4030 - val_accuracy: 0.8119\n",
      "Epoch 432/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8375\n",
      "Epoch 00432: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3581 - accuracy: 0.8375 - val_loss: 0.4157 - val_accuracy: 0.8072\n",
      "Epoch 433/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8322\n",
      "Epoch 00433: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3686 - accuracy: 0.8320 - val_loss: 0.3874 - val_accuracy: 0.8233\n",
      "Epoch 434/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8345\n",
      "Epoch 00434: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3629 - accuracy: 0.8344 - val_loss: 0.3850 - val_accuracy: 0.8237\n",
      "Epoch 435/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8362\n",
      "Epoch 00435: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3585 - accuracy: 0.8358 - val_loss: 0.3953 - val_accuracy: 0.8179\n",
      "Epoch 436/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8382\n",
      "Epoch 00436: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3558 - accuracy: 0.8382 - val_loss: 0.4037 - val_accuracy: 0.8106\n",
      "Epoch 437/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8359\n",
      "Epoch 00437: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3601 - accuracy: 0.8355 - val_loss: 0.3892 - val_accuracy: 0.8203\n",
      "Epoch 438/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8341\n",
      "Epoch 00438: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3581 - accuracy: 0.8340 - val_loss: 0.3887 - val_accuracy: 0.8250\n",
      "Epoch 439/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8416\n",
      "Epoch 00439: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3567 - accuracy: 0.8412 - val_loss: 0.3996 - val_accuracy: 0.8156\n",
      "Epoch 440/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8389\n",
      "Epoch 00440: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3580 - accuracy: 0.8389 - val_loss: 0.3901 - val_accuracy: 0.8224\n",
      "Epoch 441/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8404\n",
      "Epoch 00441: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3558 - accuracy: 0.8402 - val_loss: 0.4001 - val_accuracy: 0.8115\n",
      "Epoch 442/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8423\n",
      "Epoch 00442: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3553 - accuracy: 0.8420 - val_loss: 0.3897 - val_accuracy: 0.8201\n",
      "Epoch 443/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8392\n",
      "Epoch 00443: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3545 - accuracy: 0.8391 - val_loss: 0.3915 - val_accuracy: 0.8190\n",
      "Epoch 444/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8351\n",
      "Epoch 00444: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3598 - accuracy: 0.8350 - val_loss: 0.3915 - val_accuracy: 0.8173\n",
      "Epoch 445/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8346\n",
      "Epoch 00445: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3623 - accuracy: 0.8344 - val_loss: 0.4104 - val_accuracy: 0.8052\n",
      "Epoch 446/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8378\n",
      "Epoch 00446: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3589 - accuracy: 0.8374 - val_loss: 0.3945 - val_accuracy: 0.8177\n",
      "Epoch 447/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 00447: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3549 - accuracy: 0.8394 - val_loss: 0.4227 - val_accuracy: 0.8009\n",
      "Epoch 448/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8392\n",
      "Epoch 00448: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3534 - accuracy: 0.8393 - val_loss: 0.4197 - val_accuracy: 0.8022\n",
      "Epoch 449/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8383\n",
      "Epoch 00449: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3568 - accuracy: 0.8382 - val_loss: 0.4032 - val_accuracy: 0.8117\n",
      "Epoch 450/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8395\n",
      "Epoch 00450: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3516 - accuracy: 0.8395 - val_loss: 0.4118 - val_accuracy: 0.8065\n",
      "Epoch 451/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.8397\n",
      "Epoch 00451: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3522 - accuracy: 0.8401 - val_loss: 0.4103 - val_accuracy: 0.8085\n",
      "Epoch 452/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8387\n",
      "Epoch 00452: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3543 - accuracy: 0.8387 - val_loss: 0.3924 - val_accuracy: 0.8166\n",
      "Epoch 453/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.8417\n",
      "Epoch 00453: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3514 - accuracy: 0.8416 - val_loss: 0.4057 - val_accuracy: 0.8065\n",
      "Epoch 454/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8385\n",
      "Epoch 00454: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3558 - accuracy: 0.8387 - val_loss: 0.3931 - val_accuracy: 0.8201\n",
      "Epoch 455/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8401\n",
      "Epoch 00455: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3519 - accuracy: 0.8402 - val_loss: 0.4004 - val_accuracy: 0.8110\n",
      "Epoch 456/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8394\n",
      "Epoch 00456: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3518 - accuracy: 0.8392 - val_loss: 0.3915 - val_accuracy: 0.8207\n",
      "Epoch 457/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8395\n",
      "Epoch 00457: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3513 - accuracy: 0.8394 - val_loss: 0.3858 - val_accuracy: 0.8244\n",
      "Epoch 458/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8430\n",
      "Epoch 00458: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3494 - accuracy: 0.8427 - val_loss: 0.3944 - val_accuracy: 0.8184\n",
      "Epoch 459/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8427\n",
      "Epoch 00459: val_loss did not improve from 0.38433\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3517 - accuracy: 0.8428 - val_loss: 0.4100 - val_accuracy: 0.8091\n",
      "Epoch 460/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8419\n",
      "Epoch 00460: val_loss improved from 0.38433 to 0.38427, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3489 - accuracy: 0.8419 - val_loss: 0.3843 - val_accuracy: 0.8242\n",
      "Epoch 461/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8404\n",
      "Epoch 00461: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3513 - accuracy: 0.8400 - val_loss: 0.3935 - val_accuracy: 0.8175\n",
      "Epoch 462/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8424\n",
      "Epoch 00462: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3493 - accuracy: 0.8428 - val_loss: 0.4050 - val_accuracy: 0.8130\n",
      "Epoch 463/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8399\n",
      "Epoch 00463: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3530 - accuracy: 0.8402 - val_loss: 0.4070 - val_accuracy: 0.8100\n",
      "Epoch 464/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8452\n",
      "Epoch 00464: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3432 - accuracy: 0.8453 - val_loss: 0.4009 - val_accuracy: 0.8102\n",
      "Epoch 465/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8409\n",
      "Epoch 00465: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3457 - accuracy: 0.8407 - val_loss: 0.3865 - val_accuracy: 0.8239\n",
      "Epoch 466/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8454\n",
      "Epoch 00466: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3439 - accuracy: 0.8453 - val_loss: 0.4082 - val_accuracy: 0.8078\n",
      "Epoch 467/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8439\n",
      "Epoch 00467: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3448 - accuracy: 0.8436 - val_loss: 0.4009 - val_accuracy: 0.8113\n",
      "Epoch 468/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8479\n",
      "Epoch 00468: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3414 - accuracy: 0.8477 - val_loss: 0.4033 - val_accuracy: 0.8121\n",
      "Epoch 469/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8464\n",
      "Epoch 00469: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3401 - accuracy: 0.8462 - val_loss: 0.4070 - val_accuracy: 0.8074\n",
      "Epoch 470/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8425\n",
      "Epoch 00470: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3487 - accuracy: 0.8422 - val_loss: 0.4021 - val_accuracy: 0.8117\n",
      "Epoch 471/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8473\n",
      "Epoch 00471: val_loss did not improve from 0.38427\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3409 - accuracy: 0.8475 - val_loss: 0.3911 - val_accuracy: 0.8201\n",
      "Epoch 472/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8475\n",
      "Epoch 00472: val_loss improved from 0.38427 to 0.38286, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3441 - accuracy: 0.8472 - val_loss: 0.3829 - val_accuracy: 0.8261\n",
      "Epoch 473/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8449\n",
      "Epoch 00473: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3440 - accuracy: 0.8447 - val_loss: 0.3881 - val_accuracy: 0.8239\n",
      "Epoch 474/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8453\n",
      "Epoch 00474: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3464 - accuracy: 0.8452 - val_loss: 0.3940 - val_accuracy: 0.8119\n",
      "Epoch 475/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.8465\n",
      "Epoch 00475: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3403 - accuracy: 0.8464 - val_loss: 0.4006 - val_accuracy: 0.8098\n",
      "Epoch 476/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8424\n",
      "Epoch 00476: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3478 - accuracy: 0.8426 - val_loss: 0.4283 - val_accuracy: 0.7954\n",
      "Epoch 477/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8413\n",
      "Epoch 00477: val_loss did not improve from 0.38286\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3502 - accuracy: 0.8415 - val_loss: 0.4001 - val_accuracy: 0.8098\n",
      "Epoch 478/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3380 - accuracy: 0.8472\n",
      "Epoch 00478: val_loss improved from 0.38286 to 0.38008, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3386 - accuracy: 0.8469 - val_loss: 0.3801 - val_accuracy: 0.8276\n",
      "Epoch 479/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8487\n",
      "Epoch 00479: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3399 - accuracy: 0.8488 - val_loss: 0.4013 - val_accuracy: 0.8132\n",
      "Epoch 480/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8500\n",
      "Epoch 00480: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3430 - accuracy: 0.8498 - val_loss: 0.4000 - val_accuracy: 0.8093\n",
      "Epoch 481/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8445\n",
      "Epoch 00481: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3443 - accuracy: 0.8447 - val_loss: 0.3904 - val_accuracy: 0.8190\n",
      "Epoch 482/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8488\n",
      "Epoch 00482: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3392 - accuracy: 0.8490 - val_loss: 0.3947 - val_accuracy: 0.8171\n",
      "Epoch 483/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8477\n",
      "Epoch 00483: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3409 - accuracy: 0.8474 - val_loss: 0.3974 - val_accuracy: 0.8138\n",
      "Epoch 484/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8435\n",
      "Epoch 00484: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3455 - accuracy: 0.8436 - val_loss: 0.3851 - val_accuracy: 0.8194\n",
      "Epoch 485/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8447\n",
      "Epoch 00485: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3416 - accuracy: 0.8450 - val_loss: 0.3914 - val_accuracy: 0.8214\n",
      "Epoch 486/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8441\n",
      "Epoch 00486: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3412 - accuracy: 0.8442 - val_loss: 0.4228 - val_accuracy: 0.8003\n",
      "Epoch 487/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8428\n",
      "Epoch 00487: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3439 - accuracy: 0.8428 - val_loss: 0.3819 - val_accuracy: 0.8250\n",
      "Epoch 488/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8477\n",
      "Epoch 00488: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3394 - accuracy: 0.8479 - val_loss: 0.4003 - val_accuracy: 0.8110\n",
      "Epoch 489/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8490\n",
      "Epoch 00489: val_loss did not improve from 0.38008\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3374 - accuracy: 0.8490 - val_loss: 0.3849 - val_accuracy: 0.8218\n",
      "Epoch 490/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8499\n",
      "Epoch 00490: val_loss improved from 0.38008 to 0.37859, saving model to pickled_objects/batch_size_256_lr_0.02_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3366 - accuracy: 0.8499 - val_loss: 0.3786 - val_accuracy: 0.8274\n",
      "Epoch 491/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8471\n",
      "Epoch 00491: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3361 - accuracy: 0.8471 - val_loss: 0.3986 - val_accuracy: 0.8132\n",
      "Epoch 492/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8497\n",
      "Epoch 00492: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3352 - accuracy: 0.8493 - val_loss: 0.3992 - val_accuracy: 0.8106\n",
      "Epoch 493/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8467\n",
      "Epoch 00493: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3348 - accuracy: 0.8466 - val_loss: 0.3872 - val_accuracy: 0.8220\n",
      "Epoch 494/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8500\n",
      "Epoch 00494: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3364 - accuracy: 0.8496 - val_loss: 0.3823 - val_accuracy: 0.8222\n",
      "Epoch 495/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8493\n",
      "Epoch 00495: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3376 - accuracy: 0.8487 - val_loss: 0.3844 - val_accuracy: 0.8218\n",
      "Epoch 496/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8496\n",
      "Epoch 00496: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3352 - accuracy: 0.8499 - val_loss: 0.3810 - val_accuracy: 0.8239\n",
      "Epoch 497/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8519\n",
      "Epoch 00497: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3324 - accuracy: 0.8519 - val_loss: 0.4126 - val_accuracy: 0.8009\n",
      "Epoch 498/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8526\n",
      "Epoch 00498: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3295 - accuracy: 0.8526 - val_loss: 0.3826 - val_accuracy: 0.8252\n",
      "Epoch 499/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8498\n",
      "Epoch 00499: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3311 - accuracy: 0.8500 - val_loss: 0.3890 - val_accuracy: 0.8186\n",
      "Epoch 500/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8497\n",
      "Epoch 00500: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3344 - accuracy: 0.8495 - val_loss: 0.3786 - val_accuracy: 0.8265\n",
      "Epoch 501/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8503\n",
      "Epoch 00501: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3340 - accuracy: 0.8503 - val_loss: 0.4061 - val_accuracy: 0.8100\n",
      "Epoch 502/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8497\n",
      "Epoch 00502: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3338 - accuracy: 0.8493 - val_loss: 0.3906 - val_accuracy: 0.8186\n",
      "Epoch 503/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8506\n",
      "Epoch 00503: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3313 - accuracy: 0.8507 - val_loss: 0.3988 - val_accuracy: 0.8121\n",
      "Epoch 504/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8529\n",
      "Epoch 00504: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3313 - accuracy: 0.8528 - val_loss: 0.3907 - val_accuracy: 0.8203\n",
      "Epoch 505/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8536\n",
      "Epoch 00505: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3294 - accuracy: 0.8535 - val_loss: 0.3896 - val_accuracy: 0.8199\n",
      "Epoch 506/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8529\n",
      "Epoch 00506: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3290 - accuracy: 0.8531 - val_loss: 0.4089 - val_accuracy: 0.8050\n",
      "Epoch 507/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3263 - accuracy: 0.8511\n",
      "Epoch 00507: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3263 - accuracy: 0.8513 - val_loss: 0.3841 - val_accuracy: 0.8186\n",
      "Epoch 508/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8519\n",
      "Epoch 00508: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3299 - accuracy: 0.8523 - val_loss: 0.4239 - val_accuracy: 0.7966\n",
      "Epoch 509/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8522\n",
      "Epoch 00509: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3317 - accuracy: 0.8526 - val_loss: 0.3947 - val_accuracy: 0.8181\n",
      "Epoch 510/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8529\n",
      "Epoch 00510: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3306 - accuracy: 0.8522 - val_loss: 0.4080 - val_accuracy: 0.8072\n",
      "Epoch 511/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8520\n",
      "Epoch 00511: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3314 - accuracy: 0.8523 - val_loss: 0.3837 - val_accuracy: 0.8239\n",
      "Epoch 512/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8507\n",
      "Epoch 00512: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3313 - accuracy: 0.8508 - val_loss: 0.4017 - val_accuracy: 0.8102\n",
      "Epoch 513/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8580\n",
      "Epoch 00513: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3228 - accuracy: 0.8580 - val_loss: 0.3932 - val_accuracy: 0.8160\n",
      "Epoch 514/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8552\n",
      "Epoch 00514: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3241 - accuracy: 0.8555 - val_loss: 0.3932 - val_accuracy: 0.8149\n",
      "Epoch 515/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8535\n",
      "Epoch 00515: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3275 - accuracy: 0.8534 - val_loss: 0.3878 - val_accuracy: 0.8177\n",
      "Epoch 516/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8547\n",
      "Epoch 00516: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3243 - accuracy: 0.8546 - val_loss: 0.3888 - val_accuracy: 0.8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8563\n",
      "Epoch 00517: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3214 - accuracy: 0.8562 - val_loss: 0.3871 - val_accuracy: 0.8205\n",
      "Epoch 518/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8552\n",
      "Epoch 00518: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3271 - accuracy: 0.8551 - val_loss: 0.4172 - val_accuracy: 0.8033\n",
      "Epoch 519/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8555\n",
      "Epoch 00519: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3217 - accuracy: 0.8556 - val_loss: 0.3925 - val_accuracy: 0.8188\n",
      "Epoch 520/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8560\n",
      "Epoch 00520: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3264 - accuracy: 0.8559 - val_loss: 0.3959 - val_accuracy: 0.8173\n",
      "Epoch 521/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8536\n",
      "Epoch 00521: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3245 - accuracy: 0.8539 - val_loss: 0.4093 - val_accuracy: 0.8063\n",
      "Epoch 522/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8548\n",
      "Epoch 00522: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3264 - accuracy: 0.8549 - val_loss: 0.3994 - val_accuracy: 0.8126\n",
      "Epoch 523/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8580\n",
      "Epoch 00523: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3185 - accuracy: 0.8582 - val_loss: 0.3888 - val_accuracy: 0.8181\n",
      "Epoch 524/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8585\n",
      "Epoch 00524: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3201 - accuracy: 0.8581 - val_loss: 0.3891 - val_accuracy: 0.8190\n",
      "Epoch 525/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8538\n",
      "Epoch 00525: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3234 - accuracy: 0.8539 - val_loss: 0.3907 - val_accuracy: 0.8179\n",
      "Epoch 526/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8553\n",
      "Epoch 00526: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3240 - accuracy: 0.8553 - val_loss: 0.3952 - val_accuracy: 0.8141\n",
      "Epoch 527/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8569\n",
      "Epoch 00527: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3208 - accuracy: 0.8567 - val_loss: 0.4507 - val_accuracy: 0.7915\n",
      "Epoch 528/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8560\n",
      "Epoch 00528: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3245 - accuracy: 0.8560 - val_loss: 0.4056 - val_accuracy: 0.8083\n",
      "Epoch 529/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8553\n",
      "Epoch 00529: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3271 - accuracy: 0.8550 - val_loss: 0.4253 - val_accuracy: 0.8007\n",
      "Epoch 530/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8569\n",
      "Epoch 00530: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3206 - accuracy: 0.8566 - val_loss: 0.3917 - val_accuracy: 0.8192\n",
      "Epoch 531/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8564\n",
      "Epoch 00531: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3222 - accuracy: 0.8562 - val_loss: 0.3883 - val_accuracy: 0.8201\n",
      "Epoch 532/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3166 - accuracy: 0.8593\n",
      "Epoch 00532: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3169 - accuracy: 0.8593 - val_loss: 0.3879 - val_accuracy: 0.8196\n",
      "Epoch 533/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8568\n",
      "Epoch 00533: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3225 - accuracy: 0.8566 - val_loss: 0.3945 - val_accuracy: 0.8166\n",
      "Epoch 534/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8578\n",
      "Epoch 00534: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3171 - accuracy: 0.8580 - val_loss: 0.3998 - val_accuracy: 0.8138\n",
      "Epoch 535/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8592\n",
      "Epoch 00535: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3153 - accuracy: 0.8590 - val_loss: 0.3927 - val_accuracy: 0.8160\n",
      "Epoch 536/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8589\n",
      "Epoch 00536: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3179 - accuracy: 0.8589 - val_loss: 0.4349 - val_accuracy: 0.7969\n",
      "Epoch 537/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8572\n",
      "Epoch 00537: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3192 - accuracy: 0.8572 - val_loss: 0.4090 - val_accuracy: 0.8083\n",
      "Epoch 538/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8551\n",
      "Epoch 00538: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3201 - accuracy: 0.8554 - val_loss: 0.4168 - val_accuracy: 0.8037\n",
      "Epoch 539/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8580\n",
      "Epoch 00539: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3181 - accuracy: 0.8578 - val_loss: 0.4158 - val_accuracy: 0.8020\n",
      "Epoch 540/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8568\n",
      "Epoch 00540: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3178 - accuracy: 0.8567 - val_loss: 0.3868 - val_accuracy: 0.8218\n",
      "Epoch 541/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8592\n",
      "Epoch 00541: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3137 - accuracy: 0.8595 - val_loss: 0.4259 - val_accuracy: 0.7997\n",
      "Epoch 542/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8575\n",
      "Epoch 00542: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3208 - accuracy: 0.8574 - val_loss: 0.3888 - val_accuracy: 0.8186\n",
      "Epoch 543/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8611\n",
      "Epoch 00543: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3159 - accuracy: 0.8610 - val_loss: 0.3911 - val_accuracy: 0.8164\n",
      "Epoch 544/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8614\n",
      "Epoch 00544: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3142 - accuracy: 0.8612 - val_loss: 0.3861 - val_accuracy: 0.8181\n",
      "Epoch 545/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8614\n",
      "Epoch 00545: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3134 - accuracy: 0.8614 - val_loss: 0.4110 - val_accuracy: 0.8093\n",
      "Epoch 546/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8611\n",
      "Epoch 00546: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3109 - accuracy: 0.8615 - val_loss: 0.4254 - val_accuracy: 0.8025\n",
      "Epoch 547/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8607\n",
      "Epoch 00547: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3100 - accuracy: 0.8607 - val_loss: 0.4269 - val_accuracy: 0.8007\n",
      "Epoch 548/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8594\n",
      "Epoch 00548: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3142 - accuracy: 0.8597 - val_loss: 0.3957 - val_accuracy: 0.8184\n",
      "Epoch 549/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8592\n",
      "Epoch 00549: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3155 - accuracy: 0.8593 - val_loss: 0.3832 - val_accuracy: 0.8188\n",
      "Epoch 550/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8592\n",
      "Epoch 00550: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3130 - accuracy: 0.8591 - val_loss: 0.3832 - val_accuracy: 0.8224\n",
      "Epoch 551/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8624\n",
      "Epoch 00551: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3118 - accuracy: 0.8625 - val_loss: 0.4106 - val_accuracy: 0.8091\n",
      "Epoch 552/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8612\n",
      "Epoch 00552: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3138 - accuracy: 0.8611 - val_loss: 0.4357 - val_accuracy: 0.7960\n",
      "Epoch 553/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8626\n",
      "Epoch 00553: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3112 - accuracy: 0.8625 - val_loss: 0.3907 - val_accuracy: 0.8186\n",
      "Epoch 554/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8617\n",
      "Epoch 00554: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3103 - accuracy: 0.8615 - val_loss: 0.3871 - val_accuracy: 0.8216\n",
      "Epoch 555/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8633\n",
      "Epoch 00555: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3086 - accuracy: 0.8632 - val_loss: 0.4406 - val_accuracy: 0.7949\n",
      "Epoch 556/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8632\n",
      "Epoch 00556: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3095 - accuracy: 0.8632 - val_loss: 0.4002 - val_accuracy: 0.8126\n",
      "Epoch 557/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8623\n",
      "Epoch 00557: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3089 - accuracy: 0.8624 - val_loss: 0.3954 - val_accuracy: 0.8153\n",
      "Epoch 558/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8630\n",
      "Epoch 00558: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3055 - accuracy: 0.8632 - val_loss: 0.4084 - val_accuracy: 0.8121\n",
      "Epoch 559/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8597\n",
      "Epoch 00559: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3095 - accuracy: 0.8601 - val_loss: 0.3962 - val_accuracy: 0.8156\n",
      "Epoch 560/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8613\n",
      "Epoch 00560: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3089 - accuracy: 0.8614 - val_loss: 0.3954 - val_accuracy: 0.8149\n",
      "Epoch 561/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8634\n",
      "Epoch 00561: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3083 - accuracy: 0.8634 - val_loss: 0.3804 - val_accuracy: 0.8270\n",
      "Epoch 562/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8634\n",
      "Epoch 00562: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3087 - accuracy: 0.8631 - val_loss: 0.3985 - val_accuracy: 0.8132\n",
      "Epoch 563/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8630\n",
      "Epoch 00563: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3036 - accuracy: 0.8628 - val_loss: 0.4022 - val_accuracy: 0.8134\n",
      "Epoch 564/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8677\n",
      "Epoch 00564: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3053 - accuracy: 0.8678 - val_loss: 0.3886 - val_accuracy: 0.8233\n",
      "Epoch 565/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8678\n",
      "Epoch 00565: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3005 - accuracy: 0.8679 - val_loss: 0.4001 - val_accuracy: 0.8181\n",
      "Epoch 566/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8656\n",
      "Epoch 00566: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3097 - accuracy: 0.8657 - val_loss: 0.4063 - val_accuracy: 0.8138\n",
      "Epoch 567/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8623\n",
      "Epoch 00567: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3056 - accuracy: 0.8627 - val_loss: 0.3850 - val_accuracy: 0.8227\n",
      "Epoch 568/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8667\n",
      "Epoch 00568: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3043 - accuracy: 0.8671 - val_loss: 0.4621 - val_accuracy: 0.7835\n",
      "Epoch 569/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8655\n",
      "Epoch 00569: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3038 - accuracy: 0.8656 - val_loss: 0.3916 - val_accuracy: 0.8171\n",
      "Epoch 570/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8632\n",
      "Epoch 00570: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3049 - accuracy: 0.8634 - val_loss: 0.3859 - val_accuracy: 0.8207\n",
      "Epoch 571/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8660\n",
      "Epoch 00571: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3056 - accuracy: 0.8660 - val_loss: 0.4049 - val_accuracy: 0.8134\n",
      "Epoch 572/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.8644\n",
      "Epoch 00572: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3009 - accuracy: 0.8644 - val_loss: 0.3947 - val_accuracy: 0.8162\n",
      "Epoch 573/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8649\n",
      "Epoch 00573: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3050 - accuracy: 0.8651 - val_loss: 0.4198 - val_accuracy: 0.8063\n",
      "Epoch 574/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8664\n",
      "Epoch 00574: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3031 - accuracy: 0.8665 - val_loss: 0.4054 - val_accuracy: 0.8128\n",
      "Epoch 575/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8656\n",
      "Epoch 00575: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3030 - accuracy: 0.8658 - val_loss: 0.4543 - val_accuracy: 0.7865\n",
      "Epoch 576/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8602\n",
      "Epoch 00576: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3101 - accuracy: 0.8604 - val_loss: 0.4062 - val_accuracy: 0.8102\n",
      "Epoch 577/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8668\n",
      "Epoch 00577: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3024 - accuracy: 0.8671 - val_loss: 0.4177 - val_accuracy: 0.8059\n",
      "Epoch 578/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8678\n",
      "Epoch 00578: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2978 - accuracy: 0.8675 - val_loss: 0.3875 - val_accuracy: 0.8203\n",
      "Epoch 579/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8670\n",
      "Epoch 00579: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3023 - accuracy: 0.8674 - val_loss: 0.4052 - val_accuracy: 0.8123\n",
      "Epoch 580/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8705\n",
      "Epoch 00580: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2978 - accuracy: 0.8704 - val_loss: 0.4127 - val_accuracy: 0.8089\n",
      "Epoch 581/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8697\n",
      "Epoch 00581: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.2955 - accuracy: 0.8696 - val_loss: 0.3832 - val_accuracy: 0.8212\n",
      "Epoch 582/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8686\n",
      "Epoch 00582: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2961 - accuracy: 0.8689 - val_loss: 0.4076 - val_accuracy: 0.8095\n",
      "Epoch 583/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8663\n",
      "Epoch 00583: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3021 - accuracy: 0.8665 - val_loss: 0.4125 - val_accuracy: 0.8072\n",
      "Epoch 584/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8662\n",
      "Epoch 00584: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3020 - accuracy: 0.8663 - val_loss: 0.4348 - val_accuracy: 0.7962\n",
      "Epoch 585/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8675\n",
      "Epoch 00585: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2938 - accuracy: 0.8678 - val_loss: 0.3935 - val_accuracy: 0.8162\n",
      "Epoch 586/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8673\n",
      "Epoch 00586: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2936 - accuracy: 0.8674 - val_loss: 0.3940 - val_accuracy: 0.8212\n",
      "Epoch 587/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8677\n",
      "Epoch 00587: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2972 - accuracy: 0.8680 - val_loss: 0.3959 - val_accuracy: 0.8166\n",
      "Epoch 588/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8716\n",
      "Epoch 00588: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2952 - accuracy: 0.8716 - val_loss: 0.3987 - val_accuracy: 0.8177\n",
      "Epoch 589/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8713\n",
      "Epoch 00589: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2913 - accuracy: 0.8716 - val_loss: 0.4261 - val_accuracy: 0.8033\n",
      "Epoch 590/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8714\n",
      "Epoch 00590: val_loss did not improve from 0.37859\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2917 - accuracy: 0.8717 - val_loss: 0.3987 - val_accuracy: 0.8179\n",
      "Epoch 00590: early stopping\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 8s 106ms/step - loss: 0.6933 - accuracy: 0.5059\n",
      "Epoch 00001: val_loss improved from inf to 0.69318, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.4899\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5350\n",
      "Epoch 00002: val_loss improved from 0.69318 to 0.69248, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6914 - accuracy: 0.5349 - val_loss: 0.6925 - val_accuracy: 0.4918\n",
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5576\n",
      "Epoch 00003: val_loss improved from 0.69248 to 0.69187, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6888 - accuracy: 0.5578 - val_loss: 0.6919 - val_accuracy: 0.4938\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5720\n",
      "Epoch 00004: val_loss did not improve from 0.69187\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6849 - accuracy: 0.5717 - val_loss: 0.6934 - val_accuracy: 0.4938\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5805\n",
      "Epoch 00005: val_loss did not improve from 0.69187\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6807 - accuracy: 0.5801 - val_loss: 0.6936 - val_accuracy: 0.4974\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.5819\n",
      "Epoch 00006: val_loss did not improve from 0.69187\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6773 - accuracy: 0.5823 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.5782\n",
      "Epoch 00007: val_loss did not improve from 0.69187\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6752 - accuracy: 0.5786 - val_loss: 0.6920 - val_accuracy: 0.5105\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.5887\n",
      "Epoch 00008: val_loss did not improve from 0.69187\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6731 - accuracy: 0.5881 - val_loss: 0.7069 - val_accuracy: 0.4979\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6715 - accuracy: 0.5867\n",
      "Epoch 00009: val_loss improved from 0.69187 to 0.68713, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6714 - accuracy: 0.5871 - val_loss: 0.6871 - val_accuracy: 0.5327\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6695 - accuracy: 0.5918\n",
      "Epoch 00010: val_loss improved from 0.68713 to 0.67948, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6693 - accuracy: 0.5921 - val_loss: 0.6795 - val_accuracy: 0.5604\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6665 - accuracy: 0.5953\n",
      "Epoch 00011: val_loss improved from 0.67948 to 0.67242, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6661 - accuracy: 0.5959 - val_loss: 0.6724 - val_accuracy: 0.5776\n",
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6657 - accuracy: 0.5966\n",
      "Epoch 00012: val_loss improved from 0.67242 to 0.67055, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6653 - accuracy: 0.5973 - val_loss: 0.6706 - val_accuracy: 0.5798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6628 - accuracy: 0.6017\n",
      "Epoch 00013: val_loss improved from 0.67055 to 0.66391, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6625 - accuracy: 0.6020 - val_loss: 0.6639 - val_accuracy: 0.5942\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.6045\n",
      "Epoch 00014: val_loss did not improve from 0.66391\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6585 - accuracy: 0.6048 - val_loss: 0.6654 - val_accuracy: 0.5894\n",
      "Epoch 15/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.6057\n",
      "Epoch 00015: val_loss improved from 0.66391 to 0.65509, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6562 - accuracy: 0.6062 - val_loss: 0.6551 - val_accuracy: 0.6079\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6522 - accuracy: 0.6147\n",
      "Epoch 00016: val_loss improved from 0.65509 to 0.65057, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6519 - accuracy: 0.6149 - val_loss: 0.6506 - val_accuracy: 0.6113\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6502 - accuracy: 0.6153\n",
      "Epoch 00017: val_loss did not improve from 0.65057\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6498 - accuracy: 0.6154 - val_loss: 0.6617 - val_accuracy: 0.5933\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6498 - accuracy: 0.6170\n",
      "Epoch 00018: val_loss improved from 0.65057 to 0.64882, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6494 - accuracy: 0.6171 - val_loss: 0.6488 - val_accuracy: 0.6148\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6467 - accuracy: 0.6217\n",
      "Epoch 00019: val_loss did not improve from 0.64882\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6466 - accuracy: 0.6219 - val_loss: 0.6513 - val_accuracy: 0.6090\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6445 - accuracy: 0.6224\n",
      "Epoch 00020: val_loss did not improve from 0.64882\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6445 - accuracy: 0.6226 - val_loss: 0.6535 - val_accuracy: 0.6045\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6426 - accuracy: 0.6249\n",
      "Epoch 00021: val_loss did not improve from 0.64882\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6428 - accuracy: 0.6249 - val_loss: 0.6504 - val_accuracy: 0.6098\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6398 - accuracy: 0.6303\n",
      "Epoch 00022: val_loss improved from 0.64882 to 0.64065, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6397 - accuracy: 0.6306 - val_loss: 0.6406 - val_accuracy: 0.6232\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6375 - accuracy: 0.6302\n",
      "Epoch 00023: val_loss improved from 0.64065 to 0.63558, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6375 - accuracy: 0.6300 - val_loss: 0.6356 - val_accuracy: 0.6301\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6349 - accuracy: 0.6334\n",
      "Epoch 00024: val_loss improved from 0.63558 to 0.63044, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6348 - accuracy: 0.6337 - val_loss: 0.6304 - val_accuracy: 0.6363\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.6388\n",
      "Epoch 00025: val_loss did not improve from 0.63044\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6319 - accuracy: 0.6391 - val_loss: 0.6331 - val_accuracy: 0.6331\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.6446\n",
      "Epoch 00026: val_loss did not improve from 0.63044\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6281 - accuracy: 0.6447 - val_loss: 0.6350 - val_accuracy: 0.6292\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6209 - accuracy: 0.6494\n",
      "Epoch 00027: val_loss improved from 0.63044 to 0.62997, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6211 - accuracy: 0.6494 - val_loss: 0.6300 - val_accuracy: 0.6361\n",
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6154 - accuracy: 0.6570\n",
      "Epoch 00028: val_loss did not improve from 0.62997\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6153 - accuracy: 0.6570 - val_loss: 0.6343 - val_accuracy: 0.6318\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6089 - accuracy: 0.6612\n",
      "Epoch 00029: val_loss improved from 0.62997 to 0.62804, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6092 - accuracy: 0.6610 - val_loss: 0.6280 - val_accuracy: 0.6361\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.6680\n",
      "Epoch 00030: val_loss did not improve from 0.62804\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6038 - accuracy: 0.6679 - val_loss: 0.6353 - val_accuracy: 0.6313\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5993 - accuracy: 0.6757\n",
      "Epoch 00031: val_loss improved from 0.62804 to 0.62534, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5998 - accuracy: 0.6753 - val_loss: 0.6253 - val_accuracy: 0.6382\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.6803\n",
      "Epoch 00032: val_loss improved from 0.62534 to 0.60997, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5948 - accuracy: 0.6798 - val_loss: 0.6100 - val_accuracy: 0.6535\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.6847\n",
      "Epoch 00033: val_loss did not improve from 0.60997\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5935 - accuracy: 0.6838 - val_loss: 0.6239 - val_accuracy: 0.6425\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.6861\n",
      "Epoch 00034: val_loss improved from 0.60997 to 0.59919, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5890 - accuracy: 0.6857 - val_loss: 0.5992 - val_accuracy: 0.6606\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5819 - accuracy: 0.6877\n",
      "Epoch 00035: val_loss improved from 0.59919 to 0.59111, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5825 - accuracy: 0.6871 - val_loss: 0.5911 - val_accuracy: 0.6702\n",
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.6972\n",
      "Epoch 00036: val_loss improved from 0.59111 to 0.58760, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5796 - accuracy: 0.6960 - val_loss: 0.5876 - val_accuracy: 0.6750\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5754 - accuracy: 0.6988\n",
      "Epoch 00037: val_loss improved from 0.58760 to 0.58667, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5758 - accuracy: 0.6981 - val_loss: 0.5867 - val_accuracy: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7021\n",
      "Epoch 00038: val_loss improved from 0.58667 to 0.57060, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5683 - accuracy: 0.7013 - val_loss: 0.5706 - val_accuracy: 0.6905\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7042\n",
      "Epoch 00039: val_loss improved from 0.57060 to 0.55211, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5670 - accuracy: 0.7033 - val_loss: 0.5521 - val_accuracy: 0.7122\n",
      "Epoch 40/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7083\n",
      "Epoch 00040: val_loss improved from 0.55211 to 0.54958, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5652 - accuracy: 0.7075 - val_loss: 0.5496 - val_accuracy: 0.7242\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7113\n",
      "Epoch 00041: val_loss did not improve from 0.54958\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5614 - accuracy: 0.7105 - val_loss: 0.5825 - val_accuracy: 0.6836\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5593 - accuracy: 0.7104\n",
      "Epoch 00042: val_loss did not improve from 0.54958\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5594 - accuracy: 0.7104 - val_loss: 0.5569 - val_accuracy: 0.7006\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.7123\n",
      "Epoch 00043: val_loss did not improve from 0.54958\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5587 - accuracy: 0.7113 - val_loss: 0.5569 - val_accuracy: 0.7027\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7146\n",
      "Epoch 00044: val_loss improved from 0.54958 to 0.54780, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5549 - accuracy: 0.7138 - val_loss: 0.5478 - val_accuracy: 0.7135\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7172\n",
      "Epoch 00045: val_loss improved from 0.54780 to 0.53352, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5513 - accuracy: 0.7167 - val_loss: 0.5335 - val_accuracy: 0.7255\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5480 - accuracy: 0.7181\n",
      "Epoch 00046: val_loss improved from 0.53352 to 0.52743, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5488 - accuracy: 0.7171 - val_loss: 0.5274 - val_accuracy: 0.7358\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5476 - accuracy: 0.7180\n",
      "Epoch 00047: val_loss improved from 0.52743 to 0.51959, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5483 - accuracy: 0.7171 - val_loss: 0.5196 - val_accuracy: 0.7388\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7220\n",
      "Epoch 00048: val_loss did not improve from 0.51959\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5464 - accuracy: 0.7212 - val_loss: 0.5371 - val_accuracy: 0.7313\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5404 - accuracy: 0.7247\n",
      "Epoch 00049: val_loss did not improve from 0.51959\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5412 - accuracy: 0.7244 - val_loss: 0.5272 - val_accuracy: 0.7296\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7233\n",
      "Epoch 00050: val_loss did not improve from 0.51959\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5407 - accuracy: 0.7225 - val_loss: 0.5225 - val_accuracy: 0.7345\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7223\n",
      "Epoch 00051: val_loss did not improve from 0.51959\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5407 - accuracy: 0.7214 - val_loss: 0.5370 - val_accuracy: 0.7214\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.7321\n",
      "Epoch 00052: val_loss did not improve from 0.51959\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5364 - accuracy: 0.7313 - val_loss: 0.5642 - val_accuracy: 0.6971\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7222\n",
      "Epoch 00053: val_loss improved from 0.51959 to 0.51904, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5384 - accuracy: 0.7213 - val_loss: 0.5190 - val_accuracy: 0.7429\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5313 - accuracy: 0.7306\n",
      "Epoch 00054: val_loss did not improve from 0.51904\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5321 - accuracy: 0.7300 - val_loss: 0.5206 - val_accuracy: 0.7433\n",
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5300 - accuracy: 0.7307\n",
      "Epoch 00055: val_loss improved from 0.51904 to 0.51573, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5310 - accuracy: 0.7301 - val_loss: 0.5157 - val_accuracy: 0.7334\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5273 - accuracy: 0.7326\n",
      "Epoch 00056: val_loss improved from 0.51573 to 0.50790, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5281 - accuracy: 0.7319 - val_loss: 0.5079 - val_accuracy: 0.7506\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7348\n",
      "Epoch 00057: val_loss improved from 0.50790 to 0.50312, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5253 - accuracy: 0.7344 - val_loss: 0.5031 - val_accuracy: 0.7470\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7377\n",
      "Epoch 00058: val_loss did not improve from 0.50312\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5249 - accuracy: 0.7368 - val_loss: 0.5171 - val_accuracy: 0.7377\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7325\n",
      "Epoch 00059: val_loss did not improve from 0.50312\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5249 - accuracy: 0.7319 - val_loss: 0.5209 - val_accuracy: 0.7304\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7381\n",
      "Epoch 00060: val_loss did not improve from 0.50312\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5199 - accuracy: 0.7375 - val_loss: 0.5203 - val_accuracy: 0.7264\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5181 - accuracy: 0.7427\n",
      "Epoch 00061: val_loss did not improve from 0.50312\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5188 - accuracy: 0.7424 - val_loss: 0.5116 - val_accuracy: 0.7332\n",
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5164 - accuracy: 0.7427\n",
      "Epoch 00062: val_loss did not improve from 0.50312\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5172 - accuracy: 0.7422 - val_loss: 0.5299 - val_accuracy: 0.7259\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7438\n",
      "Epoch 00063: val_loss improved from 0.50312 to 0.49771, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5120 - accuracy: 0.7435 - val_loss: 0.4977 - val_accuracy: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5129 - accuracy: 0.7464\n",
      "Epoch 00064: val_loss improved from 0.49771 to 0.49513, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5131 - accuracy: 0.7461 - val_loss: 0.4951 - val_accuracy: 0.7515\n",
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5105 - accuracy: 0.7442\n",
      "Epoch 00065: val_loss did not improve from 0.49513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5116 - accuracy: 0.7434 - val_loss: 0.5256 - val_accuracy: 0.7276\n",
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5070 - accuracy: 0.7499\n",
      "Epoch 00066: val_loss did not improve from 0.49513\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5076 - accuracy: 0.7494 - val_loss: 0.5318 - val_accuracy: 0.7253\n",
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7491\n",
      "Epoch 00067: val_loss did not improve from 0.49513\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5065 - accuracy: 0.7490 - val_loss: 0.4976 - val_accuracy: 0.7539\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7528\n",
      "Epoch 00068: val_loss improved from 0.49513 to 0.49359, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5027 - accuracy: 0.7523 - val_loss: 0.4936 - val_accuracy: 0.7607\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5055 - accuracy: 0.7509\n",
      "Epoch 00069: val_loss improved from 0.49359 to 0.48230, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5063 - accuracy: 0.7503 - val_loss: 0.4823 - val_accuracy: 0.7655\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4996 - accuracy: 0.7550\n",
      "Epoch 00070: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5003 - accuracy: 0.7544 - val_loss: 0.5213 - val_accuracy: 0.7354\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7601\n",
      "Epoch 00071: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4970 - accuracy: 0.7594 - val_loss: 0.4871 - val_accuracy: 0.7633\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4947 - accuracy: 0.7547\n",
      "Epoch 00072: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4957 - accuracy: 0.7541 - val_loss: 0.4836 - val_accuracy: 0.7700\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7584\n",
      "Epoch 00073: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4953 - accuracy: 0.7574 - val_loss: 0.4863 - val_accuracy: 0.7653\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.7609\n",
      "Epoch 00074: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4905 - accuracy: 0.7606 - val_loss: 0.4848 - val_accuracy: 0.7646\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4944 - accuracy: 0.7573\n",
      "Epoch 00075: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4949 - accuracy: 0.7571 - val_loss: 0.4956 - val_accuracy: 0.7567\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4894 - accuracy: 0.7594\n",
      "Epoch 00076: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4902 - accuracy: 0.7591 - val_loss: 0.5082 - val_accuracy: 0.7433\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4877 - accuracy: 0.7635\n",
      "Epoch 00077: val_loss did not improve from 0.48230\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4885 - accuracy: 0.7629 - val_loss: 0.4930 - val_accuracy: 0.7560\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4884 - accuracy: 0.7618\n",
      "Epoch 00078: val_loss improved from 0.48230 to 0.47932, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4887 - accuracy: 0.7614 - val_loss: 0.4793 - val_accuracy: 0.7747\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4846 - accuracy: 0.7644\n",
      "Epoch 00079: val_loss did not improve from 0.47932\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4851 - accuracy: 0.7640 - val_loss: 0.4984 - val_accuracy: 0.7494\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.7662\n",
      "Epoch 00080: val_loss did not improve from 0.47932\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4849 - accuracy: 0.7653 - val_loss: 0.4804 - val_accuracy: 0.7648\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4818 - accuracy: 0.7673\n",
      "Epoch 00081: val_loss improved from 0.47932 to 0.47666, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4824 - accuracy: 0.7670 - val_loss: 0.4767 - val_accuracy: 0.7689\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4777 - accuracy: 0.7696\n",
      "Epoch 00082: val_loss improved from 0.47666 to 0.46694, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4784 - accuracy: 0.7693 - val_loss: 0.4669 - val_accuracy: 0.7792\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7662\n",
      "Epoch 00083: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4787 - accuracy: 0.7658 - val_loss: 0.4877 - val_accuracy: 0.7584\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7676\n",
      "Epoch 00084: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4785 - accuracy: 0.7672 - val_loss: 0.4705 - val_accuracy: 0.7754\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.7701\n",
      "Epoch 00085: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4779 - accuracy: 0.7694 - val_loss: 0.4733 - val_accuracy: 0.7672\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.7722\n",
      "Epoch 00086: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4735 - accuracy: 0.7716 - val_loss: 0.4789 - val_accuracy: 0.7676\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7699\n",
      "Epoch 00087: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4724 - accuracy: 0.7692 - val_loss: 0.4714 - val_accuracy: 0.7734\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7702\n",
      "Epoch 00088: val_loss did not improve from 0.46694\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4710 - accuracy: 0.7696 - val_loss: 0.4801 - val_accuracy: 0.7668\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7714\n",
      "Epoch 00089: val_loss improved from 0.46694 to 0.46263, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4725 - accuracy: 0.7707 - val_loss: 0.4626 - val_accuracy: 0.7797\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.7756\n",
      "Epoch 00090: val_loss did not improve from 0.46263\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4676 - accuracy: 0.7753 - val_loss: 0.4666 - val_accuracy: 0.7736\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.7780\n",
      "Epoch 00091: val_loss did not improve from 0.46263\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4685 - accuracy: 0.7776 - val_loss: 0.4693 - val_accuracy: 0.7711\n",
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.7735\n",
      "Epoch 00092: val_loss did not improve from 0.46263\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4654 - accuracy: 0.7735 - val_loss: 0.4689 - val_accuracy: 0.7724\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7773\n",
      "Epoch 00093: val_loss did not improve from 0.46263\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4647 - accuracy: 0.7767 - val_loss: 0.4662 - val_accuracy: 0.7756\n",
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.7820\n",
      "Epoch 00094: val_loss improved from 0.46263 to 0.45858, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4589 - accuracy: 0.7818 - val_loss: 0.4586 - val_accuracy: 0.7833\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4604 - accuracy: 0.7796\n",
      "Epoch 00095: val_loss improved from 0.45858 to 0.45746, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4610 - accuracy: 0.7794 - val_loss: 0.4575 - val_accuracy: 0.7818\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4627 - accuracy: 0.7784\n",
      "Epoch 00096: val_loss improved from 0.45746 to 0.45743, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4630 - accuracy: 0.7779 - val_loss: 0.4574 - val_accuracy: 0.7794\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.7805\n",
      "Epoch 00097: val_loss did not improve from 0.45743\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4581 - accuracy: 0.7805 - val_loss: 0.4650 - val_accuracy: 0.7756\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.7829\n",
      "Epoch 00098: val_loss did not improve from 0.45743\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4605 - accuracy: 0.7825 - val_loss: 0.4608 - val_accuracy: 0.7799\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4579 - accuracy: 0.7831\n",
      "Epoch 00099: val_loss improved from 0.45743 to 0.45045, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4587 - accuracy: 0.7826 - val_loss: 0.4504 - val_accuracy: 0.7861\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4599 - accuracy: 0.7809\n",
      "Epoch 00100: val_loss did not improve from 0.45045\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4601 - accuracy: 0.7806 - val_loss: 0.4600 - val_accuracy: 0.7792\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4515 - accuracy: 0.7862\n",
      "Epoch 00101: val_loss did not improve from 0.45045\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4516 - accuracy: 0.7859 - val_loss: 0.4719 - val_accuracy: 0.7721\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.7847\n",
      "Epoch 00102: val_loss improved from 0.45045 to 0.44543, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4527 - accuracy: 0.7839 - val_loss: 0.4454 - val_accuracy: 0.7917\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7829\n",
      "Epoch 00103: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4559 - accuracy: 0.7822 - val_loss: 0.4594 - val_accuracy: 0.7829\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7864\n",
      "Epoch 00104: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4461 - accuracy: 0.7861 - val_loss: 0.4574 - val_accuracy: 0.7816\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7884\n",
      "Epoch 00105: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4470 - accuracy: 0.7882 - val_loss: 0.4485 - val_accuracy: 0.7855\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.7852\n",
      "Epoch 00106: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4519 - accuracy: 0.7850 - val_loss: 0.4546 - val_accuracy: 0.7814\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.7853\n",
      "Epoch 00107: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4498 - accuracy: 0.7847 - val_loss: 0.4479 - val_accuracy: 0.7855\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7930\n",
      "Epoch 00108: val_loss did not improve from 0.44543\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4426 - accuracy: 0.7931 - val_loss: 0.4676 - val_accuracy: 0.7745\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.7932\n",
      "Epoch 00109: val_loss improved from 0.44543 to 0.43762, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4424 - accuracy: 0.7929 - val_loss: 0.4376 - val_accuracy: 0.7926\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7868\n",
      "Epoch 00110: val_loss improved from 0.43762 to 0.43399, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4471 - accuracy: 0.7868 - val_loss: 0.4340 - val_accuracy: 0.7930\n",
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.7932\n",
      "Epoch 00111: val_loss did not improve from 0.43399\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4422 - accuracy: 0.7926 - val_loss: 0.4521 - val_accuracy: 0.7814\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.7951\n",
      "Epoch 00112: val_loss did not improve from 0.43399\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4387 - accuracy: 0.7948 - val_loss: 0.4354 - val_accuracy: 0.7930\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4403 - accuracy: 0.7923\n",
      "Epoch 00113: val_loss did not improve from 0.43399\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4402 - accuracy: 0.7923 - val_loss: 0.4566 - val_accuracy: 0.7797\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.7965\n",
      "Epoch 00114: val_loss improved from 0.43399 to 0.43240, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4388 - accuracy: 0.7965 - val_loss: 0.4324 - val_accuracy: 0.7971\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.7928\n",
      "Epoch 00115: val_loss did not improve from 0.43240\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4394 - accuracy: 0.7929 - val_loss: 0.4651 - val_accuracy: 0.7803\n",
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4383 - accuracy: 0.7947\n",
      "Epoch 00116: val_loss improved from 0.43240 to 0.42912, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4387 - accuracy: 0.7944 - val_loss: 0.4291 - val_accuracy: 0.7958\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4394 - accuracy: 0.7961\n",
      "Epoch 00117: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4395 - accuracy: 0.7962 - val_loss: 0.4449 - val_accuracy: 0.7853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.7941\n",
      "Epoch 00118: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4348 - accuracy: 0.7937 - val_loss: 0.4327 - val_accuracy: 0.7921\n",
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.7961\n",
      "Epoch 00119: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4350 - accuracy: 0.7957 - val_loss: 0.4301 - val_accuracy: 0.7966\n",
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.7958\n",
      "Epoch 00120: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4321 - accuracy: 0.7956 - val_loss: 0.4331 - val_accuracy: 0.7941\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7951\n",
      "Epoch 00121: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4342 - accuracy: 0.7951 - val_loss: 0.4325 - val_accuracy: 0.7939\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.7996\n",
      "Epoch 00122: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4278 - accuracy: 0.7992 - val_loss: 0.4330 - val_accuracy: 0.7928\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.7987\n",
      "Epoch 00123: val_loss did not improve from 0.42912\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4319 - accuracy: 0.7984 - val_loss: 0.4423 - val_accuracy: 0.7878\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.8018\n",
      "Epoch 00124: val_loss improved from 0.42912 to 0.42579, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4261 - accuracy: 0.8016 - val_loss: 0.4258 - val_accuracy: 0.7990\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8022\n",
      "Epoch 00125: val_loss did not improve from 0.42579\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4252 - accuracy: 0.8020 - val_loss: 0.4447 - val_accuracy: 0.7857\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.7983\n",
      "Epoch 00126: val_loss did not improve from 0.42579\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4269 - accuracy: 0.7983 - val_loss: 0.4320 - val_accuracy: 0.7975\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8046\n",
      "Epoch 00127: val_loss did not improve from 0.42579\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4227 - accuracy: 0.8043 - val_loss: 0.4286 - val_accuracy: 0.7966\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8040\n",
      "Epoch 00128: val_loss improved from 0.42579 to 0.41907, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4213 - accuracy: 0.8036 - val_loss: 0.4191 - val_accuracy: 0.8063\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8054\n",
      "Epoch 00129: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4223 - accuracy: 0.8052 - val_loss: 0.4408 - val_accuracy: 0.7893\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8029\n",
      "Epoch 00130: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4226 - accuracy: 0.8025 - val_loss: 0.4255 - val_accuracy: 0.7969\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8039\n",
      "Epoch 00131: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4221 - accuracy: 0.8038 - val_loss: 0.4192 - val_accuracy: 0.8037\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8082\n",
      "Epoch 00132: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4209 - accuracy: 0.8083 - val_loss: 0.4210 - val_accuracy: 0.7986\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8067\n",
      "Epoch 00133: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4200 - accuracy: 0.8068 - val_loss: 0.4203 - val_accuracy: 0.8044\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4157 - accuracy: 0.8069\n",
      "Epoch 00134: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4157 - accuracy: 0.8070 - val_loss: 0.4348 - val_accuracy: 0.7930\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4179 - accuracy: 0.8050\n",
      "Epoch 00135: val_loss did not improve from 0.41907\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4180 - accuracy: 0.8048 - val_loss: 0.4226 - val_accuracy: 0.8009\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8129\n",
      "Epoch 00136: val_loss improved from 0.41907 to 0.41325, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4123 - accuracy: 0.8125 - val_loss: 0.4132 - val_accuracy: 0.8083\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8081\n",
      "Epoch 00137: val_loss improved from 0.41325 to 0.40809, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4133 - accuracy: 0.8076 - val_loss: 0.4081 - val_accuracy: 0.8121\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.8094\n",
      "Epoch 00138: val_loss did not improve from 0.40809\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4114 - accuracy: 0.8092 - val_loss: 0.4130 - val_accuracy: 0.8055\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8118\n",
      "Epoch 00139: val_loss improved from 0.40809 to 0.40615, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4135 - accuracy: 0.8113 - val_loss: 0.4062 - val_accuracy: 0.8121\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8078\n",
      "Epoch 00140: val_loss did not improve from 0.40615\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4111 - accuracy: 0.8073 - val_loss: 0.4183 - val_accuracy: 0.8095\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8103\n",
      "Epoch 00141: val_loss did not improve from 0.40615\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4140 - accuracy: 0.8098 - val_loss: 0.4117 - val_accuracy: 0.8040\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8142\n",
      "Epoch 00142: val_loss did not improve from 0.40615\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4066 - accuracy: 0.8143 - val_loss: 0.4231 - val_accuracy: 0.7982\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4088 - accuracy: 0.8156\n",
      "Epoch 00143: val_loss improved from 0.40615 to 0.40550, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4091 - accuracy: 0.8153 - val_loss: 0.4055 - val_accuracy: 0.8149\n",
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8140\n",
      "Epoch 00144: val_loss improved from 0.40550 to 0.40477, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4080 - accuracy: 0.8136 - val_loss: 0.4048 - val_accuracy: 0.8143\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4060 - accuracy: 0.8114\n",
      "Epoch 00145: val_loss did not improve from 0.40477\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4061 - accuracy: 0.8112 - val_loss: 0.4058 - val_accuracy: 0.8110\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8151\n",
      "Epoch 00146: val_loss improved from 0.40477 to 0.40154, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4047 - accuracy: 0.8148 - val_loss: 0.4015 - val_accuracy: 0.8136\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8129\n",
      "Epoch 00147: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4060 - accuracy: 0.8132 - val_loss: 0.4203 - val_accuracy: 0.7992\n",
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8135\n",
      "Epoch 00148: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4050 - accuracy: 0.8133 - val_loss: 0.4028 - val_accuracy: 0.8113\n",
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.8176\n",
      "Epoch 00149: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4020 - accuracy: 0.8175 - val_loss: 0.4070 - val_accuracy: 0.8083\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4042 - accuracy: 0.8128\n",
      "Epoch 00150: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4047 - accuracy: 0.8126 - val_loss: 0.4043 - val_accuracy: 0.8130\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8129\n",
      "Epoch 00151: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4040 - accuracy: 0.8131 - val_loss: 0.4100 - val_accuracy: 0.8067\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4013 - accuracy: 0.8148\n",
      "Epoch 00152: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4016 - accuracy: 0.8147 - val_loss: 0.4045 - val_accuracy: 0.8115\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8143\n",
      "Epoch 00153: val_loss did not improve from 0.40154\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4047 - accuracy: 0.8142 - val_loss: 0.4397 - val_accuracy: 0.7930\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8133\n",
      "Epoch 00154: val_loss improved from 0.40154 to 0.39996, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4003 - accuracy: 0.8132 - val_loss: 0.4000 - val_accuracy: 0.8201\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8193\n",
      "Epoch 00155: val_loss did not improve from 0.39996\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3981 - accuracy: 0.8192 - val_loss: 0.4160 - val_accuracy: 0.8057\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8134\n",
      "Epoch 00156: val_loss did not improve from 0.39996\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3997 - accuracy: 0.8133 - val_loss: 0.4053 - val_accuracy: 0.8132\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8180\n",
      "Epoch 00157: val_loss did not improve from 0.39996\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3962 - accuracy: 0.8176 - val_loss: 0.4101 - val_accuracy: 0.8100\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8230\n",
      "Epoch 00158: val_loss improved from 0.39996 to 0.39841, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3925 - accuracy: 0.8227 - val_loss: 0.3984 - val_accuracy: 0.8203\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3966 - accuracy: 0.8168\n",
      "Epoch 00159: val_loss did not improve from 0.39841\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3960 - accuracy: 0.8172 - val_loss: 0.4011 - val_accuracy: 0.8138\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8192\n",
      "Epoch 00160: val_loss improved from 0.39841 to 0.39524, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3970 - accuracy: 0.8186 - val_loss: 0.3952 - val_accuracy: 0.8194\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8169\n",
      "Epoch 00161: val_loss did not improve from 0.39524\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3949 - accuracy: 0.8162 - val_loss: 0.4003 - val_accuracy: 0.8153\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8237\n",
      "Epoch 00162: val_loss improved from 0.39524 to 0.39450, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3912 - accuracy: 0.8234 - val_loss: 0.3945 - val_accuracy: 0.8220\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8218\n",
      "Epoch 00163: val_loss did not improve from 0.39450\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3892 - accuracy: 0.8217 - val_loss: 0.3945 - val_accuracy: 0.8184\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8264\n",
      "Epoch 00164: val_loss improved from 0.39450 to 0.39395, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3891 - accuracy: 0.8260 - val_loss: 0.3940 - val_accuracy: 0.8188\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8205\n",
      "Epoch 00165: val_loss did not improve from 0.39395\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3892 - accuracy: 0.8204 - val_loss: 0.4026 - val_accuracy: 0.8164\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8225\n",
      "Epoch 00166: val_loss did not improve from 0.39395\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3884 - accuracy: 0.8225 - val_loss: 0.4091 - val_accuracy: 0.8102\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8248\n",
      "Epoch 00167: val_loss did not improve from 0.39395\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3877 - accuracy: 0.8244 - val_loss: 0.4045 - val_accuracy: 0.8130\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3879 - accuracy: 0.8235\n",
      "Epoch 00168: val_loss improved from 0.39395 to 0.39218, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3884 - accuracy: 0.8231 - val_loss: 0.3922 - val_accuracy: 0.8192\n",
      "Epoch 169/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8256\n",
      "Epoch 00169: val_loss did not improve from 0.39218\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3840 - accuracy: 0.8253 - val_loss: 0.3930 - val_accuracy: 0.8192\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8255\n",
      "Epoch 00170: val_loss did not improve from 0.39218\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3868 - accuracy: 0.8255 - val_loss: 0.3938 - val_accuracy: 0.8201\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8269\n",
      "Epoch 00171: val_loss improved from 0.39218 to 0.38963, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3824 - accuracy: 0.8267 - val_loss: 0.3896 - val_accuracy: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3833 - accuracy: 0.8252\n",
      "Epoch 00172: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3843 - accuracy: 0.8244 - val_loss: 0.3913 - val_accuracy: 0.8212\n",
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8253\n",
      "Epoch 00173: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3825 - accuracy: 0.8251 - val_loss: 0.4081 - val_accuracy: 0.8104\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8239\n",
      "Epoch 00174: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3828 - accuracy: 0.8240 - val_loss: 0.4025 - val_accuracy: 0.8136\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8261\n",
      "Epoch 00175: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3859 - accuracy: 0.8260 - val_loss: 0.4093 - val_accuracy: 0.8115\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8273\n",
      "Epoch 00176: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3843 - accuracy: 0.8272 - val_loss: 0.4002 - val_accuracy: 0.8158\n",
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8295\n",
      "Epoch 00177: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3783 - accuracy: 0.8294 - val_loss: 0.3919 - val_accuracy: 0.8184\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8271\n",
      "Epoch 00178: val_loss did not improve from 0.38963\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3820 - accuracy: 0.8268 - val_loss: 0.3957 - val_accuracy: 0.8184\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8294\n",
      "Epoch 00179: val_loss improved from 0.38963 to 0.38934, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3759 - accuracy: 0.8296 - val_loss: 0.3893 - val_accuracy: 0.8220\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8308\n",
      "Epoch 00180: val_loss did not improve from 0.38934\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3783 - accuracy: 0.8307 - val_loss: 0.3946 - val_accuracy: 0.8175\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8315\n",
      "Epoch 00181: val_loss did not improve from 0.38934\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3755 - accuracy: 0.8314 - val_loss: 0.3991 - val_accuracy: 0.8166\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8283\n",
      "Epoch 00182: val_loss improved from 0.38934 to 0.38464, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3768 - accuracy: 0.8283 - val_loss: 0.3846 - val_accuracy: 0.8239\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.8297\n",
      "Epoch 00183: val_loss did not improve from 0.38464\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3736 - accuracy: 0.8298 - val_loss: 0.3888 - val_accuracy: 0.8216\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8277\n",
      "Epoch 00184: val_loss did not improve from 0.38464\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3768 - accuracy: 0.8278 - val_loss: 0.3852 - val_accuracy: 0.8239\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8274\n",
      "Epoch 00185: val_loss did not improve from 0.38464\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3733 - accuracy: 0.8272 - val_loss: 0.3996 - val_accuracy: 0.8143\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.8320\n",
      "Epoch 00186: val_loss improved from 0.38464 to 0.38029, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3705 - accuracy: 0.8320 - val_loss: 0.3803 - val_accuracy: 0.8270\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8295\n",
      "Epoch 00187: val_loss did not improve from 0.38029\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3733 - accuracy: 0.8296 - val_loss: 0.3893 - val_accuracy: 0.8199\n",
      "Epoch 188/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8279\n",
      "Epoch 00188: val_loss did not improve from 0.38029\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3767 - accuracy: 0.8278 - val_loss: 0.4198 - val_accuracy: 0.8014\n",
      "Epoch 189/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8332\n",
      "Epoch 00189: val_loss did not improve from 0.38029\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3729 - accuracy: 0.8328 - val_loss: 0.3838 - val_accuracy: 0.8218\n",
      "Epoch 190/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8322\n",
      "Epoch 00190: val_loss did not improve from 0.38029\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3721 - accuracy: 0.8318 - val_loss: 0.3948 - val_accuracy: 0.8177\n",
      "Epoch 191/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8324\n",
      "Epoch 00191: val_loss improved from 0.38029 to 0.37950, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3688 - accuracy: 0.8325 - val_loss: 0.3795 - val_accuracy: 0.8261\n",
      "Epoch 192/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8328\n",
      "Epoch 00192: val_loss did not improve from 0.37950\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3696 - accuracy: 0.8328 - val_loss: 0.3824 - val_accuracy: 0.8267\n",
      "Epoch 193/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3675 - accuracy: 0.8340\n",
      "Epoch 00193: val_loss did not improve from 0.37950\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3675 - accuracy: 0.8339 - val_loss: 0.3874 - val_accuracy: 0.8235\n",
      "Epoch 194/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8358\n",
      "Epoch 00194: val_loss improved from 0.37950 to 0.37850, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3656 - accuracy: 0.8356 - val_loss: 0.3785 - val_accuracy: 0.8282\n",
      "Epoch 195/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3638 - accuracy: 0.8338\n",
      "Epoch 00195: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3634 - accuracy: 0.8337 - val_loss: 0.3887 - val_accuracy: 0.8181\n",
      "Epoch 196/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.8361\n",
      "Epoch 00196: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3626 - accuracy: 0.8357 - val_loss: 0.3934 - val_accuracy: 0.8190\n",
      "Epoch 197/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8336\n",
      "Epoch 00197: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3656 - accuracy: 0.8334 - val_loss: 0.3806 - val_accuracy: 0.8255\n",
      "Epoch 198/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8342\n",
      "Epoch 00198: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3656 - accuracy: 0.8341 - val_loss: 0.4088 - val_accuracy: 0.8126\n",
      "Epoch 199/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8339\n",
      "Epoch 00199: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3638 - accuracy: 0.8344 - val_loss: 0.3961 - val_accuracy: 0.8171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8371\n",
      "Epoch 00200: val_loss did not improve from 0.37850\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3600 - accuracy: 0.8368 - val_loss: 0.3826 - val_accuracy: 0.8252\n",
      "Epoch 201/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.8400\n",
      "Epoch 00201: val_loss improved from 0.37850 to 0.37153, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3596 - accuracy: 0.8397 - val_loss: 0.3715 - val_accuracy: 0.8278\n",
      "Epoch 202/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3575 - accuracy: 0.8407\n",
      "Epoch 00202: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3584 - accuracy: 0.8402 - val_loss: 0.3909 - val_accuracy: 0.8199\n",
      "Epoch 203/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8371\n",
      "Epoch 00203: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3611 - accuracy: 0.8370 - val_loss: 0.3846 - val_accuracy: 0.8227\n",
      "Epoch 204/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3605 - accuracy: 0.8379\n",
      "Epoch 00204: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3608 - accuracy: 0.8376 - val_loss: 0.3790 - val_accuracy: 0.8250\n",
      "Epoch 205/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8398\n",
      "Epoch 00205: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3585 - accuracy: 0.8397 - val_loss: 0.3791 - val_accuracy: 0.8246\n",
      "Epoch 206/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8356\n",
      "Epoch 00206: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3614 - accuracy: 0.8355 - val_loss: 0.3934 - val_accuracy: 0.8205\n",
      "Epoch 207/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8415\n",
      "Epoch 00207: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3563 - accuracy: 0.8413 - val_loss: 0.3751 - val_accuracy: 0.8280\n",
      "Epoch 208/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8392\n",
      "Epoch 00208: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3562 - accuracy: 0.8390 - val_loss: 0.3752 - val_accuracy: 0.8293\n",
      "Epoch 209/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.8413\n",
      "Epoch 00209: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3524 - accuracy: 0.8412 - val_loss: 0.3960 - val_accuracy: 0.8149\n",
      "Epoch 210/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8413\n",
      "Epoch 00210: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3573 - accuracy: 0.8409 - val_loss: 0.3795 - val_accuracy: 0.8231\n",
      "Epoch 211/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8423\n",
      "Epoch 00211: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3547 - accuracy: 0.8422 - val_loss: 0.3786 - val_accuracy: 0.8224\n",
      "Epoch 212/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8430\n",
      "Epoch 00212: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3537 - accuracy: 0.8428 - val_loss: 0.3977 - val_accuracy: 0.8186\n",
      "Epoch 213/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.8433\n",
      "Epoch 00213: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3522 - accuracy: 0.8431 - val_loss: 0.3988 - val_accuracy: 0.8166\n",
      "Epoch 214/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8427\n",
      "Epoch 00214: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3507 - accuracy: 0.8428 - val_loss: 0.3836 - val_accuracy: 0.8235\n",
      "Epoch 215/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3519 - accuracy: 0.8427\n",
      "Epoch 00215: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3520 - accuracy: 0.8427 - val_loss: 0.3836 - val_accuracy: 0.8237\n",
      "Epoch 216/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8445\n",
      "Epoch 00216: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3514 - accuracy: 0.8442 - val_loss: 0.3894 - val_accuracy: 0.8190\n",
      "Epoch 217/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8424\n",
      "Epoch 00217: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3510 - accuracy: 0.8423 - val_loss: 0.3935 - val_accuracy: 0.8177\n",
      "Epoch 218/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8417\n",
      "Epoch 00218: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3507 - accuracy: 0.8417 - val_loss: 0.3897 - val_accuracy: 0.8171\n",
      "Epoch 219/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3477 - accuracy: 0.8436\n",
      "Epoch 00219: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3483 - accuracy: 0.8434 - val_loss: 0.3850 - val_accuracy: 0.8250\n",
      "Epoch 220/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8456\n",
      "Epoch 00220: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3486 - accuracy: 0.8456 - val_loss: 0.3885 - val_accuracy: 0.8222\n",
      "Epoch 221/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8476\n",
      "Epoch 00221: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3467 - accuracy: 0.8475 - val_loss: 0.3759 - val_accuracy: 0.8274\n",
      "Epoch 222/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8452\n",
      "Epoch 00222: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3443 - accuracy: 0.8453 - val_loss: 0.4009 - val_accuracy: 0.8177\n",
      "Epoch 223/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8460\n",
      "Epoch 00223: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3453 - accuracy: 0.8459 - val_loss: 0.3960 - val_accuracy: 0.8177\n",
      "Epoch 224/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8443\n",
      "Epoch 00224: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3463 - accuracy: 0.8442 - val_loss: 0.3815 - val_accuracy: 0.8276\n",
      "Epoch 225/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8478\n",
      "Epoch 00225: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3423 - accuracy: 0.8475 - val_loss: 0.3791 - val_accuracy: 0.8263\n",
      "Epoch 226/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.8464\n",
      "Epoch 00226: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3460 - accuracy: 0.8462 - val_loss: 0.3758 - val_accuracy: 0.8261\n",
      "Epoch 227/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8474\n",
      "Epoch 00227: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3400 - accuracy: 0.8473 - val_loss: 0.3806 - val_accuracy: 0.8272\n",
      "Epoch 228/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8491\n",
      "Epoch 00228: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3422 - accuracy: 0.8493 - val_loss: 0.3809 - val_accuracy: 0.8252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8499\n",
      "Epoch 00229: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3407 - accuracy: 0.8498 - val_loss: 0.3942 - val_accuracy: 0.8209\n",
      "Epoch 230/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8456\n",
      "Epoch 00230: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3476 - accuracy: 0.8456 - val_loss: 0.3837 - val_accuracy: 0.8242\n",
      "Epoch 231/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8471\n",
      "Epoch 00231: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3428 - accuracy: 0.8470 - val_loss: 0.3839 - val_accuracy: 0.8237\n",
      "Epoch 232/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8491\n",
      "Epoch 00232: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3371 - accuracy: 0.8492 - val_loss: 0.3730 - val_accuracy: 0.8304\n",
      "Epoch 233/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8476\n",
      "Epoch 00233: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3362 - accuracy: 0.8476 - val_loss: 0.3881 - val_accuracy: 0.8201\n",
      "Epoch 234/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8515\n",
      "Epoch 00234: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3374 - accuracy: 0.8510 - val_loss: 0.3907 - val_accuracy: 0.8173\n",
      "Epoch 235/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8529\n",
      "Epoch 00235: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3327 - accuracy: 0.8530 - val_loss: 0.3840 - val_accuracy: 0.8190\n",
      "Epoch 236/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8468\n",
      "Epoch 00236: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3404 - accuracy: 0.8468 - val_loss: 0.3740 - val_accuracy: 0.8293\n",
      "Epoch 237/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8528\n",
      "Epoch 00237: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3354 - accuracy: 0.8525 - val_loss: 0.3821 - val_accuracy: 0.8209\n",
      "Epoch 238/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8486\n",
      "Epoch 00238: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3400 - accuracy: 0.8484 - val_loss: 0.3829 - val_accuracy: 0.8229\n",
      "Epoch 239/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8519\n",
      "Epoch 00239: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3348 - accuracy: 0.8519 - val_loss: 0.3813 - val_accuracy: 0.8229\n",
      "Epoch 240/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8497\n",
      "Epoch 00240: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3368 - accuracy: 0.8495 - val_loss: 0.3780 - val_accuracy: 0.8244\n",
      "Epoch 241/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8517\n",
      "Epoch 00241: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3354 - accuracy: 0.8519 - val_loss: 0.3749 - val_accuracy: 0.8270\n",
      "Epoch 242/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3296 - accuracy: 0.8547\n",
      "Epoch 00242: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3291 - accuracy: 0.8546 - val_loss: 0.3781 - val_accuracy: 0.8248\n",
      "Epoch 243/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8546\n",
      "Epoch 00243: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3313 - accuracy: 0.8546 - val_loss: 0.3831 - val_accuracy: 0.8194\n",
      "Epoch 244/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8532\n",
      "Epoch 00244: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3273 - accuracy: 0.8534 - val_loss: 0.3801 - val_accuracy: 0.8214\n",
      "Epoch 245/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8537\n",
      "Epoch 00245: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3335 - accuracy: 0.8535 - val_loss: 0.3748 - val_accuracy: 0.8304\n",
      "Epoch 246/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8506\n",
      "Epoch 00246: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3308 - accuracy: 0.8506 - val_loss: 0.3774 - val_accuracy: 0.8274\n",
      "Epoch 247/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8594\n",
      "Epoch 00247: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3249 - accuracy: 0.8594 - val_loss: 0.3856 - val_accuracy: 0.8151\n",
      "Epoch 248/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8524\n",
      "Epoch 00248: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3284 - accuracy: 0.8522 - val_loss: 0.3781 - val_accuracy: 0.8222\n",
      "Epoch 249/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8573\n",
      "Epoch 00249: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3241 - accuracy: 0.8571 - val_loss: 0.3732 - val_accuracy: 0.8306\n",
      "Epoch 250/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8590\n",
      "Epoch 00250: val_loss did not improve from 0.37153\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3224 - accuracy: 0.8589 - val_loss: 0.3760 - val_accuracy: 0.8282\n",
      "Epoch 251/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8545\n",
      "Epoch 00251: val_loss improved from 0.37153 to 0.36901, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3258 - accuracy: 0.8543 - val_loss: 0.3690 - val_accuracy: 0.8287\n",
      "Epoch 252/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.8560\n",
      "Epoch 00252: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3271 - accuracy: 0.8564 - val_loss: 0.3896 - val_accuracy: 0.8231\n",
      "Epoch 253/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8592\n",
      "Epoch 00253: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3227 - accuracy: 0.8587 - val_loss: 0.3876 - val_accuracy: 0.8207\n",
      "Epoch 254/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8582\n",
      "Epoch 00254: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3244 - accuracy: 0.8585 - val_loss: 0.3843 - val_accuracy: 0.8196\n",
      "Epoch 255/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8547\n",
      "Epoch 00255: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3259 - accuracy: 0.8548 - val_loss: 0.3760 - val_accuracy: 0.8250\n",
      "Epoch 256/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8582\n",
      "Epoch 00256: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3230 - accuracy: 0.8585 - val_loss: 0.3848 - val_accuracy: 0.8216\n",
      "Epoch 257/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8592\n",
      "Epoch 00257: val_loss did not improve from 0.36901\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3195 - accuracy: 0.8594 - val_loss: 0.3699 - val_accuracy: 0.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8587\n",
      "Epoch 00258: val_loss improved from 0.36901 to 0.36894, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3217 - accuracy: 0.8588 - val_loss: 0.3689 - val_accuracy: 0.8317\n",
      "Epoch 259/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8568\n",
      "Epoch 00259: val_loss improved from 0.36894 to 0.36804, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3224 - accuracy: 0.8567 - val_loss: 0.3680 - val_accuracy: 0.8308\n",
      "Epoch 260/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8582\n",
      "Epoch 00260: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3209 - accuracy: 0.8584 - val_loss: 0.3805 - val_accuracy: 0.8270\n",
      "Epoch 261/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8580\n",
      "Epoch 00261: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3200 - accuracy: 0.8582 - val_loss: 0.3722 - val_accuracy: 0.8308\n",
      "Epoch 262/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8612\n",
      "Epoch 00262: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3184 - accuracy: 0.8612 - val_loss: 0.3795 - val_accuracy: 0.8261\n",
      "Epoch 263/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8588\n",
      "Epoch 00263: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3159 - accuracy: 0.8583 - val_loss: 0.3694 - val_accuracy: 0.8308\n",
      "Epoch 264/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8594\n",
      "Epoch 00264: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3174 - accuracy: 0.8591 - val_loss: 0.3898 - val_accuracy: 0.8186\n",
      "Epoch 265/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8594\n",
      "Epoch 00265: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3199 - accuracy: 0.8595 - val_loss: 0.3762 - val_accuracy: 0.8255\n",
      "Epoch 266/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8577\n",
      "Epoch 00266: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3230 - accuracy: 0.8573 - val_loss: 0.3709 - val_accuracy: 0.8282\n",
      "Epoch 267/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8627\n",
      "Epoch 00267: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3130 - accuracy: 0.8629 - val_loss: 0.3793 - val_accuracy: 0.8272\n",
      "Epoch 268/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8605\n",
      "Epoch 00268: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3131 - accuracy: 0.8606 - val_loss: 0.3755 - val_accuracy: 0.8259\n",
      "Epoch 269/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8616\n",
      "Epoch 00269: val_loss did not improve from 0.36804\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3130 - accuracy: 0.8611 - val_loss: 0.3818 - val_accuracy: 0.8239\n",
      "Epoch 270/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8638\n",
      "Epoch 00270: val_loss improved from 0.36804 to 0.36562, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3060 - accuracy: 0.8638 - val_loss: 0.3656 - val_accuracy: 0.8317\n",
      "Epoch 271/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8608\n",
      "Epoch 00271: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3139 - accuracy: 0.8606 - val_loss: 0.3742 - val_accuracy: 0.8287\n",
      "Epoch 272/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8630\n",
      "Epoch 00272: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3109 - accuracy: 0.8631 - val_loss: 0.3769 - val_accuracy: 0.8250\n",
      "Epoch 273/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8644\n",
      "Epoch 00273: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3114 - accuracy: 0.8644 - val_loss: 0.3843 - val_accuracy: 0.8212\n",
      "Epoch 274/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8652\n",
      "Epoch 00274: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3039 - accuracy: 0.8652 - val_loss: 0.3758 - val_accuracy: 0.8261\n",
      "Epoch 275/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8663\n",
      "Epoch 00275: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3065 - accuracy: 0.8664 - val_loss: 0.3826 - val_accuracy: 0.8248\n",
      "Epoch 276/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8619\n",
      "Epoch 00276: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3102 - accuracy: 0.8620 - val_loss: 0.3748 - val_accuracy: 0.8270\n",
      "Epoch 277/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8627\n",
      "Epoch 00277: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3080 - accuracy: 0.8627 - val_loss: 0.3678 - val_accuracy: 0.8313\n",
      "Epoch 278/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8655\n",
      "Epoch 00278: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3075 - accuracy: 0.8656 - val_loss: 0.3911 - val_accuracy: 0.8186\n",
      "Epoch 279/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8664\n",
      "Epoch 00279: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3045 - accuracy: 0.8661 - val_loss: 0.3866 - val_accuracy: 0.8196\n",
      "Epoch 280/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8649\n",
      "Epoch 00280: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3045 - accuracy: 0.8651 - val_loss: 0.3711 - val_accuracy: 0.8321\n",
      "Epoch 281/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8666\n",
      "Epoch 00281: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3009 - accuracy: 0.8670 - val_loss: 0.3844 - val_accuracy: 0.8222\n",
      "Epoch 282/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8665\n",
      "Epoch 00282: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3042 - accuracy: 0.8666 - val_loss: 0.3822 - val_accuracy: 0.8255\n",
      "Epoch 283/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8633\n",
      "Epoch 00283: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3087 - accuracy: 0.8634 - val_loss: 0.3682 - val_accuracy: 0.8291\n",
      "Epoch 284/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8643\n",
      "Epoch 00284: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3068 - accuracy: 0.8644 - val_loss: 0.3748 - val_accuracy: 0.8250\n",
      "Epoch 285/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.8679\n",
      "Epoch 00285: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3004 - accuracy: 0.8684 - val_loss: 0.3707 - val_accuracy: 0.8325\n",
      "Epoch 286/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8676\n",
      "Epoch 00286: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3037 - accuracy: 0.8674 - val_loss: 0.3692 - val_accuracy: 0.8310\n",
      "Epoch 287/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8705\n",
      "Epoch 00287: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2968 - accuracy: 0.8704 - val_loss: 0.3878 - val_accuracy: 0.8192\n",
      "Epoch 288/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8702\n",
      "Epoch 00288: val_loss did not improve from 0.36562\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2954 - accuracy: 0.8706 - val_loss: 0.3700 - val_accuracy: 0.8300\n",
      "Epoch 289/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8699\n",
      "Epoch 00289: val_loss improved from 0.36562 to 0.36428, saving model to pickled_objects/batch_size_256_lr_0.04_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2940 - accuracy: 0.8699 - val_loss: 0.3643 - val_accuracy: 0.8319\n",
      "Epoch 290/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8660\n",
      "Epoch 00290: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3004 - accuracy: 0.8661 - val_loss: 0.3753 - val_accuracy: 0.8259\n",
      "Epoch 291/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8712\n",
      "Epoch 00291: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2946 - accuracy: 0.8714 - val_loss: 0.3735 - val_accuracy: 0.8261\n",
      "Epoch 292/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8727\n",
      "Epoch 00292: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2956 - accuracy: 0.8725 - val_loss: 0.3719 - val_accuracy: 0.8291\n",
      "Epoch 293/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8652\n",
      "Epoch 00293: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3013 - accuracy: 0.8656 - val_loss: 0.3776 - val_accuracy: 0.8244\n",
      "Epoch 294/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8700\n",
      "Epoch 00294: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2934 - accuracy: 0.8702 - val_loss: 0.3943 - val_accuracy: 0.8164\n",
      "Epoch 295/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8745\n",
      "Epoch 00295: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2902 - accuracy: 0.8746 - val_loss: 0.3838 - val_accuracy: 0.8248\n",
      "Epoch 296/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8734\n",
      "Epoch 00296: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2910 - accuracy: 0.8736 - val_loss: 0.3867 - val_accuracy: 0.8220\n",
      "Epoch 297/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8717\n",
      "Epoch 00297: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2949 - accuracy: 0.8720 - val_loss: 0.3778 - val_accuracy: 0.8235\n",
      "Epoch 298/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8718\n",
      "Epoch 00298: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2917 - accuracy: 0.8721 - val_loss: 0.3764 - val_accuracy: 0.8285\n",
      "Epoch 299/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8772\n",
      "Epoch 00299: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2856 - accuracy: 0.8772 - val_loss: 0.3752 - val_accuracy: 0.8300\n",
      "Epoch 300/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.8746\n",
      "Epoch 00300: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2862 - accuracy: 0.8748 - val_loss: 0.3835 - val_accuracy: 0.8246\n",
      "Epoch 301/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8704\n",
      "Epoch 00301: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2961 - accuracy: 0.8703 - val_loss: 0.3911 - val_accuracy: 0.8177\n",
      "Epoch 302/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8750\n",
      "Epoch 00302: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2902 - accuracy: 0.8755 - val_loss: 0.3830 - val_accuracy: 0.8235\n",
      "Epoch 303/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8732\n",
      "Epoch 00303: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2920 - accuracy: 0.8732 - val_loss: 0.3820 - val_accuracy: 0.8203\n",
      "Epoch 304/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.8731\n",
      "Epoch 00304: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2886 - accuracy: 0.8732 - val_loss: 0.3712 - val_accuracy: 0.8293\n",
      "Epoch 305/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.8766\n",
      "Epoch 00305: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2857 - accuracy: 0.8768 - val_loss: 0.3742 - val_accuracy: 0.8295\n",
      "Epoch 306/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8739\n",
      "Epoch 00306: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2903 - accuracy: 0.8740 - val_loss: 0.3749 - val_accuracy: 0.8261\n",
      "Epoch 307/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.8728\n",
      "Epoch 00307: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2894 - accuracy: 0.8731 - val_loss: 0.3727 - val_accuracy: 0.8287\n",
      "Epoch 308/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.8719\n",
      "Epoch 00308: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2858 - accuracy: 0.8717 - val_loss: 0.3723 - val_accuracy: 0.8313\n",
      "Epoch 309/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8787\n",
      "Epoch 00309: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2811 - accuracy: 0.8787 - val_loss: 0.3788 - val_accuracy: 0.8265\n",
      "Epoch 310/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.8757\n",
      "Epoch 00310: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2841 - accuracy: 0.8756 - val_loss: 0.3729 - val_accuracy: 0.8280\n",
      "Epoch 311/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8788\n",
      "Epoch 00311: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2827 - accuracy: 0.8789 - val_loss: 0.3882 - val_accuracy: 0.8166\n",
      "Epoch 312/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8759\n",
      "Epoch 00312: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2857 - accuracy: 0.8759 - val_loss: 0.3829 - val_accuracy: 0.8237\n",
      "Epoch 313/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8745\n",
      "Epoch 00313: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2862 - accuracy: 0.8749 - val_loss: 0.3804 - val_accuracy: 0.8229\n",
      "Epoch 314/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2772 - accuracy: 0.8778\n",
      "Epoch 00314: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2768 - accuracy: 0.8780 - val_loss: 0.3743 - val_accuracy: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8750\n",
      "Epoch 00315: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2834 - accuracy: 0.8749 - val_loss: 0.3685 - val_accuracy: 0.8302\n",
      "Epoch 316/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.8804\n",
      "Epoch 00316: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2801 - accuracy: 0.8804 - val_loss: 0.3837 - val_accuracy: 0.8205\n",
      "Epoch 317/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.8791\n",
      "Epoch 00317: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2848 - accuracy: 0.8796 - val_loss: 0.3781 - val_accuracy: 0.8227\n",
      "Epoch 318/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8812\n",
      "Epoch 00318: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2827 - accuracy: 0.8814 - val_loss: 0.3831 - val_accuracy: 0.8203\n",
      "Epoch 319/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.8793\n",
      "Epoch 00319: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2762 - accuracy: 0.8794 - val_loss: 0.3817 - val_accuracy: 0.8242\n",
      "Epoch 320/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8785\n",
      "Epoch 00320: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2780 - accuracy: 0.8784 - val_loss: 0.3768 - val_accuracy: 0.8239\n",
      "Epoch 321/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8803\n",
      "Epoch 00321: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2776 - accuracy: 0.8805 - val_loss: 0.3758 - val_accuracy: 0.8267\n",
      "Epoch 322/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8815\n",
      "Epoch 00322: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2726 - accuracy: 0.8816 - val_loss: 0.3789 - val_accuracy: 0.8261\n",
      "Epoch 323/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8791\n",
      "Epoch 00323: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2796 - accuracy: 0.8794 - val_loss: 0.3760 - val_accuracy: 0.8295\n",
      "Epoch 324/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.8817\n",
      "Epoch 00324: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2712 - accuracy: 0.8820 - val_loss: 0.3875 - val_accuracy: 0.8229\n",
      "Epoch 325/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2771 - accuracy: 0.8812\n",
      "Epoch 00325: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2764 - accuracy: 0.8815 - val_loss: 0.3879 - val_accuracy: 0.8246\n",
      "Epoch 326/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8813\n",
      "Epoch 00326: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2724 - accuracy: 0.8814 - val_loss: 0.3828 - val_accuracy: 0.8257\n",
      "Epoch 327/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.8811\n",
      "Epoch 00327: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2768 - accuracy: 0.8810 - val_loss: 0.3884 - val_accuracy: 0.8212\n",
      "Epoch 328/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.8793\n",
      "Epoch 00328: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2704 - accuracy: 0.8797 - val_loss: 0.3771 - val_accuracy: 0.8302\n",
      "Epoch 329/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8830\n",
      "Epoch 00329: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2693 - accuracy: 0.8832 - val_loss: 0.3857 - val_accuracy: 0.8229\n",
      "Epoch 330/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.8793\n",
      "Epoch 00330: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2755 - accuracy: 0.8795 - val_loss: 0.3759 - val_accuracy: 0.8278\n",
      "Epoch 331/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.8873\n",
      "Epoch 00331: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2645 - accuracy: 0.8874 - val_loss: 0.3764 - val_accuracy: 0.8263\n",
      "Epoch 332/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8825\n",
      "Epoch 00332: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2692 - accuracy: 0.8826 - val_loss: 0.3916 - val_accuracy: 0.8222\n",
      "Epoch 333/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8855\n",
      "Epoch 00333: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2662 - accuracy: 0.8855 - val_loss: 0.3747 - val_accuracy: 0.8278\n",
      "Epoch 334/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.8805\n",
      "Epoch 00334: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2704 - accuracy: 0.8805 - val_loss: 0.3848 - val_accuracy: 0.8272\n",
      "Epoch 335/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.8823\n",
      "Epoch 00335: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2674 - accuracy: 0.8828 - val_loss: 0.3838 - val_accuracy: 0.8265\n",
      "Epoch 336/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.8870\n",
      "Epoch 00336: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2646 - accuracy: 0.8873 - val_loss: 0.3713 - val_accuracy: 0.8347\n",
      "Epoch 337/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8828\n",
      "Epoch 00337: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2689 - accuracy: 0.8831 - val_loss: 0.3781 - val_accuracy: 0.8252\n",
      "Epoch 338/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.8893\n",
      "Epoch 00338: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2609 - accuracy: 0.8894 - val_loss: 0.3746 - val_accuracy: 0.8293\n",
      "Epoch 339/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.8853\n",
      "Epoch 00339: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2619 - accuracy: 0.8856 - val_loss: 0.3872 - val_accuracy: 0.8259\n",
      "Epoch 340/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2630 - accuracy: 0.8878\n",
      "Epoch 00340: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2628 - accuracy: 0.8880 - val_loss: 0.3884 - val_accuracy: 0.8192\n",
      "Epoch 341/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.8883\n",
      "Epoch 00341: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2603 - accuracy: 0.8882 - val_loss: 0.3890 - val_accuracy: 0.8235\n",
      "Epoch 342/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.8912\n",
      "Epoch 00342: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2583 - accuracy: 0.8914 - val_loss: 0.3778 - val_accuracy: 0.8362\n",
      "Epoch 343/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.8887\n",
      "Epoch 00343: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2620 - accuracy: 0.8890 - val_loss: 0.3833 - val_accuracy: 0.8242\n",
      "Epoch 344/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.8891\n",
      "Epoch 00344: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2607 - accuracy: 0.8894 - val_loss: 0.3758 - val_accuracy: 0.8282\n",
      "Epoch 345/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8901\n",
      "Epoch 00345: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2609 - accuracy: 0.8904 - val_loss: 0.3713 - val_accuracy: 0.8304\n",
      "Epoch 346/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.8927\n",
      "Epoch 00346: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2533 - accuracy: 0.8930 - val_loss: 0.3896 - val_accuracy: 0.8218\n",
      "Epoch 347/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.8891\n",
      "Epoch 00347: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2584 - accuracy: 0.8890 - val_loss: 0.3791 - val_accuracy: 0.8293\n",
      "Epoch 348/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.8888\n",
      "Epoch 00348: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2571 - accuracy: 0.8888 - val_loss: 0.3841 - val_accuracy: 0.8295\n",
      "Epoch 349/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.8880\n",
      "Epoch 00349: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2602 - accuracy: 0.8879 - val_loss: 0.3774 - val_accuracy: 0.8302\n",
      "Epoch 350/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.8902\n",
      "Epoch 00350: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2570 - accuracy: 0.8903 - val_loss: 0.3894 - val_accuracy: 0.8231\n",
      "Epoch 351/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.8914\n",
      "Epoch 00351: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2517 - accuracy: 0.8917 - val_loss: 0.3769 - val_accuracy: 0.8302\n",
      "Epoch 352/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.8886\n",
      "Epoch 00352: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2535 - accuracy: 0.8890 - val_loss: 0.3795 - val_accuracy: 0.8300\n",
      "Epoch 353/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.8879\n",
      "Epoch 00353: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2579 - accuracy: 0.8880 - val_loss: 0.3857 - val_accuracy: 0.8233\n",
      "Epoch 354/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.8951\n",
      "Epoch 00354: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2499 - accuracy: 0.8954 - val_loss: 0.3836 - val_accuracy: 0.8274\n",
      "Epoch 355/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.8894\n",
      "Epoch 00355: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2578 - accuracy: 0.8897 - val_loss: 0.3823 - val_accuracy: 0.8287\n",
      "Epoch 356/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.8913\n",
      "Epoch 00356: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2545 - accuracy: 0.8917 - val_loss: 0.3874 - val_accuracy: 0.8265\n",
      "Epoch 357/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.8896\n",
      "Epoch 00357: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2542 - accuracy: 0.8895 - val_loss: 0.3812 - val_accuracy: 0.8295\n",
      "Epoch 358/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.8933\n",
      "Epoch 00358: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2488 - accuracy: 0.8936 - val_loss: 0.3794 - val_accuracy: 0.8263\n",
      "Epoch 359/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.8918\n",
      "Epoch 00359: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2495 - accuracy: 0.8919 - val_loss: 0.3780 - val_accuracy: 0.8321\n",
      "Epoch 360/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.8944\n",
      "Epoch 00360: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2478 - accuracy: 0.8945 - val_loss: 0.3787 - val_accuracy: 0.8332\n",
      "Epoch 361/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8934\n",
      "Epoch 00361: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2491 - accuracy: 0.8938 - val_loss: 0.3895 - val_accuracy: 0.8237\n",
      "Epoch 362/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.8971\n",
      "Epoch 00362: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2452 - accuracy: 0.8973 - val_loss: 0.3748 - val_accuracy: 0.8349\n",
      "Epoch 363/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.8971\n",
      "Epoch 00363: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2450 - accuracy: 0.8972 - val_loss: 0.3885 - val_accuracy: 0.8237\n",
      "Epoch 364/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.8974\n",
      "Epoch 00364: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2446 - accuracy: 0.8977 - val_loss: 0.3910 - val_accuracy: 0.8214\n",
      "Epoch 365/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.8965\n",
      "Epoch 00365: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2434 - accuracy: 0.8969 - val_loss: 0.3893 - val_accuracy: 0.8231\n",
      "Epoch 366/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8949\n",
      "Epoch 00366: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2428 - accuracy: 0.8952 - val_loss: 0.3830 - val_accuracy: 0.8285\n",
      "Epoch 367/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.8949\n",
      "Epoch 00367: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2467 - accuracy: 0.8949 - val_loss: 0.3845 - val_accuracy: 0.8276\n",
      "Epoch 368/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8963\n",
      "Epoch 00368: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2441 - accuracy: 0.8965 - val_loss: 0.3963 - val_accuracy: 0.8207\n",
      "Epoch 369/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.8937\n",
      "Epoch 00369: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2454 - accuracy: 0.8940 - val_loss: 0.3857 - val_accuracy: 0.8282\n",
      "Epoch 370/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8966\n",
      "Epoch 00370: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2438 - accuracy: 0.8969 - val_loss: 0.3802 - val_accuracy: 0.8295\n",
      "Epoch 371/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8951\n",
      "Epoch 00371: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2436 - accuracy: 0.8954 - val_loss: 0.3857 - val_accuracy: 0.8293\n",
      "Epoch 372/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.8981\n",
      "Epoch 00372: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2414 - accuracy: 0.8984 - val_loss: 0.3934 - val_accuracy: 0.8246\n",
      "Epoch 373/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.8979\n",
      "Epoch 00373: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2385 - accuracy: 0.8981 - val_loss: 0.3875 - val_accuracy: 0.8302\n",
      "Epoch 374/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.8923\n",
      "Epoch 00374: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2465 - accuracy: 0.8926 - val_loss: 0.3920 - val_accuracy: 0.8239\n",
      "Epoch 375/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.8995\n",
      "Epoch 00375: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2379 - accuracy: 0.8996 - val_loss: 0.3885 - val_accuracy: 0.8242\n",
      "Epoch 376/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.8981\n",
      "Epoch 00376: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2436 - accuracy: 0.8984 - val_loss: 0.3918 - val_accuracy: 0.8224\n",
      "Epoch 377/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.8933\n",
      "Epoch 00377: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2420 - accuracy: 0.8934 - val_loss: 0.3795 - val_accuracy: 0.8308\n",
      "Epoch 378/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.8977\n",
      "Epoch 00378: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2384 - accuracy: 0.8981 - val_loss: 0.3968 - val_accuracy: 0.8231\n",
      "Epoch 379/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2355 - accuracy: 0.8994\n",
      "Epoch 00379: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2352 - accuracy: 0.8991 - val_loss: 0.3873 - val_accuracy: 0.8315\n",
      "Epoch 380/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.8998\n",
      "Epoch 00380: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2366 - accuracy: 0.8996 - val_loss: 0.3817 - val_accuracy: 0.8321\n",
      "Epoch 381/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9015\n",
      "Epoch 00381: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2347 - accuracy: 0.9020 - val_loss: 0.3913 - val_accuracy: 0.8280\n",
      "Epoch 382/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9003\n",
      "Epoch 00382: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2361 - accuracy: 0.9002 - val_loss: 0.3923 - val_accuracy: 0.8255\n",
      "Epoch 383/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2378 - accuracy: 0.9019\n",
      "Epoch 00383: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2373 - accuracy: 0.9021 - val_loss: 0.3849 - val_accuracy: 0.8257\n",
      "Epoch 384/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9013\n",
      "Epoch 00384: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2329 - accuracy: 0.9015 - val_loss: 0.3952 - val_accuracy: 0.8224\n",
      "Epoch 385/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.8995\n",
      "Epoch 00385: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2368 - accuracy: 0.8996 - val_loss: 0.3859 - val_accuracy: 0.8321\n",
      "Epoch 386/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9019\n",
      "Epoch 00386: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2309 - accuracy: 0.9021 - val_loss: 0.4015 - val_accuracy: 0.8201\n",
      "Epoch 387/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.8994\n",
      "Epoch 00387: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2332 - accuracy: 0.8995 - val_loss: 0.3895 - val_accuracy: 0.8289\n",
      "Epoch 388/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9022\n",
      "Epoch 00388: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2285 - accuracy: 0.9028 - val_loss: 0.3907 - val_accuracy: 0.8237\n",
      "Epoch 389/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9014\n",
      "Epoch 00389: val_loss did not improve from 0.36428\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2313 - accuracy: 0.9015 - val_loss: 0.3979 - val_accuracy: 0.8255\n",
      "Epoch 00389: early stopping\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 8s 110ms/step - loss: 0.6924 - accuracy: 0.5163\n",
      "Epoch 00001: val_loss improved from inf to 0.69222, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 0.6924 - accuracy: 0.5163 - val_loss: 0.6922 - val_accuracy: 0.4929\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.5619\n",
      "Epoch 00002: val_loss did not improve from 0.69222\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6870 - accuracy: 0.5623 - val_loss: 0.6928 - val_accuracy: 0.4944\n",
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.5682\n",
      "Epoch 00003: val_loss improved from 0.69222 to 0.69047, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6819 - accuracy: 0.5681 - val_loss: 0.6905 - val_accuracy: 0.5071\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5717\n",
      "Epoch 00004: val_loss improved from 0.69047 to 0.68622, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6801 - accuracy: 0.5721 - val_loss: 0.6862 - val_accuracy: 0.5299\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.5687\n",
      "Epoch 00005: val_loss improved from 0.68622 to 0.68243, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6793 - accuracy: 0.5690 - val_loss: 0.6824 - val_accuracy: 0.5391\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6770 - accuracy: 0.5768\n",
      "Epoch 00006: val_loss did not improve from 0.68243\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6772 - accuracy: 0.5766 - val_loss: 0.6921 - val_accuracy: 0.5054\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6722 - accuracy: 0.5864\n",
      "Epoch 00007: val_loss improved from 0.68243 to 0.67169, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6720 - accuracy: 0.5865 - val_loss: 0.6717 - val_accuracy: 0.5761\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6730 - accuracy: 0.5832\n",
      "Epoch 00008: val_loss did not improve from 0.67169\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6729 - accuracy: 0.5832 - val_loss: 0.6721 - val_accuracy: 0.5727\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6667 - accuracy: 0.5968\n",
      "Epoch 00009: val_loss improved from 0.67169 to 0.66430, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6664 - accuracy: 0.5973 - val_loss: 0.6643 - val_accuracy: 0.5950\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6714 - accuracy: 0.5924\n",
      "Epoch 00010: val_loss did not improve from 0.66430\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6713 - accuracy: 0.5923 - val_loss: 0.6679 - val_accuracy: 0.5853\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6576 - accuracy: 0.6112\n",
      "Epoch 00011: val_loss improved from 0.66430 to 0.66276, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6573 - accuracy: 0.6114 - val_loss: 0.6628 - val_accuracy: 0.5920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.6109\n",
      "Epoch 00012: val_loss improved from 0.66276 to 0.64849, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6573 - accuracy: 0.6111 - val_loss: 0.6485 - val_accuracy: 0.6227\n",
      "Epoch 13/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6486 - accuracy: 0.6255\n",
      "Epoch 00013: val_loss improved from 0.64849 to 0.64068, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6485 - accuracy: 0.6254 - val_loss: 0.6407 - val_accuracy: 0.6395\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6392 - accuracy: 0.6338\n",
      "Epoch 00014: val_loss did not improve from 0.64068\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6393 - accuracy: 0.6332 - val_loss: 0.6420 - val_accuracy: 0.6288\n",
      "Epoch 15/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6310 - accuracy: 0.6469\n",
      "Epoch 00015: val_loss improved from 0.64068 to 0.63322, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6310 - accuracy: 0.6464 - val_loss: 0.6332 - val_accuracy: 0.6414\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6209 - accuracy: 0.6596\n",
      "Epoch 00016: val_loss did not improve from 0.63322\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6208 - accuracy: 0.6593 - val_loss: 0.6414 - val_accuracy: 0.6268\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6137 - accuracy: 0.6611\n",
      "Epoch 00017: val_loss improved from 0.63322 to 0.63197, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6135 - accuracy: 0.6609 - val_loss: 0.6320 - val_accuracy: 0.6324\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6119 - accuracy: 0.6612\n",
      "Epoch 00018: val_loss improved from 0.63197 to 0.62315, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6121 - accuracy: 0.6612 - val_loss: 0.6231 - val_accuracy: 0.6410\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.6743\n",
      "Epoch 00019: val_loss improved from 0.62315 to 0.61589, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5998 - accuracy: 0.6743 - val_loss: 0.6159 - val_accuracy: 0.6509\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.6759\n",
      "Epoch 00020: val_loss improved from 0.61589 to 0.61147, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5961 - accuracy: 0.6756 - val_loss: 0.6115 - val_accuracy: 0.6507\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5959 - accuracy: 0.6769\n",
      "Epoch 00021: val_loss improved from 0.61147 to 0.59314, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5966 - accuracy: 0.6762 - val_loss: 0.5931 - val_accuracy: 0.6694\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.6897\n",
      "Epoch 00022: val_loss improved from 0.59314 to 0.58360, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5836 - accuracy: 0.6889 - val_loss: 0.5836 - val_accuracy: 0.6877\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5907 - accuracy: 0.6797\n",
      "Epoch 00023: val_loss improved from 0.58360 to 0.56413, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5906 - accuracy: 0.6797 - val_loss: 0.5641 - val_accuracy: 0.6967\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.6954\n",
      "Epoch 00024: val_loss improved from 0.56413 to 0.56001, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5750 - accuracy: 0.6949 - val_loss: 0.5600 - val_accuracy: 0.7038\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7037\n",
      "Epoch 00025: val_loss did not improve from 0.56001\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5669 - accuracy: 0.7029 - val_loss: 0.5668 - val_accuracy: 0.6937\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7064\n",
      "Epoch 00026: val_loss improved from 0.56001 to 0.54173, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5663 - accuracy: 0.7057 - val_loss: 0.5417 - val_accuracy: 0.7137\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.7093\n",
      "Epoch 00027: val_loss improved from 0.54173 to 0.52772, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5555 - accuracy: 0.7089 - val_loss: 0.5277 - val_accuracy: 0.7537\n",
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5517 - accuracy: 0.7159\n",
      "Epoch 00028: val_loss did not improve from 0.52772\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5518 - accuracy: 0.7156 - val_loss: 0.5360 - val_accuracy: 0.7197\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5464 - accuracy: 0.7210\n",
      "Epoch 00029: val_loss did not improve from 0.52772\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5472 - accuracy: 0.7204 - val_loss: 0.5397 - val_accuracy: 0.7208\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7221\n",
      "Epoch 00030: val_loss improved from 0.52772 to 0.52060, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5411 - accuracy: 0.7220 - val_loss: 0.5206 - val_accuracy: 0.7337\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7252\n",
      "Epoch 00031: val_loss improved from 0.52060 to 0.51487, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5403 - accuracy: 0.7244 - val_loss: 0.5149 - val_accuracy: 0.7498\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5321 - accuracy: 0.7301\n",
      "Epoch 00032: val_loss improved from 0.51487 to 0.49831, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5327 - accuracy: 0.7298 - val_loss: 0.4983 - val_accuracy: 0.7638\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5264 - accuracy: 0.7324\n",
      "Epoch 00033: val_loss improved from 0.49831 to 0.49792, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5269 - accuracy: 0.7320 - val_loss: 0.4979 - val_accuracy: 0.7569\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7351\n",
      "Epoch 00034: val_loss improved from 0.49792 to 0.49350, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5277 - accuracy: 0.7352 - val_loss: 0.4935 - val_accuracy: 0.7646\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7390\n",
      "Epoch 00035: val_loss improved from 0.49350 to 0.48915, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5244 - accuracy: 0.7385 - val_loss: 0.4891 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.7421\n",
      "Epoch 00036: val_loss improved from 0.48915 to 0.47767, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5171 - accuracy: 0.7418 - val_loss: 0.4777 - val_accuracy: 0.7760\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5126 - accuracy: 0.7469\n",
      "Epoch 00037: val_loss did not improve from 0.47767\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5128 - accuracy: 0.7467 - val_loss: 0.4808 - val_accuracy: 0.7724\n",
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5047 - accuracy: 0.7491\n",
      "Epoch 00038: val_loss did not improve from 0.47767\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5047 - accuracy: 0.7488 - val_loss: 0.4810 - val_accuracy: 0.7683\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4969 - accuracy: 0.7552\n",
      "Epoch 00039: val_loss did not improve from 0.47767\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4971 - accuracy: 0.7550 - val_loss: 0.4961 - val_accuracy: 0.7547\n",
      "Epoch 40/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4991 - accuracy: 0.7552\n",
      "Epoch 00040: val_loss improved from 0.47767 to 0.46150, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4996 - accuracy: 0.7549 - val_loss: 0.4615 - val_accuracy: 0.7861\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7593\n",
      "Epoch 00041: val_loss improved from 0.46150 to 0.46086, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4955 - accuracy: 0.7593 - val_loss: 0.4609 - val_accuracy: 0.7833\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.7610\n",
      "Epoch 00042: val_loss improved from 0.46086 to 0.45876, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4887 - accuracy: 0.7611 - val_loss: 0.4588 - val_accuracy: 0.7874\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4823 - accuracy: 0.7667\n",
      "Epoch 00043: val_loss did not improve from 0.45876\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4825 - accuracy: 0.7664 - val_loss: 0.4659 - val_accuracy: 0.7835\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7690\n",
      "Epoch 00044: val_loss improved from 0.45876 to 0.44992, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4824 - accuracy: 0.7687 - val_loss: 0.4499 - val_accuracy: 0.7928\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7677\n",
      "Epoch 00045: val_loss did not improve from 0.44992\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4765 - accuracy: 0.7678 - val_loss: 0.4558 - val_accuracy: 0.7846\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.7731\n",
      "Epoch 00046: val_loss improved from 0.44992 to 0.44879, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4730 - accuracy: 0.7728 - val_loss: 0.4488 - val_accuracy: 0.7885\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.7762\n",
      "Epoch 00047: val_loss improved from 0.44879 to 0.44044, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4709 - accuracy: 0.7759 - val_loss: 0.4404 - val_accuracy: 0.8003\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.7776\n",
      "Epoch 00048: val_loss did not improve from 0.44044\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4637 - accuracy: 0.7771 - val_loss: 0.4411 - val_accuracy: 0.7979\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7821\n",
      "Epoch 00049: val_loss improved from 0.44044 to 0.43820, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4611 - accuracy: 0.7821 - val_loss: 0.4382 - val_accuracy: 0.8037\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4624 - accuracy: 0.7819\n",
      "Epoch 00050: val_loss improved from 0.43820 to 0.43360, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4623 - accuracy: 0.7818 - val_loss: 0.4336 - val_accuracy: 0.8044\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4579 - accuracy: 0.7819\n",
      "Epoch 00051: val_loss did not improve from 0.43360\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4576 - accuracy: 0.7820 - val_loss: 0.4353 - val_accuracy: 0.7964\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7828\n",
      "Epoch 00052: val_loss did not improve from 0.43360\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4568 - accuracy: 0.7829 - val_loss: 0.4352 - val_accuracy: 0.8020\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4554 - accuracy: 0.7834\n",
      "Epoch 00053: val_loss did not improve from 0.43360\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4551 - accuracy: 0.7837 - val_loss: 0.4402 - val_accuracy: 0.7941\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7873\n",
      "Epoch 00054: val_loss improved from 0.43360 to 0.41779, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4457 - accuracy: 0.7870 - val_loss: 0.4178 - val_accuracy: 0.8095\n",
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4472 - accuracy: 0.7879\n",
      "Epoch 00055: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4471 - accuracy: 0.7880 - val_loss: 0.4251 - val_accuracy: 0.8065\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7889\n",
      "Epoch 00056: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4475 - accuracy: 0.7886 - val_loss: 0.4256 - val_accuracy: 0.8025\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7911\n",
      "Epoch 00057: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4426 - accuracy: 0.7910 - val_loss: 0.4239 - val_accuracy: 0.8057\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7935\n",
      "Epoch 00058: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4388 - accuracy: 0.7937 - val_loss: 0.4216 - val_accuracy: 0.8046\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4388 - accuracy: 0.7899\n",
      "Epoch 00059: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4392 - accuracy: 0.7898 - val_loss: 0.4233 - val_accuracy: 0.8065\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4366 - accuracy: 0.7935\n",
      "Epoch 00060: val_loss did not improve from 0.41779\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4365 - accuracy: 0.7936 - val_loss: 0.4234 - val_accuracy: 0.8074\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.7956\n",
      "Epoch 00061: val_loss improved from 0.41779 to 0.41281, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4351 - accuracy: 0.7956 - val_loss: 0.4128 - val_accuracy: 0.8123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4285 - accuracy: 0.8014\n",
      "Epoch 00062: val_loss did not improve from 0.41281\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4287 - accuracy: 0.8013 - val_loss: 0.4291 - val_accuracy: 0.8012\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.7990\n",
      "Epoch 00063: val_loss did not improve from 0.41281\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4287 - accuracy: 0.7994 - val_loss: 0.4263 - val_accuracy: 0.7977\n",
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4279 - accuracy: 0.8001\n",
      "Epoch 00064: val_loss improved from 0.41281 to 0.40409, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4276 - accuracy: 0.8003 - val_loss: 0.4041 - val_accuracy: 0.8164\n",
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.7992\n",
      "Epoch 00065: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4256 - accuracy: 0.7994 - val_loss: 0.4071 - val_accuracy: 0.8181\n",
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4224 - accuracy: 0.8027\n",
      "Epoch 00066: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4228 - accuracy: 0.8024 - val_loss: 0.4058 - val_accuracy: 0.8134\n",
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8022\n",
      "Epoch 00067: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4213 - accuracy: 0.8021 - val_loss: 0.4041 - val_accuracy: 0.8156\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8066\n",
      "Epoch 00068: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4185 - accuracy: 0.8065 - val_loss: 0.4085 - val_accuracy: 0.8072\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8078\n",
      "Epoch 00069: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4179 - accuracy: 0.8079 - val_loss: 0.4052 - val_accuracy: 0.8151\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8091\n",
      "Epoch 00070: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4136 - accuracy: 0.8091 - val_loss: 0.4120 - val_accuracy: 0.8110\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8060\n",
      "Epoch 00071: val_loss did not improve from 0.40409\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4153 - accuracy: 0.8060 - val_loss: 0.4228 - val_accuracy: 0.8001\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8088\n",
      "Epoch 00072: val_loss improved from 0.40409 to 0.40379, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4116 - accuracy: 0.8091 - val_loss: 0.4038 - val_accuracy: 0.8115\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8073\n",
      "Epoch 00073: val_loss did not improve from 0.40379\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4130 - accuracy: 0.8075 - val_loss: 0.4226 - val_accuracy: 0.8007\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8145\n",
      "Epoch 00074: val_loss did not improve from 0.40379\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4072 - accuracy: 0.8150 - val_loss: 0.4067 - val_accuracy: 0.8123\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8101\n",
      "Epoch 00075: val_loss did not improve from 0.40379\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4104 - accuracy: 0.8099 - val_loss: 0.4051 - val_accuracy: 0.8123\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8109\n",
      "Epoch 00076: val_loss did not improve from 0.40379\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4083 - accuracy: 0.8110 - val_loss: 0.4128 - val_accuracy: 0.8057\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4027 - accuracy: 0.8167\n",
      "Epoch 00077: val_loss improved from 0.40379 to 0.39256, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4023 - accuracy: 0.8169 - val_loss: 0.3926 - val_accuracy: 0.8242\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.8146\n",
      "Epoch 00078: val_loss did not improve from 0.39256\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4033 - accuracy: 0.8147 - val_loss: 0.4006 - val_accuracy: 0.8166\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8132\n",
      "Epoch 00079: val_loss did not improve from 0.39256\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4019 - accuracy: 0.8135 - val_loss: 0.3942 - val_accuracy: 0.8192\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8173\n",
      "Epoch 00080: val_loss did not improve from 0.39256\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3995 - accuracy: 0.8174 - val_loss: 0.3997 - val_accuracy: 0.8147\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8142\n",
      "Epoch 00081: val_loss did not improve from 0.39256\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4018 - accuracy: 0.8142 - val_loss: 0.3969 - val_accuracy: 0.8166\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8180\n",
      "Epoch 00082: val_loss did not improve from 0.39256\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3963 - accuracy: 0.8183 - val_loss: 0.3930 - val_accuracy: 0.8212\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8203\n",
      "Epoch 00083: val_loss improved from 0.39256 to 0.38809, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3923 - accuracy: 0.8200 - val_loss: 0.3881 - val_accuracy: 0.8207\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8193\n",
      "Epoch 00084: val_loss improved from 0.38809 to 0.38529, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3953 - accuracy: 0.8196 - val_loss: 0.3853 - val_accuracy: 0.8231\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3939 - accuracy: 0.8199\n",
      "Epoch 00085: val_loss did not improve from 0.38529\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3942 - accuracy: 0.8199 - val_loss: 0.3970 - val_accuracy: 0.8156\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3897 - accuracy: 0.8234\n",
      "Epoch 00086: val_loss did not improve from 0.38529\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3898 - accuracy: 0.8236 - val_loss: 0.4040 - val_accuracy: 0.8080\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8201\n",
      "Epoch 00087: val_loss improved from 0.38529 to 0.38369, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3895 - accuracy: 0.8199 - val_loss: 0.3837 - val_accuracy: 0.8248\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8195\n",
      "Epoch 00088: val_loss improved from 0.38369 to 0.38178, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3878 - accuracy: 0.8198 - val_loss: 0.3818 - val_accuracy: 0.8267\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8216\n",
      "Epoch 00089: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3896 - accuracy: 0.8214 - val_loss: 0.4059 - val_accuracy: 0.8098\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.8225\n",
      "Epoch 00090: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3874 - accuracy: 0.8228 - val_loss: 0.3934 - val_accuracy: 0.8192\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8260\n",
      "Epoch 00091: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3833 - accuracy: 0.8263 - val_loss: 0.3870 - val_accuracy: 0.8222\n",
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8268\n",
      "Epoch 00092: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3833 - accuracy: 0.8268 - val_loss: 0.3943 - val_accuracy: 0.8175\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8239\n",
      "Epoch 00093: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3826 - accuracy: 0.8241 - val_loss: 0.3875 - val_accuracy: 0.8214\n",
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8268\n",
      "Epoch 00094: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3806 - accuracy: 0.8271 - val_loss: 0.3828 - val_accuracy: 0.8220\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3831 - accuracy: 0.8243\n",
      "Epoch 00095: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3829 - accuracy: 0.8244 - val_loss: 0.3843 - val_accuracy: 0.8250\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8275\n",
      "Epoch 00096: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3781 - accuracy: 0.8278 - val_loss: 0.3826 - val_accuracy: 0.8237\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8305\n",
      "Epoch 00097: val_loss did not improve from 0.38178\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3754 - accuracy: 0.8306 - val_loss: 0.3837 - val_accuracy: 0.8252\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.8270\n",
      "Epoch 00098: val_loss improved from 0.38178 to 0.38154, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3718 - accuracy: 0.8274 - val_loss: 0.3815 - val_accuracy: 0.8257\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8305\n",
      "Epoch 00099: val_loss did not improve from 0.38154\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3743 - accuracy: 0.8300 - val_loss: 0.3945 - val_accuracy: 0.8181\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3760 - accuracy: 0.8299\n",
      "Epoch 00100: val_loss did not improve from 0.38154\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3754 - accuracy: 0.8304 - val_loss: 0.3847 - val_accuracy: 0.8186\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8313\n",
      "Epoch 00101: val_loss improved from 0.38154 to 0.37769, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3723 - accuracy: 0.8315 - val_loss: 0.3777 - val_accuracy: 0.8291\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8338\n",
      "Epoch 00102: val_loss did not improve from 0.37769\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3691 - accuracy: 0.8342 - val_loss: 0.3815 - val_accuracy: 0.8242\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8322\n",
      "Epoch 00103: val_loss did not improve from 0.37769\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3699 - accuracy: 0.8321 - val_loss: 0.3781 - val_accuracy: 0.8255\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8319\n",
      "Epoch 00104: val_loss did not improve from 0.37769\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3711 - accuracy: 0.8315 - val_loss: 0.3819 - val_accuracy: 0.8276\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8388\n",
      "Epoch 00105: val_loss improved from 0.37769 to 0.37417, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3604 - accuracy: 0.8389 - val_loss: 0.3742 - val_accuracy: 0.8276\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8360\n",
      "Epoch 00106: val_loss improved from 0.37417 to 0.37415, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3651 - accuracy: 0.8359 - val_loss: 0.3742 - val_accuracy: 0.8313\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8362\n",
      "Epoch 00107: val_loss did not improve from 0.37415\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3634 - accuracy: 0.8364 - val_loss: 0.3849 - val_accuracy: 0.8242\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8366\n",
      "Epoch 00108: val_loss improved from 0.37415 to 0.37115, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3607 - accuracy: 0.8371 - val_loss: 0.3712 - val_accuracy: 0.8317\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8354\n",
      "Epoch 00109: val_loss improved from 0.37115 to 0.36908, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3605 - accuracy: 0.8356 - val_loss: 0.3691 - val_accuracy: 0.8293\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.8372\n",
      "Epoch 00110: val_loss improved from 0.36908 to 0.36648, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3609 - accuracy: 0.8371 - val_loss: 0.3665 - val_accuracy: 0.8343\n",
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8410\n",
      "Epoch 00111: val_loss did not improve from 0.36648\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3556 - accuracy: 0.8412 - val_loss: 0.3760 - val_accuracy: 0.8289\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8390\n",
      "Epoch 00112: val_loss improved from 0.36648 to 0.36598, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3568 - accuracy: 0.8390 - val_loss: 0.3660 - val_accuracy: 0.8371\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8419\n",
      "Epoch 00113: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3537 - accuracy: 0.8421 - val_loss: 0.3884 - val_accuracy: 0.8237\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8446\n",
      "Epoch 00114: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3505 - accuracy: 0.8449 - val_loss: 0.3666 - val_accuracy: 0.8336\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8453\n",
      "Epoch 00115: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3476 - accuracy: 0.8454 - val_loss: 0.3803 - val_accuracy: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8390\n",
      "Epoch 00116: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3516 - accuracy: 0.8390 - val_loss: 0.3683 - val_accuracy: 0.8349\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8429\n",
      "Epoch 00117: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3498 - accuracy: 0.8432 - val_loss: 0.3695 - val_accuracy: 0.8291\n",
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8422\n",
      "Epoch 00118: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3499 - accuracy: 0.8423 - val_loss: 0.3720 - val_accuracy: 0.8276\n",
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8434\n",
      "Epoch 00119: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3476 - accuracy: 0.8431 - val_loss: 0.3796 - val_accuracy: 0.8235\n",
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8419\n",
      "Epoch 00120: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3487 - accuracy: 0.8421 - val_loss: 0.3703 - val_accuracy: 0.8373\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8453\n",
      "Epoch 00121: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3448 - accuracy: 0.8452 - val_loss: 0.3864 - val_accuracy: 0.8216\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8459\n",
      "Epoch 00122: val_loss did not improve from 0.36598\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3455 - accuracy: 0.8458 - val_loss: 0.3700 - val_accuracy: 0.8285\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8429\n",
      "Epoch 00123: val_loss improved from 0.36598 to 0.36550, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3465 - accuracy: 0.8430 - val_loss: 0.3655 - val_accuracy: 0.8334\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8446\n",
      "Epoch 00124: val_loss did not improve from 0.36550\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3442 - accuracy: 0.8450 - val_loss: 0.3679 - val_accuracy: 0.8306\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8458\n",
      "Epoch 00125: val_loss improved from 0.36550 to 0.36459, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3443 - accuracy: 0.8458 - val_loss: 0.3646 - val_accuracy: 0.8351\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.8473\n",
      "Epoch 00126: val_loss did not improve from 0.36459\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3397 - accuracy: 0.8476 - val_loss: 0.3718 - val_accuracy: 0.8276\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8495\n",
      "Epoch 00127: val_loss improved from 0.36459 to 0.36119, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3358 - accuracy: 0.8495 - val_loss: 0.3612 - val_accuracy: 0.8381\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.8568\n",
      "Epoch 00128: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3303 - accuracy: 0.8569 - val_loss: 0.3666 - val_accuracy: 0.8353\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8564\n",
      "Epoch 00129: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3284 - accuracy: 0.8565 - val_loss: 0.3655 - val_accuracy: 0.8347\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8511\n",
      "Epoch 00130: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3348 - accuracy: 0.8512 - val_loss: 0.3680 - val_accuracy: 0.8295\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8482\n",
      "Epoch 00131: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3368 - accuracy: 0.8485 - val_loss: 0.3772 - val_accuracy: 0.8220\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3385 - accuracy: 0.8513\n",
      "Epoch 00132: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3379 - accuracy: 0.8515 - val_loss: 0.3618 - val_accuracy: 0.8332\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8531\n",
      "Epoch 00133: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3325 - accuracy: 0.8531 - val_loss: 0.3682 - val_accuracy: 0.8282\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8511\n",
      "Epoch 00134: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3285 - accuracy: 0.8512 - val_loss: 0.3622 - val_accuracy: 0.8338\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8521\n",
      "Epoch 00135: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3269 - accuracy: 0.8524 - val_loss: 0.3651 - val_accuracy: 0.8340\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8579\n",
      "Epoch 00136: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3247 - accuracy: 0.8581 - val_loss: 0.3635 - val_accuracy: 0.8353\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8593\n",
      "Epoch 00137: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3186 - accuracy: 0.8595 - val_loss: 0.3734 - val_accuracy: 0.8334\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8575\n",
      "Epoch 00138: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3273 - accuracy: 0.8578 - val_loss: 0.3856 - val_accuracy: 0.8252\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8564\n",
      "Epoch 00139: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3240 - accuracy: 0.8567 - val_loss: 0.3746 - val_accuracy: 0.8308\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8555\n",
      "Epoch 00140: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3242 - accuracy: 0.8556 - val_loss: 0.3710 - val_accuracy: 0.8349\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8573\n",
      "Epoch 00141: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3229 - accuracy: 0.8576 - val_loss: 0.3803 - val_accuracy: 0.8310\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8570\n",
      "Epoch 00142: val_loss did not improve from 0.36119\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3225 - accuracy: 0.8573 - val_loss: 0.3636 - val_accuracy: 0.8371\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8606\n",
      "Epoch 00143: val_loss improved from 0.36119 to 0.35507, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3190 - accuracy: 0.8608 - val_loss: 0.3551 - val_accuracy: 0.8366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3166 - accuracy: 0.8613\n",
      "Epoch 00144: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3165 - accuracy: 0.8615 - val_loss: 0.3610 - val_accuracy: 0.8343\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8615\n",
      "Epoch 00145: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3133 - accuracy: 0.8617 - val_loss: 0.3750 - val_accuracy: 0.8291\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8589\n",
      "Epoch 00146: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3188 - accuracy: 0.8592 - val_loss: 0.3572 - val_accuracy: 0.8424\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8606\n",
      "Epoch 00147: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3171 - accuracy: 0.8611 - val_loss: 0.3570 - val_accuracy: 0.8409\n",
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8624\n",
      "Epoch 00148: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3162 - accuracy: 0.8623 - val_loss: 0.3566 - val_accuracy: 0.8375\n",
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8646\n",
      "Epoch 00149: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3087 - accuracy: 0.8651 - val_loss: 0.3617 - val_accuracy: 0.8347\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8599\n",
      "Epoch 00150: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3134 - accuracy: 0.8603 - val_loss: 0.3606 - val_accuracy: 0.8388\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8653\n",
      "Epoch 00151: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3068 - accuracy: 0.8654 - val_loss: 0.3806 - val_accuracy: 0.8233\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8624\n",
      "Epoch 00152: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3106 - accuracy: 0.8627 - val_loss: 0.3653 - val_accuracy: 0.8375\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3103 - accuracy: 0.8645\n",
      "Epoch 00153: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3094 - accuracy: 0.8647 - val_loss: 0.3634 - val_accuracy: 0.8377\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8659\n",
      "Epoch 00154: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3055 - accuracy: 0.8661 - val_loss: 0.3672 - val_accuracy: 0.8308\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8687\n",
      "Epoch 00155: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.3056 - accuracy: 0.8689 - val_loss: 0.3657 - val_accuracy: 0.8278\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8654\n",
      "Epoch 00156: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3006 - accuracy: 0.8658 - val_loss: 0.3568 - val_accuracy: 0.8377\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8660\n",
      "Epoch 00157: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3044 - accuracy: 0.8661 - val_loss: 0.3557 - val_accuracy: 0.8353\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8681\n",
      "Epoch 00158: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2994 - accuracy: 0.8683 - val_loss: 0.3580 - val_accuracy: 0.8368\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8678\n",
      "Epoch 00159: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3000 - accuracy: 0.8679 - val_loss: 0.3774 - val_accuracy: 0.8220\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8710\n",
      "Epoch 00160: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2995 - accuracy: 0.8712 - val_loss: 0.3641 - val_accuracy: 0.8360\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8700\n",
      "Epoch 00161: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2966 - accuracy: 0.8701 - val_loss: 0.3613 - val_accuracy: 0.8407\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8657\n",
      "Epoch 00162: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3023 - accuracy: 0.8658 - val_loss: 0.3617 - val_accuracy: 0.8379\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8723\n",
      "Epoch 00163: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2973 - accuracy: 0.8727 - val_loss: 0.3593 - val_accuracy: 0.8375\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8704\n",
      "Epoch 00164: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2967 - accuracy: 0.8703 - val_loss: 0.3625 - val_accuracy: 0.8325\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8736\n",
      "Epoch 00165: val_loss did not improve from 0.35507\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2901 - accuracy: 0.8738 - val_loss: 0.3577 - val_accuracy: 0.8377\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8714\n",
      "Epoch 00166: val_loss improved from 0.35507 to 0.35434, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2937 - accuracy: 0.8717 - val_loss: 0.3543 - val_accuracy: 0.8407\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8745\n",
      "Epoch 00167: val_loss did not improve from 0.35434\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2890 - accuracy: 0.8747 - val_loss: 0.3559 - val_accuracy: 0.8394\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8768\n",
      "Epoch 00168: val_loss did not improve from 0.35434\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2898 - accuracy: 0.8769 - val_loss: 0.3596 - val_accuracy: 0.8356\n",
      "Epoch 169/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8717\n",
      "Epoch 00169: val_loss did not improve from 0.35434\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2922 - accuracy: 0.8719 - val_loss: 0.3620 - val_accuracy: 0.8394\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8694\n",
      "Epoch 00170: val_loss improved from 0.35434 to 0.35295, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2912 - accuracy: 0.8699 - val_loss: 0.3530 - val_accuracy: 0.8409\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.8752\n",
      "Epoch 00171: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2846 - accuracy: 0.8754 - val_loss: 0.3672 - val_accuracy: 0.8306\n",
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.8781\n",
      "Epoch 00172: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2862 - accuracy: 0.8783 - val_loss: 0.3549 - val_accuracy: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8802\n",
      "Epoch 00173: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2822 - accuracy: 0.8803 - val_loss: 0.3589 - val_accuracy: 0.8407\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8773\n",
      "Epoch 00174: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2796 - accuracy: 0.8774 - val_loss: 0.3711 - val_accuracy: 0.8358\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8779\n",
      "Epoch 00175: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2794 - accuracy: 0.8783 - val_loss: 0.3572 - val_accuracy: 0.8396\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.8763\n",
      "Epoch 00176: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2829 - accuracy: 0.8768 - val_loss: 0.3598 - val_accuracy: 0.8383\n",
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8797\n",
      "Epoch 00177: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2758 - accuracy: 0.8800 - val_loss: 0.3733 - val_accuracy: 0.8358\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2750 - accuracy: 0.8815\n",
      "Epoch 00178: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.2746 - accuracy: 0.8817 - val_loss: 0.3557 - val_accuracy: 0.8394\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.8802\n",
      "Epoch 00179: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2738 - accuracy: 0.8807 - val_loss: 0.3650 - val_accuracy: 0.8396\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8808\n",
      "Epoch 00180: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2770 - accuracy: 0.8811 - val_loss: 0.3665 - val_accuracy: 0.8405\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.8830\n",
      "Epoch 00181: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2720 - accuracy: 0.8832 - val_loss: 0.3647 - val_accuracy: 0.8371\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.8817\n",
      "Epoch 00182: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2729 - accuracy: 0.8819 - val_loss: 0.3752 - val_accuracy: 0.8336\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.8787\n",
      "Epoch 00183: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2737 - accuracy: 0.8792 - val_loss: 0.3665 - val_accuracy: 0.8321\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8838\n",
      "Epoch 00184: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2724 - accuracy: 0.8838 - val_loss: 0.3662 - val_accuracy: 0.8392\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.8853\n",
      "Epoch 00185: val_loss did not improve from 0.35295\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2664 - accuracy: 0.8853 - val_loss: 0.3539 - val_accuracy: 0.8420\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.8856\n",
      "Epoch 00186: val_loss improved from 0.35295 to 0.35291, saving model to pickled_objects/batch_size_256_lr_0.08_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2646 - accuracy: 0.8860 - val_loss: 0.3529 - val_accuracy: 0.8435\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8855\n",
      "Epoch 00187: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2664 - accuracy: 0.8857 - val_loss: 0.3621 - val_accuracy: 0.8360\n",
      "Epoch 188/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8845\n",
      "Epoch 00188: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2690 - accuracy: 0.8845 - val_loss: 0.3914 - val_accuracy: 0.8207\n",
      "Epoch 189/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.8873\n",
      "Epoch 00189: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2658 - accuracy: 0.8872 - val_loss: 0.3617 - val_accuracy: 0.8360\n",
      "Epoch 190/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2632 - accuracy: 0.8867\n",
      "Epoch 00190: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2628 - accuracy: 0.8870 - val_loss: 0.3672 - val_accuracy: 0.8366\n",
      "Epoch 191/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.8880\n",
      "Epoch 00191: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2621 - accuracy: 0.8882 - val_loss: 0.3764 - val_accuracy: 0.8319\n",
      "Epoch 192/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2651 - accuracy: 0.8869\n",
      "Epoch 00192: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2641 - accuracy: 0.8874 - val_loss: 0.3588 - val_accuracy: 0.8431\n",
      "Epoch 193/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.8876\n",
      "Epoch 00193: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2592 - accuracy: 0.8881 - val_loss: 0.3550 - val_accuracy: 0.8418\n",
      "Epoch 194/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8871\n",
      "Epoch 00194: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2590 - accuracy: 0.8875 - val_loss: 0.3789 - val_accuracy: 0.8323\n",
      "Epoch 195/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.8894\n",
      "Epoch 00195: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2599 - accuracy: 0.8897 - val_loss: 0.3596 - val_accuracy: 0.8416\n",
      "Epoch 196/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8911\n",
      "Epoch 00196: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2520 - accuracy: 0.8915 - val_loss: 0.3688 - val_accuracy: 0.8347\n",
      "Epoch 197/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.8894\n",
      "Epoch 00197: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2551 - accuracy: 0.8897 - val_loss: 0.3657 - val_accuracy: 0.8431\n",
      "Epoch 198/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.8920\n",
      "Epoch 00198: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2514 - accuracy: 0.8924 - val_loss: 0.3682 - val_accuracy: 0.8401\n",
      "Epoch 199/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.8920\n",
      "Epoch 00199: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2533 - accuracy: 0.8923 - val_loss: 0.3714 - val_accuracy: 0.8409\n",
      "Epoch 200/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.8912\n",
      "Epoch 00200: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2551 - accuracy: 0.8914 - val_loss: 0.3635 - val_accuracy: 0.8403\n",
      "Epoch 201/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.8962\n",
      "Epoch 00201: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2464 - accuracy: 0.8963 - val_loss: 0.3538 - val_accuracy: 0.8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.8971\n",
      "Epoch 00202: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2438 - accuracy: 0.8967 - val_loss: 0.3894 - val_accuracy: 0.8330\n",
      "Epoch 203/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8942\n",
      "Epoch 00203: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2457 - accuracy: 0.8940 - val_loss: 0.3740 - val_accuracy: 0.8388\n",
      "Epoch 204/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8955\n",
      "Epoch 00204: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2433 - accuracy: 0.8958 - val_loss: 0.3782 - val_accuracy: 0.8340\n",
      "Epoch 205/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.8950\n",
      "Epoch 00205: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2448 - accuracy: 0.8952 - val_loss: 0.3635 - val_accuracy: 0.8426\n",
      "Epoch 206/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.8931\n",
      "Epoch 00206: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2465 - accuracy: 0.8934 - val_loss: 0.3673 - val_accuracy: 0.8368\n",
      "Epoch 207/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.8998\n",
      "Epoch 00207: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2387 - accuracy: 0.8998 - val_loss: 0.3730 - val_accuracy: 0.8399\n",
      "Epoch 208/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.8986\n",
      "Epoch 00208: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2419 - accuracy: 0.8986 - val_loss: 0.3690 - val_accuracy: 0.8347\n",
      "Epoch 209/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.8998\n",
      "Epoch 00209: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2372 - accuracy: 0.9002 - val_loss: 0.3647 - val_accuracy: 0.8426\n",
      "Epoch 210/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.8965\n",
      "Epoch 00210: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2424 - accuracy: 0.8969 - val_loss: 0.3634 - val_accuracy: 0.8366\n",
      "Epoch 211/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.8996\n",
      "Epoch 00211: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2357 - accuracy: 0.8997 - val_loss: 0.3627 - val_accuracy: 0.8388\n",
      "Epoch 212/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.8952\n",
      "Epoch 00212: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2461 - accuracy: 0.8953 - val_loss: 0.3681 - val_accuracy: 0.8431\n",
      "Epoch 213/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9006\n",
      "Epoch 00213: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2394 - accuracy: 0.9009 - val_loss: 0.3731 - val_accuracy: 0.8377\n",
      "Epoch 214/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9027\n",
      "Epoch 00214: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2288 - accuracy: 0.9027 - val_loss: 0.3774 - val_accuracy: 0.8364\n",
      "Epoch 215/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9007\n",
      "Epoch 00215: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2362 - accuracy: 0.9011 - val_loss: 0.3664 - val_accuracy: 0.8414\n",
      "Epoch 216/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.9046\n",
      "Epoch 00216: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2342 - accuracy: 0.9046 - val_loss: 0.3643 - val_accuracy: 0.8409\n",
      "Epoch 217/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9004\n",
      "Epoch 00217: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2372 - accuracy: 0.9005 - val_loss: 0.3748 - val_accuracy: 0.8364\n",
      "Epoch 218/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9007\n",
      "Epoch 00218: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2306 - accuracy: 0.9011 - val_loss: 0.3706 - val_accuracy: 0.8386\n",
      "Epoch 219/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9041\n",
      "Epoch 00219: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2302 - accuracy: 0.9045 - val_loss: 0.3726 - val_accuracy: 0.8338\n",
      "Epoch 220/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9017\n",
      "Epoch 00220: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2315 - accuracy: 0.9020 - val_loss: 0.3743 - val_accuracy: 0.8381\n",
      "Epoch 221/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9037\n",
      "Epoch 00221: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2291 - accuracy: 0.9041 - val_loss: 0.3687 - val_accuracy: 0.8448\n",
      "Epoch 222/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9068\n",
      "Epoch 00222: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2263 - accuracy: 0.9071 - val_loss: 0.3641 - val_accuracy: 0.8399\n",
      "Epoch 223/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2210 - accuracy: 0.9080\n",
      "Epoch 00223: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2204 - accuracy: 0.9083 - val_loss: 0.3755 - val_accuracy: 0.8358\n",
      "Epoch 224/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9048\n",
      "Epoch 00224: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2273 - accuracy: 0.9046 - val_loss: 0.3670 - val_accuracy: 0.8426\n",
      "Epoch 225/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9057\n",
      "Epoch 00225: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2253 - accuracy: 0.9059 - val_loss: 0.3569 - val_accuracy: 0.8452\n",
      "Epoch 226/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9079\n",
      "Epoch 00226: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2192 - accuracy: 0.9084 - val_loss: 0.3693 - val_accuracy: 0.8386\n",
      "Epoch 227/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9077\n",
      "Epoch 00227: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2210 - accuracy: 0.9075 - val_loss: 0.3867 - val_accuracy: 0.8343\n",
      "Epoch 228/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.9062\n",
      "Epoch 00228: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2219 - accuracy: 0.9064 - val_loss: 0.3997 - val_accuracy: 0.8287\n",
      "Epoch 229/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.9080\n",
      "Epoch 00229: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2220 - accuracy: 0.9084 - val_loss: 0.3819 - val_accuracy: 0.8379\n",
      "Epoch 230/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.9089\n",
      "Epoch 00230: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2166 - accuracy: 0.9092 - val_loss: 0.3653 - val_accuracy: 0.8418\n",
      "Epoch 231/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9048\n",
      "Epoch 00231: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2249 - accuracy: 0.9051 - val_loss: 0.3681 - val_accuracy: 0.8386\n",
      "Epoch 232/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2186 - accuracy: 0.9093\n",
      "Epoch 00232: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2185 - accuracy: 0.9093 - val_loss: 0.3758 - val_accuracy: 0.8405\n",
      "Epoch 233/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2118 - accuracy: 0.9125\n",
      "Epoch 00233: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2110 - accuracy: 0.9127 - val_loss: 0.3730 - val_accuracy: 0.8381\n",
      "Epoch 234/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2201 - accuracy: 0.9093\n",
      "Epoch 00234: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2193 - accuracy: 0.9093 - val_loss: 0.3703 - val_accuracy: 0.8388\n",
      "Epoch 235/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9127\n",
      "Epoch 00235: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2123 - accuracy: 0.9128 - val_loss: 0.3744 - val_accuracy: 0.8401\n",
      "Epoch 236/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2143 - accuracy: 0.9091\n",
      "Epoch 00236: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2134 - accuracy: 0.9095 - val_loss: 0.3842 - val_accuracy: 0.8373\n",
      "Epoch 237/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9140\n",
      "Epoch 00237: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2055 - accuracy: 0.9142 - val_loss: 0.3759 - val_accuracy: 0.8411\n",
      "Epoch 238/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9098\n",
      "Epoch 00238: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2130 - accuracy: 0.9099 - val_loss: 0.3711 - val_accuracy: 0.8444\n",
      "Epoch 239/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9116\n",
      "Epoch 00239: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2148 - accuracy: 0.9116 - val_loss: 0.3765 - val_accuracy: 0.8431\n",
      "Epoch 240/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9150\n",
      "Epoch 00240: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2056 - accuracy: 0.9152 - val_loss: 0.3732 - val_accuracy: 0.8373\n",
      "Epoch 241/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9129\n",
      "Epoch 00241: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2106 - accuracy: 0.9129 - val_loss: 0.3836 - val_accuracy: 0.8429\n",
      "Epoch 242/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9155\n",
      "Epoch 00242: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2037 - accuracy: 0.9159 - val_loss: 0.3852 - val_accuracy: 0.8422\n",
      "Epoch 243/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9127\n",
      "Epoch 00243: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2097 - accuracy: 0.9128 - val_loss: 0.3714 - val_accuracy: 0.8418\n",
      "Epoch 244/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9155\n",
      "Epoch 00244: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2027 - accuracy: 0.9159 - val_loss: 0.3703 - val_accuracy: 0.8429\n",
      "Epoch 245/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2062 - accuracy: 0.9134\n",
      "Epoch 00245: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2052 - accuracy: 0.9136 - val_loss: 0.3760 - val_accuracy: 0.8435\n",
      "Epoch 246/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9144\n",
      "Epoch 00246: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2075 - accuracy: 0.9145 - val_loss: 0.3804 - val_accuracy: 0.8409\n",
      "Epoch 247/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2075 - accuracy: 0.9137\n",
      "Epoch 00247: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2071 - accuracy: 0.9139 - val_loss: 0.3778 - val_accuracy: 0.8366\n",
      "Epoch 248/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9203\n",
      "Epoch 00248: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1965 - accuracy: 0.9206 - val_loss: 0.3740 - val_accuracy: 0.8418\n",
      "Epoch 249/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9186\n",
      "Epoch 00249: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1969 - accuracy: 0.9188 - val_loss: 0.3858 - val_accuracy: 0.8347\n",
      "Epoch 250/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9156\n",
      "Epoch 00250: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1994 - accuracy: 0.9159 - val_loss: 0.3924 - val_accuracy: 0.8362\n",
      "Epoch 251/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9169\n",
      "Epoch 00251: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2017 - accuracy: 0.9172 - val_loss: 0.3870 - val_accuracy: 0.8358\n",
      "Epoch 252/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9188\n",
      "Epoch 00252: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1994 - accuracy: 0.9188 - val_loss: 0.3835 - val_accuracy: 0.8414\n",
      "Epoch 253/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9155\n",
      "Epoch 00253: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2002 - accuracy: 0.9159 - val_loss: 0.3832 - val_accuracy: 0.8371\n",
      "Epoch 254/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.9161\n",
      "Epoch 00254: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1984 - accuracy: 0.9161 - val_loss: 0.3905 - val_accuracy: 0.8375\n",
      "Epoch 255/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9209\n",
      "Epoch 00255: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1998 - accuracy: 0.9210 - val_loss: 0.3963 - val_accuracy: 0.8293\n",
      "Epoch 256/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9173\n",
      "Epoch 00256: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1963 - accuracy: 0.9177 - val_loss: 0.3921 - val_accuracy: 0.8411\n",
      "Epoch 257/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.9244\n",
      "Epoch 00257: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1903 - accuracy: 0.9245 - val_loss: 0.3963 - val_accuracy: 0.8345\n",
      "Epoch 258/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9205\n",
      "Epoch 00258: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1959 - accuracy: 0.9206 - val_loss: 0.3796 - val_accuracy: 0.8433\n",
      "Epoch 259/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9210\n",
      "Epoch 00259: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1921 - accuracy: 0.9214 - val_loss: 0.3841 - val_accuracy: 0.8411\n",
      "Epoch 260/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.9208\n",
      "Epoch 00260: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1904 - accuracy: 0.9209 - val_loss: 0.3836 - val_accuracy: 0.8399\n",
      "Epoch 261/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9236\n",
      "Epoch 00261: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1879 - accuracy: 0.9239 - val_loss: 0.3947 - val_accuracy: 0.8416\n",
      "Epoch 262/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1882 - accuracy: 0.9221\n",
      "Epoch 00262: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1874 - accuracy: 0.9224 - val_loss: 0.3984 - val_accuracy: 0.8381\n",
      "Epoch 263/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9197\n",
      "Epoch 00263: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1936 - accuracy: 0.9196 - val_loss: 0.3961 - val_accuracy: 0.8388\n",
      "Epoch 264/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9205\n",
      "Epoch 00264: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1874 - accuracy: 0.9209 - val_loss: 0.3892 - val_accuracy: 0.8416\n",
      "Epoch 265/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9246\n",
      "Epoch 00265: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1839 - accuracy: 0.9249 - val_loss: 0.3989 - val_accuracy: 0.8362\n",
      "Epoch 266/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9208\n",
      "Epoch 00266: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1935 - accuracy: 0.9213 - val_loss: 0.3867 - val_accuracy: 0.8396\n",
      "Epoch 267/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9240\n",
      "Epoch 00267: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1877 - accuracy: 0.9241 - val_loss: 0.4036 - val_accuracy: 0.8351\n",
      "Epoch 268/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1869 - accuracy: 0.9261\n",
      "Epoch 00268: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1858 - accuracy: 0.9264 - val_loss: 0.3972 - val_accuracy: 0.8356\n",
      "Epoch 269/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9236\n",
      "Epoch 00269: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1849 - accuracy: 0.9238 - val_loss: 0.3927 - val_accuracy: 0.8368\n",
      "Epoch 270/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9250\n",
      "Epoch 00270: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1819 - accuracy: 0.9253 - val_loss: 0.3960 - val_accuracy: 0.8401\n",
      "Epoch 271/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9259\n",
      "Epoch 00271: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.1814 - accuracy: 0.9261 - val_loss: 0.4101 - val_accuracy: 0.8261\n",
      "Epoch 272/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9244\n",
      "Epoch 00272: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1867 - accuracy: 0.9249 - val_loss: 0.3957 - val_accuracy: 0.8364\n",
      "Epoch 273/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9265\n",
      "Epoch 00273: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1771 - accuracy: 0.9266 - val_loss: 0.4031 - val_accuracy: 0.8304\n",
      "Epoch 274/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9291\n",
      "Epoch 00274: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1753 - accuracy: 0.9294 - val_loss: 0.3927 - val_accuracy: 0.8420\n",
      "Epoch 275/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9284\n",
      "Epoch 00275: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1777 - accuracy: 0.9286 - val_loss: 0.3880 - val_accuracy: 0.8403\n",
      "Epoch 276/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9277\n",
      "Epoch 00276: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1753 - accuracy: 0.9281 - val_loss: 0.3945 - val_accuracy: 0.8392\n",
      "Epoch 277/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9259\n",
      "Epoch 00277: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1795 - accuracy: 0.9262 - val_loss: 0.4093 - val_accuracy: 0.8343\n",
      "Epoch 278/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9280\n",
      "Epoch 00278: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1761 - accuracy: 0.9280 - val_loss: 0.3992 - val_accuracy: 0.8366\n",
      "Epoch 279/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9302\n",
      "Epoch 00279: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.1683 - accuracy: 0.9305 - val_loss: 0.3974 - val_accuracy: 0.8360\n",
      "Epoch 280/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9282\n",
      "Epoch 00280: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1761 - accuracy: 0.9285 - val_loss: 0.4004 - val_accuracy: 0.8399\n",
      "Epoch 281/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9281\n",
      "Epoch 00281: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1735 - accuracy: 0.9284 - val_loss: 0.4115 - val_accuracy: 0.8351\n",
      "Epoch 282/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9296\n",
      "Epoch 00282: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1725 - accuracy: 0.9295 - val_loss: 0.3960 - val_accuracy: 0.8351\n",
      "Epoch 283/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9280\n",
      "Epoch 00283: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1763 - accuracy: 0.9282 - val_loss: 0.3976 - val_accuracy: 0.8349\n",
      "Epoch 284/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9285\n",
      "Epoch 00284: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1722 - accuracy: 0.9287 - val_loss: 0.3943 - val_accuracy: 0.8362\n",
      "Epoch 285/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9285\n",
      "Epoch 00285: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1712 - accuracy: 0.9287 - val_loss: 0.4179 - val_accuracy: 0.8358\n",
      "Epoch 286/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.9339\n",
      "Epoch 00286: val_loss did not improve from 0.35291\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1651 - accuracy: 0.9336 - val_loss: 0.4024 - val_accuracy: 0.8379\n",
      "Epoch 00286: early stopping\n",
      "Epoch 1/10000\n",
      "     73/Unknown - 8s 106ms/step - loss: 0.6903 - accuracy: 0.5330\n",
      "Epoch 00001: val_loss improved from inf to 0.69185, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 0.6903 - accuracy: 0.5330 - val_loss: 0.6918 - val_accuracy: 0.4961\n",
      "Epoch 2/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5471\n",
      "Epoch 00002: val_loss improved from 0.69185 to 0.69083, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6872 - accuracy: 0.5472 - val_loss: 0.6908 - val_accuracy: 0.5019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5616\n",
      "Epoch 00003: val_loss improved from 0.69083 to 0.67703, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6835 - accuracy: 0.5621 - val_loss: 0.6770 - val_accuracy: 0.5933\n",
      "Epoch 4/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5208\n",
      "Epoch 00004: val_loss did not improve from 0.67703\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6925 - accuracy: 0.5208 - val_loss: 0.6917 - val_accuracy: 0.5739\n",
      "Epoch 5/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6830 - accuracy: 0.5707\n",
      "Epoch 00005: val_loss did not improve from 0.67703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6828 - accuracy: 0.5707 - val_loss: 0.6774 - val_accuracy: 0.5638\n",
      "Epoch 6/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6749 - accuracy: 0.5830\n",
      "Epoch 00006: val_loss did not improve from 0.67703\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6751 - accuracy: 0.5825 - val_loss: 0.6795 - val_accuracy: 0.5503\n",
      "Epoch 7/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6694 - accuracy: 0.5889\n",
      "Epoch 00007: val_loss improved from 0.67703 to 0.66267, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.6694 - accuracy: 0.5885 - val_loss: 0.6627 - val_accuracy: 0.6043\n",
      "Epoch 8/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.5830\n",
      "Epoch 00008: val_loss improved from 0.66267 to 0.66265, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6724 - accuracy: 0.5828 - val_loss: 0.6626 - val_accuracy: 0.5997\n",
      "Epoch 9/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.6088\n",
      "Epoch 00009: val_loss improved from 0.66265 to 0.65714, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6597 - accuracy: 0.6090 - val_loss: 0.6571 - val_accuracy: 0.6116\n",
      "Epoch 10/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6247\n",
      "Epoch 00010: val_loss improved from 0.65714 to 0.65461, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6495 - accuracy: 0.6244 - val_loss: 0.6546 - val_accuracy: 0.6146\n",
      "Epoch 11/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6416 - accuracy: 0.6379\n",
      "Epoch 00011: val_loss did not improve from 0.65461\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6416 - accuracy: 0.6379 - val_loss: 0.6576 - val_accuracy: 0.5972\n",
      "Epoch 12/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6319 - accuracy: 0.6450\n",
      "Epoch 00012: val_loss did not improve from 0.65461\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6321 - accuracy: 0.6449 - val_loss: 0.6793 - val_accuracy: 0.5628\n",
      "Epoch 13/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6296 - accuracy: 0.6463\n",
      "Epoch 00013: val_loss improved from 0.65461 to 0.62190, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6296 - accuracy: 0.6464 - val_loss: 0.6219 - val_accuracy: 0.6531\n",
      "Epoch 14/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6199 - accuracy: 0.6581\n",
      "Epoch 00014: val_loss improved from 0.62190 to 0.61925, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6202 - accuracy: 0.6577 - val_loss: 0.6192 - val_accuracy: 0.6533\n",
      "Epoch 15/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.6668\n",
      "Epoch 00015: val_loss improved from 0.61925 to 0.59910, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6082 - accuracy: 0.6665 - val_loss: 0.5991 - val_accuracy: 0.6672\n",
      "Epoch 16/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.6109 - accuracy: 0.6642\n",
      "Epoch 00016: val_loss improved from 0.59910 to 0.59731, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6111 - accuracy: 0.6639 - val_loss: 0.5973 - val_accuracy: 0.6696\n",
      "Epoch 17/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5843 - accuracy: 0.6865\n",
      "Epoch 00017: val_loss improved from 0.59731 to 0.57016, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5841 - accuracy: 0.6869 - val_loss: 0.5702 - val_accuracy: 0.6920\n",
      "Epoch 18/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5968 - accuracy: 0.6726\n",
      "Epoch 00018: val_loss improved from 0.57016 to 0.56733, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5966 - accuracy: 0.6728 - val_loss: 0.5673 - val_accuracy: 0.6967\n",
      "Epoch 19/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.6945\n",
      "Epoch 00019: val_loss improved from 0.56733 to 0.55470, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5736 - accuracy: 0.6945 - val_loss: 0.5547 - val_accuracy: 0.7025\n",
      "Epoch 20/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7017\n",
      "Epoch 00020: val_loss improved from 0.55470 to 0.53337, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5681 - accuracy: 0.7016 - val_loss: 0.5334 - val_accuracy: 0.7455\n",
      "Epoch 21/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5988 - accuracy: 0.6797\n",
      "Epoch 00021: val_loss did not improve from 0.53337\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5990 - accuracy: 0.6793 - val_loss: 0.6062 - val_accuracy: 0.6619\n",
      "Epoch 22/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.6905\n",
      "Epoch 00022: val_loss did not improve from 0.53337\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5840 - accuracy: 0.6906 - val_loss: 0.6035 - val_accuracy: 0.6561\n",
      "Epoch 23/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5569 - accuracy: 0.7115\n",
      "Epoch 00023: val_loss improved from 0.53337 to 0.53321, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5568 - accuracy: 0.7117 - val_loss: 0.5332 - val_accuracy: 0.7268\n",
      "Epoch 24/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7204\n",
      "Epoch 00024: val_loss improved from 0.53321 to 0.51743, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5430 - accuracy: 0.7201 - val_loss: 0.5174 - val_accuracy: 0.7373\n",
      "Epoch 25/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7248\n",
      "Epoch 00025: val_loss improved from 0.51743 to 0.50709, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5390 - accuracy: 0.7246 - val_loss: 0.5071 - val_accuracy: 0.7545\n",
      "Epoch 26/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5367 - accuracy: 0.7285\n",
      "Epoch 00026: val_loss did not improve from 0.50709\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5371 - accuracy: 0.7278 - val_loss: 0.5122 - val_accuracy: 0.7530\n",
      "Epoch 27/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5321 - accuracy: 0.7288\n",
      "Epoch 00027: val_loss did not improve from 0.50709\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5324 - accuracy: 0.7289 - val_loss: 0.5246 - val_accuracy: 0.7289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7369\n",
      "Epoch 00028: val_loss did not improve from 0.50709\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5238 - accuracy: 0.7366 - val_loss: 0.5140 - val_accuracy: 0.7339\n",
      "Epoch 29/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7326\n",
      "Epoch 00029: val_loss improved from 0.50709 to 0.49721, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5250 - accuracy: 0.7323 - val_loss: 0.4972 - val_accuracy: 0.7629\n",
      "Epoch 30/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5177 - accuracy: 0.7404\n",
      "Epoch 00030: val_loss improved from 0.49721 to 0.48645, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5178 - accuracy: 0.7401 - val_loss: 0.4864 - val_accuracy: 0.7610\n",
      "Epoch 31/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7423\n",
      "Epoch 00031: val_loss did not improve from 0.48645\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5148 - accuracy: 0.7420 - val_loss: 0.5126 - val_accuracy: 0.7558\n",
      "Epoch 32/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5146 - accuracy: 0.7411\n",
      "Epoch 00032: val_loss did not improve from 0.48645\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5147 - accuracy: 0.7409 - val_loss: 0.4974 - val_accuracy: 0.7487\n",
      "Epoch 33/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5056 - accuracy: 0.7459\n",
      "Epoch 00033: val_loss did not improve from 0.48645\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5055 - accuracy: 0.7459 - val_loss: 0.4905 - val_accuracy: 0.7590\n",
      "Epoch 34/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.7456\n",
      "Epoch 00034: val_loss did not improve from 0.48645\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5081 - accuracy: 0.7458 - val_loss: 0.5001 - val_accuracy: 0.7425\n",
      "Epoch 35/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5050 - accuracy: 0.7496\n",
      "Epoch 00035: val_loss did not improve from 0.48645\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5050 - accuracy: 0.7498 - val_loss: 0.5006 - val_accuracy: 0.7399\n",
      "Epoch 36/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7496\n",
      "Epoch 00036: val_loss improved from 0.48645 to 0.48288, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5022 - accuracy: 0.7494 - val_loss: 0.4829 - val_accuracy: 0.7681\n",
      "Epoch 37/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7541\n",
      "Epoch 00037: val_loss did not improve from 0.48288\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4978 - accuracy: 0.7538 - val_loss: 0.5326 - val_accuracy: 0.7169\n",
      "Epoch 38/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4928 - accuracy: 0.7568\n",
      "Epoch 00038: val_loss did not improve from 0.48288\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4932 - accuracy: 0.7566 - val_loss: 0.4981 - val_accuracy: 0.7483\n",
      "Epoch 39/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4952 - accuracy: 0.7536\n",
      "Epoch 00039: val_loss improved from 0.48288 to 0.48042, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4948 - accuracy: 0.7541 - val_loss: 0.4804 - val_accuracy: 0.7584\n",
      "Epoch 40/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4910 - accuracy: 0.7572\n",
      "Epoch 00040: val_loss did not improve from 0.48042\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4908 - accuracy: 0.7572 - val_loss: 0.4806 - val_accuracy: 0.7592\n",
      "Epoch 41/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4868 - accuracy: 0.7623\n",
      "Epoch 00041: val_loss did not improve from 0.48042\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4864 - accuracy: 0.7622 - val_loss: 0.4815 - val_accuracy: 0.7545\n",
      "Epoch 42/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4783 - accuracy: 0.7654\n",
      "Epoch 00042: val_loss did not improve from 0.48042\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4782 - accuracy: 0.7654 - val_loss: 0.5169 - val_accuracy: 0.7276\n",
      "Epoch 43/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4740 - accuracy: 0.7673\n",
      "Epoch 00043: val_loss improved from 0.48042 to 0.47444, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4744 - accuracy: 0.7670 - val_loss: 0.4744 - val_accuracy: 0.7633\n",
      "Epoch 44/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.7692\n",
      "Epoch 00044: val_loss improved from 0.47444 to 0.46912, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4778 - accuracy: 0.7690 - val_loss: 0.4691 - val_accuracy: 0.7726\n",
      "Epoch 45/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4788 - accuracy: 0.7656\n",
      "Epoch 00045: val_loss improved from 0.46912 to 0.45412, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4792 - accuracy: 0.7653 - val_loss: 0.4541 - val_accuracy: 0.7837\n",
      "Epoch 46/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4703 - accuracy: 0.7693\n",
      "Epoch 00046: val_loss did not improve from 0.45412\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4709 - accuracy: 0.7693 - val_loss: 0.4659 - val_accuracy: 0.7693\n",
      "Epoch 47/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.7721\n",
      "Epoch 00047: val_loss did not improve from 0.45412\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4664 - accuracy: 0.7723 - val_loss: 0.4964 - val_accuracy: 0.7457\n",
      "Epoch 48/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4622 - accuracy: 0.7782\n",
      "Epoch 00048: val_loss did not improve from 0.45412\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4620 - accuracy: 0.7784 - val_loss: 0.4707 - val_accuracy: 0.7623\n",
      "Epoch 49/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4607 - accuracy: 0.7781 ETA: 1s - loss: 0\n",
      "Epoch 00049: val_loss did not improve from 0.45412\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4604 - accuracy: 0.7781 - val_loss: 0.4657 - val_accuracy: 0.7687\n",
      "Epoch 50/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.7791\n",
      "Epoch 00050: val_loss did not improve from 0.45412\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4596 - accuracy: 0.7797 - val_loss: 0.4708 - val_accuracy: 0.7625\n",
      "Epoch 51/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4547 - accuracy: 0.7798\n",
      "Epoch 00051: val_loss improved from 0.45412 to 0.44698, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4543 - accuracy: 0.7802 - val_loss: 0.4470 - val_accuracy: 0.7807\n",
      "Epoch 52/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.7809\n",
      "Epoch 00052: val_loss did not improve from 0.44698\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4541 - accuracy: 0.7809 - val_loss: 0.4586 - val_accuracy: 0.7717\n",
      "Epoch 53/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4529 - accuracy: 0.7811\n",
      "Epoch 00053: val_loss did not improve from 0.44698\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4527 - accuracy: 0.7813 - val_loss: 0.4662 - val_accuracy: 0.7696\n",
      "Epoch 54/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.7803\n",
      "Epoch 00054: val_loss improved from 0.44698 to 0.44071, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4530 - accuracy: 0.7802 - val_loss: 0.4407 - val_accuracy: 0.7900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4443 - accuracy: 0.7853\n",
      "Epoch 00055: val_loss improved from 0.44071 to 0.43698, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4439 - accuracy: 0.7855 - val_loss: 0.4370 - val_accuracy: 0.7880\n",
      "Epoch 56/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.7892\n",
      "Epoch 00056: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4420 - accuracy: 0.7891 - val_loss: 0.4446 - val_accuracy: 0.7855\n",
      "Epoch 57/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.7911\n",
      "Epoch 00057: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4360 - accuracy: 0.7910 - val_loss: 0.4603 - val_accuracy: 0.7758\n",
      "Epoch 58/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.7906\n",
      "Epoch 00058: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4425 - accuracy: 0.7910 - val_loss: 0.4395 - val_accuracy: 0.7812\n",
      "Epoch 59/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.7923\n",
      "Epoch 00059: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4346 - accuracy: 0.7924 - val_loss: 0.4566 - val_accuracy: 0.7711\n",
      "Epoch 60/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.7949\n",
      "Epoch 00060: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4339 - accuracy: 0.7948 - val_loss: 0.4453 - val_accuracy: 0.7807\n",
      "Epoch 61/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4263 - accuracy: 0.7986\n",
      "Epoch 00061: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4262 - accuracy: 0.7987 - val_loss: 0.4860 - val_accuracy: 0.7567\n",
      "Epoch 62/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.7972\n",
      "Epoch 00062: val_loss did not improve from 0.43698\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4287 - accuracy: 0.7973 - val_loss: 0.4392 - val_accuracy: 0.7878\n",
      "Epoch 63/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8006\n",
      "Epoch 00063: val_loss improved from 0.43698 to 0.43563, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4215 - accuracy: 0.8009 - val_loss: 0.4356 - val_accuracy: 0.7853\n",
      "Epoch 64/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8011\n",
      "Epoch 00064: val_loss improved from 0.43563 to 0.43325, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4216 - accuracy: 0.8010 - val_loss: 0.4332 - val_accuracy: 0.7893\n",
      "Epoch 65/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8027\n",
      "Epoch 00065: val_loss did not improve from 0.43325\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4139 - accuracy: 0.8025 - val_loss: 0.4736 - val_accuracy: 0.7616\n",
      "Epoch 66/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4175 - accuracy: 0.8044\n",
      "Epoch 00066: val_loss improved from 0.43325 to 0.42686, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4173 - accuracy: 0.8044 - val_loss: 0.4269 - val_accuracy: 0.7956\n",
      "Epoch 67/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8056\n",
      "Epoch 00067: val_loss did not improve from 0.42686\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4104 - accuracy: 0.8062 - val_loss: 0.4343 - val_accuracy: 0.7874\n",
      "Epoch 68/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8078\n",
      "Epoch 00068: val_loss did not improve from 0.42686\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4104 - accuracy: 0.8080 - val_loss: 0.4646 - val_accuracy: 0.7693\n",
      "Epoch 69/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8096\n",
      "Epoch 00069: val_loss improved from 0.42686 to 0.42371, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4091 - accuracy: 0.8097 - val_loss: 0.4237 - val_accuracy: 0.8009\n",
      "Epoch 70/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4062 - accuracy: 0.8100\n",
      "Epoch 00070: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4062 - accuracy: 0.8102 - val_loss: 0.4395 - val_accuracy: 0.7829\n",
      "Epoch 71/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8090\n",
      "Epoch 00071: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4032 - accuracy: 0.8089 - val_loss: 0.4291 - val_accuracy: 0.7893\n",
      "Epoch 72/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.4020 - accuracy: 0.8135\n",
      "Epoch 00072: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4016 - accuracy: 0.8135 - val_loss: 0.4262 - val_accuracy: 0.7923\n",
      "Epoch 73/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8169\n",
      "Epoch 00073: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3953 - accuracy: 0.8170 - val_loss: 0.4373 - val_accuracy: 0.7844\n",
      "Epoch 74/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8131\n",
      "Epoch 00074: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3974 - accuracy: 0.8131 - val_loss: 0.4473 - val_accuracy: 0.7799\n",
      "Epoch 75/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8192\n",
      "Epoch 00075: val_loss did not improve from 0.42371\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3917 - accuracy: 0.8190 - val_loss: 0.4411 - val_accuracy: 0.7876\n",
      "Epoch 76/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8211\n",
      "Epoch 00076: val_loss improved from 0.42371 to 0.41883, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3884 - accuracy: 0.8210 - val_loss: 0.4188 - val_accuracy: 0.8025\n",
      "Epoch 77/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.8241\n",
      "Epoch 00077: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3841 - accuracy: 0.8243 - val_loss: 0.4402 - val_accuracy: 0.7853\n",
      "Epoch 78/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8238\n",
      "Epoch 00078: val_loss did not improve from 0.41883\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3858 - accuracy: 0.8239 - val_loss: 0.4403 - val_accuracy: 0.7829\n",
      "Epoch 79/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8215\n",
      "Epoch 00079: val_loss improved from 0.41883 to 0.41549, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3829 - accuracy: 0.8220 - val_loss: 0.4155 - val_accuracy: 0.8009\n",
      "Epoch 80/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8248\n",
      "Epoch 00080: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3804 - accuracy: 0.8249 - val_loss: 0.4192 - val_accuracy: 0.7975\n",
      "Epoch 81/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8261\n",
      "Epoch 00081: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3805 - accuracy: 0.8261 - val_loss: 0.4161 - val_accuracy: 0.7971\n",
      "Epoch 82/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8272\n",
      "Epoch 00082: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3778 - accuracy: 0.8278 - val_loss: 0.4313 - val_accuracy: 0.7893\n",
      "Epoch 83/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3743 - accuracy: 0.8279\n",
      "Epoch 00083: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3747 - accuracy: 0.8279 - val_loss: 0.5000 - val_accuracy: 0.7470\n",
      "Epoch 84/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3771 - accuracy: 0.8303\n",
      "Epoch 00084: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3772 - accuracy: 0.8300 - val_loss: 0.4318 - val_accuracy: 0.7934\n",
      "Epoch 85/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8319\n",
      "Epoch 00085: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3683 - accuracy: 0.8322 - val_loss: 0.4207 - val_accuracy: 0.8031\n",
      "Epoch 86/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8310\n",
      "Epoch 00086: val_loss did not improve from 0.41549\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3690 - accuracy: 0.8315 - val_loss: 0.4236 - val_accuracy: 0.7988\n",
      "Epoch 87/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8371\n",
      "Epoch 00087: val_loss improved from 0.41549 to 0.40710, saving model to pickled_objects/batch_size_256_lr_0.16_best_weights_trial_4.h5\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3644 - accuracy: 0.8375 - val_loss: 0.4071 - val_accuracy: 0.8067\n",
      "Epoch 88/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3618 - accuracy: 0.8340\n",
      "Epoch 00088: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3607 - accuracy: 0.8345 - val_loss: 0.4281 - val_accuracy: 0.7906\n",
      "Epoch 89/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8364\n",
      "Epoch 00089: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3624 - accuracy: 0.8362 - val_loss: 0.4129 - val_accuracy: 0.8076\n",
      "Epoch 90/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8387\n",
      "Epoch 00090: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3522 - accuracy: 0.8388 - val_loss: 0.4235 - val_accuracy: 0.7979\n",
      "Epoch 91/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3558 - accuracy: 0.8415\n",
      "Epoch 00091: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3585 - accuracy: 0.8405 - val_loss: 0.4411 - val_accuracy: 0.7906\n",
      "Epoch 92/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8424\n",
      "Epoch 00092: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3531 - accuracy: 0.8426 - val_loss: 0.4187 - val_accuracy: 0.7979\n",
      "Epoch 93/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8387\n",
      "Epoch 00093: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3540 - accuracy: 0.8392 - val_loss: 0.4169 - val_accuracy: 0.7964\n",
      "Epoch 94/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3483 - accuracy: 0.8433\n",
      "Epoch 00094: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3476 - accuracy: 0.8436 - val_loss: 0.4293 - val_accuracy: 0.7906\n",
      "Epoch 95/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8416\n",
      "Epoch 00095: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3496 - accuracy: 0.8418 - val_loss: 0.4156 - val_accuracy: 0.7990\n",
      "Epoch 96/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8422\n",
      "Epoch 00096: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3474 - accuracy: 0.8420 - val_loss: 0.4084 - val_accuracy: 0.8044\n",
      "Epoch 97/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8465\n",
      "Epoch 00097: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3369 - accuracy: 0.8469 - val_loss: 0.4158 - val_accuracy: 0.8016\n",
      "Epoch 98/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.8502\n",
      "Epoch 00098: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3339 - accuracy: 0.8502 - val_loss: 0.4288 - val_accuracy: 0.7962\n",
      "Epoch 99/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8519\n",
      "Epoch 00099: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3321 - accuracy: 0.8523 - val_loss: 0.4081 - val_accuracy: 0.8033\n",
      "Epoch 100/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8499\n",
      "Epoch 00100: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3381 - accuracy: 0.8497 - val_loss: 0.4621 - val_accuracy: 0.7706\n",
      "Epoch 101/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8511\n",
      "Epoch 00101: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3351 - accuracy: 0.8516 - val_loss: 0.4255 - val_accuracy: 0.7951\n",
      "Epoch 102/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8544\n",
      "Epoch 00102: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3267 - accuracy: 0.8546 - val_loss: 0.4251 - val_accuracy: 0.8003\n",
      "Epoch 103/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.8549\n",
      "Epoch 00103: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3291 - accuracy: 0.8545 - val_loss: 0.4383 - val_accuracy: 0.7893\n",
      "Epoch 104/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8561\n",
      "Epoch 00104: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3211 - accuracy: 0.8563 - val_loss: 0.4115 - val_accuracy: 0.8059\n",
      "Epoch 105/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8602\n",
      "Epoch 00105: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3193 - accuracy: 0.8601 - val_loss: 0.4129 - val_accuracy: 0.8072\n",
      "Epoch 106/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8556\n",
      "Epoch 00106: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3227 - accuracy: 0.8556 - val_loss: 0.4209 - val_accuracy: 0.7984\n",
      "Epoch 107/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8586\n",
      "Epoch 00107: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3193 - accuracy: 0.8588 - val_loss: 0.4302 - val_accuracy: 0.7906\n",
      "Epoch 108/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8589\n",
      "Epoch 00108: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3166 - accuracy: 0.8587 - val_loss: 0.4262 - val_accuracy: 0.7951\n",
      "Epoch 109/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8625\n",
      "Epoch 00109: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3151 - accuracy: 0.8626 - val_loss: 0.4229 - val_accuracy: 0.7994\n",
      "Epoch 110/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8598\n",
      "Epoch 00110: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3128 - accuracy: 0.8602 - val_loss: 0.4233 - val_accuracy: 0.7992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8650\n",
      "Epoch 00111: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3040 - accuracy: 0.8650 - val_loss: 0.4776 - val_accuracy: 0.7687\n",
      "Epoch 112/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8625\n",
      "Epoch 00112: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3087 - accuracy: 0.8631 - val_loss: 0.4337 - val_accuracy: 0.7960\n",
      "Epoch 113/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8657\n",
      "Epoch 00113: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3030 - accuracy: 0.8657 - val_loss: 0.4350 - val_accuracy: 0.7997\n",
      "Epoch 114/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8697\n",
      "Epoch 00114: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2993 - accuracy: 0.8692 - val_loss: 0.4544 - val_accuracy: 0.7859\n",
      "Epoch 115/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8617\n",
      "Epoch 00115: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3058 - accuracy: 0.8617 - val_loss: 0.4299 - val_accuracy: 0.8001\n",
      "Epoch 116/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8695\n",
      "Epoch 00116: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2951 - accuracy: 0.8699 - val_loss: 0.4216 - val_accuracy: 0.8016\n",
      "Epoch 117/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8662\n",
      "Epoch 00117: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3011 - accuracy: 0.8659 - val_loss: 0.4544 - val_accuracy: 0.7754\n",
      "Epoch 118/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8694\n",
      "Epoch 00118: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2981 - accuracy: 0.8694 - val_loss: 0.4217 - val_accuracy: 0.8016\n",
      "Epoch 119/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.8740\n",
      "Epoch 00119: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2889 - accuracy: 0.8740 - val_loss: 0.4193 - val_accuracy: 0.8005\n",
      "Epoch 120/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8739\n",
      "Epoch 00120: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2927 - accuracy: 0.8739 - val_loss: 0.4402 - val_accuracy: 0.7928\n",
      "Epoch 121/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8742\n",
      "Epoch 00121: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2933 - accuracy: 0.8745 - val_loss: 0.4271 - val_accuracy: 0.7949\n",
      "Epoch 122/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8789\n",
      "Epoch 00122: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2820 - accuracy: 0.8792 - val_loss: 0.4724 - val_accuracy: 0.7730\n",
      "Epoch 123/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8798\n",
      "Epoch 00123: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2797 - accuracy: 0.8802 - val_loss: 0.4633 - val_accuracy: 0.7844\n",
      "Epoch 124/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8794\n",
      "Epoch 00124: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2778 - accuracy: 0.8796 - val_loss: 0.4164 - val_accuracy: 0.8033\n",
      "Epoch 125/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8781\n",
      "Epoch 00125: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2857 - accuracy: 0.8782 - val_loss: 0.4196 - val_accuracy: 0.8001\n",
      "Epoch 126/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8797\n",
      "Epoch 00126: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2813 - accuracy: 0.8793 - val_loss: 0.4304 - val_accuracy: 0.7975\n",
      "Epoch 127/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.8819\n",
      "Epoch 00127: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2758 - accuracy: 0.8817 - val_loss: 0.4356 - val_accuracy: 0.8005\n",
      "Epoch 128/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8830\n",
      "Epoch 00128: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2723 - accuracy: 0.8834 - val_loss: 0.4320 - val_accuracy: 0.7997\n",
      "Epoch 129/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.8848\n",
      "Epoch 00129: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2656 - accuracy: 0.8850 - val_loss: 0.4379 - val_accuracy: 0.7939\n",
      "Epoch 130/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8812\n",
      "Epoch 00130: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2710 - accuracy: 0.8814 - val_loss: 0.4243 - val_accuracy: 0.8050\n",
      "Epoch 131/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.8824\n",
      "Epoch 00131: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2726 - accuracy: 0.8824 - val_loss: 0.4537 - val_accuracy: 0.7812\n",
      "Epoch 132/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8843\n",
      "Epoch 00132: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2723 - accuracy: 0.8845 - val_loss: 0.4284 - val_accuracy: 0.7999\n",
      "Epoch 133/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.8842\n",
      "Epoch 00133: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2640 - accuracy: 0.8845 - val_loss: 0.5026 - val_accuracy: 0.7655\n",
      "Epoch 134/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8867\n",
      "Epoch 00134: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2662 - accuracy: 0.8871 - val_loss: 0.4451 - val_accuracy: 0.7915\n",
      "Epoch 135/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.8910\n",
      "Epoch 00135: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2581 - accuracy: 0.8908 - val_loss: 0.5155 - val_accuracy: 0.7689\n",
      "Epoch 136/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.8882\n",
      "Epoch 00136: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2568 - accuracy: 0.8887 - val_loss: 0.4392 - val_accuracy: 0.7979\n",
      "Epoch 137/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.8931\n",
      "Epoch 00137: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2542 - accuracy: 0.8934 - val_loss: 0.4730 - val_accuracy: 0.7861\n",
      "Epoch 138/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.8922\n",
      "Epoch 00138: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2549 - accuracy: 0.8924 - val_loss: 0.4370 - val_accuracy: 0.8014\n",
      "Epoch 139/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.8913\n",
      "Epoch 00139: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2505 - accuracy: 0.8914 - val_loss: 0.4640 - val_accuracy: 0.7846\n",
      "Epoch 140/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.8931\n",
      "Epoch 00140: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2545 - accuracy: 0.8932 - val_loss: 0.4872 - val_accuracy: 0.7676\n",
      "Epoch 141/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.8943\n",
      "Epoch 00141: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2471 - accuracy: 0.8945 - val_loss: 0.4401 - val_accuracy: 0.7988\n",
      "Epoch 142/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.8966\n",
      "Epoch 00142: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2459 - accuracy: 0.8965 - val_loss: 0.4378 - val_accuracy: 0.7997\n",
      "Epoch 143/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2478 - accuracy: 0.8934\n",
      "Epoch 00143: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2476 - accuracy: 0.8934 - val_loss: 0.5702 - val_accuracy: 0.7384\n",
      "Epoch 144/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.8966\n",
      "Epoch 00144: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2439 - accuracy: 0.8968 - val_loss: 0.4362 - val_accuracy: 0.7964\n",
      "Epoch 145/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.8968\n",
      "Epoch 00145: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2437 - accuracy: 0.8971 - val_loss: 0.4525 - val_accuracy: 0.7941\n",
      "Epoch 146/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.8972\n",
      "Epoch 00146: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2420 - accuracy: 0.8972 - val_loss: 0.4481 - val_accuracy: 0.7926\n",
      "Epoch 147/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9019\n",
      "Epoch 00147: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2359 - accuracy: 0.9021 - val_loss: 0.4446 - val_accuracy: 0.7947\n",
      "Epoch 148/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.8977\n",
      "Epoch 00148: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2422 - accuracy: 0.8976 - val_loss: 0.5382 - val_accuracy: 0.7670\n",
      "Epoch 149/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.8965\n",
      "Epoch 00149: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2436 - accuracy: 0.8969 - val_loss: 0.4568 - val_accuracy: 0.7941\n",
      "Epoch 150/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.8994\n",
      "Epoch 00150: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2345 - accuracy: 0.8997 - val_loss: 0.4421 - val_accuracy: 0.8027\n",
      "Epoch 151/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9006\n",
      "Epoch 00151: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2339 - accuracy: 0.9008 - val_loss: 0.4527 - val_accuracy: 0.7951\n",
      "Epoch 152/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.8991\n",
      "Epoch 00152: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2394 - accuracy: 0.8994 - val_loss: 0.4845 - val_accuracy: 0.7758\n",
      "Epoch 153/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.8990\n",
      "Epoch 00153: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2370 - accuracy: 0.8990 - val_loss: 0.4417 - val_accuracy: 0.7994\n",
      "Epoch 154/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.9025\n",
      "Epoch 00154: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2282 - accuracy: 0.9027 - val_loss: 0.4940 - val_accuracy: 0.7764\n",
      "Epoch 155/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9025\n",
      "Epoch 00155: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2335 - accuracy: 0.9028 - val_loss: 0.4555 - val_accuracy: 0.7904\n",
      "Epoch 156/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9032\n",
      "Epoch 00156: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2249 - accuracy: 0.9033 - val_loss: 0.4848 - val_accuracy: 0.7799\n",
      "Epoch 157/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9046\n",
      "Epoch 00157: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2252 - accuracy: 0.9045 - val_loss: 0.4879 - val_accuracy: 0.7857\n",
      "Epoch 158/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9083\n",
      "Epoch 00158: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2206 - accuracy: 0.9085 - val_loss: 0.4888 - val_accuracy: 0.7794\n",
      "Epoch 159/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9035\n",
      "Epoch 00159: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2280 - accuracy: 0.9038 - val_loss: 0.4388 - val_accuracy: 0.7982\n",
      "Epoch 160/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9038\n",
      "Epoch 00160: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2245 - accuracy: 0.9042 - val_loss: 0.5123 - val_accuracy: 0.7702\n",
      "Epoch 161/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9093\n",
      "Epoch 00161: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2157 - accuracy: 0.9095 - val_loss: 0.4948 - val_accuracy: 0.7801\n",
      "Epoch 162/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9147\n",
      "Epoch 00162: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2085 - accuracy: 0.9150 - val_loss: 0.4611 - val_accuracy: 0.7917\n",
      "Epoch 163/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9088\n",
      "Epoch 00163: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2201 - accuracy: 0.9090 - val_loss: 0.4479 - val_accuracy: 0.8003\n",
      "Epoch 164/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2144 - accuracy: 0.9112\n",
      "Epoch 00164: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2136 - accuracy: 0.9116 - val_loss: 0.4981 - val_accuracy: 0.7816\n",
      "Epoch 165/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9141\n",
      "Epoch 00165: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2073 - accuracy: 0.9142 - val_loss: 0.4484 - val_accuracy: 0.7986\n",
      "Epoch 166/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9123\n",
      "Epoch 00166: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2132 - accuracy: 0.9124 - val_loss: 0.4582 - val_accuracy: 0.7958\n",
      "Epoch 167/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9110\n",
      "Epoch 00167: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2150 - accuracy: 0.9104 - val_loss: 0.4876 - val_accuracy: 0.7745\n",
      "Epoch 168/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9127\n",
      "Epoch 00168: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2092 - accuracy: 0.9127 - val_loss: 0.4637 - val_accuracy: 0.7887\n",
      "Epoch 169/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/73 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9113\n",
      "Epoch 00169: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2101 - accuracy: 0.9116 - val_loss: 0.4898 - val_accuracy: 0.7833\n",
      "Epoch 170/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9182\n",
      "Epoch 00170: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2027 - accuracy: 0.9181 - val_loss: 0.4894 - val_accuracy: 0.7868\n",
      "Epoch 171/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9134\n",
      "Epoch 00171: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2060 - accuracy: 0.9139 - val_loss: 0.4904 - val_accuracy: 0.7891\n",
      "Epoch 172/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9163\n",
      "Epoch 00172: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2035 - accuracy: 0.9161 - val_loss: 0.5461 - val_accuracy: 0.7629\n",
      "Epoch 173/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9166\n",
      "Epoch 00173: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.2001 - accuracy: 0.9166 - val_loss: 0.4971 - val_accuracy: 0.7861\n",
      "Epoch 174/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9141\n",
      "Epoch 00174: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2019 - accuracy: 0.9145 - val_loss: 0.5375 - val_accuracy: 0.7623\n",
      "Epoch 175/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2024 - accuracy: 0.9156\n",
      "Epoch 00175: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.2009 - accuracy: 0.9161 - val_loss: 0.4939 - val_accuracy: 0.7788\n",
      "Epoch 176/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1928 - accuracy: 0.9211\n",
      "Epoch 00176: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1917 - accuracy: 0.9215 - val_loss: 0.4684 - val_accuracy: 0.7911\n",
      "Epoch 177/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9201\n",
      "Epoch 00177: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1940 - accuracy: 0.9205 - val_loss: 0.4606 - val_accuracy: 0.8014\n",
      "Epoch 178/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1957 - accuracy: 0.9185\n",
      "Epoch 00178: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1951 - accuracy: 0.9187 - val_loss: 0.5046 - val_accuracy: 0.7803\n",
      "Epoch 179/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9182\n",
      "Epoch 00179: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1995 - accuracy: 0.9183 - val_loss: 0.4615 - val_accuracy: 0.7951\n",
      "Epoch 180/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9234\n",
      "Epoch 00180: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1935 - accuracy: 0.9236 - val_loss: 0.4795 - val_accuracy: 0.7906\n",
      "Epoch 181/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9221\n",
      "Epoch 00181: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1901 - accuracy: 0.9222 - val_loss: 0.4843 - val_accuracy: 0.7936\n",
      "Epoch 182/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9228\n",
      "Epoch 00182: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1889 - accuracy: 0.9229 - val_loss: 0.4686 - val_accuracy: 0.8029\n",
      "Epoch 183/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9245\n",
      "Epoch 00183: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1801 - accuracy: 0.9249 - val_loss: 0.4960 - val_accuracy: 0.7928\n",
      "Epoch 184/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9221\n",
      "Epoch 00184: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1912 - accuracy: 0.9220 - val_loss: 0.4722 - val_accuracy: 0.8031\n",
      "Epoch 185/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9251\n",
      "Epoch 00185: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1809 - accuracy: 0.9250 - val_loss: 0.5020 - val_accuracy: 0.7822\n",
      "Epoch 186/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9230\n",
      "Epoch 00186: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.1911 - accuracy: 0.9231 - val_loss: 0.4819 - val_accuracy: 0.7951\n",
      "Epoch 187/10000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9271\n",
      "Epoch 00187: val_loss did not improve from 0.40710\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.1758 - accuracy: 0.9272 - val_loss: 0.5069 - val_accuracy: 0.7874\n",
      "Epoch 00187: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 64, 128, 256]\n",
    "learning_rates = [0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "model_state_by_batch_size_and_learning_rate_trial_4 = {}\n",
    "        \n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "    model_state_by_batch_size_and_learning_rate_trial_4[batch_size] = {}\n",
    "\n",
    "    for learning_rate in learning_rates:        \n",
    "        # Build and train model\n",
    "        model = ml_utils.build_model(optimizer=keras.optimizers.SGD(learning_rate))\n",
    "        es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "        mc = keras.callbacks.ModelCheckpoint(\n",
    "            'pickled_objects/batch_size_{}_lr_{}_best_weights_trial_4.h5'.format(batch_size, learning_rate),\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "        model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate] = ml_utils.train_model(\n",
    "            model,\n",
    "            train,\n",
    "            validation,\n",
    "            epochs=10000,\n",
    "            extra_callbacks=[es, mc],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.save_model_state(model_state_by_batch_size_and_learning_rate_trial_4, 'model_state_by_batch_size_and_learning_rate_trial_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize effect of learning rate for each batch size\n",
    "for batch_size in batch_sizes:\n",
    "    plt.figure()\n",
    "    for learning_rate in learning_rates:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        plt.plot(history['val_loss'], label='lr_{}'.format(learning_rate))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation loss')\n",
    "        plt.title('Effect of learning rate on validation loss for batch size {}'.format(batch_size))\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.savefig('graphs/batch_size_{}_lr_all'.format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of learning rate on different batch sizes above. Looking at the smallest batch size, 32, we see that the optimal learning rate is 0.01. On the other hand, for batch size 256, the optimal value is 0.08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize effect of batch size for each learning rate\n",
    "for learning_rate in learning_rates:\n",
    "    plt.figure()\n",
    "    for batch_size in batch_sizes:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        plt.plot(history['val_loss'], label='batch_{}'.format(batch_size))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation loss')\n",
    "        plt.title('Effect of batch size on validation loss for learning rate {}'.format(learning_rate))\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.savefig('graphs/batch_size_all_lr_{:.0f}_e-2'.format(learning_rate*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a similar relationship when we invert the grouping (plot different batch sizes in one graph). For low learning rates such as 0.01, small batch sizes perform the best, whereas for higher learning rates, the larger batch sizes perform better.\n",
    "\n",
    "The graphs below attempt to visualize this more succintly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize min val loss achieved by learning rate, for each batch size\n",
    "plt.figure()\n",
    "for batch_size in batch_sizes:\n",
    "    min_val_losses = []\n",
    "    for learning_rate in learning_rates:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        min_val_losses.append(np.min(history['val_loss']))\n",
    "    plt.plot(learning_rates, min_val_losses, label='batch_{}'.format(batch_size))\n",
    "    plt.xlabel('Learning rate')\n",
    "    plt.ylabel('Min validation loss achieved')\n",
    "    plt.title('Effect of learning rate on validation loss for different batch sizes')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.savefig(\"graphs/min_val_loss_group_by_batch_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize min val loss achieved by learning rate, for each batch size\n",
    "plt.figure()\n",
    "for learning_rate in learning_rates:\n",
    "    min_val_losses = []\n",
    "    for batch_size in batch_sizes:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        min_val_losses.append(np.min(history['val_loss']))\n",
    "    plt.plot(batch_sizes, min_val_losses, label='lr_{}'.format(learning_rate))\n",
    "    plt.xlabel('Batch size')\n",
    "    plt.ylabel('Min validation loss achieved')\n",
    "    plt.title('Effect of batch size on validation loss for different learning rates')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.savefig(\"graphs/min_val_loss_group_by_learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df5xcdX3v8dc7u/lFyA9ISCS/kYAaEJKYC1qxTQUt2NaohQoUhVvaaG+xev1R8RelXG3F1lKv8NCLFyyiV1D8FW0sWunW1gICOyEQEFliMpsfAkl2kmx+7+7n/nHO6swwuzsLO3tmdt7Px2MeO3PO98z5nO/OzmfP+Z7v96uIwMzMrNy4rAMwM7P65ARhZmYVOUGYmVlFThBmZlaRE4SZmVXkBGFmZhU5QdhzSFooqVtSSw3e+1pJXx5g3SpJW0d6n9WQ9EeSfpDFvkeapCsk/ecIvVebpD8ZifcaYj8jErOkD0v6vyMRkzlBjAnpH9cjkg5I+qWkz0maMYztN0s6r/91ROQj4tiI6K1NxPUnIr4SEa/POg4Y2S/4LEn6J0kfH819RsTfRETNE1qzcIJocJLeB1wPfACYDrwSWAT8UNKELGOrF5Jas46hXz3FYjYUJ4gGJmka8NfAuyLiXyLiaERsBv6QJElclpa7VtJdku6UtE9Su6Qz03W3AwuB76aXlf5S0mJJ0f9lll5m+Lik/0rLfFfSTElfkbRX0gOSFhfF9RlJnem6hyS95nke31xJ35D0rKRfSPqLonVnSbpXUkHSDkk3FifENP4/l/Qk8GTRsndKelJSl6SbJCldV/Jf+xBlWyR9WtLONK6riuurwnFslvRBSRuA/ZJaJV0t6an09/GYpDenZV8GfB54VVrXhXT5REl/Lykv6WlJn5c0efDq02cl7ZH0M0nnpgsvkvRQWcH3Sfr2IO91sqSfpu/1HUnHF2379fSsdY+kH0s6LV2+Bvgj4C/7PzPp8gWSvpn+TndJurEslr9P6/sXki4Y5OA+KGlbWn9PFB3fry5hpp+J7qJHj6Rr03UDfrasSET40aAP4HygB2itsO424Kvp82uBo8CFwHjg/cAvgPHp+s3AeUXbLgai/32BNqADOJnkLOUx4OfAeUAr8CXgi0XbXwbMTNe9D/glMKkoli8PcDyrgK3p83HAQ8A1wATgxcAm4HfS9a8gOVtqTeN9HHhP0XsF8EPgeGBy0bLvATNIkuKzwPnpuiuA/yzbfqCy70zrYD5wHPCvxfVV4bg2A+uBBUWxXATMTY/zrcB+4MRKsaTL/hFYmx7PVOC7wN8OsL8r0s/F/0x/328F9qTbTgR2Ay8rKp8D/mCA92oDtgGnA1OAbxT//oA/TuOZmMa4vmjdPwEfL3rdAjwM3JC+1yTgnKKYjwJ/mpb7M2A7oAoxvQToBOYWfV5PHuzzBSxLf4fLGeKz5UdRvWUdgB8v4JeXfBH/coB1nwR+mD6/FrivaN04YAfwmvT1ZoZOEB8pWv9p4PtFr3+/+IuhQixdwJlFsVSTIM4G8mXrP0RRIipb9x7gW0WvA3htWZno/0JKX38NuDp9fgXPTRADlb0HeEfRuvMYOkH88RC/y/XA6gFiEUkCOblo2auAXwzwXleUf7kCPwXelj7/HPCJ9Plp6e9n4gDv1QZ8suj1UuAI0FKh7Iy0Hqanr/+J0gTxKpIv6Ur/0FwBdBS9PiZ9rxdVKLsEeCat9/Fl657z+QJOSH8HFz+fz1YzP3w9tLHtBGZJao2InrJ1J6br+3X2P4mIPiV3C80dxr6eLnp+sMLrY/tfKGkX+ZP0/QOYBswaxr4guUQ2t/8SS6oF+I90H6cC/wCsJPkyaSX5r7BYJ8/1y6LnB4rjHkbZuWXvXWk/5UrKSHo78F6SZEz63gPV0Qkkx/hQepULkqQx2F1m2yL95ktt4de/79uAr0r6KPA24GsRcbjK2LeQnJXMkrQT+ATJ2dAJQF9aZhbJGUu5BcCWCp/Vfr+q74g4kB7rc34/EdEh6T0kyeA0SXcD742I7eVlJY0H7gL+X0TckS4e9LNlv+Y2iMZ2L3AYeEvxQklTgAuAHxUtXlC0fhzJ5ZH+P6gRG9I3bW/4IEk7yHERMYPky0KDbvhcnST/Ic8oekyNiDek6z8H/Aw4JSKmAR+usI9aDVW8g6T++i0YqGClWCQtAr4AXAXMTOvoUX4df3ncO0mS8GlFdTE9IgZLbvNUlE1ILpNtB4iI+0jOAl4DXArcPkTsxce3kORS0M5029Uk/8lP59fJbqDj6AQWDtRWMxwR8f8i4hySL/sguVGjks8C+4CPlsUx2GfLUk4QDSwi9pA0Un9W0vmSxitpLP46sJXSP/xXSHpL+sf5HpLEcl+67mmS67AjYSrJ9e9ngVZJ15CcQQzXT4G9aWPk5LRh+HRJ/61oP3uBbkkvJblmPVq+Brxb0jwltxN/cJjbTyH5UnsWQNJ/J7nG3+9pYL7SRveI6CNJKDdImp1uM0/S7wyyj9nAX6SfiYuAlwHritZ/CbgR6ImIoW6pvUzSUknHANcBd0VyC/RUks/RLpIznL8p2678c/VTkuT6SUlTJE2S9Ooh9v0ckl4i6bWSJgKHSJLnc27JlvQO4LeAS9M6LI5jsM+WpZwgGlxEfIrkv+e/J/nCvJ/kP6Rzyy4bfIeksbKL5LLCWyLiaLrub4GPKrkj6P0vMKS7ge+TNGJvIfkDruYSTIn0C+j3SRoXf0HyH+v/JflPFZKG9ktJ/jv8AnDnC4x7OL4A/ADYQNLAu44kKVbVbyQiHiNpx7mX5Ev05cBPiorcA2wEfplexoEkCXUA90naS9Iw/pJBdnM/cApJvX0CuDAidhWtv50kKQ119tBf9p9IbzYA+u/4+RLJ73gbSaP9fWXb3QIsTT9X3y76nS4B8iT/xLy1iv2Xm0jSxrYzjWk2yd9AuUtIEtT2ojuZPlzFZ8tSKr1MaWNRemvfkoi4LOtYxqL0dszPR8SirGOpVnqL7DPAioh4Mut4rD75DMJsmNLLEm9Q0p9hHvBXwLeyjmuY/gx4wMnBBuO7mMyGTyRtP3eSXP/+Z5J76huCpM0kx/CmjEOxOudLTGZmVpEvMZmZWUVj5hLTrFmzYvHixVmHMaD9+/czZcqUrMOoG66PUq6PUq6PUrWsj4ceemhnRJxQad2YSRCLFy/mwQcfzDqMAbW1tbFq1aqsw6gbro9Sro9Sro9StawPSVsGWudLTGZmVpEThJmZVeQEYWZmFTlBmJlZRU4QZmZWUU0TRDrC6BOSOiRdXWH9RCXTYHZIuj8diZR0BMrbJD0i6XFJH6plnGZm9lw1SxCSWoCbSOYlWApcImlpWbErga6IWEIyDWH/mO4Xkcxw9XKSqSXf0Z88zMxsdNSyH8RZJFMIbgKQdAfJ5CKPFZVZTTIrFCSzPt2YTnISwJR07oLJJJOb7K1hrA3joS27+fcnns06jBds85YjtB95Iusw6obro5Tro9RQ9XHqi6bye2cMZ4LI6tQyQcyjdB6ArSRzwVYsExE9kvaQTHZ/F0ny2EEyEcn/jIjd5TuQtAZYAzBnzhza2tpG+BBGTnd394jE91f/dZAte/uGPT1b/Ql4qiPrIOqI66OU66PU4PVx1otaOHb3z0d8r7VMEJW+w8pHBhyozFkkk6/MBY4D/kPSv/afjfyqYMTNwM0AK1eujHrueTkSPSEPHOlh6w9+wLteu4T3vX6wuWLqn3vKlnJ9lHJ9lMqqPmrZSL2V0rlsi+dAfk6Z9HLSdGA3yUxh/xIRRyPiGZLZtlbWMNaG8MjWPfT2BcsXzsg6FDNrArVMEA8Ap0g6KZ1b92JgbVmZtcDl6fMLgXsiGX88D7xWiSnAK0kmqG9quc4CAMsXHJdxJGbWDGqWICKiB7iKZI7ix4GvRcRGSddJemNa7BZgpqQO4L1A/62wNwHHAo+SJJovRsSGWsXaKHL5Lk6aNYXjpkzIOhQzawI1Hc01ItaRTOhevOyaoueHSG5pLd+uu9LyZhYRtOcLvGbJrKxDMbMm4Z7UDWJb4SDP7jvs9gczGzVOEA0il0/bHxa6/cHMRocTRINoz3cxafw4XvqiqVmHYmZNwgmiQeTyBc6YP4PWFv/KzGx0+NumARzu6eWx7XtZ4ctLZjaKnCAawKPb9nKkt88N1GY2qpwgGkAu3wXgBGFmo8oJogHkOgvMP24ys6dOyjoUM2siThANILely7e3mtmoc4Koc7/cc4jtew6xfIEvL5nZ6HKCqHPrO5P2hxWLfAZhZqPLCaLOtecLTGgdx9ITp2Udipk1GSeIOpfLd3H63GlMaPWvysxGl7916tjR3j42bN3jDnJmlgkniDr2sx37ONzT5zuYzCwTThB1rN0d5MwsQ04QdSyX72LOtImcON0d5Mxs9DlB1LFcZ4EVC49DUtahmFkTcoKoUzu7D7Nl1wFfXjKzzNQ0QUg6X9ITkjokXV1h/URJd6br75e0OF3+R5LWFz36JC2rZaz1Zr1nkDOzjNUsQUhqAW4CLgCWApdIWlpW7EqgKyKWADcA1wNExFciYllELAPeBmyOiPW1irUe5Tq7aB0nXj5vetahmFmTquUZxFlAR0RsiogjwB3A6rIyq4Hb0ud3AefquRfcLwG+WsM461L7lgJL505j0viWrEMxsybVWsP3ngd0Fr3eCpw9UJmI6JG0B5gJ7Cwq81aem1gAkLQGWAMwZ84c2traRiTwWuju7q46vr4I2rcc4Jx5rXV9TC/EcOqjGbg+Srk+SmVVH7VMEJVuvYnhlJF0NnAgIh6ttIOIuBm4GWDlypWxatWq5xfpKGhra6Pa+B7fsZfDd/8Hb/yN01m1bF5tA8vIcOqjGbg+Srk+SmVVH7W8xLQVWFD0ej6wfaAyklqB6cDuovUX04yXl/o7yC1wA7WZZaeWCeIB4BRJJ0maQPJlv7aszFrg8vT5hcA9EREAksYBF5G0XTSVXL7AzCkTWHD85KxDMbMmVrNLTGmbwlXA3UALcGtEbJR0HfBgRKwFbgFul9RBcuZwcdFb/CawNSI21SrGepXLJzPIuYOcmWWplm0QRMQ6YF3ZsmuKnh8iOUuotG0b8MpaxlePCgeO8NSz+3nLivlZh2JmTc49qevM+s7+DnLuQW1m2XKCqDO5fIFxgjPnO0GYWbacIOpMrrPAS140jSkTa3r1z8xsSE4QdaSvL9IGap89mFn2nCDqyKad3ew71MPyBU4QZpY9J4g60p6O4LpikTvImVn2nCDqSC7fxfTJ4zlp5pSsQzEzc4KoJ7l8gWULZjBunDvImVn2nCDqRPfhHp54eh8rPEGQmdUJJ4g68XBngQh3kDOz+uEEUSdy6QiuZ/oOJjOrE04QdSKXL3DK7GOZPnl81qGYmQFOEHUhIsh1Fnx5yczqihNEHdiy6wC79x9huRuozayOOEHUgVxnOoOczyDMrI44QdSB9i0Fjp3Yyimzp2YdipnZrzhB1IFcZxdnLphOizvImVkdcYLI2MEjvTy+Yx/LF7j9wczqixNExjZsLdDbF6xY5PYHM6svThAZy6VTjC7zGYSZ1ZmaJghJ50t6QlKHpKsrrJ8o6c50/f2SFhetO0PSvZI2SnpE0qRaxpqVXL6LxTOP4fgpE7IOxcysRM0ShKQW4CbgAmApcImkpWXFrgS6ImIJcANwfbptK/Bl4J0RcRqwCjhaq1izEhG05wseoM/M6lJVCULSIknnpc8nS6rmfsyzgI6I2BQRR4A7gNVlZVYDt6XP7wLOlSTg9cCGiHgYICJ2RURvNbE2km2Fgzy777D7P5hZXWodqoCkPwXWAMcDJwPzgc8D5w6x6Tygs+j1VuDsgcpERI+kPcBM4FQgJN0NnADcERGfqhDbmjQ25syZQ1tb21CHk5nu7u7nxHf/jh4Aep95ira2zaMfVIYq1Uczc32Ucn2Uyqo+hkwQwJ+TnA3cDxART0qaXcV2lW7qjyrLtALnAP8NOAD8SNJDEfGjkoIRNwM3A6xcuTJWrVpVRVjZaGtrozy+H3/3MSaN38Jlv/fbtLY01/0Cleqjmbk+Srk+SmVVH9V8Kx1OLxEBv2ofKP+ir2QrsKDo9Xxg+0Bl0vedDuxOl/97ROyMiAPAOmBFFftsKO35Ls6YP6PpkoOZNYZqvpn+XdKHgcmSXgd8HfhuFds9AJwi6SRJE4CLgbVlZdYCl6fPLwTuiYgA7gbOkHRMmjh+C3isin02jMM9vTy2fa/bH8ysblWTIK4GngUeAd4BrIuIjwy1UUT0AFeRfNk/DnwtIjZKuk7SG9NitwAzJXUA7033RUR0Af9AkmTWA+0R8c/DOrI6t3H7Xo709vkOJjOrW9W0QbwrIj4DfKF/gaR3p8sGFRHrSC4PFS+7puj5IeCiAbb9MsmtrmNS+5Z0BFfPIGdmdaqaM4jLKyy7YoTjaDq5zgLzZkxm9rQx2f/PzMaAAc8gJF0CXAqcJKm47WAqsKvWgY116/OeQc7M6ttgl5j+C9gBzAI+XbR8H7ChlkGNdU/vPcS2wkGuPOekrEMxMxvQgAkiIrYAW4BXjV44zSGX9wxyZlb/hmyDkPRKSQ9I6pZ0RFKvpL2jEdxYlcsXmNAyjqVzp2UdipnZgKpppL4RuAR4EpgM/Anw2VoGNda157s4fd40Jra2ZB2KmdmAqurCGxEdQEtE9EbEF4Hfrm1YY9fR3j42bN3Dcvd/MLM6V00/iANpT+j1kj5F0nA9pbZhjV0/27GPwz19bn8ws7pXzRnE29JyVwH7ScZO+oNaBjWW5TqTBmr3oDazejfoGUQ66c8nIuIy4BDw16MS1RjWvqWLOdMmcuJ0d5Azs/o26BlEOknPCeklJhsBuc4CyxccRzIvkplZ/aqmDWIz8JO0N/X+/oUR8Q+1Cmqs2tV9mC27DvBHZy/MOhQzsyFVkyC2p49xJMNs2POUyxcAfAeTmTWEIRNERLjdYYTkOrtoHSdOnzs961DMzIbkqcxGUS5f4GUnTmPyBHeQM7P65wQxSvoieLizwAr3fzCzBuEEMUq2dQf7j/S6/cHMGsaQbRCS/neFxXuAByPiOyMf0tj0VKEX8AiuZtY4qjmDmAQsIxms70ngDOB44EpJ/1jD2MaUjkIfM6dMYOHxx2QdiplZVapJEEuA10bEZyPis8B5wMuANwOvH2xDSedLekJSh6SrK6yfKOnOdP39khanyxdLOihpffr4/HAPrN48Vehl+cIZ7iBnZg2jmn4Q80gG59uTvp4CzI2IXkmHB9ooHabjJuB1wFbgAUlrI+KxomJXAl0RsUTSxcD1wFvTdU9FxLLhHU592nPgKDv2B5e5/cHMGkg1CeJTJCO5tgECfhP4G0lTgH8dZLuzgI6I2AQg6Q5gNVCcIFYD16bP7wJu1Bj8F7t/gD63P5hZI6mmo9wtktaRfOEL+HBEbE9Xf2CQTecBnUWvtwJnD1QmInok7QFmputOkpQD9gIfjYj/KN+BpDXAGoA5c+bQ1tY21OFk4ltPHkEEe37xCG2dYy7/PS/d3d11+/vKguujlOujVFb1Uc0ZBCRtFc+m5ZdIWhIRPx5im0rfhFFlmR3AwojYJekVwLclnRYRJVOdRsTNwM0AK1eujFWrVg19JBm4ddNPmT91Fxec53mW+rW1tVGvv68suD5KuT5KZVUf1dzm2t8usBHoSxcHMFSC2Eoyd0S/+SRjOlUqs1VSKzAd2B0RARwGiIiHJD0FnAo8OFS89aavL1if72LFCe5yYmaNpZoziDcBL4mIARukB/AAcIqkk4BtwMXApWVl1gKXA/cCFwL3RERIOoEkUfRKejFwCrBpmPuvC5t2drP3UA8nT/eI6WbWWKpJEJuA8aT/0VcrbVO4CrgbaAFujYiNkq4j6WS3FrgFuF1SB7CbJIlA0hB+naQeoBd4Z0TsHs7+60V7OoLryTM8/pKZNZaq5qQmuYvpRxQliYj4i6E2jIh1wLqyZdcUPT8EXFRhu28A36gitrqXyxeYPnk8L5rixmkzayzVJIi16cOeh1y+i2ULZjBOB7IOxcxsWKq5zfW20QhkLOo+3MMTT+/j/NNfRHIiZmbWOAZMEJK+FhF/KOkRnnt7KhFxRk0jGwM2dBaISGaQi+3lN3CZmdW3wc4g3p3+/L3RCGQsas8nPaiXLZhBzvnBzBrMgAkiInakP7eMXjhjSy5fYMnsY5k+eXzWoZiZDduQvbckvUXSk5L2SNoraZ+kvUNt1+wiglxngeULPP6SmTWmagfr+/2IeLzWwYwlW3YdYPf+I6xY5BFczawxVTP+w9NODsPnEVzNrNFVcwbxoKQ7gW9T2lHumzWLagzI5QscO7GVU2ZPzToUM7PnpZoEMY3kJv7i2eMCcIIYRHu+izMXTKdlnHtQm1ljGjRBpLPCbYiIG0YpnjHh4JFeHt+xjz/7rZOzDsXM7HkbtA0iInqBN45SLGPGI9v20NsXbn8ws4ZWzSWm/5J0I3AnsL9/YUS01yyqBtffQW6556A2swZWTYL4jfTndUXLAnjtyIczNuTyXSyeeQzHT/EcEGbWuKoZrM/zZA5DRNCeL3DOkllZh2Jm9oJUNSe1pN8FTgMm9S+LiOsG3qJ5bd9ziGf3HXb7g5k1vGqG2vg8yZzU7wJEMsHPohrH1bDatyTtDyvc/mBmDa6antS/ERFvB7oi4q+BVwELahtW48rlC0waP46XvMgd5MyssVWTIA6mPw9ImgscBU6qXUiNLdfZxRnzZjC+pZqqNTOrX9V8i31P0gzg74B2YDNwRzVvLul8SU9I6pB0dYX1EyXdma6/X9LisvULJXVLen81+8va4Z5eNm7by/JFbn8ws8ZXzV1M/yt9+g1J3wMmRcSeobZLe2HfBLwO2Ao8IGltRDxWVOxKkktXSyRdDFxP0t7R7wbg+9UdSvY2bt/Lkd4+li9w+4OZNb5qGqmPkfQxSV+IiMPAbEnVzDJ3FtAREZsi4gjJWcfqsjKrgf45r+8CzpWkdL9vAjYBG6s8lszl8gUAVvgOJjMbA6q5zfWLwEMkjdOQnA18HfjeENvNAzqLXm8Fzh6oTET0SNoDzJR0EPggydnHgJeXJK0B1gDMmTOHtra2Kg6ndu5ef4iZk8Rj7ffxWNm67u7uzOOrJ66PUq6PUq6PUlnVRzUJ4uSIeKukSwAi4mD/f/lDqFQmqizz18ANEdE92K4i4mbgZoCVK1fGqlWrqgirdj5y3z286tQZrFq14jnr2trayDq+euL6KOX6KOX6KJVVfVSTII5Imkz65S7pZIrmhRjEVkpvh50PbB+gzFZJrcB0YDfJmcaFkj4FzAD6JB2KiBur2G8mnt57iG2Fg/zxOb7By8zGhmoSxF8B/wIskPQV4NXAFVVs9wBwiqSTgG3AxcClZWXWApcD9wIXAvdERACv6S8g6Vqgu56TAyTjL4HbH8xs7KjmLqYfSmoHXklySejdEbGziu16JF0F3A20ALdGxEZJ1wEPRsRa4BbgdkkdJGcOF7+AY8lULl9gQss4ls6dlnUoZmYjYsAEIan8QvqO9OdCSQurGe47ItYB68qWXVP0/BDJ0B2Dvce1Q+2nHuTyBU6bN42JrS1Zh2JmNiIGO4P49CDrPNx3kaO9fWzYVuDSszxElZmNHQMmCA/zXb2f7djHoaN9rHAPajMbQzxg0AjIdXoGOTMbe5wgRkAuX2D21InMnT5p6MJmZg3CCWIEtOe7WLHwOKrrP2hm1hiqGYvpR9Usa1a7ug+zZdcBzyBnZmPOYLe5TgKOAWZJOo5fD4sxDZg7CrE1hPWdyQB9bn8ws7FmsNtc3wG8hyQZPMSvE8RekmG8jeTyUus48fJ507MOxcxsRA12m+tngM9IeldEfHYUY2oouXyBl504jckT3EHOzMaWahqpfylpKoCkj0r6ZoVe1k2pty94uLPg9gczG5OqSRAfi4h9ks4Bfodkgp/P1TasxvDzp/ex/0gvK9z+YGZjUDUJojf9+bvA5yLiO8CE2oXUOPpnkPMZhJmNRdUkiG2S/g/wh8A6SROr3G7My+W7OH7KBBYef0zWoZiZjbhqvuj/kGTI7vMjogAcD3ygplE1iKSD3Ax3kDOzMWnIBBERB4BngHPSRT3Ak7UMqhHsOXCUp57d7/4PZjZmVdOT+q+ADwIfSheNB75cy6AawfqtafvDArc/mNnYVM0lpjcDbwT2A0TEdmBqLYNqBO1buhgnOMMJwszGqGoSxJF0nugAkDSltiE1hlxngVPnTOXYidVM621m1niqSRBfS+9imiHpT4F/Bb5Q27DqW19fsD7f5fYHMxvTqmmk/nvgLuAbwEuAa6odekPS+ZKekNQh6eoK6ydKujNdf7+kxenysyStTx8PS3rzcA6q1jbt3M/eQz3u/2BmY1pV10ci4ofADyXNAnZVs42kFpJB/V4HbAUekLQ2Ih4rKnYl0BURSyRdDFwPvBV4FFgZET2STgQelvTdiOip+shqqD2fzCDnHtRmNpYNeAYh6ZWS2tKxl5ZLepTki/tpSedX8d5nAR0RsSkijgB3AKvLyqwmGboDkrOUcyUpIg4UJYNJpO0f9SKXLzBtUisvnuXmGDMbuwY7g7gR+DAwHbgHuCAi7pP0UuCrwL8M8d7zgM6i11uBswcqk54t7AFmAjslnQ3cCiwC3lbp7EHSGmANwJw5c2hraxsipJHxn48fZOGx4sc//veqt+nu7h61+BqB66OU66OU66NUVvUxWIJojYgfAEi6LiLuA4iIn1XZc7hSofIzgQHLRMT9wGmSXgbcJun7EXGopGDEzcDNACtXroxVq1ZVE9cL0n24h213380fnH0Kq1adWvV2bW1tjEZ8jcL1Ucr1Ucr1USqr+hiskbqv6PnBsnXVXPLZCiwoej0f2D5QGUmtJGcru0t2FPE4SR+M06vYZ81t6CzQF55BzszGvsHOIM6UtJfkv/zJ6XPS15OqeO8HgFMknQRsAy4GLi0rsxa4HLgXuBC4JyIi3aYzvey0iOTuqc1VHlNN5dIpRpfN9x1MZja2DTaj3AuaIi39cr+KZKC/FuDWiNgo6TrgwYhYC9wC3C6pg+TM4eJ083OAqyUdJTmT+R8RsfOFxDNS2rd0sWT2sUw/ZnzWoY0u0YUAAAwmSURBVJiZ1VRNuwFHxDpgXdmya4qeHwIuqrDd7cDttYzt+YgIcp0Fzn3p7KxDMTOrOc/rMAz53QfYvf+I2x/MrCk4QQxDfwc596A2s2bgBDEMuXyBKRNaOHVO0w9ma2ZNwAliGHL5AmcumEHLOM8gZ2ZjnxNElQ4e6eXxHXt9ecnMmoYTRJUe2baHnr7wAH1m1jScIKqUSxuol3kGOTNrEk4QVWrPd7Fo5jHMPHZi1qGYmY0KJ4gqRATt+YIvL5lZU3GCqML2PYd4dt9hN1CbWVNxgqhCf/vD8gU+gzCz5uEEUYX2LQUmjR/HS090Bzkzax5OEFXIdXZxxrwZjG9xdZlZ8/A33hAO9/SycZs7yJlZ83GCGMLG7Xs50tvnEVzNrOk4QQwhl09mkPMZhJk1GyeIIeTyXcybMZk506qZZdXMbOxwghhCLl9gmc8ezKwJOUEM4um9h9hWOOge1GbWlJwgBuH2BzNrZjVNEJLOl/SEpA5JV1dYP1HSnen6+yUtTpe/TtJDkh5Jf762lnEOJJfvYkLLOE6bOy2L3ZuZZapmCUJSC3ATcAGwFLhE0tKyYlcCXRGxBLgBuD5dvhP4/Yh4OXA5cHut4hxMLl/gtHnTmNjaksXuzcwyVcsziLOAjojYFBFHgDuA1WVlVgO3pc/vAs6VpIjIRcT2dPlGYJKkUR1n+2hvHxu2FTz+kpk1rdYavvc8oLPo9Vbg7IHKRESPpD3ATJIziH5/AOQi4nD5DiStAdYAzJkzh7a2thELfvOeXg4d7WNi93ba2p55we/X3d09ovE1OtdHKddHKddHqazqo5YJQhWWxXDKSDqN5LLT6yvtICJuBm4GWLlyZaxatep5BVrJl+7dDGzksgtezbwZk1/w+7W1tTGS8TU610cp10cp10eprOqjlpeYtgILil7PB7YPVEZSKzAd2J2+ng98C3h7RDxVwzgryuULzJ46kbnT3UHOzJpTLRPEA8Apkk6SNAG4GFhbVmYtSSM0wIXAPRERkmYA/wx8KCJ+UsMYB5TLd7F84QykSic5ZmZjX80SRET0AFcBdwOPA1+LiI2SrpP0xrTYLcBMSR3Ae4H+W2GvApYAH5O0Pn3MrlWs5XZ1H2bzrgPuIGdmTa2WbRBExDpgXdmya4qeHwIuqrDdx4GP1zK2wazv7O8g5wRhZs3LPakryOULtIwTL583PetQzMwy4wRRQXu+i5edOJXJE9xBzsyalxNEmd6+4OHOgtsfzKzpOUGUefKZfew/0usB+sys6TlBlGnfkjZQe4gNM2tyThBlcvkujp8ygUUzj8k6FDOzTDlBlMl1Fli+wB3kzMycIIrsOXCUjme63f5gZoYTRIn1W5P2B9/BZGbmBFEil+9CgjMW+AzCzMwJokh7vsBL5kzl2Ik1HYHEzKwhOEGk+vqC9fkuj79kZpZygkht2rmfvYd63EBtZpZygkjl8l0ArHCCMDMDnCB+pT1fYNqkVl4869isQzEzqwtOEKlcvotlC49j3Dh3kDMzAycIALoP9/Dzp/ex3Le3mpn9ihMEsKGzQF/gBmozsyJOECTjL4FHcDUzK1bTBCHpfElPSOqQdHWF9RMl3Zmuv1/S4nT5TEn/Jqlb0o21jBGS9oeTT5jC9GPG13pXZmYNo2YJQlILcBNwAbAUuETS0rJiVwJdEbEEuAG4Pl1+CPgY8P5axdcvImjPF9xBzsysTC3PIM4COiJiU0QcAe4AVpeVWQ3clj6/CzhXkiJif0T8J0miqKn87gPs3n/EA/SZmZWp5aBD84DOotdbgbMHKhMRPZL2ADOBndXsQNIaYA3AnDlzaGtrG3aQ27v7WDmnhXi2g7a2TcPevlrd3d3PK76xyvVRyvVRyvVRKqv6qGWCqNShIJ5HmQFFxM3AzQArV66MVatWVR1csUuf11bD09bWxvONbyxyfZRyfZRyfZTKqj5qeYlpK7Cg6PV8YPtAZSS1AtOB3TWMyczMqlTLBPEAcIqkkyRNAC4G1paVWQtcnj6/ELgnIqo+gzAzs9qp2SWmtE3hKuBuoAW4NSI2SroOeDAi1gK3ALdL6iA5c7i4f3tJm4FpwARJbwJeHxGP1SpeMzMrVdOZcSJiHbCubNk1Rc8PARcNsO3iWsZmZmaDc09qMzOryAnCzMwqcoIwM7OKnCDMzKwijZW7SiU9C2zJOo5BzKLKHuJNwvVRyvVRyvVRqpb1sSgiTqi0YswkiHon6cGIWJl1HPXC9VHK9VHK9VEqq/rwJSYzM6vICcLMzCpyghg9N2cdQJ1xfZRyfZRyfZTKpD7cBmFmZhX5DMLMzCpygjAzs4qcIGpE0mZJj0haL+nBdNnxkn4o6cn055id51TSrZKekfRo0bKKx6/E/5bUIWmDpBXZRV4bA9THtZK2pZ+R9ZLeULTuQ2l9PCHpd7KJujYkLZD0b5Iel7RR0rvT5U35+RikPrL/fESEHzV4AJuBWWXLPgVcnT6/Grg+6zhrePy/CawAHh3q+IE3AN8nmWHwlcD9Wcc/SvVxLfD+CmWXAg8DE4GTgKeAlqyPYQTr4kRgRfp8KvDz9Jib8vMxSH1k/vnwGcToWg3clj6/DXhThrHUVET8mOfODjjQ8a8GvhSJ+4AZkk4cnUhHxwD1MZDVwB0RcTgifgF0AGfVLLhRFhE7IqI9fb4PeJxkfvqm/HwMUh8DGbXPhxNE7QTwA0kPSVqTLpsTETsg+VAAszOLLhsDHf88oLOo3FYG/wMZS65KL5vcWnTJsWnqQ9JiYDlwP/58lNcHZPz5cIKonVdHxArgAuDPJf1m1gHVMVVY1gz3X38OOBlYBuwAPp0ub4r6kHQs8A3gPRGxd7CiFZY1Q31k/vlwgqiRiNie/nwG+BbJKeDT/afG6c9nsoswEwMd/1ZgQVG5+cD2UY5t1EXE0xHRGxF9wBf49WWCMV8fksaTfBl+JSK+mS5u2s9Hpfqoh8+HE0QNSJoiaWr/c+D1wKPAWuDytNjlwHeyiTAzAx3/WuDt6d0qrwT29F9qGMvKrqO/meQzAkl9XCxpoqSTgFOAn452fLUiSSTz0T8eEf9QtKopPx8D1UddfD6ybsEfiw/gxSR3GTwMbAQ+ki6fCfwIeDL9eXzWsdawDr5Kclp8lOQ/nisHOn6SU+abSO7GeARYmXX8o1Qft6fHuyH9oz+xqPxH0vp4Argg6/hHuC7OIbkksgFYnz7e0Kyfj0HqI/PPh4faMDOzinyJyczMKnKCMDOzipwgzMysIicIMzOryAnCzMwqcoKwpiepNx0t82FJ7ZJ+Y4jyMyT9jyret03S85poXtI6STOez7ZmI8UJwgwORsSyiDgT+BDwt0OUnwEMmSBeiIh4Q0QUarkPs6E4QZiVmgZ0QTI2jqQfpWcVj0hanZb5JHByetbxd2nZv0zLPCzpk0Xvd5Gkn0r6uaTXlO9M0omSfpy+16P9ZZTMJzJL0juL5gP4haR/S9e/XtK9aWxfT8fxMRtR7ihnTU9SL0mP1UkkY/O/NiIektQKHBMReyXNAu4jGdZgEfC9iDg93f4C4GPAeRFxQNLxEbFbUhvwUES8L53s5b0RcV7Zvt8HTIqIT0hqSfe3T9Jmkh7DO9Ny44F7SOZMuBf4JkkP2v2SPghMjIjrallP1nxasw7ArA4cjIhlAJJeBXxJ0ukkQzz8TToSbx/JkMpzKmx/HvDFiDgAEBHF8z70D0T3ELC4wrYPALemCeDbEbF+gBg/A9wTEd+V9Hskk8b8JBnGhwkkScNsRDlBmBWJiHvTs4UTSMbDOQF4RUQcTf+rn1RhMzHwcMuH05+9VPh7i4gfpwnod4HbJf1dRHyp5M2lK0jOWq4q2t8PI+KS4Ryb2XC5DcKsiKSXAi3ALmA68EyaHH6b5EsaYB/J1JD9fgD8saRj0vc4fhj7W5Tu4wskI3quKFv/CuD9wGWRDPsMyaWuV0takpY5RtKpwztSs6H5DMIMJkvqv7Qj4PKI6JX0FeC7kh4kGWHzZwARsUvSTyQ9Cnw/Ij4gaRnwoKQjwDrgw1XuexXwAUlHgW7g7WXrrwKOB/4tvZz0YET8SXpW8VVJE9NyHyWZy9hsxLiR2szMKvIlJjMzq8gJwszMKnKCMDOzipwgzMysIicIMzOryAnCzMwqcoIwM7OK/j8t4CzBhyAwjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "y_values = []\n",
    "for batch_size in batch_sizes:\n",
    "    min_val_losses = {}\n",
    "    for learning_rate in learning_rates:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        min_val_losses[learning_rate] = np.min(history['val_loss'])\n",
    "    best_learning_rate = min(min_val_losses.items(), key=lambda x: x[1])[0]\n",
    "    y_values.append(best_learning_rate)\n",
    "\n",
    "plt.plot(batch_sizes, y_values)\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Best learning rate')\n",
    "plt.title('Optimal learning rate by batch size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(\"graphs/optimal_lr_by_batch_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this all mean? Does batch size matter when I'm training a model? Let's look at the best validation loss we achieved on each batch size and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "y_values = []\n",
    "for batch_size in batch_sizes:\n",
    "    min_val_losses = {}\n",
    "    for learning_rate in learning_rates:\n",
    "        history = model_state_by_batch_size_and_learning_rate_trial_4[batch_size][learning_rate].history\n",
    "        min_val_losses[learning_rate] = np.min(history['val_loss'])\n",
    "    min_val_loss_overall = min(min_val_losses.items(), key=lambda x: x[1])[1]\n",
    "    y_values.append(min_val_loss_overall)\n",
    "\n",
    "plt.plot(batch_sizes, y_values)\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Min val loss achieved')\n",
    "plt.title('Min val loss by batch size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(\"graphs/min_val_loss_by_batch_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, performance across different batch sizes is quite close! We do see that generally the larger the batch size the worse the performance, but the difference is not that large, compared to the difference observed with learning rate, for example. This suggests that as long as you find the right learning rate for your batch size, you should be fine; you can concentrate on other aspects of training. However, if you want to wring out every last drop of performance, you can do so by reducing the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The tl;dr from the batch experiments so far is that batch size is closely linked to learning rate - the larger the batch size, the larger the optimal learning rate. The experiments suggest that as long as you find the optimal learning rate for a given batch size, the difference in performance between batch sizes is actually not that large.**\n",
    "\n",
    "Some other questions to explore:\n",
    "- How does Keras handle batch udpates? Does it average the gradients over a batch, or sum them?\n",
    "- Why is the ratio of the batch-level update magnitude almost exactly the inverse of the batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 29 nearest neighbors...\n",
      "[t-SNE] Indexed 30 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 30 samples in 0.002s...\n",
      "[t-SNE] Computed conditional probabilities for sample 30 / 30\n",
      "[t-SNE] Mean sigma: 1125899906842624.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 43.379154\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.501490\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "weights_across_epochs = []\n",
    "for epoch, weights in model_state_by_batch_size_trial_1[32].weights_by_epoch.items():\n",
    "    weights_across_epochs.append(weights[-2].flatten())\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(weights_across_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(transformed_weights[:,0], transformed_weights[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/1500\n",
      "    582/Unknown - 27s 46ms/step - loss: 0.6860 - accuracy: 0.5435\n",
      "Saving weights for epoch 0\n",
      "582/582 [==============================] - 33s 57ms/step - loss: 0.6860 - accuracy: 0.5435 - val_loss: 0.6627 - val_accuracy: 0.6279\n",
      "Epoch 2/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6911 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5099\n",
      "Epoch 3/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6900 - accuracy: 0.5264 - val_loss: 0.6677 - val_accuracy: 0.6131\n",
      "Epoch 4/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6494 - accuracy: 0.6271 - val_loss: 0.6357 - val_accuracy: 0.6464\n",
      "Epoch 5/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6255 - accuracy: 0.6538 - val_loss: 0.6208 - val_accuracy: 0.6582\n",
      "Epoch 6/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.6749\n",
      "Saving weights for epoch 5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6082 - accuracy: 0.6749 - val_loss: 0.5961 - val_accuracy: 0.6930\n",
      "Epoch 7/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5909 - accuracy: 0.6867 - val_loss: 0.5770 - val_accuracy: 0.7096\n",
      "Epoch 8/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5729 - accuracy: 0.7031 - val_loss: 0.5760 - val_accuracy: 0.7016\n",
      "Epoch 9/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5585 - accuracy: 0.7104 - val_loss: 0.5525 - val_accuracy: 0.7197\n",
      "Epoch 10/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5435 - accuracy: 0.7195 - val_loss: 0.5312 - val_accuracy: 0.7341\n",
      "Epoch 11/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7294\n",
      "Saving weights for epoch 10\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5364 - accuracy: 0.7294 - val_loss: 0.5351 - val_accuracy: 0.7489\n",
      "Epoch 12/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5233 - accuracy: 0.7365 - val_loss: 0.5556 - val_accuracy: 0.7197\n",
      "Epoch 13/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5132 - accuracy: 0.7459 - val_loss: 0.4851 - val_accuracy: 0.7700\n",
      "Epoch 14/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5013 - accuracy: 0.7510 - val_loss: 0.4809 - val_accuracy: 0.7689\n",
      "Epoch 15/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4926 - accuracy: 0.7577 - val_loss: 0.5318 - val_accuracy: 0.7377\n",
      "Epoch 16/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4798 - accuracy: 0.7699\n",
      "Saving weights for epoch 15\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4800 - accuracy: 0.7698 - val_loss: 0.4698 - val_accuracy: 0.7820\n",
      "Epoch 17/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4731 - accuracy: 0.7707 - val_loss: 0.4790 - val_accuracy: 0.7741\n",
      "Epoch 18/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4685 - accuracy: 0.7741 - val_loss: 0.4502 - val_accuracy: 0.7900\n",
      "Epoch 19/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4628 - accuracy: 0.7762 - val_loss: 0.4516 - val_accuracy: 0.7850\n",
      "Epoch 20/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4569 - accuracy: 0.7828 - val_loss: 0.4395 - val_accuracy: 0.7883\n",
      "Epoch 21/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4494 - accuracy: 0.7860\n",
      "Saving weights for epoch 20\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4494 - accuracy: 0.7859 - val_loss: 0.4357 - val_accuracy: 0.7947\n",
      "Epoch 22/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4453 - accuracy: 0.7882 - val_loss: 0.4603 - val_accuracy: 0.7827\n",
      "Epoch 23/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4356 - accuracy: 0.7938 - val_loss: 0.4388 - val_accuracy: 0.7911\n",
      "Epoch 24/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4373 - accuracy: 0.7917 - val_loss: 0.4355 - val_accuracy: 0.8001\n",
      "Epoch 25/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4307 - accuracy: 0.7961 - val_loss: 0.4403 - val_accuracy: 0.7865\n",
      "Epoch 26/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8017\n",
      "Saving weights for epoch 25\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4252 - accuracy: 0.8015 - val_loss: 0.4356 - val_accuracy: 0.7913\n",
      "Epoch 27/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4189 - accuracy: 0.8077 - val_loss: 0.4106 - val_accuracy: 0.8072\n",
      "Epoch 28/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4213 - accuracy: 0.8042 - val_loss: 0.4330 - val_accuracy: 0.7936\n",
      "Epoch 29/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4118 - accuracy: 0.8071 - val_loss: 0.4518 - val_accuracy: 0.7853\n",
      "Epoch 30/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4044 - accuracy: 0.8132 - val_loss: 0.4123 - val_accuracy: 0.8091\n",
      "Epoch 31/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.4015 - accuracy: 0.8119\n",
      "Saving weights for epoch 30\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4017 - accuracy: 0.8119 - val_loss: 0.4116 - val_accuracy: 0.8093\n",
      "Epoch 32/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3991 - accuracy: 0.8150 - val_loss: 0.4009 - val_accuracy: 0.8119\n",
      "Epoch 33/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3952 - accuracy: 0.8180 - val_loss: 0.3920 - val_accuracy: 0.8192\n",
      "Epoch 34/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3907 - accuracy: 0.8235 - val_loss: 0.3878 - val_accuracy: 0.8224\n",
      "Epoch 35/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3867 - accuracy: 0.8242 - val_loss: 0.4230 - val_accuracy: 0.8001\n",
      "Epoch 36/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8223\n",
      "Saving weights for epoch 35\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3831 - accuracy: 0.8219 - val_loss: 0.4248 - val_accuracy: 0.7960\n",
      "Epoch 37/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3848 - accuracy: 0.8237 - val_loss: 0.3934 - val_accuracy: 0.8201\n",
      "Epoch 38/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3766 - accuracy: 0.8275 - val_loss: 0.3940 - val_accuracy: 0.8181\n",
      "Epoch 39/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3721 - accuracy: 0.8298 - val_loss: 0.4245 - val_accuracy: 0.7962\n",
      "Epoch 40/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3667 - accuracy: 0.8347 - val_loss: 0.4021 - val_accuracy: 0.8141\n",
      "Epoch 41/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8343\n",
      "Saving weights for epoch 40\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3636 - accuracy: 0.8341 - val_loss: 0.4226 - val_accuracy: 0.8037\n",
      "Epoch 42/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3650 - accuracy: 0.8326 - val_loss: 0.3992 - val_accuracy: 0.8160\n",
      "Epoch 43/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3646 - accuracy: 0.8346 - val_loss: 0.4001 - val_accuracy: 0.8106\n",
      "Epoch 44/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3574 - accuracy: 0.8368 - val_loss: 0.3845 - val_accuracy: 0.8239\n",
      "Epoch 45/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3610 - accuracy: 0.8340 - val_loss: 0.3737 - val_accuracy: 0.8233\n",
      "Epoch 46/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8396\n",
      "Saving weights for epoch 45\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3513 - accuracy: 0.8395 - val_loss: 0.3925 - val_accuracy: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3523 - accuracy: 0.8424 - val_loss: 0.3995 - val_accuracy: 0.8164\n",
      "Epoch 48/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3422 - accuracy: 0.8476 - val_loss: 0.3691 - val_accuracy: 0.8261\n",
      "Epoch 49/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3462 - accuracy: 0.8440 - val_loss: 0.3781 - val_accuracy: 0.8214\n",
      "Epoch 50/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3397 - accuracy: 0.8486 - val_loss: 0.3985 - val_accuracy: 0.8126\n",
      "Epoch 51/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8477\n",
      "Saving weights for epoch 50\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3431 - accuracy: 0.8475 - val_loss: 0.3889 - val_accuracy: 0.8222\n",
      "Epoch 52/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3328 - accuracy: 0.8495 - val_loss: 0.3731 - val_accuracy: 0.8265\n",
      "Epoch 53/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3283 - accuracy: 0.8558 - val_loss: 0.3992 - val_accuracy: 0.8132\n",
      "Epoch 54/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3260 - accuracy: 0.8539 - val_loss: 0.3845 - val_accuracy: 0.8194\n",
      "Epoch 55/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3278 - accuracy: 0.8552 - val_loss: 0.3840 - val_accuracy: 0.8246\n",
      "Epoch 56/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.8555\n",
      "Saving weights for epoch 55\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3281 - accuracy: 0.8557 - val_loss: 0.4000 - val_accuracy: 0.8147\n",
      "Epoch 57/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3231 - accuracy: 0.8590 - val_loss: 0.4176 - val_accuracy: 0.8102\n",
      "Epoch 58/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3202 - accuracy: 0.8583 - val_loss: 0.4105 - val_accuracy: 0.8136\n",
      "Epoch 59/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3139 - accuracy: 0.8615 - val_loss: 0.3849 - val_accuracy: 0.8199\n",
      "Epoch 60/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3153 - accuracy: 0.8617 - val_loss: 0.3995 - val_accuracy: 0.8145\n",
      "Epoch 61/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.8614\n",
      "Saving weights for epoch 60\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3169 - accuracy: 0.8611 - val_loss: 0.3813 - val_accuracy: 0.8184\n",
      "Epoch 62/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3083 - accuracy: 0.8638 - val_loss: 0.3823 - val_accuracy: 0.8261\n",
      "Epoch 63/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3130 - accuracy: 0.8607 - val_loss: 0.3831 - val_accuracy: 0.8280\n",
      "Epoch 64/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3108 - accuracy: 0.8659 - val_loss: 0.3875 - val_accuracy: 0.8259\n",
      "Epoch 65/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3010 - accuracy: 0.8698 - val_loss: 0.4109 - val_accuracy: 0.8100\n",
      "Epoch 66/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8682\n",
      "Saving weights for epoch 65\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3039 - accuracy: 0.8682 - val_loss: 0.3982 - val_accuracy: 0.8218\n",
      "Epoch 67/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3049 - accuracy: 0.8649 - val_loss: 0.3802 - val_accuracy: 0.8261\n",
      "Epoch 68/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2974 - accuracy: 0.8716 - val_loss: 0.3890 - val_accuracy: 0.8220\n",
      "Epoch 69/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3003 - accuracy: 0.8703 - val_loss: 0.3763 - val_accuracy: 0.8289\n",
      "Epoch 70/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2943 - accuracy: 0.8676 - val_loss: 0.3852 - val_accuracy: 0.8289\n",
      "Epoch 71/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8733\n",
      "Saving weights for epoch 70\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2963 - accuracy: 0.8732 - val_loss: 0.3878 - val_accuracy: 0.8227\n",
      "Epoch 72/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2837 - accuracy: 0.8764 - val_loss: 0.3824 - val_accuracy: 0.8141\n",
      "Epoch 73/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2903 - accuracy: 0.8746 - val_loss: 0.4015 - val_accuracy: 0.8123\n",
      "Epoch 74/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2880 - accuracy: 0.8750 - val_loss: 0.3712 - val_accuracy: 0.8248\n",
      "Epoch 75/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2824 - accuracy: 0.8765 - val_loss: 0.3849 - val_accuracy: 0.8222\n",
      "Epoch 76/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8799\n",
      "Saving weights for epoch 75\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2900 - accuracy: 0.8801 - val_loss: 0.4006 - val_accuracy: 0.8166\n",
      "Epoch 77/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2743 - accuracy: 0.8826 - val_loss: 0.4092 - val_accuracy: 0.8141\n",
      "Epoch 78/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2867 - accuracy: 0.8761 - val_loss: 0.3956 - val_accuracy: 0.8166\n",
      "Epoch 79/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2770 - accuracy: 0.8825 - val_loss: 0.3796 - val_accuracy: 0.8237\n",
      "Epoch 80/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2785 - accuracy: 0.8832 - val_loss: 0.3818 - val_accuracy: 0.8267\n",
      "Epoch 81/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8852\n",
      "Saving weights for epoch 80\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2687 - accuracy: 0.8852 - val_loss: 0.4812 - val_accuracy: 0.7915\n",
      "Epoch 82/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2802 - accuracy: 0.8802 - val_loss: 0.3905 - val_accuracy: 0.8222\n",
      "Epoch 83/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2728 - accuracy: 0.8823 - val_loss: 0.4001 - val_accuracy: 0.8156\n",
      "Epoch 84/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2747 - accuracy: 0.8807 - val_loss: 0.4112 - val_accuracy: 0.8134\n",
      "Epoch 85/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2583 - accuracy: 0.8904 - val_loss: 0.3929 - val_accuracy: 0.8310\n",
      "Epoch 86/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.8893\n",
      "Saving weights for epoch 85\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2658 - accuracy: 0.8891 - val_loss: 0.3949 - val_accuracy: 0.8181\n",
      "Epoch 87/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2630 - accuracy: 0.8887 - val_loss: 0.3986 - val_accuracy: 0.8229\n",
      "Epoch 88/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2604 - accuracy: 0.8892 - val_loss: 0.3974 - val_accuracy: 0.8289\n",
      "Epoch 89/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2629 - accuracy: 0.8922 - val_loss: 0.3888 - val_accuracy: 0.8203\n",
      "Epoch 90/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2608 - accuracy: 0.8854 - val_loss: 0.4328 - val_accuracy: 0.8009\n",
      "Epoch 91/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8887\n",
      "Saving weights for epoch 90\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2616 - accuracy: 0.8888 - val_loss: 0.3941 - val_accuracy: 0.8237\n",
      "Epoch 92/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2557 - accuracy: 0.8930 - val_loss: 0.4210 - val_accuracy: 0.8117\n",
      "Epoch 93/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2539 - accuracy: 0.8919 - val_loss: 0.3901 - val_accuracy: 0.8246\n",
      "Epoch 94/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2506 - accuracy: 0.8954 - val_loss: 0.4062 - val_accuracy: 0.8216\n",
      "Epoch 95/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2441 - accuracy: 0.8962 - val_loss: 0.4021 - val_accuracy: 0.8184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8962\n",
      "Saving weights for epoch 95\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2512 - accuracy: 0.8961 - val_loss: 0.4149 - val_accuracy: 0.8138\n",
      "Epoch 97/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2482 - accuracy: 0.8976 - val_loss: 0.4469 - val_accuracy: 0.8042\n",
      "Epoch 98/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2530 - accuracy: 0.8927 - val_loss: 0.4063 - val_accuracy: 0.8188\n",
      "Epoch 99/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2378 - accuracy: 0.9014 - val_loss: 0.3968 - val_accuracy: 0.8248\n",
      "Epoch 100/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2451 - accuracy: 0.8955 - val_loss: 0.3981 - val_accuracy: 0.8285\n",
      "Epoch 101/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9038\n",
      "Saving weights for epoch 100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2433 - accuracy: 0.9038 - val_loss: 0.3978 - val_accuracy: 0.8205\n",
      "Epoch 102/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2444 - accuracy: 0.8972 - val_loss: 0.3724 - val_accuracy: 0.8267\n",
      "Epoch 103/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2408 - accuracy: 0.9000 - val_loss: 0.4003 - val_accuracy: 0.8282\n",
      "Epoch 104/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2446 - accuracy: 0.8983 - val_loss: 0.3905 - val_accuracy: 0.8237\n",
      "Epoch 105/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2410 - accuracy: 0.8976 - val_loss: 0.3931 - val_accuracy: 0.8248\n",
      "Epoch 106/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9014\n",
      "Saving weights for epoch 105\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2402 - accuracy: 0.9013 - val_loss: 0.3990 - val_accuracy: 0.8231\n",
      "Epoch 107/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2352 - accuracy: 0.9025 - val_loss: 0.4107 - val_accuracy: 0.8175\n",
      "Epoch 108/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2404 - accuracy: 0.8997 - val_loss: 0.3955 - val_accuracy: 0.8252\n",
      "Epoch 109/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2333 - accuracy: 0.9018 - val_loss: 0.4964 - val_accuracy: 0.7818\n",
      "Epoch 110/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2298 - accuracy: 0.9060 - val_loss: 0.4123 - val_accuracy: 0.8102\n",
      "Epoch 111/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9050\n",
      "Saving weights for epoch 110\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2355 - accuracy: 0.9049 - val_loss: 0.4054 - val_accuracy: 0.8209\n",
      "Epoch 112/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2343 - accuracy: 0.9026 - val_loss: 0.4177 - val_accuracy: 0.8134\n",
      "Epoch 113/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2356 - accuracy: 0.9006 - val_loss: 0.4196 - val_accuracy: 0.8123\n",
      "Epoch 114/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2269 - accuracy: 0.9081 - val_loss: 0.3873 - val_accuracy: 0.8270\n",
      "Epoch 115/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2325 - accuracy: 0.9041 - val_loss: 0.4112 - val_accuracy: 0.8128\n",
      "Epoch 116/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9071\n",
      "Saving weights for epoch 115\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2239 - accuracy: 0.9070 - val_loss: 0.4048 - val_accuracy: 0.8190\n",
      "Epoch 117/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2300 - accuracy: 0.9080 - val_loss: 0.4072 - val_accuracy: 0.8199\n",
      "Epoch 118/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2335 - accuracy: 0.9047 - val_loss: 0.4057 - val_accuracy: 0.8248\n",
      "Epoch 119/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2256 - accuracy: 0.9109 - val_loss: 0.3924 - val_accuracy: 0.8212\n",
      "Epoch 120/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2243 - accuracy: 0.9099 - val_loss: 0.4066 - val_accuracy: 0.8242\n",
      "Epoch 121/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2237 - accuracy: 0.9080\n",
      "Saving weights for epoch 120\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2242 - accuracy: 0.9079 - val_loss: 0.4191 - val_accuracy: 0.8031\n",
      "Epoch 122/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2270 - accuracy: 0.9073 - val_loss: 0.4296 - val_accuracy: 0.8149\n",
      "Epoch 123/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2274 - accuracy: 0.9047 - val_loss: 0.4143 - val_accuracy: 0.8126\n",
      "Epoch 124/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2264 - accuracy: 0.9067 - val_loss: 0.4199 - val_accuracy: 0.8212\n",
      "Epoch 125/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2173 - accuracy: 0.9096 - val_loss: 0.4367 - val_accuracy: 0.8166\n",
      "Epoch 126/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9098\n",
      "Saving weights for epoch 125\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2219 - accuracy: 0.9098 - val_loss: 0.4128 - val_accuracy: 0.8203\n",
      "Epoch 127/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2228 - accuracy: 0.9086 - val_loss: 0.3948 - val_accuracy: 0.8214\n",
      "Epoch 128/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2213 - accuracy: 0.9109 - val_loss: 0.4025 - val_accuracy: 0.8220\n",
      "Epoch 129/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2221 - accuracy: 0.9095 - val_loss: 0.4138 - val_accuracy: 0.8126\n",
      "Epoch 130/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2206 - accuracy: 0.9120 - val_loss: 0.4581 - val_accuracy: 0.7926\n",
      "Epoch 131/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9153\n",
      "Saving weights for epoch 130\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2090 - accuracy: 0.9154 - val_loss: 0.4406 - val_accuracy: 0.8141\n",
      "Epoch 132/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2188 - accuracy: 0.9095 - val_loss: 0.4174 - val_accuracy: 0.8162\n",
      "Epoch 133/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2214 - accuracy: 0.9109 - val_loss: 0.4459 - val_accuracy: 0.8018\n",
      "Epoch 134/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2111 - accuracy: 0.9160 - val_loss: 0.4128 - val_accuracy: 0.8209\n",
      "Epoch 135/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2101 - accuracy: 0.9175 - val_loss: 0.4206 - val_accuracy: 0.8160\n",
      "Epoch 136/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9105\n",
      "Saving weights for epoch 135\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2171 - accuracy: 0.9104 - val_loss: 0.4186 - val_accuracy: 0.8218\n",
      "Epoch 137/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2106 - accuracy: 0.9155 - val_loss: 0.4696 - val_accuracy: 0.8031\n",
      "Epoch 138/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2137 - accuracy: 0.9107 - val_loss: 0.4039 - val_accuracy: 0.8173\n",
      "Epoch 139/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2140 - accuracy: 0.9122 - val_loss: 0.4210 - val_accuracy: 0.8149\n",
      "Epoch 140/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2097 - accuracy: 0.9140 - val_loss: 0.4335 - val_accuracy: 0.8192\n",
      "Epoch 141/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9125\n",
      "Saving weights for epoch 140\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2192 - accuracy: 0.9123 - val_loss: 0.4096 - val_accuracy: 0.8128\n",
      "Epoch 142/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2048 - accuracy: 0.9188 - val_loss: 0.4300 - val_accuracy: 0.8102\n",
      "Epoch 143/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2082 - accuracy: 0.9143 - val_loss: 0.4247 - val_accuracy: 0.8153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2150 - accuracy: 0.9119 - val_loss: 0.4260 - val_accuracy: 0.8020\n",
      "Epoch 145/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2101 - accuracy: 0.9164 - val_loss: 0.4286 - val_accuracy: 0.8164\n",
      "Epoch 146/1500\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9159\n",
      "Saving weights for epoch 145\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2113 - accuracy: 0.9160 - val_loss: 0.4109 - val_accuracy: 0.8153\n",
      "Epoch 147/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2081 - accuracy: 0.9143 - val_loss: 0.4378 - val_accuracy: 0.8227\n",
      "Epoch 148/1500\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2049 - accuracy: 0.9173 - val_loss: 0.4219 - val_accuracy: 0.8237\n",
      "Epoch 00148: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/1500\n",
      "    146/Unknown - 23s 156ms/step - loss: 0.6887 - accuracy: 0.5431\n",
      "Saving weights for epoch 0\n",
      "146/146 [==============================] - 29s 196ms/step - loss: 0.6887 - accuracy: 0.5431 - val_loss: 0.6813 - val_accuracy: 0.6028\n",
      "Epoch 2/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6816 - accuracy: 0.5656 - val_loss: 0.6733 - val_accuracy: 0.6182\n",
      "Epoch 3/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6759 - accuracy: 0.5823 - val_loss: 0.6660 - val_accuracy: 0.6129\n",
      "Epoch 4/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6681 - accuracy: 0.5913 - val_loss: 0.6598 - val_accuracy: 0.6195\n",
      "Epoch 5/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6589 - accuracy: 0.6068 - val_loss: 0.6393 - val_accuracy: 0.6535\n",
      "Epoch 6/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.6230\n",
      "Saving weights for epoch 5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6463 - accuracy: 0.6227 - val_loss: 0.6285 - val_accuracy: 0.6632\n",
      "Epoch 7/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.6290 - accuracy: 0.6468 - val_loss: 0.6179 - val_accuracy: 0.6606\n",
      "Epoch 8/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6112 - accuracy: 0.6642 - val_loss: 0.6071 - val_accuracy: 0.6730\n",
      "Epoch 9/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5866 - accuracy: 0.6891 - val_loss: 0.5569 - val_accuracy: 0.7261\n",
      "Epoch 10/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5658 - accuracy: 0.7091 - val_loss: 0.5332 - val_accuracy: 0.7384\n",
      "Epoch 11/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.7167\n",
      "Saving weights for epoch 10\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5580 - accuracy: 0.7167 - val_loss: 0.5603 - val_accuracy: 0.7384\n",
      "Epoch 12/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5424 - accuracy: 0.7245 - val_loss: 0.5487 - val_accuracy: 0.7332\n",
      "Epoch 13/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5322 - accuracy: 0.7327 - val_loss: 0.5277 - val_accuracy: 0.7496\n",
      "Epoch 14/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5175 - accuracy: 0.7414 - val_loss: 0.5453 - val_accuracy: 0.7186\n",
      "Epoch 15/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5074 - accuracy: 0.7507 - val_loss: 0.5170 - val_accuracy: 0.7442\n",
      "Epoch 16/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.7512\n",
      "Saving weights for epoch 15\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.5037 - accuracy: 0.7510 - val_loss: 0.4882 - val_accuracy: 0.7663\n",
      "Epoch 17/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4967 - accuracy: 0.7574 - val_loss: 0.5554 - val_accuracy: 0.7083\n",
      "Epoch 18/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4873 - accuracy: 0.7610 - val_loss: 0.4878 - val_accuracy: 0.7597\n",
      "Epoch 19/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4831 - accuracy: 0.7617 - val_loss: 0.4635 - val_accuracy: 0.7732\n",
      "Epoch 20/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4775 - accuracy: 0.7693 - val_loss: 0.4676 - val_accuracy: 0.7779\n",
      "Epoch 21/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7700 ETA: 0s - loss: 0.4744 - accuracy: \n",
      "Saving weights for epoch 20\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4753 - accuracy: 0.7697 - val_loss: 0.4630 - val_accuracy: 0.7805\n",
      "Epoch 22/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4674 - accuracy: 0.7759 - val_loss: 0.4563 - val_accuracy: 0.7754\n",
      "Epoch 23/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4637 - accuracy: 0.7777 - val_loss: 0.4480 - val_accuracy: 0.7792\n",
      "Epoch 24/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4628 - accuracy: 0.7762 - val_loss: 0.4458 - val_accuracy: 0.7880\n",
      "Epoch 25/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4546 - accuracy: 0.7809 - val_loss: 0.4501 - val_accuracy: 0.7870\n",
      "Epoch 26/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.7837\n",
      "Saving weights for epoch 25\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4514 - accuracy: 0.7839 - val_loss: 0.4294 - val_accuracy: 0.7934\n",
      "Epoch 27/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4436 - accuracy: 0.7896 - val_loss: 0.4342 - val_accuracy: 0.7979\n",
      "Epoch 28/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4400 - accuracy: 0.7916 - val_loss: 0.4303 - val_accuracy: 0.7943\n",
      "Epoch 29/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4385 - accuracy: 0.7928 - val_loss: 0.4301 - val_accuracy: 0.8003\n",
      "Epoch 30/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4342 - accuracy: 0.7956 - val_loss: 0.4222 - val_accuracy: 0.8018\n",
      "Epoch 31/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.7979\n",
      "Saving weights for epoch 30\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4301 - accuracy: 0.7977 - val_loss: 0.4141 - val_accuracy: 0.8046\n",
      "Epoch 32/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4239 - accuracy: 0.7999 - val_loss: 0.4226 - val_accuracy: 0.7975\n",
      "Epoch 33/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4200 - accuracy: 0.8049 - val_loss: 0.4145 - val_accuracy: 0.8044\n",
      "Epoch 34/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4148 - accuracy: 0.8044 - val_loss: 0.4131 - val_accuracy: 0.8018\n",
      "Epoch 35/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4159 - accuracy: 0.8069 - val_loss: 0.4110 - val_accuracy: 0.8089\n",
      "Epoch 36/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8089 ETA: 0s - loss: 0.4127 - accuracy: 0.\n",
      "Saving weights for epoch 35\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4125 - accuracy: 0.8089 - val_loss: 0.4190 - val_accuracy: 0.8074\n",
      "Epoch 37/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4060 - accuracy: 0.8084 - val_loss: 0.4069 - val_accuracy: 0.8089\n",
      "Epoch 38/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4064 - accuracy: 0.8103 - val_loss: 0.4044 - val_accuracy: 0.8102\n",
      "Epoch 39/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.4011 - accuracy: 0.8136 - val_loss: 0.3974 - val_accuracy: 0.8145\n",
      "Epoch 40/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3987 - accuracy: 0.8149 - val_loss: 0.3938 - val_accuracy: 0.8126\n",
      "Epoch 41/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3922 - accuracy: 0.8194\n",
      "Saving weights for epoch 40\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3929 - accuracy: 0.8191 - val_loss: 0.4153 - val_accuracy: 0.8087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3898 - accuracy: 0.8211 - val_loss: 0.4042 - val_accuracy: 0.8055\n",
      "Epoch 43/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3886 - accuracy: 0.8207 - val_loss: 0.4051 - val_accuracy: 0.8087\n",
      "Epoch 44/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3861 - accuracy: 0.8230 - val_loss: 0.4191 - val_accuracy: 0.8040\n",
      "Epoch 45/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3816 - accuracy: 0.8245 - val_loss: 0.3921 - val_accuracy: 0.8143\n",
      "Epoch 46/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8304 ETA: \n",
      "Saving weights for epoch 45\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3757 - accuracy: 0.8302 - val_loss: 0.4069 - val_accuracy: 0.8048\n",
      "Epoch 47/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3818 - accuracy: 0.8263 - val_loss: 0.3898 - val_accuracy: 0.8199\n",
      "Epoch 48/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3735 - accuracy: 0.8290 - val_loss: 0.3898 - val_accuracy: 0.8134\n",
      "Epoch 49/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3707 - accuracy: 0.8289 - val_loss: 0.3982 - val_accuracy: 0.8128\n",
      "Epoch 50/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3731 - accuracy: 0.8320 - val_loss: 0.3862 - val_accuracy: 0.8227\n",
      "Epoch 51/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8320 E\n",
      "Saving weights for epoch 50\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3651 - accuracy: 0.8322 - val_loss: 0.3790 - val_accuracy: 0.8278\n",
      "Epoch 52/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3608 - accuracy: 0.8375 - val_loss: 0.4194 - val_accuracy: 0.8059\n",
      "Epoch 53/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3601 - accuracy: 0.8385 - val_loss: 0.3798 - val_accuracy: 0.8237\n",
      "Epoch 54/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3581 - accuracy: 0.8399 - val_loss: 0.3932 - val_accuracy: 0.8179\n",
      "Epoch 55/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3530 - accuracy: 0.8402 - val_loss: 0.4001 - val_accuracy: 0.8119\n",
      "Epoch 56/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8445\n",
      "Saving weights for epoch 55\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3499 - accuracy: 0.8447 - val_loss: 0.4497 - val_accuracy: 0.7971\n",
      "Epoch 57/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3506 - accuracy: 0.8402 - val_loss: 0.3790 - val_accuracy: 0.8216\n",
      "Epoch 58/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3460 - accuracy: 0.8449 - val_loss: 0.3869 - val_accuracy: 0.8145\n",
      "Epoch 59/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3499 - accuracy: 0.8402 - val_loss: 0.3824 - val_accuracy: 0.8235\n",
      "Epoch 60/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3364 - accuracy: 0.8478 - val_loss: 0.3743 - val_accuracy: 0.8321\n",
      "Epoch 61/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8469\n",
      "Saving weights for epoch 60\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3395 - accuracy: 0.8468 - val_loss: 0.4051 - val_accuracy: 0.8166\n",
      "Epoch 62/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3382 - accuracy: 0.8495 - val_loss: 0.3833 - val_accuracy: 0.8184\n",
      "Epoch 63/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3304 - accuracy: 0.8515 - val_loss: 0.3651 - val_accuracy: 0.8321\n",
      "Epoch 64/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3278 - accuracy: 0.8542 - val_loss: 0.3712 - val_accuracy: 0.8280\n",
      "Epoch 65/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3279 - accuracy: 0.8543 - val_loss: 0.3848 - val_accuracy: 0.8188\n",
      "Epoch 66/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.8577\n",
      "Saving weights for epoch 65\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3249 - accuracy: 0.8579 - val_loss: 0.3818 - val_accuracy: 0.8246\n",
      "Epoch 67/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3230 - accuracy: 0.8578 - val_loss: 0.4174 - val_accuracy: 0.8063\n",
      "Epoch 68/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3221 - accuracy: 0.8568 - val_loss: 0.3783 - val_accuracy: 0.8278\n",
      "Epoch 69/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3182 - accuracy: 0.8588 - val_loss: 0.3783 - val_accuracy: 0.8291\n",
      "Epoch 70/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3132 - accuracy: 0.8608 - val_loss: 0.3959 - val_accuracy: 0.8171\n",
      "Epoch 71/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8620\n",
      "Saving weights for epoch 70\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3139 - accuracy: 0.8621 - val_loss: 0.3753 - val_accuracy: 0.8291\n",
      "Epoch 72/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3159 - accuracy: 0.8608 - val_loss: 0.4106 - val_accuracy: 0.8012\n",
      "Epoch 73/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3063 - accuracy: 0.8631 - val_loss: 0.3852 - val_accuracy: 0.8285\n",
      "Epoch 74/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3057 - accuracy: 0.8654 - val_loss: 0.3695 - val_accuracy: 0.8332\n",
      "Epoch 75/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2981 - accuracy: 0.8699 - val_loss: 0.3789 - val_accuracy: 0.8276\n",
      "Epoch 76/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8671\n",
      "Saving weights for epoch 75\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3031 - accuracy: 0.8674 - val_loss: 0.3836 - val_accuracy: 0.8259\n",
      "Epoch 77/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2966 - accuracy: 0.8692 - val_loss: 0.3772 - val_accuracy: 0.8323\n",
      "Epoch 78/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3018 - accuracy: 0.8692 - val_loss: 0.3828 - val_accuracy: 0.8237\n",
      "Epoch 79/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.3006 - accuracy: 0.8671 - val_loss: 0.3713 - val_accuracy: 0.8315\n",
      "Epoch 80/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2936 - accuracy: 0.8699 - val_loss: 0.4094 - val_accuracy: 0.8117\n",
      "Epoch 81/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8737\n",
      "Saving weights for epoch 80\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2879 - accuracy: 0.8739 - val_loss: 0.3905 - val_accuracy: 0.8259\n",
      "Epoch 82/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2887 - accuracy: 0.8733 - val_loss: 0.3874 - val_accuracy: 0.8203\n",
      "Epoch 83/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2864 - accuracy: 0.8738 - val_loss: 0.4075 - val_accuracy: 0.8166\n",
      "Epoch 84/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2871 - accuracy: 0.8751 - val_loss: 0.3760 - val_accuracy: 0.8325\n",
      "Epoch 85/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2823 - accuracy: 0.8766 - val_loss: 0.3806 - val_accuracy: 0.8347\n",
      "Epoch 86/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.8822\n",
      "Saving weights for epoch 85\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2714 - accuracy: 0.8822 - val_loss: 0.3788 - val_accuracy: 0.8285\n",
      "Epoch 87/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2754 - accuracy: 0.8794 - val_loss: 0.4110 - val_accuracy: 0.8164\n",
      "Epoch 88/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2813 - accuracy: 0.8753 - val_loss: 0.3933 - val_accuracy: 0.8227\n",
      "Epoch 89/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2687 - accuracy: 0.8836 - val_loss: 0.3923 - val_accuracy: 0.8235\n",
      "Epoch 90/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2720 - accuracy: 0.8840 - val_loss: 0.3819 - val_accuracy: 0.8332\n",
      "Epoch 91/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/146 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8829\n",
      "Saving weights for epoch 90\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2701 - accuracy: 0.8831 - val_loss: 0.3762 - val_accuracy: 0.8285\n",
      "Epoch 92/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2599 - accuracy: 0.8898 - val_loss: 0.4021 - val_accuracy: 0.8218\n",
      "Epoch 93/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2639 - accuracy: 0.8849 - val_loss: 0.3738 - val_accuracy: 0.8293\n",
      "Epoch 94/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2650 - accuracy: 0.8852 - val_loss: 0.3639 - val_accuracy: 0.8336\n",
      "Epoch 95/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2557 - accuracy: 0.8912 - val_loss: 0.3869 - val_accuracy: 0.8343\n",
      "Epoch 96/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.8899 ETA: 0s - loss: 0\n",
      "Saving weights for epoch 95\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2518 - accuracy: 0.8902 - val_loss: 0.4080 - val_accuracy: 0.8224\n",
      "Epoch 97/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2540 - accuracy: 0.8916 - val_loss: 0.3756 - val_accuracy: 0.8338\n",
      "Epoch 98/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2529 - accuracy: 0.8916 - val_loss: 0.3910 - val_accuracy: 0.8239\n",
      "Epoch 99/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2526 - accuracy: 0.8924 - val_loss: 0.3828 - val_accuracy: 0.8368\n",
      "Epoch 100/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2525 - accuracy: 0.8923 - val_loss: 0.3785 - val_accuracy: 0.8358\n",
      "Epoch 101/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.8982\n",
      "Saving weights for epoch 100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2421 - accuracy: 0.8983 - val_loss: 0.3980 - val_accuracy: 0.8259\n",
      "Epoch 102/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2473 - accuracy: 0.8924 - val_loss: 0.3817 - val_accuracy: 0.8409\n",
      "Epoch 103/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2408 - accuracy: 0.8973 - val_loss: 0.3987 - val_accuracy: 0.8278\n",
      "Epoch 104/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2345 - accuracy: 0.9015 - val_loss: 0.3866 - val_accuracy: 0.8349\n",
      "Epoch 105/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2335 - accuracy: 0.9023 - val_loss: 0.4005 - val_accuracy: 0.8224\n",
      "Epoch 106/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.8973\n",
      "Saving weights for epoch 105\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2372 - accuracy: 0.8975 - val_loss: 0.3841 - val_accuracy: 0.8353\n",
      "Epoch 107/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2345 - accuracy: 0.9008 - val_loss: 0.3933 - val_accuracy: 0.8285\n",
      "Epoch 108/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2336 - accuracy: 0.9046 - val_loss: 0.3911 - val_accuracy: 0.8287\n",
      "Epoch 109/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2286 - accuracy: 0.9051 - val_loss: 0.4033 - val_accuracy: 0.8227\n",
      "Epoch 110/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2286 - accuracy: 0.9044 - val_loss: 0.4034 - val_accuracy: 0.8248\n",
      "Epoch 111/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9032\n",
      "Saving weights for epoch 110\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2288 - accuracy: 0.9032 - val_loss: 0.4741 - val_accuracy: 0.7975\n",
      "Epoch 112/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.2305 - accuracy: 0.9046 - val_loss: 0.4114 - val_accuracy: 0.8218\n",
      "Epoch 113/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2265 - accuracy: 0.9055 - val_loss: 0.4040 - val_accuracy: 0.8194\n",
      "Epoch 114/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2217 - accuracy: 0.9081 - val_loss: 0.4138 - val_accuracy: 0.8227\n",
      "Epoch 115/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2147 - accuracy: 0.9110 - val_loss: 0.3920 - val_accuracy: 0.8360\n",
      "Epoch 116/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.2194 - accuracy: 0.9082\n",
      "Saving weights for epoch 115\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2185 - accuracy: 0.9084 - val_loss: 0.4035 - val_accuracy: 0.8306\n",
      "Epoch 117/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2127 - accuracy: 0.9108 - val_loss: 0.4210 - val_accuracy: 0.8248\n",
      "Epoch 118/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2110 - accuracy: 0.9116 - val_loss: 0.4199 - val_accuracy: 0.8289\n",
      "Epoch 119/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2074 - accuracy: 0.9136 - val_loss: 0.4165 - val_accuracy: 0.8304\n",
      "Epoch 120/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2182 - accuracy: 0.9091 - val_loss: 0.4118 - val_accuracy: 0.8313\n",
      "Epoch 121/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9170\n",
      "Saving weights for epoch 120\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2050 - accuracy: 0.9170 - val_loss: 0.4027 - val_accuracy: 0.8336\n",
      "Epoch 122/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2082 - accuracy: 0.9142 - val_loss: 0.4212 - val_accuracy: 0.8237\n",
      "Epoch 123/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2003 - accuracy: 0.9156 - val_loss: 0.4183 - val_accuracy: 0.8289\n",
      "Epoch 124/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2105 - accuracy: 0.9128 - val_loss: 0.4487 - val_accuracy: 0.8194\n",
      "Epoch 125/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1998 - accuracy: 0.9159 - val_loss: 0.4240 - val_accuracy: 0.8308\n",
      "Epoch 126/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9202\n",
      "Saving weights for epoch 125\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1930 - accuracy: 0.9202 - val_loss: 0.4247 - val_accuracy: 0.8188\n",
      "Epoch 127/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.2032 - accuracy: 0.9138 - val_loss: 0.4193 - val_accuracy: 0.8280\n",
      "Epoch 128/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1964 - accuracy: 0.9184 - val_loss: 0.4136 - val_accuracy: 0.8300\n",
      "Epoch 129/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1939 - accuracy: 0.9210 - val_loss: 0.4216 - val_accuracy: 0.8291\n",
      "Epoch 130/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1893 - accuracy: 0.9218 - val_loss: 0.4287 - val_accuracy: 0.8270\n",
      "Epoch 131/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9206\n",
      "Saving weights for epoch 130\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1926 - accuracy: 0.9207 - val_loss: 0.4077 - val_accuracy: 0.8295\n",
      "Epoch 132/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1995 - accuracy: 0.9162 - val_loss: 0.4042 - val_accuracy: 0.8308\n",
      "Epoch 133/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1890 - accuracy: 0.9194 - val_loss: 0.4121 - val_accuracy: 0.8280 0s - loss: 0.1894 - \n",
      "Epoch 134/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1872 - accuracy: 0.9226 - val_loss: 0.4087 - val_accuracy: 0.8388\n",
      "Epoch 135/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1816 - accuracy: 0.9264 - val_loss: 0.4343 - val_accuracy: 0.8196\n",
      "Epoch 136/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9245\n",
      "Saving weights for epoch 135\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1830 - accuracy: 0.9246 - val_loss: 0.4052 - val_accuracy: 0.8343\n",
      "Epoch 137/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1820 - accuracy: 0.9232 - val_loss: 0.4289 - val_accuracy: 0.8205\n",
      "Epoch 138/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1836 - accuracy: 0.9242 - val_loss: 0.4149 - val_accuracy: 0.8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1809 - accuracy: 0.9231 - val_loss: 0.4211 - val_accuracy: 0.8216\n",
      "Epoch 140/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1848 - accuracy: 0.9234 - val_loss: 0.4026 - val_accuracy: 0.8308\n",
      "Epoch 141/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1754 - accuracy: 0.9295\n",
      "Saving weights for epoch 140\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1753 - accuracy: 0.9294 - val_loss: 0.4578 - val_accuracy: 0.8102\n",
      "Epoch 142/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1830 - accuracy: 0.9250 - val_loss: 0.4445 - val_accuracy: 0.8203\n",
      "Epoch 143/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1721 - accuracy: 0.9300 - val_loss: 0.4417 - val_accuracy: 0.8239\n",
      "Epoch 144/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1731 - accuracy: 0.9281 - val_loss: 0.4341 - val_accuracy: 0.8222\n",
      "Epoch 145/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1725 - accuracy: 0.9285 - val_loss: 0.4910 - val_accuracy: 0.7876\n",
      "Epoch 146/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.9274\n",
      "Saving weights for epoch 145\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1783 - accuracy: 0.9275 - val_loss: 0.4348 - val_accuracy: 0.8235\n",
      "Epoch 147/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1759 - accuracy: 0.9294 - val_loss: 0.4570 - val_accuracy: 0.8196\n",
      "Epoch 148/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1770 - accuracy: 0.9286 - val_loss: 0.4682 - val_accuracy: 0.8046\n",
      "Epoch 149/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1660 - accuracy: 0.9307 - val_loss: 0.4527 - val_accuracy: 0.8190\n",
      "Epoch 150/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1712 - accuracy: 0.9315 - val_loss: 0.4613 - val_accuracy: 0.8164\n",
      "Epoch 151/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9316\n",
      "Saving weights for epoch 150\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1634 - accuracy: 0.9319 - val_loss: 0.4453 - val_accuracy: 0.8259\n",
      "Epoch 152/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1687 - accuracy: 0.9321 - val_loss: 0.4361 - val_accuracy: 0.8298\n",
      "Epoch 153/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1591 - accuracy: 0.9344 - val_loss: 0.4480 - val_accuracy: 0.8212\n",
      "Epoch 154/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1665 - accuracy: 0.9312 - val_loss: 0.4459 - val_accuracy: 0.8255\n",
      "Epoch 155/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1524 - accuracy: 0.9386 - val_loss: 0.4417 - val_accuracy: 0.8302\n",
      "Epoch 156/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9381\n",
      "Saving weights for epoch 155\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1550 - accuracy: 0.9382 - val_loss: 0.4916 - val_accuracy: 0.8173\n",
      "Epoch 157/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1659 - accuracy: 0.9354 - val_loss: 0.4336 - val_accuracy: 0.8293\n",
      "Epoch 158/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1568 - accuracy: 0.9363 - val_loss: 0.4411 - val_accuracy: 0.8340\n",
      "Epoch 159/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1610 - accuracy: 0.9357 - val_loss: 0.4737 - val_accuracy: 0.8188\n",
      "Epoch 160/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1581 - accuracy: 0.9373 - val_loss: 0.4431 - val_accuracy: 0.8310\n",
      "Epoch 161/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9384\n",
      "Saving weights for epoch 160\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1535 - accuracy: 0.9386 - val_loss: 0.4410 - val_accuracy: 0.8276\n",
      "Epoch 162/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1571 - accuracy: 0.9379 - val_loss: 0.4619 - val_accuracy: 0.8138\n",
      "Epoch 163/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1514 - accuracy: 0.9395 - val_loss: 0.4643 - val_accuracy: 0.8257\n",
      "Epoch 164/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1521 - accuracy: 0.9385 - val_loss: 0.4789 - val_accuracy: 0.8214\n",
      "Epoch 165/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1530 - accuracy: 0.9378 - val_loss: 0.4144 - val_accuracy: 0.8351\n",
      "Epoch 166/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9372\n",
      "Saving weights for epoch 165\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1549 - accuracy: 0.9374 - val_loss: 0.4634 - val_accuracy: 0.8207\n",
      "Epoch 167/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1525 - accuracy: 0.9403 - val_loss: 0.4394 - val_accuracy: 0.8315\n",
      "Epoch 168/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1471 - accuracy: 0.9415 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "Epoch 169/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1463 - accuracy: 0.9421 - val_loss: 0.4299 - val_accuracy: 0.8332\n",
      "Epoch 170/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1504 - accuracy: 0.9398 - val_loss: 0.4343 - val_accuracy: 0.8280\n",
      "Epoch 171/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9394\n",
      "Saving weights for epoch 170\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1463 - accuracy: 0.9395 - val_loss: 0.4547 - val_accuracy: 0.8156\n",
      "Epoch 172/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1480 - accuracy: 0.9410 - val_loss: 0.4944 - val_accuracy: 0.8091\n",
      "Epoch 173/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1441 - accuracy: 0.9430 - val_loss: 0.4475 - val_accuracy: 0.8235\n",
      "Epoch 174/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1394 - accuracy: 0.9451 - val_loss: 0.4727 - val_accuracy: 0.8166\n",
      "Epoch 175/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1379 - accuracy: 0.9440 - val_loss: 0.4719 - val_accuracy: 0.8224\n",
      "Epoch 176/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9442\n",
      "Saving weights for epoch 175\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1414 - accuracy: 0.9442 - val_loss: 0.4919 - val_accuracy: 0.8089\n",
      "Epoch 177/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1416 - accuracy: 0.9437 - val_loss: 0.4468 - val_accuracy: 0.8280\n",
      "Epoch 178/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1399 - accuracy: 0.9442 - val_loss: 0.4741 - val_accuracy: 0.8220\n",
      "Epoch 179/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1327 - accuracy: 0.9471 - val_loss: 0.5007 - val_accuracy: 0.8123\n",
      "Epoch 180/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1443 - accuracy: 0.9445 - val_loss: 0.4891 - val_accuracy: 0.8190\n",
      "Epoch 181/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9456\n",
      "Saving weights for epoch 180\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1391 - accuracy: 0.9455 - val_loss: 0.4620 - val_accuracy: 0.8199\n",
      "Epoch 182/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1337 - accuracy: 0.9476 - val_loss: 0.4900 - val_accuracy: 0.8145\n",
      "Epoch 183/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1325 - accuracy: 0.9495 - val_loss: 0.4615 - val_accuracy: 0.8259\n",
      "Epoch 184/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1314 - accuracy: 0.9476 - val_loss: 0.4727 - val_accuracy: 0.8259\n",
      "Epoch 185/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1395 - accuracy: 0.9455 - val_loss: 0.4288 - val_accuracy: 0.8308\n",
      "Epoch 186/1500\n",
      "144/146 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9480\n",
      "Saving weights for epoch 185\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1335 - accuracy: 0.9482 - val_loss: 0.4650 - val_accuracy: 0.8233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1314 - accuracy: 0.9490 - val_loss: 0.4667 - val_accuracy: 0.8201\n",
      "Epoch 188/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1328 - accuracy: 0.9478 - val_loss: 0.4988 - val_accuracy: 0.8083\n",
      "Epoch 189/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1388 - accuracy: 0.9454 - val_loss: 0.4462 - val_accuracy: 0.8282\n",
      "Epoch 190/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1318 - accuracy: 0.9462 - val_loss: 0.4596 - val_accuracy: 0.8278\n",
      "Epoch 191/1500\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9498\n",
      "Saving weights for epoch 190\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1259 - accuracy: 0.9499 - val_loss: 0.4933 - val_accuracy: 0.8033\n",
      "Epoch 192/1500\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.1344 - accuracy: 0.9492 - val_loss: 0.4917 - val_accuracy: 0.8209\n",
      "Epoch 193/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1297 - accuracy: 0.9495 - val_loss: 0.4819 - val_accuracy: 0.8220\n",
      "Epoch 194/1500\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.1248 - accuracy: 0.9521 - val_loss: 0.4704 - val_accuracy: 0.8239\n",
      "Epoch 00194: early stopping\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/1500\n",
      "     37/Unknown - 22s 590ms/step - loss: 0.6935 - accuracy: 0.4990\n",
      "Saving weights for epoch 0\n",
      "37/37 [==============================] - 28s 753ms/step - loss: 0.6935 - accuracy: 0.4990 - val_loss: 0.6930 - val_accuracy: 0.4910\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.6906 - accuracy: 0.5421 - val_loss: 0.6915 - val_accuracy: 0.4946\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6869 - accuracy: 0.5658 - val_loss: 0.6916 - val_accuracy: 0.4951\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6830 - accuracy: 0.5721 - val_loss: 0.6983 - val_accuracy: 0.4910\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6818 - accuracy: 0.5663 - val_loss: 0.7025 - val_accuracy: 0.4914\n",
      "Epoch 6/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5577\n",
      "Saving weights for epoch 5\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6832 - accuracy: 0.5578 - val_loss: 0.6895 - val_accuracy: 0.5080\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.6892 - accuracy: 0.5451 - val_loss: 0.6907 - val_accuracy: 0.4996\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.6798 - accuracy: 0.5797 - val_loss: 0.6800 - val_accuracy: 0.5617\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6778 - accuracy: 0.5756 - val_loss: 0.6842 - val_accuracy: 0.5327\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.6790 - accuracy: 0.5736 - val_loss: 0.6808 - val_accuracy: 0.5497\n",
      "Epoch 11/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6743 - accuracy: 0.5841\n",
      "Saving weights for epoch 10\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.6740 - accuracy: 0.5838 - val_loss: 0.6998 - val_accuracy: 0.4989\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.6721 - accuracy: 0.5836 - val_loss: 0.6939 - val_accuracy: 0.5112\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.6700 - accuracy: 0.5912 - val_loss: 0.6745 - val_accuracy: 0.5636\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6674 - accuracy: 0.5961 - val_loss: 0.6805 - val_accuracy: 0.5486\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.6637 - accuracy: 0.5990 - val_loss: 0.6707 - val_accuracy: 0.5735\n",
      "Epoch 16/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6616 - accuracy: 0.6070\n",
      "Saving weights for epoch 15\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.6608 - accuracy: 0.6071 - val_loss: 0.6655 - val_accuracy: 0.5868\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6545 - accuracy: 0.6141 - val_loss: 0.6746 - val_accuracy: 0.5649\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6568 - accuracy: 0.6127 - val_loss: 0.6611 - val_accuracy: 0.5978\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6486 - accuracy: 0.6224 - val_loss: 0.6611 - val_accuracy: 0.5965\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6468 - accuracy: 0.6258 - val_loss: 0.6501 - val_accuracy: 0.6217\n",
      "Epoch 21/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6481 - accuracy: 0.6254\n",
      "Saving weights for epoch 20\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6478 - accuracy: 0.6256 - val_loss: 0.6514 - val_accuracy: 0.6152\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6385 - accuracy: 0.6354 - val_loss: 0.6326 - val_accuracy: 0.6554\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.6309 - accuracy: 0.6447 - val_loss: 0.6471 - val_accuracy: 0.6182\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6377 - accuracy: 0.6363 - val_loss: 0.6508 - val_accuracy: 0.6215\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6298 - accuracy: 0.6494 - val_loss: 0.6589 - val_accuracy: 0.6036\n",
      "Epoch 26/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6315 - accuracy: 0.6448\n",
      "Saving weights for epoch 25\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6318 - accuracy: 0.6448 - val_loss: 0.6440 - val_accuracy: 0.6212\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6150 - accuracy: 0.6622 - val_loss: 0.6198 - val_accuracy: 0.6496\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6083 - accuracy: 0.6672 - val_loss: 0.6242 - val_accuracy: 0.6410\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6193 - accuracy: 0.6550 - val_loss: 0.6368 - val_accuracy: 0.6318\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6365 - accuracy: 0.6337 - val_loss: 0.6549 - val_accuracy: 0.6051\n",
      "Epoch 31/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6286 - accuracy: 0.6491\n",
      "Saving weights for epoch 30\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6289 - accuracy: 0.6487 - val_loss: 0.6438 - val_accuracy: 0.6152\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6024 - accuracy: 0.6723 - val_loss: 0.6416 - val_accuracy: 0.6240\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6278 - accuracy: 0.6450 - val_loss: 0.6532 - val_accuracy: 0.6010\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6041 - accuracy: 0.6714 - val_loss: 0.6075 - val_accuracy: 0.6586\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5910 - accuracy: 0.6834 - val_loss: 0.6246 - val_accuracy: 0.6449\n",
      "Epoch 36/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.6808\n",
      "Saving weights for epoch 35\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5921 - accuracy: 0.6807 - val_loss: 0.6281 - val_accuracy: 0.6348\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5848 - accuracy: 0.6887 - val_loss: 0.6008 - val_accuracy: 0.6623\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5791 - accuracy: 0.6955 - val_loss: 0.6010 - val_accuracy: 0.6670\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.6106 - accuracy: 0.6669 - val_loss: 0.6496 - val_accuracy: 0.6124\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5845 - accuracy: 0.6887 - val_loss: 0.5943 - val_accuracy: 0.6690\n",
      "Epoch 41/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.6972\n",
      "Saving weights for epoch 40\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5748 - accuracy: 0.6969 - val_loss: 0.5941 - val_accuracy: 0.6660\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5670 - accuracy: 0.7071 - val_loss: 0.5775 - val_accuracy: 0.6780\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5744 - accuracy: 0.7010 - val_loss: 0.5521 - val_accuracy: 0.7044\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5669 - accuracy: 0.7057 - val_loss: 0.5499 - val_accuracy: 0.7081\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5588 - accuracy: 0.7077 - val_loss: 0.5628 - val_accuracy: 0.6883\n",
      "Epoch 46/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5571 - accuracy: 0.7132\n",
      "Saving weights for epoch 45\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5578 - accuracy: 0.7129 - val_loss: 0.5273 - val_accuracy: 0.7324\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5556 - accuracy: 0.7121 - val_loss: 0.5468 - val_accuracy: 0.7077\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5564 - accuracy: 0.7110 - val_loss: 0.5641 - val_accuracy: 0.6877\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5525 - accuracy: 0.7149 - val_loss: 0.5384 - val_accuracy: 0.7120\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5495 - accuracy: 0.7171 - val_loss: 0.5430 - val_accuracy: 0.7096\n",
      "Epoch 51/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5448 - accuracy: 0.7234\n",
      "Saving weights for epoch 50\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5451 - accuracy: 0.7233 - val_loss: 0.5453 - val_accuracy: 0.7064\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5444 - accuracy: 0.7229 - val_loss: 0.5125 - val_accuracy: 0.7429\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5402 - accuracy: 0.7260 - val_loss: 0.5313 - val_accuracy: 0.7182\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5385 - accuracy: 0.7244 - val_loss: 0.5094 - val_accuracy: 0.7418\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5407 - accuracy: 0.7244 - val_loss: 0.5140 - val_accuracy: 0.7358\n",
      "Epoch 56/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5367 - accuracy: 0.7284\n",
      "Saving weights for epoch 55\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5373 - accuracy: 0.7280 - val_loss: 0.5196 - val_accuracy: 0.7309\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5322 - accuracy: 0.7297 - val_loss: 0.5173 - val_accuracy: 0.7304\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5282 - accuracy: 0.7312 - val_loss: 0.5312 - val_accuracy: 0.7169\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5289 - accuracy: 0.7332 - val_loss: 0.5512 - val_accuracy: 0.6999\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5263 - accuracy: 0.7349 - val_loss: 0.5244 - val_accuracy: 0.7195\n",
      "Epoch 61/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7354\n",
      "Saving weights for epoch 60\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5229 - accuracy: 0.7352 - val_loss: 0.5392 - val_accuracy: 0.7072\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5173 - accuracy: 0.7382 - val_loss: 0.5147 - val_accuracy: 0.7272\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5243 - accuracy: 0.7311 - val_loss: 0.4992 - val_accuracy: 0.7418\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.5091 - accuracy: 0.7480 - val_loss: 0.4928 - val_accuracy: 0.7457\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.5150 - accuracy: 0.7433 - val_loss: 0.5033 - val_accuracy: 0.7384\n",
      "Epoch 66/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7459\n",
      "Saving weights for epoch 65\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.5121 - accuracy: 0.7460 - val_loss: 0.5189 - val_accuracy: 0.7266\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5131 - accuracy: 0.7426 - val_loss: 0.4927 - val_accuracy: 0.7479\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5035 - accuracy: 0.7494 - val_loss: 0.4962 - val_accuracy: 0.7451\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.5052 - accuracy: 0.7508 - val_loss: 0.5133 - val_accuracy: 0.7371\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4995 - accuracy: 0.7549 - val_loss: 0.4976 - val_accuracy: 0.7433\n",
      "Epoch 71/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4971 - accuracy: 0.7554\n",
      "Saving weights for epoch 70\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4973 - accuracy: 0.7550 - val_loss: 0.5074 - val_accuracy: 0.7498\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4974 - accuracy: 0.7520 - val_loss: 0.4716 - val_accuracy: 0.7706\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4951 - accuracy: 0.7557 - val_loss: 0.4743 - val_accuracy: 0.7642\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.4912 - accuracy: 0.7601 - val_loss: 0.5058 - val_accuracy: 0.7405\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4880 - accuracy: 0.7606 - val_loss: 0.5399 - val_accuracy: 0.7221\n",
      "Epoch 76/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.7623\n",
      "Saving weights for epoch 75\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4869 - accuracy: 0.7623 - val_loss: 0.5649 - val_accuracy: 0.7132\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4892 - accuracy: 0.7633 - val_loss: 0.5465 - val_accuracy: 0.7130\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4812 - accuracy: 0.7634 - val_loss: 0.4682 - val_accuracy: 0.7739\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4838 - accuracy: 0.7679 - val_loss: 0.4797 - val_accuracy: 0.7584\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4775 - accuracy: 0.7702 - val_loss: 0.4749 - val_accuracy: 0.7616\n",
      "Epoch 81/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4704 - accuracy: 0.7729\n",
      "Saving weights for epoch 80\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4700 - accuracy: 0.7729 - val_loss: 0.4731 - val_accuracy: 0.7687\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4694 - accuracy: 0.7728 - val_loss: 0.4883 - val_accuracy: 0.7612\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4655 - accuracy: 0.7776 - val_loss: 0.4881 - val_accuracy: 0.7513\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4863 - accuracy: 0.7595 - val_loss: 0.4725 - val_accuracy: 0.7644\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4572 - accuracy: 0.7802 - val_loss: 0.4537 - val_accuracy: 0.7792\n",
      "Epoch 86/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4683 - accuracy: 0.7737\n",
      "Saving weights for epoch 85\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4674 - accuracy: 0.7738 - val_loss: 0.4447 - val_accuracy: 0.7863\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4556 - accuracy: 0.7845 - val_loss: 0.5275 - val_accuracy: 0.7388\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4633 - accuracy: 0.7747 - val_loss: 0.4891 - val_accuracy: 0.7573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4542 - accuracy: 0.7836 - val_loss: 0.4479 - val_accuracy: 0.7861\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4520 - accuracy: 0.7855 - val_loss: 0.4451 - val_accuracy: 0.7853\n",
      "Epoch 91/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7874\n",
      "Saving weights for epoch 90\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4478 - accuracy: 0.7874 - val_loss: 0.5100 - val_accuracy: 0.7438\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4510 - accuracy: 0.7858 - val_loss: 0.4266 - val_accuracy: 0.8003\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4545 - accuracy: 0.7818 - val_loss: 0.4413 - val_accuracy: 0.7900\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4429 - accuracy: 0.7887 - val_loss: 0.4343 - val_accuracy: 0.7928\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4416 - accuracy: 0.7923 - val_loss: 0.4384 - val_accuracy: 0.7943\n",
      "Epoch 96/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.7923\n",
      "Saving weights for epoch 95\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4418 - accuracy: 0.7926 - val_loss: 0.4416 - val_accuracy: 0.7878\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4377 - accuracy: 0.7926 - val_loss: 0.4111 - val_accuracy: 0.8095\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4402 - accuracy: 0.7927 - val_loss: 0.4256 - val_accuracy: 0.8007\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4423 - accuracy: 0.7925 - val_loss: 0.4393 - val_accuracy: 0.7921\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4356 - accuracy: 0.7954 - val_loss: 0.4110 - val_accuracy: 0.8108\n",
      "Epoch 101/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4352 - accuracy: 0.7953\n",
      "Saving weights for epoch 100\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4342 - accuracy: 0.7954 - val_loss: 0.4609 - val_accuracy: 0.7764\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4376 - accuracy: 0.7962 - val_loss: 0.4448 - val_accuracy: 0.7863\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4326 - accuracy: 0.7962 - val_loss: 0.4148 - val_accuracy: 0.8063\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4284 - accuracy: 0.7981 - val_loss: 0.4244 - val_accuracy: 0.7969\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4237 - accuracy: 0.8005 - val_loss: 0.4109 - val_accuracy: 0.8048\n",
      "Epoch 106/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.7974\n",
      "Saving weights for epoch 105\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4274 - accuracy: 0.7978 - val_loss: 0.4168 - val_accuracy: 0.8037\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4236 - accuracy: 0.7999 - val_loss: 0.4600 - val_accuracy: 0.7805\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4187 - accuracy: 0.8038 - val_loss: 0.4096 - val_accuracy: 0.8091\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4188 - accuracy: 0.8041 - val_loss: 0.3984 - val_accuracy: 0.8153\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4185 - accuracy: 0.8022 - val_loss: 0.4345 - val_accuracy: 0.7928\n",
      "Epoch 111/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8013\n",
      "Saving weights for epoch 110\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4211 - accuracy: 0.8018 - val_loss: 0.4257 - val_accuracy: 0.7979\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4186 - accuracy: 0.8035 - val_loss: 0.4251 - val_accuracy: 0.7992\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4170 - accuracy: 0.8064 - val_loss: 0.4338 - val_accuracy: 0.7923\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4182 - accuracy: 0.8054 - val_loss: 0.3942 - val_accuracy: 0.8171\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.4159 - accuracy: 0.8038 - val_loss: 0.4199 - val_accuracy: 0.8007\n",
      "Epoch 116/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.8064\n",
      "Saving weights for epoch 115\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4111 - accuracy: 0.8067 - val_loss: 0.4092 - val_accuracy: 0.8022\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4120 - accuracy: 0.8063 - val_loss: 0.4268 - val_accuracy: 0.7956\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4083 - accuracy: 0.8086 - val_loss: 0.4028 - val_accuracy: 0.8136\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4102 - accuracy: 0.8121 - val_loss: 0.4531 - val_accuracy: 0.7797\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4075 - accuracy: 0.8110 - val_loss: 0.3944 - val_accuracy: 0.8162\n",
      "Epoch 121/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8128\n",
      "Saving weights for epoch 120\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4068 - accuracy: 0.8130 - val_loss: 0.4351 - val_accuracy: 0.7934\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4122 - accuracy: 0.8089 - val_loss: 0.4083 - val_accuracy: 0.8089\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.4035 - accuracy: 0.8146 - val_loss: 0.3950 - val_accuracy: 0.8158\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4013 - accuracy: 0.8132 - val_loss: 0.3982 - val_accuracy: 0.8123\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4068 - accuracy: 0.8141 - val_loss: 0.3913 - val_accuracy: 0.8186\n",
      "Epoch 126/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4014 - accuracy: 0.8150\n",
      "Saving weights for epoch 125\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3996 - accuracy: 0.8156 - val_loss: 0.3789 - val_accuracy: 0.8259\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.4055 - accuracy: 0.8114 - val_loss: 0.3859 - val_accuracy: 0.8194\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3949 - accuracy: 0.8162 - val_loss: 0.4032 - val_accuracy: 0.8093\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3992 - accuracy: 0.8159 - val_loss: 0.4066 - val_accuracy: 0.8065\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3966 - accuracy: 0.8178 - val_loss: 0.4149 - val_accuracy: 0.8057\n",
      "Epoch 131/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4011 - accuracy: 0.8130\n",
      "Saving weights for epoch 130\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3995 - accuracy: 0.8132 - val_loss: 0.3987 - val_accuracy: 0.8117\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3888 - accuracy: 0.8223 - val_loss: 0.3995 - val_accuracy: 0.8126\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3948 - accuracy: 0.8195 - val_loss: 0.3842 - val_accuracy: 0.8216\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3857 - accuracy: 0.8263 - val_loss: 0.3821 - val_accuracy: 0.8242\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3887 - accuracy: 0.8208 - val_loss: 0.4113 - val_accuracy: 0.8074\n",
      "Epoch 136/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.8156\n",
      "Saving weights for epoch 135\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3943 - accuracy: 0.8162 - val_loss: 0.4044 - val_accuracy: 0.8070\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3913 - accuracy: 0.8231 - val_loss: 0.4633 - val_accuracy: 0.7805\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3953 - accuracy: 0.8210 - val_loss: 0.3942 - val_accuracy: 0.8126\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3862 - accuracy: 0.8227 - val_loss: 0.4062 - val_accuracy: 0.8138\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3896 - accuracy: 0.8202 - val_loss: 0.4109 - val_accuracy: 0.8072\n",
      "Epoch 141/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8219\n",
      "Saving weights for epoch 140\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3835 - accuracy: 0.8224 - val_loss: 0.3911 - val_accuracy: 0.8147\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3816 - accuracy: 0.8241 - val_loss: 0.3806 - val_accuracy: 0.8235\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3773 - accuracy: 0.8293 - val_loss: 0.3934 - val_accuracy: 0.8153\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3866 - accuracy: 0.8249 - val_loss: 0.4222 - val_accuracy: 0.8025\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3799 - accuracy: 0.8261 - val_loss: 0.4120 - val_accuracy: 0.8076\n",
      "Epoch 146/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8252\n",
      "Saving weights for epoch 145\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3788 - accuracy: 0.8255 - val_loss: 0.3921 - val_accuracy: 0.8141\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3800 - accuracy: 0.8263 - val_loss: 0.3858 - val_accuracy: 0.8199\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3786 - accuracy: 0.8257 - val_loss: 0.3732 - val_accuracy: 0.8233\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3724 - accuracy: 0.8309 - val_loss: 0.3759 - val_accuracy: 0.8287\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3749 - accuracy: 0.8279 - val_loss: 0.3767 - val_accuracy: 0.8302\n",
      "Epoch 151/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8277\n",
      "Saving weights for epoch 150\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3738 - accuracy: 0.8279 - val_loss: 0.4109 - val_accuracy: 0.8076\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3717 - accuracy: 0.8310 - val_loss: 0.3826 - val_accuracy: 0.8188\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3720 - accuracy: 0.8314 - val_loss: 0.3931 - val_accuracy: 0.8138\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3735 - accuracy: 0.8305 - val_loss: 0.3737 - val_accuracy: 0.8250\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3675 - accuracy: 0.8330 - val_loss: 0.3941 - val_accuracy: 0.8153\n",
      "Epoch 156/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8287\n",
      "Saving weights for epoch 155\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3726 - accuracy: 0.8293 - val_loss: 0.3655 - val_accuracy: 0.8308\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3689 - accuracy: 0.8348 - val_loss: 0.3655 - val_accuracy: 0.8315\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3634 - accuracy: 0.8361 - val_loss: 0.3740 - val_accuracy: 0.8235\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3727 - accuracy: 0.8303 - val_loss: 0.3701 - val_accuracy: 0.8263\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3684 - accuracy: 0.8336 - val_loss: 0.3642 - val_accuracy: 0.8308\n",
      "Epoch 161/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8317\n",
      "Saving weights for epoch 160\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3676 - accuracy: 0.8320 - val_loss: 0.3827 - val_accuracy: 0.8207\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3637 - accuracy: 0.8347 - val_loss: 0.3765 - val_accuracy: 0.8237\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3637 - accuracy: 0.8371 - val_loss: 0.4154 - val_accuracy: 0.8052\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3617 - accuracy: 0.8371 - val_loss: 0.3979 - val_accuracy: 0.8115\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3648 - accuracy: 0.8374 - val_loss: 0.3744 - val_accuracy: 0.8244\n",
      "Epoch 166/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3603 - accuracy: 0.8358\n",
      "Saving weights for epoch 165\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3574 - accuracy: 0.8365 - val_loss: 0.3664 - val_accuracy: 0.8295\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3560 - accuracy: 0.8388 - val_loss: 0.3867 - val_accuracy: 0.8177\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3567 - accuracy: 0.8405 - val_loss: 0.4023 - val_accuracy: 0.8022\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3614 - accuracy: 0.8373 - val_loss: 0.3565 - val_accuracy: 0.8340\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3540 - accuracy: 0.8407 - val_loss: 0.3721 - val_accuracy: 0.8274\n",
      "Epoch 171/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3559 - accuracy: 0.8398\n",
      "Saving weights for epoch 170\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3536 - accuracy: 0.8405 - val_loss: 0.3664 - val_accuracy: 0.8265\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3549 - accuracy: 0.8387 - val_loss: 0.4184 - val_accuracy: 0.7982\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3558 - accuracy: 0.8385 - val_loss: 0.3658 - val_accuracy: 0.8319\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3532 - accuracy: 0.8405 - val_loss: 0.3730 - val_accuracy: 0.8282\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3549 - accuracy: 0.8390 - val_loss: 0.3833 - val_accuracy: 0.8173\n",
      "Epoch 176/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.8361\n",
      "Saving weights for epoch 175\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3543 - accuracy: 0.8369 - val_loss: 0.3788 - val_accuracy: 0.8244\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3461 - accuracy: 0.8456 - val_loss: 0.3772 - val_accuracy: 0.8242\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3563 - accuracy: 0.8393 - val_loss: 0.3690 - val_accuracy: 0.8293\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3495 - accuracy: 0.8442 - val_loss: 0.3647 - val_accuracy: 0.8310\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3460 - accuracy: 0.8451 - val_loss: 0.3988 - val_accuracy: 0.8115\n",
      "Epoch 181/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8430\n",
      "Saving weights for epoch 180\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3482 - accuracy: 0.8437 - val_loss: 0.3578 - val_accuracy: 0.8351\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3477 - accuracy: 0.8436 - val_loss: 0.3631 - val_accuracy: 0.8321\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3459 - accuracy: 0.8442 - val_loss: 0.3585 - val_accuracy: 0.8340\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3463 - accuracy: 0.8450 - val_loss: 0.3580 - val_accuracy: 0.8345\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3446 - accuracy: 0.8456 - val_loss: 0.3871 - val_accuracy: 0.8190\n",
      "Epoch 186/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/37 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.8442\n",
      "Saving weights for epoch 185\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3424 - accuracy: 0.8451 - val_loss: 0.3608 - val_accuracy: 0.8356\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3400 - accuracy: 0.8444 - val_loss: 0.3568 - val_accuracy: 0.8347\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3398 - accuracy: 0.8474 - val_loss: 0.3724 - val_accuracy: 0.8272\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3468 - accuracy: 0.8433 - val_loss: 0.3686 - val_accuracy: 0.8280\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3404 - accuracy: 0.8452 - val_loss: 0.3770 - val_accuracy: 0.8252\n",
      "Epoch 191/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8474\n",
      "Saving weights for epoch 190\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3364 - accuracy: 0.8481 - val_loss: 0.3539 - val_accuracy: 0.8420\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3443 - accuracy: 0.8464 - val_loss: 0.3766 - val_accuracy: 0.8239\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3318 - accuracy: 0.8533 - val_loss: 0.3920 - val_accuracy: 0.8149\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3289 - accuracy: 0.8537 - val_loss: 0.3716 - val_accuracy: 0.8293\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3257 - accuracy: 0.8541 - val_loss: 0.3699 - val_accuracy: 0.8306\n",
      "Epoch 196/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8498\n",
      "Saving weights for epoch 195\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.3365 - accuracy: 0.8495 - val_loss: 0.3716 - val_accuracy: 0.8298\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3256 - accuracy: 0.8545 - val_loss: 0.3831 - val_accuracy: 0.8192\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3331 - accuracy: 0.8504 - val_loss: 0.3580 - val_accuracy: 0.8345\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3374 - accuracy: 0.8489 - val_loss: 0.3809 - val_accuracy: 0.8220\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3281 - accuracy: 0.8556 - val_loss: 0.3557 - val_accuracy: 0.8334\n",
      "Epoch 201/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.8534\n",
      "Saving weights for epoch 200\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3320 - accuracy: 0.8539 - val_loss: 0.3861 - val_accuracy: 0.8222\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3264 - accuracy: 0.8550 - val_loss: 0.3711 - val_accuracy: 0.8308\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3265 - accuracy: 0.8550 - val_loss: 0.3831 - val_accuracy: 0.8192\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3226 - accuracy: 0.8569 - val_loss: 0.4046 - val_accuracy: 0.8123\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3262 - accuracy: 0.8567 - val_loss: 0.3607 - val_accuracy: 0.8334\n",
      "Epoch 206/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8492\n",
      "Saving weights for epoch 205\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3312 - accuracy: 0.8500 - val_loss: 0.3549 - val_accuracy: 0.8351\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3175 - accuracy: 0.8596 - val_loss: 0.3686 - val_accuracy: 0.8224\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3261 - accuracy: 0.8543 - val_loss: 0.3490 - val_accuracy: 0.8399\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3133 - accuracy: 0.8617 - val_loss: 0.3641 - val_accuracy: 0.8302\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3161 - accuracy: 0.8608 - val_loss: 0.3605 - val_accuracy: 0.8315\n",
      "Epoch 211/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8592\n",
      "Saving weights for epoch 210\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3161 - accuracy: 0.8598 - val_loss: 0.3592 - val_accuracy: 0.8315\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3191 - accuracy: 0.8588 - val_loss: 0.3567 - val_accuracy: 0.8366\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3197 - accuracy: 0.8563 - val_loss: 0.3965 - val_accuracy: 0.8128\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3092 - accuracy: 0.8654 - val_loss: 0.4194 - val_accuracy: 0.8048\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3151 - accuracy: 0.8600 - val_loss: 0.3497 - val_accuracy: 0.8349\n",
      "Epoch 216/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8590\n",
      "Saving weights for epoch 215\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3183 - accuracy: 0.8593 - val_loss: 0.4080 - val_accuracy: 0.8040\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3185 - accuracy: 0.8593 - val_loss: 0.4191 - val_accuracy: 0.8014\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3098 - accuracy: 0.8652 - val_loss: 0.3564 - val_accuracy: 0.8360\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.3136 - accuracy: 0.8619 - val_loss: 0.3520 - val_accuracy: 0.8351\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3134 - accuracy: 0.8609 - val_loss: 0.3532 - val_accuracy: 0.8317\n",
      "Epoch 221/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8631\n",
      "Saving weights for epoch 220\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3043 - accuracy: 0.8638 - val_loss: 0.3500 - val_accuracy: 0.8403\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3141 - accuracy: 0.8613 - val_loss: 0.3471 - val_accuracy: 0.8411\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3060 - accuracy: 0.8642 - val_loss: 0.3497 - val_accuracy: 0.8368\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.3063 - accuracy: 0.8624 - val_loss: 0.3594 - val_accuracy: 0.8325\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3078 - accuracy: 0.8620 - val_loss: 0.3648 - val_accuracy: 0.8280\n",
      "Epoch 226/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8625\n",
      "Saving weights for epoch 225\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3131 - accuracy: 0.8629 - val_loss: 0.3581 - val_accuracy: 0.8321\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3102 - accuracy: 0.8632 - val_loss: 0.3489 - val_accuracy: 0.8394\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2970 - accuracy: 0.8709 - val_loss: 0.3491 - val_accuracy: 0.8371\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.3049 - accuracy: 0.8656 - val_loss: 0.3528 - val_accuracy: 0.8347\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3062 - accuracy: 0.8656 - val_loss: 0.3480 - val_accuracy: 0.8377\n",
      "Epoch 231/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8679\n",
      "Saving weights for epoch 230\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2998 - accuracy: 0.8681 - val_loss: 0.3942 - val_accuracy: 0.8108\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3036 - accuracy: 0.8651 - val_loss: 0.3446 - val_accuracy: 0.8401\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2949 - accuracy: 0.8721 - val_loss: 0.3571 - val_accuracy: 0.8325\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3043 - accuracy: 0.8642 - val_loss: 0.3514 - val_accuracy: 0.8375\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2927 - accuracy: 0.8733 - val_loss: 0.3525 - val_accuracy: 0.8356\n",
      "Epoch 236/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8653\n",
      "Saving weights for epoch 235\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.3083 - accuracy: 0.8658 - val_loss: 0.3789 - val_accuracy: 0.8218\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2946 - accuracy: 0.8680 - val_loss: 0.3564 - val_accuracy: 0.8334\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.2983 - accuracy: 0.8697 - val_loss: 0.3760 - val_accuracy: 0.8244\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2947 - accuracy: 0.8709 - val_loss: 0.3557 - val_accuracy: 0.8353\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2971 - accuracy: 0.8700 - val_loss: 0.3501 - val_accuracy: 0.8373\n",
      "Epoch 241/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8743\n",
      "Saving weights for epoch 240\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.2916 - accuracy: 0.8747 - val_loss: 0.3505 - val_accuracy: 0.8373\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2937 - accuracy: 0.8721 - val_loss: 0.3569 - val_accuracy: 0.8360\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2947 - accuracy: 0.8685 - val_loss: 0.3481 - val_accuracy: 0.8420\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2888 - accuracy: 0.8730 - val_loss: 0.3519 - val_accuracy: 0.8368\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2916 - accuracy: 0.8718 - val_loss: 0.3536 - val_accuracy: 0.8401\n",
      "Epoch 246/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8757\n",
      "Saving weights for epoch 245\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2818 - accuracy: 0.8765 - val_loss: 0.3559 - val_accuracy: 0.8351\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2884 - accuracy: 0.8738 - val_loss: 0.4001 - val_accuracy: 0.8162\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2983 - accuracy: 0.8707 - val_loss: 0.3599 - val_accuracy: 0.8319\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2843 - accuracy: 0.8777 - val_loss: 0.3475 - val_accuracy: 0.8377\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2843 - accuracy: 0.8746 - val_loss: 0.3501 - val_accuracy: 0.8366\n",
      "Epoch 251/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8759\n",
      "Saving weights for epoch 250\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.2835 - accuracy: 0.8764 - val_loss: 0.3582 - val_accuracy: 0.8330\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2952 - accuracy: 0.8712 - val_loss: 0.3421 - val_accuracy: 0.8422\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2811 - accuracy: 0.8760 - val_loss: 0.3599 - val_accuracy: 0.8347\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2879 - accuracy: 0.8744 - val_loss: 0.3502 - val_accuracy: 0.8347\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2793 - accuracy: 0.8778 - val_loss: 0.3563 - val_accuracy: 0.8353\n",
      "Epoch 256/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8759\n",
      "Saving weights for epoch 255\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2821 - accuracy: 0.8767 - val_loss: 0.3711 - val_accuracy: 0.8276\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2868 - accuracy: 0.8747 - val_loss: 0.3531 - val_accuracy: 0.8360\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2740 - accuracy: 0.8814 - val_loss: 0.3427 - val_accuracy: 0.8409\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2826 - accuracy: 0.8779 - val_loss: 0.3478 - val_accuracy: 0.8429\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2758 - accuracy: 0.8804 - val_loss: 0.3574 - val_accuracy: 0.8364\n",
      "Epoch 261/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.8812\n",
      "Saving weights for epoch 260\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2681 - accuracy: 0.8819 - val_loss: 0.3468 - val_accuracy: 0.8394\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2707 - accuracy: 0.8809 - val_loss: 0.3528 - val_accuracy: 0.8388\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2756 - accuracy: 0.8797 - val_loss: 0.3489 - val_accuracy: 0.8373\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2825 - accuracy: 0.8759 - val_loss: 0.3496 - val_accuracy: 0.8366\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2766 - accuracy: 0.8788 - val_loss: 0.3564 - val_accuracy: 0.8366\n",
      "Epoch 266/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.8837\n",
      "Saving weights for epoch 265\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2663 - accuracy: 0.8846 - val_loss: 0.3514 - val_accuracy: 0.8401\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2716 - accuracy: 0.8850 - val_loss: 0.3496 - val_accuracy: 0.8396\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2673 - accuracy: 0.8833 - val_loss: 0.3665 - val_accuracy: 0.8304\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2663 - accuracy: 0.8840 - val_loss: 0.3694 - val_accuracy: 0.8319\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2702 - accuracy: 0.8816 - val_loss: 0.3526 - val_accuracy: 0.8360\n",
      "Epoch 271/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.8848\n",
      "Saving weights for epoch 270\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2644 - accuracy: 0.8857 - val_loss: 0.3407 - val_accuracy: 0.8444\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2638 - accuracy: 0.8865 - val_loss: 0.3497 - val_accuracy: 0.8386\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2565 - accuracy: 0.8901 - val_loss: 0.3514 - val_accuracy: 0.8358\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2536 - accuracy: 0.8920 - val_loss: 0.3543 - val_accuracy: 0.8345\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2640 - accuracy: 0.8854 - val_loss: 0.3437 - val_accuracy: 0.8431\n",
      "Epoch 276/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8876\n",
      "Saving weights for epoch 275\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2581 - accuracy: 0.8883 - val_loss: 0.3513 - val_accuracy: 0.8429\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2660 - accuracy: 0.8834 - val_loss: 0.3460 - val_accuracy: 0.8420\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2602 - accuracy: 0.8873 - val_loss: 0.3481 - val_accuracy: 0.8405\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2607 - accuracy: 0.8896 - val_loss: 0.3468 - val_accuracy: 0.8399\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2640 - accuracy: 0.8839 - val_loss: 0.3452 - val_accuracy: 0.8422\n",
      "Epoch 281/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8835\n",
      "Saving weights for epoch 280\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2672 - accuracy: 0.8843 - val_loss: 0.3486 - val_accuracy: 0.8407\n",
      "Epoch 282/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2675 - accuracy: 0.8824 - val_loss: 0.3805 - val_accuracy: 0.8209\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2583 - accuracy: 0.8864 - val_loss: 0.3672 - val_accuracy: 0.8285\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2533 - accuracy: 0.8868 - val_loss: 0.3456 - val_accuracy: 0.8457\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2540 - accuracy: 0.8908 - val_loss: 0.3546 - val_accuracy: 0.8392\n",
      "Epoch 286/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.8886\n",
      "Saving weights for epoch 285\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2556 - accuracy: 0.8889 - val_loss: 0.3566 - val_accuracy: 0.8338\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2496 - accuracy: 0.8940 - val_loss: 0.3552 - val_accuracy: 0.8392\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2428 - accuracy: 0.8954 - val_loss: 0.3565 - val_accuracy: 0.8379\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2699 - accuracy: 0.8831 - val_loss: 0.3480 - val_accuracy: 0.8411\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2517 - accuracy: 0.8894 - val_loss: 0.3706 - val_accuracy: 0.8274\n",
      "Epoch 291/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.8905\n",
      "Saving weights for epoch 290\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2552 - accuracy: 0.8909 - val_loss: 0.4034 - val_accuracy: 0.8156\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2416 - accuracy: 0.8983 - val_loss: 0.3564 - val_accuracy: 0.8399\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2473 - accuracy: 0.8944 - val_loss: 0.3510 - val_accuracy: 0.8461\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2427 - accuracy: 0.8952 - val_loss: 0.3538 - val_accuracy: 0.8383\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2475 - accuracy: 0.8931 - val_loss: 0.3490 - val_accuracy: 0.8429\n",
      "Epoch 296/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.8987\n",
      "Saving weights for epoch 295\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2451 - accuracy: 0.8991 - val_loss: 0.3596 - val_accuracy: 0.8336\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2461 - accuracy: 0.8909 - val_loss: 0.3867 - val_accuracy: 0.8231\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2441 - accuracy: 0.8954 - val_loss: 0.3707 - val_accuracy: 0.8334\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2424 - accuracy: 0.8962 - val_loss: 0.3533 - val_accuracy: 0.8411\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2300 - accuracy: 0.9012 - val_loss: 0.3720 - val_accuracy: 0.8298\n",
      "Epoch 301/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.8951\n",
      "Saving weights for epoch 300\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2435 - accuracy: 0.8957 - val_loss: 0.3511 - val_accuracy: 0.8439\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2437 - accuracy: 0.8946 - val_loss: 0.3768 - val_accuracy: 0.8278\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2419 - accuracy: 0.8984 - val_loss: 0.3480 - val_accuracy: 0.8435\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2380 - accuracy: 0.8984 - val_loss: 0.3545 - val_accuracy: 0.8407\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2356 - accuracy: 0.8996 - val_loss: 0.3518 - val_accuracy: 0.8414\n",
      "Epoch 306/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.8913\n",
      "Saving weights for epoch 305\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2542 - accuracy: 0.8914 - val_loss: 0.3690 - val_accuracy: 0.8343\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2410 - accuracy: 0.8983 - val_loss: 0.3783 - val_accuracy: 0.8293\n",
      "Epoch 308/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2381 - accuracy: 0.8990 - val_loss: 0.3511 - val_accuracy: 0.8437\n",
      "Epoch 309/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2358 - accuracy: 0.8969 - val_loss: 0.3522 - val_accuracy: 0.8411\n",
      "Epoch 310/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2363 - accuracy: 0.9016 - val_loss: 0.3891 - val_accuracy: 0.8282\n",
      "Epoch 311/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9007\n",
      "Saving weights for epoch 310\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2342 - accuracy: 0.9014 - val_loss: 0.3616 - val_accuracy: 0.8390\n",
      "Epoch 312/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2303 - accuracy: 0.9016 - val_loss: 0.3796 - val_accuracy: 0.8317\n",
      "Epoch 313/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2313 - accuracy: 0.9020 - val_loss: 0.3604 - val_accuracy: 0.8388\n",
      "Epoch 314/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2273 - accuracy: 0.9043 - val_loss: 0.3514 - val_accuracy: 0.8431\n",
      "Epoch 315/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2312 - accuracy: 0.9020 - val_loss: 0.3528 - val_accuracy: 0.8442\n",
      "Epoch 316/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2310 - accuracy: 0.9038\n",
      "Saving weights for epoch 315\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2282 - accuracy: 0.9044 - val_loss: 0.3714 - val_accuracy: 0.8336\n",
      "Epoch 317/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2334 - accuracy: 0.9016 - val_loss: 0.3629 - val_accuracy: 0.8386\n",
      "Epoch 318/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2201 - accuracy: 0.9077 - val_loss: 0.3608 - val_accuracy: 0.8424\n",
      "Epoch 319/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2304 - accuracy: 0.9028 - val_loss: 0.3818 - val_accuracy: 0.8250\n",
      "Epoch 320/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2287 - accuracy: 0.9023 - val_loss: 0.3495 - val_accuracy: 0.8416\n",
      "Epoch 321/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9032\n",
      "Saving weights for epoch 320\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2281 - accuracy: 0.9038 - val_loss: 0.3633 - val_accuracy: 0.8364\n",
      "Epoch 322/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2135 - accuracy: 0.9091 - val_loss: 0.3697 - val_accuracy: 0.8343\n",
      "Epoch 323/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2191 - accuracy: 0.9071 - val_loss: 0.3670 - val_accuracy: 0.8364\n",
      "Epoch 324/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2261 - accuracy: 0.9027 - val_loss: 0.3653 - val_accuracy: 0.8383\n",
      "Epoch 325/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2237 - accuracy: 0.9034 - val_loss: 0.3731 - val_accuracy: 0.8336\n",
      "Epoch 326/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2136 - accuracy: 0.9118\n",
      "Saving weights for epoch 325\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2106 - accuracy: 0.9124 - val_loss: 0.3687 - val_accuracy: 0.8377\n",
      "Epoch 327/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2189 - accuracy: 0.9093 - val_loss: 0.3688 - val_accuracy: 0.8330\n",
      "Epoch 328/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2234 - accuracy: 0.9049 - val_loss: 0.3803 - val_accuracy: 0.8302\n",
      "Epoch 329/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2154 - accuracy: 0.9092 - val_loss: 0.3553 - val_accuracy: 0.8403\n",
      "Epoch 330/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2178 - accuracy: 0.9073 - val_loss: 0.3621 - val_accuracy: 0.8405\n",
      "Epoch 331/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9034\n",
      "Saving weights for epoch 330\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2251 - accuracy: 0.9040 - val_loss: 0.3653 - val_accuracy: 0.8373\n",
      "Epoch 332/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2174 - accuracy: 0.9069 - val_loss: 0.3636 - val_accuracy: 0.8401\n",
      "Epoch 333/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2085 - accuracy: 0.9121 - val_loss: 0.3609 - val_accuracy: 0.8433\n",
      "Epoch 334/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2156 - accuracy: 0.9101 - val_loss: 0.3604 - val_accuracy: 0.8444\n",
      "Epoch 335/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2159 - accuracy: 0.9084 - val_loss: 0.3722 - val_accuracy: 0.8368\n",
      "Epoch 336/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.9106\n",
      "Saving weights for epoch 335\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2117 - accuracy: 0.9113 - val_loss: 0.3566 - val_accuracy: 0.8459\n",
      "Epoch 337/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2258 - accuracy: 0.9046 - val_loss: 0.3714 - val_accuracy: 0.8338\n",
      "Epoch 338/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2113 - accuracy: 0.9117 - val_loss: 0.3743 - val_accuracy: 0.8332\n",
      "Epoch 339/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.2136 - accuracy: 0.9128 - val_loss: 0.3638 - val_accuracy: 0.8414\n",
      "Epoch 340/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2184 - accuracy: 0.9063 - val_loss: 0.3585 - val_accuracy: 0.8373\n",
      "Epoch 341/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2061 - accuracy: 0.9138\n",
      "Saving weights for epoch 340\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2028 - accuracy: 0.9144 - val_loss: 0.3668 - val_accuracy: 0.8403\n",
      "Epoch 342/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2110 - accuracy: 0.9103 - val_loss: 0.3634 - val_accuracy: 0.8392\n",
      "Epoch 343/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2069 - accuracy: 0.9124 - val_loss: 0.3751 - val_accuracy: 0.8356\n",
      "Epoch 344/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2100 - accuracy: 0.9133 - val_loss: 0.3975 - val_accuracy: 0.8239\n",
      "Epoch 345/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2015 - accuracy: 0.9161 - val_loss: 0.3635 - val_accuracy: 0.8407\n",
      "Epoch 346/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9135\n",
      "Saving weights for epoch 345\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2061 - accuracy: 0.9140 - val_loss: 0.3752 - val_accuracy: 0.8401\n",
      "Epoch 347/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2072 - accuracy: 0.9104 - val_loss: 0.3604 - val_accuracy: 0.8429\n",
      "Epoch 348/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2056 - accuracy: 0.9118 - val_loss: 0.3723 - val_accuracy: 0.8401\n",
      "Epoch 349/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2087 - accuracy: 0.9138 - val_loss: 0.3687 - val_accuracy: 0.8392\n",
      "Epoch 350/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2068 - accuracy: 0.9142 - val_loss: 0.3573 - val_accuracy: 0.8472\n",
      "Epoch 351/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2005 - accuracy: 0.9163\n",
      "Saving weights for epoch 350\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1973 - accuracy: 0.9169 - val_loss: 0.3679 - val_accuracy: 0.8394\n",
      "Epoch 352/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2026 - accuracy: 0.9152 - val_loss: 0.3706 - val_accuracy: 0.8457\n",
      "Epoch 353/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2038 - accuracy: 0.9133 - val_loss: 0.3679 - val_accuracy: 0.8394\n",
      "Epoch 354/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1945 - accuracy: 0.9193 - val_loss: 0.3683 - val_accuracy: 0.8407\n",
      "Epoch 355/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2048 - accuracy: 0.9124 - val_loss: 0.3706 - val_accuracy: 0.8409\n",
      "Epoch 356/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9166\n",
      "Saving weights for epoch 355\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1987 - accuracy: 0.9168 - val_loss: 0.3762 - val_accuracy: 0.8386\n",
      "Epoch 357/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.1914 - accuracy: 0.9191 - val_loss: 0.3683 - val_accuracy: 0.8390\n",
      "Epoch 358/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.1954 - accuracy: 0.9182 - val_loss: 0.3705 - val_accuracy: 0.8422\n",
      "Epoch 359/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.2033 - accuracy: 0.9162 - val_loss: 0.3763 - val_accuracy: 0.8452\n",
      "Epoch 360/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1933 - accuracy: 0.9192 - val_loss: 0.3601 - val_accuracy: 0.8457\n",
      "Epoch 361/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2089 - accuracy: 0.9143\n",
      "Saving weights for epoch 360\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2051 - accuracy: 0.9149 - val_loss: 0.3693 - val_accuracy: 0.8396\n",
      "Epoch 362/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1864 - accuracy: 0.9231 - val_loss: 0.3863 - val_accuracy: 0.8358\n",
      "Epoch 363/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1912 - accuracy: 0.9198 - val_loss: 0.3923 - val_accuracy: 0.8334\n",
      "Epoch 364/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.1959 - accuracy: 0.9189 - val_loss: 0.3727 - val_accuracy: 0.8386\n",
      "Epoch 365/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.1878 - accuracy: 0.9217 - val_loss: 0.3857 - val_accuracy: 0.8371\n",
      "Epoch 366/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9129\n",
      "Saving weights for epoch 365\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.2031 - accuracy: 0.9136 - val_loss: 0.3840 - val_accuracy: 0.8330\n",
      "Epoch 367/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1881 - accuracy: 0.9214 - val_loss: 0.3750 - val_accuracy: 0.8414\n",
      "Epoch 368/1500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1862 - accuracy: 0.9206 - val_loss: 0.3691 - val_accuracy: 0.8424\n",
      "Epoch 369/1500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.1969 - accuracy: 0.9171 - val_loss: 0.3769 - val_accuracy: 0.8401\n",
      "Epoch 370/1500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.1831 - accuracy: 0.9240 - val_loss: 0.3777 - val_accuracy: 0.8431\n",
      "Epoch 371/1500\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.1959 - accuracy: 0.9187\n",
      "Saving weights for epoch 370\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1930 - accuracy: 0.9192 - val_loss: 0.3808 - val_accuracy: 0.8414\n",
      "Epoch 00371: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 128, 512]\n",
    "model_state_by_batch_size_trial_1 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    mc = ModelCheckpoint(\n",
    "        'pickled_objects/batch_size_{}_trial_1_weights.h5'.format(batch_size),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    model_state_by_batch_size_trial_1[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=1500,\n",
    "        extra_callbacks=[es, mc],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqoAAAKdCAYAAADr+kt/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUVf748feZSU9IQgoJECCBEDqEDhaKIAQEQWERG7iuRhd2baDY16/6c1FZLCzKWkFRBEEQEURAKQJKkaj0GkgCAUJIIYWUOb8/7iSGkAQCk7kpn9fzzGPm3HPv/dwz8+Bnzj3nXKW1RgghhBBCiOrGYnYAQgghhBBClEUSVSGEEEIIUS1JoiqEEEIIIaolSVSFEEIIIUS1JImqEEIIIYSoliRRFUIIIYQQ1ZIkqkIIIYQQolqSRFWIOk4p9bRS6pRSSiul+imlgpRSK5VS2UqpeLPjK6KUilRKbVZKnVdKrS2njlZKDXRyXGuVUi876Fjh9muIdMTxahqlVLxS6r5K1J+tlJpbxTFV+TmEEOWTRFWIWsyeROkyXmPt25sBLwOxQENgEzABaAx0BLo7IIaXy0ssK+lpIBuIAm51wPEuopS6z+TkPAHjczhiYgyVopRKVErd46DDdQc+q0T9h4GJDjq3EKIacjE7ACFElXsTeLVUWZr9vxGAAr7W9sfUKaWaA9u11gedF+JlaQ6s01ofNTuQqqK1LgSSzY7D0ZRS7lrr85eqp7U+XZnjaq3TrzwqIURNID2qQtR+WVrr5FKvXHsv2I/2OjZ7T+taYDwwzv5+NhjJq1LqG6XUOaXUcaXUf5VSXkUnUEp528uSlVI5SqlflVI97ed4Buhbojc3vKwglVItlVLf2/c/pZR6XSnlYt8WD/QFnrcf44UKrjdCKbVBKZWrlNqmlOpQ4hzXKKV+VEqlKaVOK6XmKaWC7Nv6Ae8DzUrE2s++rYVS6mulVIZSKl0ptVopVb/EOd2UUv9TSmXab1+PLS84Zfi3UirJHuNhpdQD9m0X3Pq3H6t0b3h8iWN1sfea59jr/l9Rm5Vzbm+l1AdKqbP2z3KRUiqkxPbZSqm59l7wVPtn/VgFx1uL0fv+cYnvT9FxPlNKTVVKpQAL7eVv2q83Wym1Syl1W6njFd/6L9EWI5VSW5RSWfZrbVo63lL7T1JKfWmvv0cpdUOpc0xSSp20f47/scc5u7xrLOOaQ5RSC+3td1Yp9aFSyrvE9tuVUnvtn22yUuq9EtseUUodUcbwlcRLfI+FEEiiKkRdNh8YY/+7of11K7AIWGB//7BSyg1YCRwAugIjMG7R/qfEsd4DBgLjgPbA/8P492U+Ro/u5hLnSCgdiFLKCnwNnAd6YE+WgSfsVboDW+znbAhMq+C6XgTeBrpg3EJfbD8+gA/wLtANGAI0Ad6xb9sETAISS8S6SSnlDnxvv57+QE/gK6DomAAPAHuBzsBsjMStQTnx/QW4A6PtWwF/A06WU7d7iViaAbuBDQBKqUBgFbAc6ADcYz/upHJbBt7ASPhHAH0wksxPS9W5GXAFegEvAP9RSnUs53i3AieAR/jz+1NkBOAJXAsUJbtngLEY35EZwKclf0iU4wVgCsb3wst+DRV5AvgGiMZoq7n27zDKGL/8b4wfTz0AN2D4JY5X2qcY35u+9n37FMWklGoIfAz8C+OzHQZst2/rDvwf8CDQEuPzr253LYSofrTW8pKXvGrpC1gL5AHnSr2a27cPNP4ZuGCfucDsEu/HAdtK1bkGI6m0YtyS10C3cmJ4GVh7iThjgBwgoETZg8DpEu9/Al64xHE0MLXEez8gCxhWTv1eQD5gtb+/D4gvVeevwCnAq4I2Xl7ivcslzjkJWA2oMraF268hsoxt7wG/FcUBPA8sLFXnDuBgOeetZ7/WoSXKWtvP187+fjawq9R++4B/VNDmicA9pcpmA4cAyyU+r++A50u8jwfuK9UWY0psvx1IKXWeuaX2f6fE+4b2Y7S3v/+yVH0rcKzk972MGIvPUaK92pb67ubbv2tdgXTAp4zjjLK3pUtFbSIvecnrwpf0qApR+72P0btU8nVRr2YFOgCd7Lc6zymlzmH05Llh9Mi1wxhesO0qYmwFHNBap5Yo2wwEKaUCKnmsLUV/aGMM4z778VFKhSmlPrXffs4E1mAklqEVHK89sEVrnV1BnT9KnLMASAHK61FdBLQF9iil3lBK9b3UBdmHBowCRpaIowNwc6nP5UMgXClV1r/tzTGu9ecSse7FGK/cqkS9naX2S67gWirym9baVuo6xitjOEaKPd4BGL2TFfmjxN/JQGCJHvLLqQ9/xt8Sew8nFI8JjrvE+UtqBWRqrXeXKNuM0a4tMH5I/A4ctg9LGFPUm4vx40QDh5RSs5RSNymlVCXOLUSdJImqELXfWa31wVKv/Ers7wOs58JEtxPG//RPYEzG0lcZoyP/h11RLLMxbqHHYtxWH20vd61gn8uJrXR7asr591VrHY/Rds9itO03SqkZ5Z5cqd4Yt5bHaq1LrgbgA3zBhZ9LB6B16QSxEtcBlbiWS7ggsVdKXY/xo+lT4EaMeFdTcduXjqfos63oWorra62L6hfFf7Xf1bLOW3w8+4+UfsBtGMM5XsMYPuJm/9HUEfg7xl2OjzCGuwghKiCJqhDiUn7DuOWZWE7CuxPwUUp1K2f/fC4cz1mWvUDLUr2nvTFu/aeWs095ehT9oZTyxVjOap+9qBcwXWu92t6bGHQZsf4BdFclJo9dLa11ltZ6odb6fozhBn8rq55SKhRjItLzWutVpTb/hnELuvRnUt64x0NAAUYbFB2/NeCP0f5X6nI+XzDG9u7WWr+ltd4BHMbohXSm/Ri354HisdHRldh/L1BPKdW2RNk1GO16CIxeWq31j1rronG1XYvOobXO01ov11o/hDG+dXgFY5mFEEiiKkRd4K2UCi318r70bsU+w+gBmq+U6q6MhfeHK6WmAWitDwOfY0xauVEZKwSMVEoVJURHgVZKqdbKeJhAWf/ufI8x8Wm2Uqq9UmoIxsSTN6/gescrpUYrpdpg9OCdxBgLCUYycbcyVhiIwVibtaSjQIhSqps9Vlf7tZ2zX39XpVSUUuoBZV8toLLst7/vUUq1UUpFASP5M5EubSFGojy3xGcXbN82E2ihlHpfKdVJKdXKfqv52bIOpLXOxOjFe1Mpdb1SqgtGD/OqUreyK+so0Mcem18F9Q5hfA+GKaVaYUymqmjIRVX4HzBGKXWvPYbpGIn6ZfWy2n/cfA98ZP8uXIsxce9jrXW6Mla6mKKM1RiaYYzvPg8ctV/3RKVUB2UsAXcbxhCRM46/TCFqD0lUhaj9HsG4RV/y9c/L3dme4PTDSFZXYfTkvWw/TpFYjKWu5mH0sD4HFN1+XogxbnQrcBpoSin2W9VFs8S3AnOATzBunVbWCxizzOMwbrHfar8lC0bvZSRG8vcSxu33ktZj3E5fbY/1Wm2s/zkY49/L9fb4bsXoRbsS6RgPVdhifwVgzIQvy7X2c5f87LYCaK0TMGacNwE22ssnY0wOKs8kjJnw39ivJQm4+wqvo8gLGL2lCVR8K3sJf9763wRk2uNwGq31aowfJ1Mx2r4A47O+5BqvJYzDaLd1wLcY7fmofVsGxrjb74E9GJO/btVan8QYC3ybvf7vGL2tw+zjZIUQ5VB/DuERQggh6g77ZKZ9wPta69fNjkcIcTF5MpUQQog6Qyk1GViBcbt/AkYP/5emBiWEKJfc+hdCCFGX9MEY9rAF48EPN9pXYhBCVENy618IIYQQQlRL0qMqhBBCCCGqJUlUhRBCCCFEtVSrJ1O5u7vr4ODgS1d0kPPnz+Pu7u6089V00l6VI+1VedJmlSPtVTnSXpUj7VU5dam9kpKS8rTWZV5srU5Ug4ODSUxMdNr5Vq5cyeDBg512vppO2qtypL0qT9qscqS9Kkfaq3KkvSqnLrWXUup0edvk1r8QQgghhKiWJFEVQgghhBDVUq2+9S+EEEIIUZ3ZbDbKWyq0sLB2PGFXKYXFcmV9o5KoCiGEEEI4mc1m4+jRo+Tm5pa5PTg4mP379zs5qqrj4eFBs2bNKp2wmpqoKqVaAnOAICANuEdrvbtUnSeBsSWKmgMfaK0fc1qgQgghhBAOdOrUKSwWCy1btkQpddH2jIwMfH19TYjM8bTWJCUlcerUKUJDQyu1r9k9qv8D3tNaz1ZKjQY+BHqXrKC1ngpMBVBKuQHHgc+cHagQQgghhCNorUlLSyM8PBwXl7JTMYvFgtVqdXJkVSckJIT4+HhCQkLKTMzLY9pkKqVUA6ALMNdetAiIUEqFV7DbSCBRa729aqMTQgghhKgaWmu01ri6upoditO4uroWX3dlqMru4ChKqa7Ap1rrtiXKtgCTtdbry9lnJbBMaz2jnO2PAcVDAry9vRsvWrTIsYFXIDc3Fw8PD6edr6aT9qocaa/KkzarHGmvypH2qhxprwsFBwcTERFR7phNrXWleh6rO5vNxpEjRzh9+uIlU2NiYpK01mFl7Wf2rf/SWXK5n4hSqglwHXB7uQfTejowveh9WFiYduZiuXVpcV5HkPaqHGmvypM2qxxpr8qR9qocaa8/FRYWsn//fnx9fcu9vZ+eno6fn5+TI6s6hYWFeHp6MnDgwEoNaTBzHdUEIEwp5QKgjJ8NTYBj5dT/K7BUa53qpPgqZd7eeazPWl/pLm0hhBBCiOpg0KBBdOzYkejoaK6//nri4uLIzc1l5MiRREVFER0dTUxMDPHx8U6LybREVWt9CtgB3GUvGgXEa63jS9e1J7H3YEy2qnbyC/NZsG8BSzOXMn7pU/x08CT5hTazwxJCCCGEuGwLFizg999/Jy4ujkmTJnHvvfcCEBsby759+4iLi2PYsGHExsY6LSazb/0/AMxWSj0NZADjAZRSy4Hntdbb7PVuwBgWsMaUKC/B1eLC7JbjuH3jm+xI+5b7vk2mjet9LJ5w3QX11ieuZ1fKLv4e/XeTIhVCCCFEdXPfnK0cPZN9QVmhzYb1ChfJL61ZoBcfjO9+yXr+/v7Ff6enp2OxWPDw8GDo0KHF5b169eLNN990SFyXw9REVWu9j1LLUdnLh5Z6vwaIcFZclaY1Lt+9xGfpSQxv2pd0/+3sTfXGZrumeJD0mZwzPLnhSTLzMunXpB9tAtuYHLQQQgghxIXGjRvHjz/+CMB333130fa3336b4cOHOy0es3tUa4V8DdNyhvMv9SaLQlsxOrWQtID1zPrtQyZ0vh+AN7a/QWZeJgBLDi6RRFUIIYQQAGX2dpo1meqTTz4BYM6cOTz++OMsX768eNsrr7zCgQMHmDVrltPiMXMyVa3harVwy93/JNU1lJA/5nCT78MU5oby7u9vM2H1BP6++u98fehrejfsTXO/5nx75FvyCvPMDlsIIYQQokzjx4/nxx9/5MyZMwBMmzaNr776ihUrVuDl5eW0OCRRdZCOTYNIanoL5KYzJHUROQn30imgN5uOb2Jj0kYGNRvEy9e9zMjIkaSfT+fL/V+Sb8tny4ktxT2tQgghhBBmyMjI4Pjx48XvFy9eTGBgIAEBAUyfPp158+axatWqC8axOoPc+negE8F96JC1kQ7H5hJZ2Ja7m79IzxYe5NvyCfYKBmB4i+F8vPNjpm6ZyowdM8jKz6JHaA/eH/Q+FiW/G4QQQgjhfOnp6YwaNYqcnBwsFgvBwcEsW7aMpKQkJk2aRPPmzenfvz8A7u7u/PLLL06JSxJVB9LKCsPeRH0wgJdcP+bwuSH4e4ReUCfIM4ivRnzF27++zd7UvXi5erEleQsL9i1gbOuxJkUuhBBCiLqsSZMmbNmypcxtZq4RL4mqo4V1JS3qL/Tav4CE41uBphdVCfIM4sVrXwQgKz+LW7++lde3vs6xzGPc1+E+AjwCnBy0EEIIIUT1I/eaq0Bhz4kAtD0695J1vV29md5vOuF+4Xy6+1P+svQvxJ2Kq+oQhRBCCCGqPUlUq4Bvs46sL+xAm7S1kFbeE2H/1C6oHV8O/5KXr32ZzPxM7vnuHt7+9W3OF56v+mCFEEIIIaopSVSrgJuLhXnW4ViwwfbZl7WPRVkYETmCz4Z+RlT9KN7/431iv48lOz/70jsLIYQQQtRCkqhWkX3e3TmpgiFuHtgKL3u/lvVb8tlNn3FXm7v49dSvTFwzkYy8jCqMVAghhBCiepJEtYoE+HiwlL6QeRwO/1ipfV0trjzR/QnubHMn205uY+yysew/u7+KIhVCCCGEqJ4kUa0igT5ufJZ7rfFmx2eV3l8pxZTuU3im5zOcyDrBY2sfI9+W7+AohRBCCCEMDz30EOHh4Sil2LlzJwC5ubmMHDmSqKgooqOjiYmJIT4+vnifbdu20bt3bzp37kybNm147bXXHBqTJKpVJNDHnXgdQn5YL9i3HPJzKn0MpRRjW48ltmMsRzOOsnD/QhYfWMzW5K1VELEQQggh6rLRo0fz008/0axZswvKY2Nj2bdvH3FxcQwbNozY2Njibffffz9PPfUUO3bsYOPGjUybNo3du3c7LCZZR7WKBHm7AZAWNoDgxJ/h2GZoccMVHWt82/F8sfcLXvnlFQACPAJYNXoVblY3h8UrhBBCCJN8PhbOHrmgyKewEKxWxxy/fgTc8cUlq/Xp0+eiMg8PD4YOHVr8vlevXrz55psX1ElLSwMgKysLNzc3AgIctx689KhWkUAfdwCOB15jFBz64YqP5eXqxaNdH8XF4kJ0cDSpuamsjF/piDCFEEIIIS7b22+/zfDhw4vff/zxxzz33HM0bdqUqKgo/v3vfxMaGlrBESpHelSrSKCP0duZ4BZOJ+8GcKhyE6pKGxk5kmHNh5GVn8WNC29k7p65DGs+DKWUI8IVQgghhFnK6O08l56On5+fCcGU75VXXuHAgQPMmjWruOz111/n9ddfZ8yYMRw+fJh+/frRo0cPWrVq5ZBzSo9qFQn0NnpUT5/Lh+b94OROyEy+qmO6WFzwc/djePPh7D6zm7d+fYsNiRuY9dsssvKzrj5oIYQQQogyTJs2ja+++ooVK1bg5eUFQEpKCosXL2bMmDEANG/enJ49e7Jp0yaHnVcS1SrSJMATgKNnsv8cm3p4rUOOHdsxltYBrflw54dMWDOBmXEzmb9vvkOOLYQQQghR0vTp05k3bx6rVq3C39+/uLx+/fp4eHiwbt06wEhcf/75Z9q3b++wc0uiWkUa+Xni5mIh/kwWhF9nFCZsccixQ7xD+Pymz3mu13M83OVh6rnVY/nh5Q45thBCCCHqpokTJxIWFkZiYiIDBw4kMjKSxMREJk2aRFpaGv379yc6OpqePXsCYLVaWbBgAY899hidOnWiT58+TJ48me7duzssJhmjWkUsFkWzAC/iU7LALwy8AuFEnMOO72pxZUwro6s9MTORRQcWcfDsQSLrRzrsHEIIIYSoO2bOnMnMmTMvKtdal7vPwIED2b59e5XFJD2qVSg8yJuEsznk2zQ07ATJO6HQ8Yv239T8JgCWH5FeVSGEEELUHpKoVqGIIG8KbZrEsznQMBoKz8PpfQ4/T9eQrjTwasDSQ0vJL8xn+eHlzNs7r8J9TmWfkgcHCCGEEKJak0S1CoUHegMYt/8bdjIKT/zm8PNYlIU7Wt/ByeyTzNgxg2c3PsvULVNJzEwsd59p26Zx78p7OXj2oMPjEUIIIYRwBElUq1B4kLF8w+GULGgUbRQ6cJxqSbe3vh1/d38+3vUx+bZ8bNrG/H3zOZV9ivj0+Ivq/376dwDm7J5TJfEIIYQQQlwtSVSrUPMgH8Deo+rfDDz8q6RHFYynV41rOw6Am1vcTIRfBAv3L2TEkhHcteIuCmwFxXXP5p4l6VwSAMsOL+Nk1skqiUkIIYQQ4mpIolqFQnzd8XS1GktUKWWfUPUH2Aqr5Hzj2o3jyR5P8mSPJ7m99e2cyz/HufxzpJ9P53D64eJ6u8/sBqB3w94U2Ar46sBXAGxN3kpKTkqVxCaEEEIIUVmSqFYhpRTNAr04kmJ/alTDTpCfDSkHquR87lZ37mxzJ/Xc6jGq5SimdJ/CpK6TANiZsrO4XtHf93e8H4XiQNoB0nLTuP/7+3n717erJDYhhBBCiMqSRLWKtWjgQ1JaDufOF5QYp1o1t/9LcrO6cVfbu4iJiAEuTFR3ndmFVVnpENSBBl4NSDqXRHxGPIW6kINpMrlKCCGEqIvCw8Np3bo10dHRREdHM3++8dTLhx56iPDwcJRS7Nz5Zz6Rm5vLyJEjiYqKIjo6mpiYGOLj4x0akySqVax9Iz+0ht3HM4wlqqDKJlSVJcQrhCDPoAsT1ZRdRPpH4uHiQVi9MBIzE0nITAAgPiO+woV9hRBCCFF7LVy4kLi4OOLi4rjtttsAGD16ND/99BPNmjW7qH5sbCz79u0jLi6OYcOGERsb69B45MlUVaxDYz8A/khKp0ezCHD3dUqPahGlFO2D2vNT4k/kFuSSfj6dUzmnuD7segAa+zRm+8ntxeNWM/MySc1NJdAz0GkxCiGEEHXZP9f8s7jDqEihrRCrxeqQ4zep14QZA2Zc8f59+vQps9zDw4OhQ4cWv+/VqxdvvvnmFZ+nLNKjWsXaN/YFYFdSOlgsENoRTvwONpvzYghsT4EuYG/qXtYlrgOge6jxHN6wemEA/Hzi5+L6RzOOOi02IYQQQlQfd955Jx06dOC+++7j9OnTldr37bffZvjw4Q6NR3pUq5i/lxth9T35IyndKGgUDUd/gtTDEBTplBg6BHUAjFn9W5O34mpxpW9YXwDCfIxEteTY1PiMeLqEdHFKbEIIIURdV1ZvZ3p6On5+fk6NY/369TRt2pT8/HyeffZZxo8fz/Lll/d49ldeeYUDBw4wa9Ysh8YkiaoTdGjsx8pdyWTnFeBV/ISqOKclql1DuxLqHcqc3XPIysuid6Pe+LgZa7wW9agC1Hevz9nzZ4nPiHdKXEIIIYSoPpo2bQqAq6srjzzyCFFRUZe137Rp0/jqq69YvXo1Xl5eDo1Jbv07QfvGftg07DmR8eejVJN/d9r53a3uTOg0gfTz6RToAm5sdmPxtsY+jYv/7tWwF1ZlLfNJVkWeWP8EM+NmVmW4QgghhHCyrKws0tLSit/PmzePzp07X3K/6dOnM2/ePFatWoW/v7/D45JE1QnaF02oSkyH+hGgLMatfye6ucXNRPpH4qJc6NekX3F5kGcQ7lZ3ACL8IwirF1buGNUCWwHfx3/PyviVzghZCCGEEE5y8uRJ+vfvT8eOHenQoQPr1q3jk08+AWDixImEhYWRmJjIwIEDiYw07ggnJiYyadIk0tLS6N+/P9HR0fTs2dOhcZl6618p1RKYAwQBacA9WuvdZdTrC0wDvAAr8Fet9WZnxno12jcyJlTtPJ4BLhHg1wRSjzg1BqvFyowbZpCclUx9j/rF5RZloZFPI46kH6FJvSaE+4az8fhGCmwFuFgu/Hqk5KRQqAtJykzCpm1YlPzOEUIIIWqD5s2bs2PHjjK3zZw5k5kzL76bGhYWVuVLWpqdafwPeE9rHQW8BnxYuoJSqhFGMjtOa90OiAb2ODXKqxTo404jPw92Fk2oCmhu9Kg6eb3SsHphdAvtdnG5fUJVUaJaYCvgWMaxi+qdzD4JQJ4tj1PZp6o2WCGEEELUeaYlqkqpBkAXYK69aBEQoZQKL1V1AjBXa70HQGudq7VOo4Zp39iPA6fOkZtfaCSq+dmQmWx2WAC0DWyLl4sXEX4RdA4xxqNsSd5yUb3krD/jLb3emxBCCCGEoymznkKklOoKfKq1bluibAswWWu9vkTZV8ARoBPGEIENwBStdXYZx3wMeKzovbe3d+NFixZV3UWUkpubi4eHR5nbvo23sfSIjSe7WOmX9S2tj37ClnYvcNa3bZn1nalAF5Bty8bX6kuOLYfnTz1Pe/f2jK8//oJ667LW8U3mNwCM8R1DD68eV3XeitpLXEzaq/KkzSpH2qtypL0qR9rrQsHBwYSHh2O1lr2ov9YapZSTo6o6hYWFxMfHl7k2a0xMTJLWOqyM3Uxfnqp0llzWJ+IK9AMGApnAR8ALwBMXHUzr6cD0ovdhYWF68ODBDgr10lauXEl553Pbe4qlR7bi3aQ1rQM0HP2EHi2CoIvz4rtcXyz7gmPnjnHjoBsvGIf629bfwD6C2K+ZH4OvMvaK2ktcTNqr8qTNKkfaq3KkvSpH2utCBw8eJD8/H19f3zIT0oyMDOrVq2dCZI6ntebMmTP4+vrSpUvl1mk3M1FNAMKUUi5a6wJlfEpNgNKDI48CO7TWZwGUUl9QRpJa3RXN/N+ZlAGRzY1CJ8/8v1w9G/Zk185d7EvdR5vANsXlJW/9J55LNCM0IYQQolZo2rQpx44dIzU1tcztOTk5eHp6OjmqquPq6lq8TmtlmJaoaq1PKaV2AHcBs4FRQLzWOr5U1c+BV5VS7lrr80AM8JszY3WE4HruhPi6G0+o8u8OqOqbqIb25KOdH/H0T0/j7erNW/3fItAzkJPZJwn0CMSiLCRmSqIqhBBCXCk3NzciIyOx2WxlzpxfvXo1AwcONCEyx1NKYbFc2bQos2/9PwDMVko9DWQA4wGUUsuB57XW27TWm5RS3wBxSqkCYCfwoGkRX4UOjf1Yu+8055Ur7n5h1TZR7RzSmXpu9Yofqzpn9xwe6/oYyVnJhHqH4m5153B69YxdCCGEqEkqSuDKG79al5i6PJXWep/WurfWOkpr3U1rvctePlRrva1Evde01m201h201rdrrdPNi/rKtW/sR4FNsy85EwIijLVUTZrMVhFPF0+WjlzK+tvWE+kfyfy98zmTc4aUnBRCvUMJqxdG2vk0MvMyzQ5VCCGEELWY2euo1intG9mfUJWUDgEtIC8Tzp00OaqyBXkGUd+jPrEdY8kuyGb69unYtI0QrxDC6hkT8+T2vxBCCCGqkiSqTtQhrMSEquDWRuHpvSZGdGmDmg2ihV8Llh5aCkCodyhR9aMA2Jq81czQhBBCCFHLSaLqRA3quRPk4248oSq4lVF4ep+5QV2C1WLlud7PFb8P8Qrh2kbX4u3qzYojK0yMTAghhBC1nSSqTqSUokNjXxAAYFwAACAASURBVPYlZ5IXUJSoVu8eVYCuIV0Z1XIUAM38muHh4sGApgPYeWZnmY9aFUIIIYRwBElUnaxDYz/yCm3sz/ICD79q36Na5Jmez/DR4I9oF9gOgCERQwBYfmQ5AFn5WZzLO2dafEIIIYSofSRRdbJ2RQv/H7ePUz21p1rO/C/N1epK99Duxe97NuxJgEcASw8tpcBWwLgV47h7xd3YtM3EKIUQQghRm0ii6mQd7ROq4hLSjEQ1JxWyUkyOqvJcLa6MajmKhMwEXtz8IvvP7udg2kHWJawzOzQhhBBC1BKSqDpZQz9PGvt7sjU+tcbM/C/P2NZjcbG4sPjgYlwsLrgoFz7d86nZYQkhhBCilpBE1QQ9IgI4dDqLjHrNjYIamqg28GpATHgMAMObD2dwxGC2Jm9lb2rNvB4hhBBCVC+SqJqgW3h9ALbnNjIKjm4yMZqr82CnB+kT1ocHOj3A3W3vBuDT3dKrKoQQQoirJ4mqCXqEBwCwMdkFIvrAnqWQcdzkqK5MM99mzBwwk8Y+jWkX2I4uDbqw/MhyTmefNjs0IYQQQtRwkqiaoEWwD/5ermw9ehZ6TQBbAWz90OywHGJc23EU2AqYu2cuugasZiCEEEKI6ksSVRNYLIpuzQLYlZROdvgAqB8B2z6CgvNmh3bV+jXpR5N6Tfho50eM+HoEn+/5nOSsZE6cO2F2aEIIIYSoYSRRNUmPiPoU2DQ7EjIg+g5jmarjcWaHddWsFiv/G/g/bm99O2dzz/LvLf/mxoU3MmjRIF7Y9IL0sgohhBDisrmYHUBd1c0+TnVrfCrXtuhlFCb8Ak17mhiVYzTxbcLTPZ/m0a6P8s2hb9h/dj8Hzh5g0YFFuFvdmdJjChYlv5GEEEIIUTFJVE3SvpEfHq4WYz3V67uAshqJai3i6eLJmFZjAMgpyOHvq//O53s/JyEzgal9ppocnRBCCCGqO+nWMombi4XOTerz69E08l28IKQdJGypEY9TvRKeLp7MGjiLES1GsCFpA0+sf0IetyqEEEKICkmiaqLu4fXJyS9k9/EMaNITsk7B2Xizw6oyHi4evHTtS4yMHMnGpI2sOreK/MJ8s8MSQgghRDUliaqJukcY41S3HEk1ElWAxK0mRlT1lFI81eMpWvi1YFXWKq774joW7l9odlhCCCGEqIYkUTVRl6b18XazsmBbArbG3YzCoxvNDcoJvFy9+GDwB8T4xODv7s9LP7/EuoR1AGTkZbAzZafJEQohhBCiOpBE1UTe7i7c1bsZB06d4/vjHhDYEvYuB1uh2aFVuSDPIAb6DOSDQR/g6+bLo2sf5ckNTzJ88XBu//Z25uyaY3aIQgghhDCZJKomu++65ri7WJi59hC67QhjnOqxn80Oy2ma+DZh1o2zaBPQhm8PfwtAuG8407ZNY/7e+SZHJ4QQQggzSaJqsuB67ozuGsYfSenEh9xoFO5eYm5QTtYusB1zh85lyYglLLtlGbNjZtO0XlOmbp3K7jO7zQ5PCCGEECaRRLUaGNg2BIAfzjaAgBaweynY6tbSTUopWvi3oJ5bPQI9A3m97+sAPLH+CTLyMkyOTgghhBBmkES1GugRHoCrVbHx0BloezOcS4YTNf9xqlejbWBbJnebzNGMo0xcPZFjGcc4ce6E2WEJIYQQwokkUa0GvN1d6Ny0Pj8fPkNBs+uNwmObzQ2qGrij9R38td1fiTsdx02Lb2LQokFM3TKVvMI8s0MTQgghhBNIolpNXBcZRHZeIb8RZTxO9egms0MynVKKR7s+ytM9n+butnfTuUFnPtvzGfeuvJe03DQAtNbsOrMLXUuf6CWEEELUZZKoVhPXRgYBsC4+BxpFG4lqHRunWhalFLe3vp0nuj/BR4M/4p529/Db6d+4e8XdpOamMm/vPMYuG8uGpA1mhyqEEEIIB5NEtZroFOaHv5cr3+9KhmbXQE4qpOwzO6xqxcXiwqRuk5jcbTLxGfH8Z9t/+GjnRwD8fKLuLOklhBBC1BWSqFYTLlYLQ9qHsjc5k+O+nY3COvCUqisxru04eoT2YOmhpZzMPglA3Km6PflMCCGEqI0kUa1GhndsBMCS1KaAgsPrzA2omlJKMaXHFCzKQj3XenQL6caeM3vIKciRiVZCCCFELSKJajXSs3kgQT7uLNydhW7aCw6uhrwss8OqlqLqR/Fqn1d5re9rXNf4Ogp0AW//+jY9P+/Jq1te5Uj6ERbsW0B2frbZoQohhBDiCrmYHYD4k9WiuKlDKHM2H+Vkp6GEHtsM+1dC+1vNDq1aigmPAcDLxQuAuXvmFv+36O/TOaeZGD3RnACFEEIIcVWkR7WaKXpK1Xe27oCCXYvNDagGaBfUDleLKwDP936ee9vfy9CIoQR4BLDiyApZukoIIYSooaRHtZrpERGAl5uVb49o7ml2LRz4HrLOgHeg2aFVW+5Wd0ZEjiArL4vRLUejlAJg6papfLbnM3af2U27oHYmRymEEEKIyjK1R1Up1VIptUkptV8ptUUp1baMOvcopdKUUnH2149mxOos7i5Wro0MYvvRs2R3uAsKcmHuLZCdanZo1dq/ev+L1/q+VpykAgyJGALASz+/xIAvB7AyfqVZ4QkhhBDiCph96/9/wHta6yjgNeDDcuqt1lpH21/9nReeOW5o3QCbhjWufaH/M3DiN1gyweywapyOQR1p7NOYXWd2cSr7FK/88grp59PNDksIIYQQl8m0RFUp1QDoAsy1Fy0CIpRS4WbFVF30axUMwKrdJ6HvE9DmZti/Ak78bnJkNYtSilf7vMpL177E/13zf6TmpvLG9jdkzKoQQghRQyiz/qetlOoKfKq1bluibAswWWu9vkTZPcDrQBKQBbyhtV5YzjEfAx4reu/t7d140aJFVXMBZcjNzcXDw8Mhx3r91wLiM+G1a6yE5sVzze9TOBF4Db9HPeKQ41cHjmyvS9Fa827quxzOP0y0RzSNXRrjbnGnp2dPrMrqlBiuljPbq7aQNqscaa/KkfaqHGmvyqlL7RUTE5OktQ4ra5vZk6lKZ8mqjDrLgAVa62ylVBvge6VUotb6omdmaq2nA9OL3oeFhenBgwc7NOCKrFy5EkedL7tBIo/O/400/1bcel0MZH1Pw8M/0rB7FAREOOQcZnNke12O3ud78/zG5/kh4QfiMJ5kddTzKNP6TCPIM8hpcVwpZ7dXbSBtVjnSXpUj7VU50l6VI+1lMHOMagIQppRyAVDGLJgmwLGSlbTWKVrrbPvfe4DlwLVOjtXphrRviJ+nK/O2HDNuVfeeANoGv80zO7Qay8/djzf7v8lHgz/i0yGfcmebO9l+cjv3rbyPs7lnzQ5PCCGEEKWYlqhqrU8BO4C77EWjgHitdXzJekqpxiX+DgFusO9Xq3m4Wrmlc2MOnDrHH0np0Lw/1GtoJKo2m9nh1VhKKbqHdie6QTRP9niSx7s9zqH0Qzyw6gEy8zLNDk8IIYQQJZg96/8B4AGl1H7gSeBvAEqp5UqpbvY6E5VSu5RSccAqjDGqP5gTrnMNbhcKwPr9p8FihY5jIO0YHNtscmS1x7h245gQPYE9qXuYsHoC2fnZHD93nH+s+QdfH/za7PCEEEKIOs3URFVrvU9r3VtrHaW17qa13mUvH6q13mb/+2mtdTv70lQdtdbvmBmzM3VtVh8vNyvr96cYBZ1uN/67rbxVvMSVeLDjg9zT7h7iTscxcOFAxiwbw7rEdby69VUy8jIq3HfHqR10n9udA2cPOClaIYQQou4wu0dVVMDNxULv5oH8euwsmbn50KANtLgBdi6CzXUmX69ySike6/oYz/Z8lobeDbHZbNza8lYy8zL5747/8vLPL/N9/Pdl7rv5+GZyC3P5I+UPJ0cthBBC1H5mz/oXl9AnKpg1e0+x+dAZBrULhVEfwkeDYeXTENoeIvqYHWKtoJTitta3cVvr2yi0FaLRbE3eyry9xuS15UeW07NhT/zc/S7Y71DaIQBOZJ1wesxCCCFEbSc9qtVcnyhj8f+1+08bBV4BcMd8cHGH5Y9DYb6J0dVOVosVF4sLT/V4ii4NunBnmzvJzMvk450fX1T3cPphAJKzkp0dphBCCFHrSaJazYUHetEqpB5f70giNSvPKAxoDtc9Cqf3wobpIE9aqhLXh13PnCFzeLzb40T6R/LZns9IyUkp3l5gKyA+Ix6QRFUIIYSoCpKoVnNKKf5xQyRZeYV8sOHwnxuufRgCWsDaV+CTEZCdal6QtZzVYmVi9ERyC3P5fM/nxeUJmQkU2AoASVSFEEKIqiCJag0wtENDIhv4MGdTPCnnzhuFrp5w70rodAccWQebZ5obZC3Xv0l/mvk244t9X5Cdnw38edsf4GT2Scx6HLEQQghRW0miWgNYLYrJg6LIyivk0flx2Gz2hMgnGEbMhPoRsP1jyM81N9BazGqxMq7tODLzMnlh0wss2r+Ifan7AGhZvyU5BTmkn083OUohhBCidpFEtYaIad+Qsd2bsOFACu+sPfjnBosFetwP2WeMZatElbm5xc008GzAivgVvLD5BT744wMArml4DQDHMo8xb+88cgvkB4MQQgjhCJKo1iAv3NyOqBAf3l5zkCMpWX9uiL4TXL1hwzQ4d9q8AGs5DxcPFo9czMLhC+kX1o98Wz4hXiG08G8BwMy4mbzyyyssOiA/GIQQQghHkES1BvFwtfLyyA7kFdp4/uudf46J9PSHgS9A6mFjYtXuryHnrJmh1lq+br60CmjFq31epXfD3gwOH0yot/Go203HNwGwIWkD8enxjFgygrhTcWaGK4QQQtRokqjWMD0iAri1S2M2HEhh5a4SM817xsLA/4NTu2DBOPhgIBQWmBdoLefl6sV7g97j8e6PFyeqRbae2Mr7f7zP4fTDF6wSIIQQQojKkUS1BnpqSBvqebjw4je7yc4rkYxe9wj8Yxt0vgvOHIQDK80Lsg4pmai2DmhNni2PpYeWArA2cW3xKgFCCCGEqBxJVGug4HruTB7UiuPpucz44eCFG4NaQv9nQVlh64fmBFjHeLp44u/uj1VZebLHk8XlbQPbklOQw7rEdSZGJ4QQQtRckqjWUHf2bErr0Hp8vPHIn0+sKuLbEFoPhUNr4MwhcwKsY2LCYxgdNZouDbrQ2Kcx3q7evN7ndSzKwlcHvuJ84fniulprPtvzGdO2TpO1V4UQQogKSKJaQ7lYLUzoH0luvo1PNsdfXKFHrPHfOTdD0nZnhlYnPdPrGZ7t9SxKKf7T7z+8O/Bdmvo2pV9YP34+8TPDFg9jZ8pO8grzeHTto0zdMpU5u+ew/+x+s0MXQgghqi1JVGuwoe1DCavvyZxN8eTkFV64MaKP8TCArNPw6a1w/pw5QdZB7QLb0blBZwBe7fMqk7tN5mzuWSavm8xLP7/EmmNr6BjUEYAfjv1gZqhCCCFEtSaJag3mYrUQ26c5Z7PzeXHZrotvI3e+C2L+Dblp8Ns8c4Ks4zxcPBjfbjyTuk0i6VwSSw4uoWdoTz6O+Rg/dz9+TPjR7BCFEEKIaksS1Rrujh5Nub5lEPO2JPDhT0curtBpLHj4wy//A5vN+QEKAMa2GsuNzW4kzCeMqX2m4mZ1o29YX/ak7uH4ueNmhyeEEEJUS5Ko1nAuVgsz7+xCRJA3b6zaf/EQADdv6DIOzhyAvcvMCVIYY1f7/odvbvmGIM8gAG5oegMAr255lc3HN7Pm2BrSz6cX75N0LolfTvxiSrxCCCFEdSCJai3g6+HKuN7NyMorZNWekxdX6PkAuPvC4gfgyHrnBygAI1l1sbgUv7+u8XX0atiLHxJ+IHZVLI/8+Ah3Lr+Trclb+ecP/2TIoiHc9/19rE1Ya17QQgghhIkkUa0lhndqhNWiWLIj6eKNfmFw92KwuBirAMy/C87GOz1GcSF3qzvvD3qfT4d8ypTuU4jtGMvRjKPcu/Je1iaspW9YXzxdPJm2bRr5hfkArE9cLysFCCGEqDNcLl1F1ARBPu70aRnEuv2nGfLWBvw9XZl7X0+sFmVUCOsGf10BP7wEe76Bk7vhvtXgFWBu4ILoBtFEN4gGoLFPY7478h1/j/47nRt05v3f3+ftHW/z6tZXST+XzndrviPQI5CltyzF183X5MiFEEKIqiU9qrXIqK5hFNo0e5Mz2Hz4zMW9q6Ht4Y75MOQ1SD0EX94D9p46UT3c2vJW3hv0XvHyVuPajaN1QGvm75vPd+e+o757fc7knuGt7W+ZHKkQQghR9SRRrUVu6tCQL2J78dOUG/DzdGX6qv3k5hdeXLFHLHS7F46sgxVTnB+ouGzuVnfm3TSPF695ke6e3flqxFd0C+nGgv0LWJ8o442FEELUbpKo1iJKKXo1D6SxvycT+7cgKS2H2Zviy6po9KpG9IFtH8Kvnzg9VnH5XCwu3NLyFm7zu40gzyBevOZF/N39mbxuMjtTdgKQkpPC2dyzJkcqhBBCOJYkqrXUuN7hNA3wYsaaA5zMyL24gtUV/jIH6jWEVf+CHElyaoomvk3474D/orVm4pqJrDm2hpuX3MytS2/lVPYps8MTQgghHMYhiapS6gGllJ/975lKqW1KqT6OOLa4Mh6uVp4f1pasvEKmrthbdiWvABjwL8hJhfXTnBuguCqdgjvxWp/XSDufxiM/PkJOfg4pOSlMWjupeIUAIYQQoqZzVI/qRK11ulLqWqA98AwgmY/JBrRpQL9WwSzekcTW+NSyK3W8DRp1hl9myRqrNUz/pv15rtdz+Ln7MbXPVMa1HUfc6The3frqBfUKbAVsTNpITkGOSZEKIYQQV8ZRy1MV2P97A/CJ1nqlUurfDjq2uEJKKZ4f1paNB9fz3JKdhPh6kJaTz8IHe+Nqtf9GsVhgxDvw0WD44i649zsIaWtu4OKyjY4azaiWo1BKMaDpAPam7mX+vvlYlIX67vVp4NWAJQeXEHc6ji4NuvDuwHfxcvUyO2whhBDisjgqUbUppcYCtwHD7GVuDjq2uArNg33423XNmbXuEHuTMwH46WAK/Vs1+LNSSFu4bS7MHQUL74UH1oGLu0kRi8pSylgr18Xiwut9X2fssrHM2zvvgjrRwdH8eupXblt2G11DurLj1A5yC3L5W4e/MarlKKwWqxmhCyGEEBVyVKL6D+BJ4H2tdbxSKgr40UHHFlfpoQGR2LQmMtiHJxb9zpIdSRcmqgDN+0LfKfDjy/DtJPAONsqa9zMjZHGFAjwCWDJiCYfSDuFmdeNE1gmCvYJpG9CWd357h8/3fM6iA4sI8DAe9PDSzy9xJP0IU3rIMmVCCCGqH4ckqlrrn4GRAMro3jmhtf6nI44trp6XmwtPD20DwPxtCazclcy58wX4uJf6+K97BPYshR2fGu83zYC/fAxthjs5YnE1vFy96BDcAYBWAa2KyydGT2RCpwmczD5JsGcwuYW5PLjqQT7f+znDmg8DBRG+EWUODSi0FXIs8xgRfhFOuw4hhBDCUbP+P1RK+Sul3IA44KRSaoIjji0ca2TnxuTm2/huZ/LFG62uxhCAwa/A2Hng6W88vSp5p9PjFFVDKUWodyhWixVvV2+e6/0cCsVdy+9i7LKx3PHtHRxOP0xCRgJ5hXnF+7229TVGLBnBH6f/MDF6IYQQdY2jZv131VqnAYOBHUAo8ICDji0caHjHhni6Wvlgw2G01hdXqN8Mek+E1kPhjgVgK4DV/3J+oMIpoupHMSF6AiHeIQyJGMLh9MOMWDKCoYuHcuPCG3n3t3dZn7ieeXvnodF8vvdzs0MWQghRhzhqjKqy/7cPsExrnaGUsjno2MKB/L3cuLNnUz746Qir95zixrYh5Vdu3AU6/AX++BLWvw71GhnLWVkd9bUR1UFsx1hiO8YCMKz5ML478h2+7r78cOwH3ol7BzAmajWt15SV8SuZ3G0yPm4+PPTDQ4T7hvNUz6fYmLQRDxcPuoZ0NfNShBBC1DKOyjiSlVKzgBjg/ymlXIFLTiNWSrUE5gBBQBpwj9Z6dzl1g4GdwAat9WgHxV0nxfZpzic/H+WtNfu5LjIIT7cKPqobnoXdX8MPLxvvM45D38edE6hwuj5hfegTZjyrY3K3yaw+upqF+xdyQ9MbCPAM4PF1j/Pe7+9RqAvZdHwTm45vonNIZ57e8DR+7n6sHr1aVhAQQgjhMI669X8nsBcYax8C0BiYfhn7/Q94T2sdBbwGfFhB3XeA5VcbqIAGvh7cc004O5MyuPXdTRw9k1V+5frhcO9KGPs5NGgH66ZC0q9Oi1WYx8XiQkxEDB8M/oA72tzBgKYDCPcN5/O9nzN/33xa1m8JwBPrniDflk9KTgpxp+NMjloIIURt4pBEVWudgpF0aqVUD+Ck1np2RfsopRoAXYC59qJFQIRSKryMuncCJ4F1johXwJMxrZl0YxR7kzMYNuMnVu8+WX7lxl2g9U1w6/8ABXNvhT3LnBarqB5cLa58MewLHu36KH3C+vDOgHe4sdmNaDSdgjsB8N2R75iyfgr/WPMPUnJS+Nemf/GvTTLGWQghxJVRZU6oqexBlLoGWIiRTCogGBittd5cwT5dgU+11m1LlG0BJmut15coawR8A/QFRgPDyrv1r5R6DHis6L23t3fjRYsWXc2lVUpubi4eHh5OO58j7Dpj48PdNrIL4P52Fro2qPi3S/DZ7bQ/OBO3gnNsb/0UKfU7X/G5a2J7mak6tldqQSo/ZP1ATL0YZpyZQWphKhrj3xQrVgopBODJoCcJcglyenzVsc2qM2mvypH2qhxpr8qpS+0VExOTpLUOK2ubo8aoTgf+orXeCMWJ6xtAr0vsVzpLVmXUeR94Qmt9rugJPOUeTOvplBhyEBYWpgcPHnyJEBxn5cqVOPN8jjAYGHUmi9GzNjN7bz79runKNS0qSigGw9k7YGYvup5ZAmMehysck1gT28tM1bW9bud2ABK2J/Dhzg+J9I9kUPggZv02i56hPfnlxC/kNM3hjMsZEjITih8ukJWfxfRt04mJiCHMJ4wJayYwru04bml5i8Niq65tVl1Je1WOtFflSHtVjrSXwVGJqkdRkgqgtd6klPK8xD4JQJhSykVrXWB/UEAT4Fiper2BD+1Jqg/gqZRaqbWWT89BmgV688m9PRgzazP/+HwHyx+6nlC/Cn7F1Q+H3hNgw39gx1zoOt5psYrqa0yrMRzNOMrDXR4m3C+c8W3H42pxpe+CvszfN5/T2acp1IUMazGMdoHteHP7myzYv4BvDn9DY5/GHEw7yIc7P2Rk5Egu9aNUCCFE3eCoyVTZSqmBRW+UUv2ACmbogNb6FMaaq3fZi0YB8Vrr+FL1ArTW4VrrcGAysEKSVMdr09CX10Z3JDUrj4fm7SC/8BKri137MHgFwfLJ8Mt7YCt0TqCi2mrk04g3+r9BuF84YDwhy9Xqyg1NbiA5KxmbNr5TX+77kq3JW/li3xe0rN8Sm7ZxMO0gIV4hHM04StzpOH4//TsZeRkmXo0QQojqwFGJ6kMYvZ77lVL7gNnA5axh9ADwgFJqP/Ak8DcApdRypVQ3B8UmLtOQDg3567XhbIlPZcrC38t+IEARDz+4+yvwbQQrHoc3O8D6aZCf47yARY1wU/ObALi77d20D2zP8iPLefiHh/GwevBGvzd4Z8A7PNT5IWYOmAnAMz89w53L7+TBVQ9SYCu44FiH0g6RkpPi9GsQQghhDkfN+t8GRAK3Ykx4igLmXcZ++7TWvbXWUVrrblrrXfbyofZjlq4/W9ZQrVrPDG3DwDYN+GpHEq9+t6/iyg07wQProc8TgIIfXoKZPeH0fqfEKmqG3o1688VNX/BY18cY02oMOQU52LAxY8AMmvk2o0fDHtzf8X5aBbSibWBbEjIT8HLx4o+UP/hk9yfFx8nIy+D2b29nwuoJxb2zQgghajdH9aiitc7XWu/UWv+htc6j7IlRoppzsVqYcXsXujT1Z9a6Qzyx8Df6vPYjLy0r8zkMRs/qDc/Aw3Ew6GVIT4DFD0BhQdn1RZ3ULqgdVouVm5rfxMNdHuaTIZ/Qq+HFcy2ndJ/Cba1uY9ktywj3DWfGjhl8tPMjCmwFrIxfSU5BDntS9/B9/PcmXIUQQghnc1iiWoarX/dKmMLTzcqH47vTItibBdsSOZaazaebj5Jy7nz5O1ld4Zp/Qu+JcPxX2Pxfo/y3L2Dtq84JXFR7blY37utwH1H1o8rc3iWkC8/2epZgr2Cm9Z1GY5/GvLH9DR5b+xhLDi7Bw+qBj6sPM3bMIDEzkW3J23j/9/eJT4937oUIIYRwiqua9a+UalvBZnkgfA1W39uNeff3YvWeU1gtMGXRH8zfmsDE/pEV79j/Gdi3Atb+G5r2hm8egYIcaDMcQir6ughxoVYBrVg4fCHPb3yeFfErAGO8a6R/JG/9+hZDvhpSXPe/cf8ltmMsE6MnmhWuEEKIKnC1yeS3FWzLvcpjC5M18PX4/+zdd3gUxRvA8e9eSS699xAS0oEAobeETuhIU1BBEVHsChawooIgKvoTRKQJiJQgIlWq9EAggZAKAVIgCQRCSC93udvfH4dRqqCQUObzPPdwuzs7O7NPCC+zs+/weCsvdHoD07emsuRAJs+H10OlvMlAvNoMen8Fi/vD4n5QdfnHIGYB9P6yZhouPDA0Kg2ftv+UrJIsEvIS6O/bn9ZurQmwC2Br5lasTKxo7tKcWXGz+OHoD3Ss0xFLtSVavbZ6iVdBEATh/vWfAlVZln3uVEOEe5daqeDJVnX5amsqM/44yRvdrv/Ytlq9jtBgICT9Cl5tobLIOAWg60Qwtbz7DRYeKKZKU2Z1mcWh3EO0dmuNJEmEe4YT7hleXcbTypMh64bw9q63OVd6Dq1BSyu3VjSqbER3uTuFlYV8c/gbdAYdk9pNolhXTH55fnUqLVmWWXZsGW3c2+BjI36tCYIg3CvE43nhlowOr8fvief49o8TNKljS6cgL5UwGwAAIABJREFU55uf0GMKGHTQcQKcPgAbxsKmd6DhINj/HbR7vWYaLjwQbDW2dKvb7YbHA+wCGBIwhBXHV+Bm4UZjp8ZszdxKtBzN8uXLqTJUUV5lTJ3WsU5Hfjj6A6cKTjG722xaubUiJjeGKQen4GHpQWTfSKxNrGuqa4IgCMJN3M2XqYQHiEat5IfhzbDWqHnrl3gKy3Q3P8HKFR5bAi4NoPEw8A4zrmL10wA4uQ22fgA3y9MqCLfpjWZv8F6r91jRZwVfdPiCDQM3EG4eTqBdIMH2wYxvOR5TpSnv7nmX45eOUyVX8caON0grSGNDmnEWU3ZJNuN3j+dsydla7o0gCIIAIlAVbkMde3Pe7x1MXkklUzcdu/UTTcxhxBro/D40HAzB/SDnCLbF/5CnVRBug4XagqFBQ7HT2AHgYelBP+t+/NjjRxb1XMQTwU/wWOBjVOgrqGNVh5mdZ1JWVcb4PePZkrmFALsAevn0Yk/2HiJWRfD90e9ruUeCIAiCCFSF2zK4mSdt6jmw7OBpfonNolyrJ+pkHilni26+7KpCCeFvweD50PkDAOqe3VhDrRYEo2dDnqWjZ0cmt59MhzodGNlwJCn5KRRri+ldrzeftf+M77p8R4BdALPiZvFL6i+kFaah1Wu5WH6RMVvHsDBxYW13QxAE4aEh5qgKt0WSJKYNbsTQOQd4c+VRPl6bRHGlMbl/S297lj/XGoXiH9Z6cAoA/+64nNgKxzZCUK8aaLkggJ3GjhldZlRvv9D4BXac3kF6UTq9fHqhVCgJ9wwn2D6YoRuG8vH+jwFwNHNEo9SQVZLFoXOH6OPbB0czx+p6cktzUSvV2Gvsa7xPgiAIDzIxoirctjr25vz6YltCvWxxtzXjrYhAOgc5czAjnz+OnWfviTy2p+TevJJeX6BTWcKqZyF2IeSn10jbBeHvTJQmzO42m3nd5+Fq4Vq938ncifnd5/NsyLM83eBpFCjIKskiwjsCrUHLoqRF1WXPlZ5j0LpBjNo8SiztKgiCcIeJEVXhX3Gx1rD6xXbV27lFFYR9voOP1iZxtrAclVJBzPtdsdaor1+BnTdHAt+i1fHPYN1rgASNHoNun4CVS810QhAAVwvXK4LUP3nbePNa09cAeCX0Fc6VnsPTypO0wjRWHF9Be4/2hDiG8M7udyisLKSwspBdZ3bRyavTLV+7WFuMpdoSSRIrTguCIFyPGFEV7ggXaw1DmnuSXVCOJEloqwxsSjx303MKrIPg9QQYstCYezV+OSx/HAz6mmiyINwyE6UJXtZeKCQFY5uNpcpQxbNbnqX10tYcPn+Y3vV6o1ao+THpRwCyirNYmLiQfr/1Y+j6oSRdTCIqO4rI45FsydhCRVUFmUWZdFzRkcXJi2u5d4IgCPcuMaIq3DGvdfFHpzcwqKknw+cfZE1cNo82r3PzkyydocEA42fTu3Dgu8t5Vl+tmUYLwm1q79GetY+sZW7CXIq1xYQ6hzI0cCgapYZVJ1bRYkkLKvTGFdlsTW05oz3D0PVDr6hjdMhoFJICrUHLwqSFPB70OGrlDZ4+CIIgPMREoCrcMc7WGqYNbgxAx0AntqbkMnFtEh62ZowOr/fPFXT5AFI3wR+TwCkIArobc62Kx6LCPcbTypOP2358xb4xjcdQVlVGsbYYZ3Nnwj3CCfMMI/VSKguTFuJn60eIYwiToyezMnUlFmoLAPLK89iUsYm+vn1royuCIAj3NBGoCnfFwKYebEnOZWFUBgC9G7nhbmt285PUZjDkR1j8CCwbCjYeUHYJRm6AsnzjSOuguWBmd/c7IAi3ydXClWnh067Z39CxIV92+LJ6+/Ggx/n80OcUVBbQz7cfWzO3sihpEd3qduN08WnizscR6hyKn63fP85dlWWZ3LLc686xFQRBeBCIQFW4KyIauDJvRHMy88v4dH0y6+NzeC7cF4NBvnn6KrfG8MxmWDUKqiqMnxXDjYGqthiOb4Imw2quI4Jwh/X368+MIzMoqypjWNAw7DX2LExayJB1Q8gqyaLKYEz3Zqm2xMfGBwczB0IcQ4jwjqCudd0r6lp6bClTD05ldtfZtPNod73LCYIg3NdEoCrcFZIk0bW+C5VVer7ZlspvR3KIO1PA8XPFrHulPeYmN/nRcwqAMXuM3w/Mhk3vgOJy+Yw9IlAV7mtWJla8HPoyCXkJNHBoQH2H+pgoTZgTPwdva2+ea/QcKfkpHMs/RmZhJikXU9h5Ziczj8ykr29fXmv6Gs7mzlTqK5mXMA+Ab498S1v3tkiSxPdx32OqMuWZhs/Uck8FQRD+OxGoCneVqUpJjwaurIzNIvlsEQA/7suggbs1W08b6C7LN3+82ep50Fca56xufhfSd4t5q8J9b3j94dXfJSReCX2FPvX64GbhhkaluWK+qs6gI/psNIuTFrP21Fr2Zu9lSvsppBWmkVeeh5uFG8kXk9lxZgd1resy6+gsNEoNjwc9jkalqY3uCYIg3DEiUBXuukdCPVgZm0Wbeg5kF5Qz84+TVFbpMcjwzNli6rtb3/hkSYJ2xlyWpG6CmAVwKQOs3EAt/hEWHhw+Nj7X3a9WqGnv0Z527u3YkrmFD/d9yPPbngfAztSO+RHzGbhmIFMPTsXfzh+ACn0FMbkxtPdof9NrlunKMFOZiTyugiDcs0QeVeGua+vrwE+jWjL/6eaM6x5AuU6Phanx/0gbEnJuvSLvMOOfy5+AyS4wqw0kr7kLLRaEe48kSUR4R7CszzJG1B/BQP+BTGo/iTpWdXi/9fucLT3L7qzdeFl5AbA7a/dN60u4kEDY8jBWHF9RE80XBEH4V0SgKtx1kiQR5u+EuYmKvo3cmTEslM2vh2NjAhvizyLL8q1V9Gegej4JXEOgMBt+ewkqiu5e4wXhHlPPph5vtXiLj9t+TLhnOGB8Qeut5m9hobbg47Yf427hzp6sPVf83Zp5ZCbj94xHlmX0Bj2fHvgUrUHLqhOraqsrgiAI/0gEqkKNUigk+jZ2x93WjGbOEhkXy0jKucVA09IJ2r8BHd6B0TshYpIxE0Dcz3e1zYJwPxjRYAR7h+6luWtzwjzDyCrJIvVSKgCXKi6xIHEBG9I2kJiXSGRqJCn5KViqLTmWf4y0gjQKKgpu+J9GnUFHWkFaTXZHEAQBEIGqUIuaORl//ObuSbv1UdWuE6HTu6BUQcgQMHeA6Nli2VVBAFSXs2NEeEcA8MK2F4i/EM/aU2vRGXQAzEmYw4zDM3C1cGVK2BQAPor6iA6RHVhZtPKaOmVZZvzu8QxYO4DTRadrqCeCIAhGIlAVao2vDXQOcmZNXA6L92dW7y+trOJMfhk6veHmFajNoPko48tVK5+CAvGPqCAAtHBtwRfhX1CsLebpTU+zIHEBNqY2BNsHs/PMTop1xYxvMZ4wjzCczJyIuxCHLMscLD/I6hOrKagoqK5rzak1bMncgkE2EH0uGlmWuVRxqRZ7JwjCw0QEqkKtkSSJrx9rgreDORPXJfHJumRG/niQBh9tJmzaDl78+fA/j7S2exXq94eUdfBNCHzfzviC1a2O0ArCA6qHTw8W9VyEg5kD+RX59PPtx7AgYw7i9h7t6ezVGaVCyQtNXqCdRzsi+0ZipbDiw6gPCVsRxkdRH3E8/zhToqfgoHEAIDY3lqXHltJ5ZWfSC9Nrs3uCIDwkRHoqoVbZmKlZ/EwrXll2mAX7jP/wdQ12obBcy9bkXH6Ly2ZAqOeNKzC1gkcXw4ltkLASjv8OkSPAtwt0+RDcm9RQTwTh3lPfoT7Ley9n9cnVDAkYgrnKnCJtEb3r9a5OSTUkYAhDAoYAMNpuNLnOuSTmJfLriV/ZmLYRvaxnVsdZfLr/U2LOxZByMYUqQxW7s3bfMKWWIAjCnSICVaHWeTmYs+qFtvx6OJsgNysaedpyqVRLt693M3FtMn5OVoR42ty8Ev+uxk/JedjyPsSvgFPbwcEfGj8GYW+KRQKEh5KDmQPPhjxbvf1Ug6duWNZd7c7IliMp05UxavMoEi8mMiVsCs1cmtHMpRmRqZHVZffn7L9pXYIgCHeCePQv3BNUSgWPtqhDI09bAOwsTPhiSCPKdXoGz45ia3LurVVk6QwD58CYvdDkSdCWwh+TIOrbu9h6QXiwmKvNmR8xn1/6/kKfen0AaO7avPq4s5kzsbmxVOorq/dtytjEoLWD6BLZhc+iP7uivqziLKKyo0grFJkDBEG4PWJEVbhndQp0ZsVzrRm1KIbxq+JpXa8jVho1AFV6AyrlTf6f5RoCj3xnzLE6vzts/Qic64N/t5ppvCDc58zV5gTaB1ZvN3VuCoC3tTf9/frzv8P/Y+aRmcRfiKfKUEV8XjzWJtaoFCoij0cypvEYKqsqmXZoGttObwPA2sSarYO3Yq42r5U+CYJw/xEjqsI9LdTLjnd6BHKxVMu320/w04FM+s3cS/0PN5OQVfjPFWisYdgy45+rx0BxrvGz5mVYNkyktRKEW+Ri4cJbzd/i/dbv08atDQALkxaSkp9CelE6YR5hrHlkDW82fxO9rGf9qfU8s/kZtp3eRhevLgzwG0CRtoj1aeup1FdyOPcw2zO3U6Yro6Kqgvf2vkfc+bhrrlteVX7r6esEQXjgiBFV4Z43qKkn8/akM3eP8WUrCxMlWr2BH/el83H/Bqw+kk2vEDccLU2vX4G9D/T5Bn4ZCbPbQ0UB6LXGY2k7wa9LzXREEO5zIxqMAEBv0ONt7Y1KoWJml5l4WHpUl+ns1RmNUsP/Dv8PrUHLq6GvMrrRaMp0ZWzL3MaipEX8nPJz9TSApxs8TZB9EGtPrSW9MJ2fe/1c/aJXWkEaT258Eh9bH74M/xI3S7ea77QgCLVKjKgK9zyVUsHkASG09LHn0/4NOPR+V1p627M+4SwvLz3Ch2uS6DZ9F9tuNo+14UBoNQYUSuNSrD0+N+6Pj7zxOYIgXJdSoWRl35X82u/XK4JUAAu1BR3qdEBr0OJh6VEd3JqrzRngP4DTxadJK0xjZIOReFh6sPbUWtacXANAQl4CRy8cBYwjqeN2jaNEV0L8hXiGbhhKYeUtPEURBOGBIgJV4b7Q0seeyOfbMLyNN+YmKp5o7YW2ysCu1Au08LZDBt785SjaqpssEtDzcxh3DIb/Cq3HgEdzY/5VbWmN9UMQHhQalaZ65PNqg/wHoZAUvNXiLUyVfz3pGF5/OK1cWzEtfBpjm49lSMAQ8ivy2X92P362fgDMPjqbjWkbeer3pzhZcJJXQl9hXLNx5FfkE5UTBUBBRQGRxyNJuph00zZ+FPUR4/eMv0M9FgShNohAVbgv9WjoiqOlKW42GuaOaM5z4fUoKNOx58SFW6+k8VDQlcK61yFj791rrCA8ZNq4t2H/sP108bpyWo2rhSvzIubR06cnAP39+qOUlACMChlFO4927MvZxzt73uFkwUlGNhzJqJBR1UvC7s/Zz+6s3XRZ2YVPD3zKW7veQm/QE3c+rnoqQdz5OJIvJqPVa1l/aj2/p/8uRmIF4T5Wq3NUJUnyBxYBjkAB8LQsy8lXlRkAfAwYADXwG/C+LGbXP9RMVUpWv9gWE5UCW3MT+jZyZ9qm46w9moNKqWDFodOknC3m80GNaOljf/1KGg6CA99DQqTx8/wecAqEgjPg6FezHRKEB8ytvNnvaOZI97rdiTobRec6nWnt1pqonChkWaala8vqOalulm742PgQlRNF8sVklAolHT06svPMTn5K/olvj3yLp5UnkX0ieXHbi1ibWjM1bCpag3Eu+sFzB+lWV2T8EIT7UW2/TPUDMEeW5YWSJA0G5gNtriqzDVgjy7JBkiQTYC8QDayt2aYK95o69uZXfG9e146NCWdZezSnev+Xm48TOebqH6nLzO3h5UOQvht+egR2fQ7aEuP2qK1g521c6crOG7xag1J9dzskCA+hT9p9QnlVOeZqc8zV5vTz7Xfdcm3c2rD02FJyy3IZFjSMZxo+w56sPXwV+xUA6YXpzDwyk2JdMcW6Yn5J/aX63AM5B0SgKgj3qVp79C9JkjPQFFhyedcqwEeSJO+/l5NluViW5T8nHmoAU4yjq4Jwhf5N3NHpZeo5WrDrzU4MaebJwYx8Zu86Rf+Ze9mdep1pAQol+HaCgB5wbL0xC4BsMK5utfRRWPsyLOoDv46u8f4IwsNAo9Jgp7H7x3Jt3dtWfx8WNAxXC9fq4LOFawsAFiUvqi6zLm0dCkmBs5kzUTlRzIqbxdSDU9Eb9CRdTCIqO+qW27jj9A6e3fwsZbqyWz5HEIQ7Q6qtJ+iSJDUDfpJluf7f9h0E3pRlefdVZdsCs4EAYBYw7nqP/iVJGguM/XPbwsLCY9WqVXepB9eqqKhAo9HU2PXud3f6fukNMofOy4Q4SFioJc6Vykw8qOfPHxR7U/i4lRJJApXEFS+C2BSfpHXiuxSb16XIvC4eecYfwTPOXTGvOItDURL7Gn1JiYXXHWvv7RI/X7dP3LPbcy/fr0pDJZ9c+ARfE1+esXsGgLyqPPaW7SXCMoKZ+TPJrcrF18SXM7ozaGUtHioPPNWeRJdHV9cTYBLAKe0pDBgY5zAOV7UrOllHli4LT7UnaunaJyezLs4iTZfGKLtRBJsGV++/WHaRJDmJ9ubtUUjilY9/ci//fN2LHqb71aNHj2xZlj2vd6y2H/1fHWxe9xVSWZajgEaSJDkBvwJhwO7rlJsOTP9z29PTU46IiLhzrf0Hmzdvpiavd7+7G/er11XbqVICh08X0LqePT/uy2D2SQtSzhYxeUAIw1r+PeiMgJbNsXL0x6qqEma1Bs/m1Hk8Es4ehbmdaCdHQ0TtjayKn6/bJ+7Z7bnX71fT4qbYmtpiaWJZve8JngAgPzGfr2O/5vnWz7M5YzPbT28n3C+cVm6tiN4RTXuP9lQZqjhw9gB2pnZcqrxElGkU9WzqserEKsqryuldrzdTw6aiM+hQSSokSaKwspB3VrwDgMHDQESzv+7PyytfZlfZLrq36E6HOh1q9mbch+71n697jbhfRrUZqJ4BPCVJUsmyXCUZh7fqAKdvdIIsyxckSdoADOE6gaogXG3ygBAAdHoDe0/kkZRTBMCauOyrAlXAq9Vf31+PB1NrkCTwaAr1OkLiKgh/y/jClSAINc7T6roDLoAx9ZW/rT/tPdqjM+jYfno7LVxb0KlOJ+Z3n0+ocyhag5blx5bTw6cH3x7+lo3pG4nKiSLYPhilpGRD2gZsTW1ZlbqKF5q8YJwHm70HvWxcwS72XOwV10yuNL77G5sbKwJVQbhLau1ZhSzL54EjwJOXdw0CMmRZzvh7OUmSAiXJ+ExFkiQroA8QX4NNFR4AaqWCn0a1YtULbegU6ERs5iVKKqvYnHSOM/l/zTsrKNMaU1xpbIxB6p86vQ+ybFx6VSy7Kgj3HLVCTZhnGJIk0adeHxb2WEhXr64oJAUt3VqiVqqxUFswKmQUHpYevN70dRo5NuLV0FdZ1nsZ0ztOx0xlxs8pP1Ohr2D5seUYZAO7zuwCIMQxhOSLydXzVDOLMrmgN857j82NpURbwi+pv1BlqKq1eyAID6LanlTzPPC8JEmpwHhgFIAkSRslSWp+ucwQIFGSpKPAfoxZAObVRmOF+5urjYZmde0JD3BCp5f5dF0yz/8US/evd7M02jiQ/+7qBIbPP0hG3lWLANRpYVzZKusgfN8Ovm0K0T/A+WNwcjsYxPt9gnCvUEgKmrk0u+GCBGBMefVz758Z3Wg0SoUSN0s3JrWbRD/ffgzyH8TZ0rPsztrN3uy9hDiGEOEdQZVcRdyFOAB2Zxkf6lmZWJF8MZkvY77k4/0fs+7UOgAMsoEp0VP49vC3d7/DgvAAq9U5qrIsH+fadFTIstzrb98nAZNqsl3Cgy08wAmAFTFnMFMrsbcw4d3VCcjIbEo8B0Bs5iXS80qZueMk859qjq25CXT5ANJ2QHEOKNTw+9t/Vdr5feO0AEEQ7lvdvbvT3bs7qZdSWXViFW/uepNKfSWDAwYTaGec8vPe3vcwV5lTqa9EhYrh9YczK24Wq04YX9xdl7aOAf4DmB4znaXHlqJWqHk25Nl/zCsry/JNA2tBeFjV9oiqINS4eo4WeNqZATA6zIfIMW0wN1Hy3upEDJdf74s7U8CSA5nEZl5i3Z95WU0s4MUD8E6mcQ5r14nQ9lVwCoKdUyEnrlb6IwjCnRVgF4C/nT+V+kq61+3OAL8BBNoHEmwfjEJSoJf15JblEmQaRDv3dtXnOZk5cejcIabHTGdR8iLMVGboDDpicmOYHjudCXsmcL1MO5lFmbRd1palKUtrspuCcF8Qgarw0JEkiUeb18HP2ZLnOvjiYWvG2G4BAAS5WuFoacrB9Hz2p10E4Le4nL+fDJKEQWUO7d+A7p/CwLmABBvG/VWu5AKseQkuZdZgzwRBuFPGNBpDR8+OTGw7EUmSUClURPaNZPuQ7fw+8HfWPbKOoTZDCXYIxkJtgaelJx+0/gCAH5N+xNvam++7fg/AL6m/sChpEevT1vPH6T+uudbso7Mp0ZXwXdx3FGuLa7SfgnCvE4Gq8FB6tYs/28Z2wNLUOPvl6bbevNDRl0mPNKRJHVuO5xZTptVjolQQm3mJI6cvEX05cN2anEv9jzaRkHV5/XC3RtBwIGTHQMHlpBVb3ocjSyBmQW10TxCE/6i7d3dmdJmBlYnVNcckScLbxhuNQoNaoWZOtznM6jqLMM8wnM2csdfY833X72nq3BRXC1d2nNmBQTaglJRMj52OTq+rriu9MJ2N6RuxMbWhSFvEkuQl11xPEB5mIlAVBEClVPBOjyCae9sT6mVbvf+Vzn4ADJgVxWNzDhB1Mo9VsVlU6AzM3HHirwqC+hj/PLYBTkdD/HLj9qntNdUFQRBqSSOnRvjY+KBSqFjcazGRfSLxtPJEkqTqqQHe1t680PgFThefZvXJ1YBxXur0mOkYZANfd/waLysvFicvplRXerPLCcJDRQSqgnCVPwNVBwsTnu/gS303a4JcjaMqyw+dYfcJY0qaLcm5rD6SxZebj1Pm1QFUGkhYCeteNb5sVacVnEuA4txa64sgCDXLw9IDFwuX6u1OdToBxjyvw+sPx0ptxbJjy5BlmfmJ89mZtZNePr1o4dqCpxo8RYmuhM0Zm5FlWSzZKgiIQFUQrtHI0xZLUxVdg10wUSnY+FoYm14PJ8TDhrVHcyjT6ule3wVZhjdWHGXmjpOsP1YEvl0gOxYuHDNmCGhuXOaRuJ9h3Wtw/HdjLlZBEB4a4Z7hrOy7kiEBQzBXm9Pfrz8nC04y7dA0ZhyZgZ+tHx+1+QiAnj490Sg1rEpdxdu736ZjZEfizhtf0pRlma2ZWzlddMM1ce6Kvdl7ySvPq9FrCsLfiUBVEK5iaapi69hwPupX/4r9/Zu4V39/u0cgT7Tyolt948hJ3JkCqN/feLDJk8ZsAL6djdvbP4bYhbBsKPxae0uwCoJQ8yRJIsg+qDr11NCgoQAsSVmCo8aRbzt/W526ysrEiu7e3YnPi2dTxibKq8p5+Y+XOZx7mLkJcxm7cyzPbX3uX4+0Vhmq2JyxmR8Tf2RP1p5/LJ9XnseL215kxpEZ/+p6gnAniEBVEK7DzcYMc5Mr0wz3aeSOJEFdB3N8nSyZPCCEOcOb4WRlytEzBRAyBIb/Bn2/MWYHsHQG91BAgu6TwDvMODXgwvHa6ZQgCLWurnVdevr0xM3CjfkR86ljVeeK44P8BwHgZ+vHVx2+okRbwlObnmLGkRnYmNqQXZLN17FfY5BvvMhIsbaYhYkLrwhoDbKBiVETeXPXm0yPnc7rO17/xwwDp4tOIyOTmJf4H3osCP+NCFQF4Ra52miYMiCET/s3rB4dkSSJJnVsOXaumNOXKnjniANHc/72IsSQRfDcTmj7CoSNNe5LXmtchjVjL+z4DPJO1nhfBEGoPVPaT+H3gb/jbeN9zbFQ51C+6PAFs7vOprt3d1b0WcGwoGGEe4azut9qmjo3Zfnx5YSvCGf1idXV5x29cJRH1z1KRmEGs4/O5qvYr/gi5gvAOG3gi0NfsObUGsI8wni+0fNoDVq2n775y57ZJdkAnCo4RaW+8s7dAEG4DbW6MpUg3G+GtvS6Zl+TOrZsTc7lhZ9jScopYvWRbD7qV5/HW3px8JIlF0rU9HaTkbzDwMwOkn+Ds3FwbL2xgnOJMEwk+haEh4VSobzhMUmS6OHdo3o70D6Qd1u9W739VcevWJi4kPVp65l2aBpd63bFysSKpSlLSclP4aOoj0jJTwGM+VvburflWP4xlqQsoZlLM6Z3nE6lvpL5ifP5Pf13HvF75IZtySrJAkAv6zl56SQNHBv8164Lwm0Tgaog/EdN6hizBCTlFOHvbElFlZ73Vify6+FsYjMvAfBj3Qyq9AZel1rSKXcz5CZCYG8oy4MTm40LBFg61WY3BEG4DziaOfJmizfxt/Pn/X3vs+L4Cp4MfpKdZ3YCcPj8YQBeavIS8xPmM3an8UlOfYf6zOw8E41Kg0alob17e/Zk7+Fi+UUczByue63s4uzq78n5ySJQFWqFePQvCP9RiKcNfy7RPa57IOtfDqNzkDOxmZdo7GnDo809ic28RMq5YhYVNDYWdAmBQfOg5XNgqIL4FbXXAUEQ7ju9fHrhauHKT8k/sSljE2VVZYxsMBILtQUelh48G/Is8yLmMabxGF5u8jI/dP0BSxPL6vN7+vREL+tZeuzGT3OyS7JRScbxrGMXj931PgnC9YgRVUH4j6w1ahp52lKpM6atUigk5o1oTlxWAQ3dbTBRKXi3VzAmKgUR09V8Uv4sL/R7GScTc+NCARobYwqr1i+CQvzfURCEf6ZWqhkdMppPD3zKB/uMS7cODRpKf7/+mCpNUSlUNHZqTGOnxtc9v2vdrgQmBjIvYR72GntSL6WSVZyFg8aBye0no1aqyS7Jxt/On3Ol54jJjeG+9p2QAAAgAElEQVTdPe/S3bs7Het0rMGeCg87EagKwh2wZFRLABQKqfrPpl521cdtzU0AeK9PQ8YsqWTVvBO80RWeauuN1OQJODALdk0FtyaQvgudfQAqBx8kSyewcAJLl2svKgjCQ21IwBC0ei1fxHxBE6cmuFu6//NJl5koTfiiwxc8tv4xph6cCoBGqaFCX4G3jTejG40mtyyXBg4NsNPYEZUTRVphGpnFmXc8UNUZdMyKm8UAvwF4WV/7HoDwcBOBqiDcAVYa9S2V69HQlW8ea8K0TceYuC6ZkxdK+LjHeygz98Guz6vLXVNby+dA2fXONVgQhPueJEk8Wf9J2nu0v+Kx/q3ysfHhf53+R9yFOHr59MLdwp2hG4YyN34uAXYBGGQDHpYeuFu6E38hHkczR5LykijRlvyr693IwbMHmZcwj7zyPD5t9+kdq1d4MIjnjIJQwx4J9WDL2A6083NgyYHT/G9PDsc7z+WooR4bVF14xfY7ntG+ybeaF6Hju2BbF46uQDLoarvpgiDcg7xtvHE0c/xX57Zxb8MLjV+grnVd1Eo1k9pNQkbm4/0fA+Bu6c6woGHsG7aPoUFD0ct6YnJj+Pzg56w7tQ6AdafW8cfpP5D/5cp78XnxAOw6swu9Qf+v6hAeXGJEVRBqgaWpigVPt6DvjL3M2X2Kg+m2HNBOQqEDQwmYqhzYUWjgqVbdsVGZwLaJOBYchTWbwKUhtB5T210QBOEBFOwQTF/fvvx28jcAPK08kSQJCYlWrq0AmB47nfTCdFSSirTCNOYlzAOggUMDcsty8bD04J0W7xDiFHJL1/xzQYFLlZeIuxBHM5dmd6Fnwv1KjKgKQi0xVSl5KyKICp2BA2n5dKvvwvynWvBMOx/GdgtAluHw6UtElhl/addPmwNHfkLe8gEUnCbuTAEnz998ZRlBEITbNabxGFQK4ziWh6VH9X5fW1/sNfakF6ZjrjIHCeYlzMPJzImePj05nn8cW1NbkvKSeHzj4zy96Wlic2MBmH10NtPzpjNy00gWJS2isLIQMC5GkHAhAWsTawB2nN5Rw70V7nUiUBWEWtQ12JnmdY0vXb3WxZ9OQc582Lc+bXyNeQ3n7k7j7T+KSTR4o9EVUCBbIBm0nFzxLoO+j6L/zH0kZBXWZhcEQXjAeFh68HSDp3E2d8bTyrN6vyT9Nao6suFI3mz+JramtnzZ4UumhU8jdngsq/uvZmXflfTw7kH8hXje3v02OoOOhUkLuaC/QPLFZL6M+ZKBawaSU5JDVkkWlyov0dOnJy7mLmzN3FodxAoCiEBVEGqVJEl890RTfn62FQ09bKr3B7tZY6ZWEnXqIpIEuT6PoEdiXcBk9hhCqJeznqam2VQZZEYsiObd1QnEZOQDsC05l98TztZWlwRBeAC8GvoqWwdvxVRpesX+J+o/QT/ffoyoP4Ingp9g12O7aOrSFACFZAwp/Oz8jBkFAh/jfNl5NqRtoFRXSgfzDuweupsJLSdwvvw8L257kd1ZuwFo7NSYRwMfJac0h6Hrh3IsX+RtFYzEHFVBqGUu1hpcrDVX7FMrFTSpY8v+tIv0CnGjy9CP2LEhmOF9h7JrlyeKHYOZ57mRw+1+4O1V8ew/GE3sYRXjhvbg5aVHAGha1+6aegVBEG7Fn/NSr3Z1btY/g9PraePehiUpS/g+7nsA/Ez8MFWa8njw42j1Wr6K/ao6NVaIYwh96vXBxsSGqYem8uTGJ3m+0fPYaewItAukgWODm15LeHCJQFUQ7lHhAU7EZObzUkc/UCjQmhinCHTo0A0uDMImcRWdNCc4+JQdhoXDqdJpmbT0CLK+MzpUzN2dxvt96qM3yKw9mo2fkxXWZiqiTl2kZ0PX6tyugiAId0Mzl2aoJBU5pTmYKEyoa1K3+thTDZ7C1dKVr2O+Rq1U42XthSRJPBb0GMEOwYzbNY5vj3xbXd7b2psfe/x4w+wGGYUZ/JT8E+Oaj8NcbX7X+ybUHBGoCsI9anSYDwObelx/VLTTe5C8BpYMRFKoUWKg2NSJT6WFfGiyjN/VXXknejgvdPRl9ZFsJm1IueL0hOxCPhsQQmllFeYmSqTLa8AeO1dE/JlChjT3rN4nCILwb1ioLWjk1IjD5w/T2Lkx6r9liJYkiR7ePejm1Q29rL9itLSRUyNW9VtF/IV4DLKBfdn7WHpsKR/s+4Ae3j04cv4Ib7V4C41Sw7myc7hbuPNR1EccPn+YUJdQ+tTrUxvdFe4SEagKwj1KpVTc+NG9gy88uhgOfA9n42HIQiw9W5O5dQZeZ7fQL3cDO/RePLvYmtRzxdSxN+M5zzNQXsjCgkasOZLNwFAPnpwfzWtdAnihoy+xmZd4esFBiiurcLc1o73/v8vLKAiC8Kc27m04fP4wLVxbwLlrjysVSpQor9lvbWJNe4/2AIR7hlOqK2XNqTXszd4LQLG2GK1ey86snbTzaMfh84cBiD4bjY+1Dx9EfcBg/8EMDhiMQTagUYlpUPcrEagKwv0qqLfxc5kKqPvIh1D2MvJ3rfhMuYSlORkA9LPV4pS6HYC6odMZsd+VpxYcpEJnYGFUOn0bu/H0goNUGWQUEizYly4CVUEQ/rP+vv05euEo/Xz7kXgu8V/XM6HVBPLK8/Cy9uJc6Tm2ZG4BwF5jz77sfViqLbE0sST6bDQVVRWcuHSCKQenMOXgFBSSgoU9FhLqHAqA3qBHkiQx5/U+IQJVQXjQmNsj9ZuBJnI4o1S/G/flAAE9IesQYUkfEmzyKSlaF5ytTMktqmTE5ZHUuSOa80vsGTYn5ZJ2oYR6TsZlEveeyGPOnjTySytxtzEjooErA5t6iOkBgiDclJulG993Nb5Mlci/D1Qt1BbM7jYbgCJtES9vf5lAu0DebPEmPyX/RKBdIDvP7CQyNZLzZedp7NSYNu5tSLmYwq6sXew8s5NQ51BkWeaVP14h+WIy41uOJ8I74oa/x8qrypmwZwL9ffvTyavTDdu2/NhyvKy9aOve9l/3T7gxEagKwoMosAfSmydAW2rcVpmChSOk7UT6aQCLLWYyynYq059sTcQ3u0m7UEqYvyNdg52x1qjYnJTLE/Oi6RrsgqlKwY9RGSglCScrU5JzitiSnIuFqYoeDV1rt5+CIDx0rE2sWdxzcfX2syHPAsbAMjI1Er2sZ5D/IAb4D0Bv0BO2PIzDucapAXuz97Inew8Ab+1+CxmZnj49Afgu7jsSLiQwo8sM1Ao1a06uYfvp7ZwsOEmHOh2uOwKbV57H5OjJBNoF0rafCFTvBjHuLQgPKjNbsPEwfiwuP8av1xE6vYdT+SnW+vyKn7Ml3YJdUEgwoWcwkiTR0seetyICUUgSPx3IZN7edDxszfj99TD2je/M7rc7oVRILD90muIKHV9vTeXR2fsZtfAQvx7OYs7uU0TGnKFCd+Wa3UUVOs4XVdT4bRAE4eHQwrUFABqlhm51uwHGObChLqEkXkykTFfGN4e/wVRpypJeS5CQ2H7aOCVqfdp6Zh+dzb6cfWxK34TeoGdR0iIAMosyq+fGXm1/zn4Ajl86TkFFwd3u4kNJjKgKwsOm/Vg4fQCOLoMWo5k6oD6vt7MnyN24hKEkSbzUyY8XO9Qju6CcvFIdAS6WmJsYf1142pnTKdCZP47lMmphDAcz8rE0VVGu07P92Pnqy0zbdIxlo1vj72LF3hN5vLb8CJIE+8Z3xlR17csTV9PpDeQWVeBpJ1LNCILwz+w0djwe9DhO5k5YmlhW72/q3JTdWbuZdGASqZdSGdlwJI2dGhNkH0T02WhySnL4ZP8nOJk5UaorZUHiArR6LVklWQzwG8DaU2v5OeVnwj3Dr7lmVE5U9feY3Bi61u1K8sVkvj/6PRPbTMTBzKFG+v4gEyOqgvCwUSig+yRAgh2TsV05iKBl7eDiqSuKSTHz8ZwTTBM7XXWQ+qfHWtTBIMPBjHwGhnoQ92E39r3TmS+HNGbZ6NaM7xlEXomWn6NPk5hdyIgF0Vws1ZJXomVPal51PQfT8ymtrLpuM7/akkqnL3eSXVB+x2+BIAgPpgmtJlRPBfhTM5dmAKxLW4eDxoHRIaMBaO3WmoLKAj458AnlVeV82OZDBgcM5mTBSSbun4iV2orXm71OF68uROVEkVmUSZG2iB+O/sDYnWNZn7aeqJwo7EyNOa4PnTtEYWUhb+x4g51ndrLjzI6a7fwDSgSqgvAwcg6C+v3g1HbI3Ae6Uvj9HZDlv8ocXgQVhZB27S/bToFOeNqZEeRqxeQBIaiUClxtNAxu5kkbXweeD6+Hh60Z21JyWRlzBoMM/xvaBIANl5d33Xcyj0d/2M8Hv137goVOb2BlzBl0epn9py5W7997Io+T54vv8M0QBOFB1sChQfVSsOOaj8PKxAqAVm6tANiXvQ8vKy/CPcMZUX8EHpYedKvbjRV9VmCvsWdI4BAA1pxcw8SoicyMm8nWzK1M2DOB/Ip8+vv1x8nMiaicKCbsmUBOaQ4AcefjaqG3Dx7x6F8QHlbhb8PxTRDYA5QmkLASDs6BFqOh8DScSzCWS98NjR694lSVUsGGV8MwUSowM7n2Mb4kSXQNdmbR/kxWxJyhnpMF/Rq7s2BfBluTc6nQ6flmWyoAq+Oy6d7Aldm7TnHqQgku1hqGt67LxVItANFpFxnczJP0vFJGLIjGycqULa93wMZcfc11BUEQrqZWqnnE7xEuVVy6YjGAUOdQ1Ao1OoOOoUFDUUgKXCxc2DRo0xXnt3RtibuFOyuOr6BIW0SYRxjvtHyH0VtGc7b0LG3d23K+7Dwb0zeSUZRB33p9OXz+MEcvHEWr17Ilcwvd6narDpaF2yNGVAXhYeXaEMamwJBF0H0yWLrA72/Dgu4Q86OxjKSAjD1Xnld0FnZOxUZtuG6Q+qcuwS4AVOgMPNLEmMqqbyM3SiqreHnpEQ5lXKKltz2yDGOWxJKQXUh9N2tOni/ho7VJADhYmHAwIx+AGdtPYJAht6iSF5fG8sS8A0xcm0SV3nDn740gCA+U91u/z1cdv7oiFZW52pzmLs2xUFvQ36//Dc9VSAr6+/WnSFsEwEtNXqKudV0W9ljI+JbjaeXWin6+/XDQODCh5QQmt59ME+cmZBRlMOPIDCbsmcDkA5PZm72XIeuGkJh3/TRdmUWZ7Mveh/z3J1uCGFEVhIeaxeWJ/lYu8FI07JgCB3+ArEOgtjAuKJAQCZn7IT8NGg+FPV/Boblg5wONH7th1a3q2WNhoqRUq6d/E3cA+jfx4KcDmWxLyUWlkPjq0cZM35rKhoSzzBwWSvcGrszYfoKvtqbSpp4DdezNiIzJIupUHr/FZdPOzwEztYptKbkA7Dt5kZyCcr4dFopG/c8vaAmCIPzdZ2GfUaItwdrE+qbl+vv1Z078HNq6t6WBYwMA3C3deSL4CQDaebRj52M7q8s3dmrMhrQNLE42ptFafXI1a0+tRS/r+SH+B2Z0nsGBswdYmrKUUl0pbdzbMCd+DuVV5XSu05lP2n3yn/pVpivDXP1gvIgqAlVBEIzM7KDXNLD1gi3vQUAE+HczBqqL+oChyjjCmvybsfyx9TcNVE1VSl7s5Me5wgrqOlgA4GRlyo5xHTmaVYAkSdSxN+eLwY2Y2K8BNmbGR/kvd/bD1sKElt72JGQXEhmTxaiFMRhkeKNrAIGuVmxLyaWdryOTN6awJi6HkT8eYs6IZlhpxHQAQRBunaOZI45m/7wKn4elB8t6L8Pd0v2W6m3s1BgAg2xgRP0RbMncQkVVBZ6Wnuw6s4tvD3/L3IS5SEioFCoOnjuIvcae1m6t+ePMHzgedqQlLYG/lot1MHNgT9YeIo9H8nn45zcMRLdlbmPcrnFE9okk0D7wmuO5pblUyVV4WHrcUl9qW60GqpIk+QOLAEegAHhaluXkq8o8BowH1IAMzJFleUZNt1UQHhptX4a6bcDeF3R/e+NepTG+cFVZaNw+ud14XG0Gp/4wznP1bn9FVS918rumeoVCItTL7q9qlQpszP6ahSRJEsNb1wXA/PLUgnKdnk/7N6C5tz0AA0I9Afj60SbYmqlZtD+TJ+ZFs+qFtqiVYkaTIAh3XrBD8C2XDbALwExlhizLPNfoOZ5r9BwAiXmJjNk2hrkJc/G09GRO9zmYqczYnLGZLl5dcLVw5dF1j7IhfQP+tv48vuFxEvISMFOZsWHABuYlzOPw+cMsP76cZxo+U329+AvxfH/0ez5r/xkb0zdikA3E5sZeN1Adu2sseWV5bBq06b5YXbC2R1R/wBh4LpQkaTAwH2hzVZksoKcsy+ckSbIBYiVJOizL8r6abqwgPDQ8jOlcMLOFXl+Coz8c/x2ijUsY0nQEHF4MabuMiwisGGEcbX093njOHeJpZ8Yrnf0IdrOmV4jbNccVComJ/Rpgb2GKlUYlglRBEO4JKoWKt1u8janSFBtTm+r9bdzb4G3tTW5ZLt90+oY6VnUAqqcQAAzyH8Sk6El8d/E7CgwFNHFqQtyFOBYkLuDI+SMALEpaxNDAodWjql/Hfk1MbgyrTqziQM4BAFIvpV7TrvKqcpLyktDLenJKc+6LUdVa+60uSZIz0BRYcnnXKsBHkiTvv5eTZXmfLMvnLn8vBI4BPjXXUkF4yLUcbQxGW78IkhJcQ6Dd68ZjKWuNKa60xcaR1j8D2b8ruQArR8L5Y7d9aUmSGNc98LpB6t/LvNbVn2fai18LgiDcOwYHDKavb98r9ikkBT90+4HlvZdfd7QToFe9XpipzCgwFBDmEcaCiAXYa+xZkrIEGZlWrq3Ir8hnZepKAJLykojJjQFgXsI8inXGFH7XC1STLyajl42rBsZfiK/en3oplSrD9XNa1zaptt4ukySpGfCTLMv1/7bvIPCmLMu7b3BOfWAPECLLcs51jo8Fxv65bWFh4bFq1ao73vYbqaioQKPR1Nj17nfift2ee+F+ORTEU2FiT6m5J60S3sW6NIMCS3/si1OoUNuhNFSyJ3QGOrVV9TlB6T9S99zvZDl3Jsl3TI229164Z/cTcb9uj7hft0fcr1v3a9GvHCk/wjjHcdgqbVlbtJbdZbtRoeID5w+YnjcdnaxjgtMEVhWtIq4iDj8TP05qTwJgobBAK2uZ7DwZCYlfi35FJamwVlqzoXgDAO3N2/OI9SOka9P5Lv87+ln1I9zi2tW3akKPHj2yZVn2vN6x2n70f3WUfMPJEpIkeQJrgDHXC1IBZFmeDkz/c9vT01OOiIi4E+28JZs3b6Ymr3e/E/fr9twb9+tv1/fXwOJ+2BengEdzNC1Hw+rn6ZzxOQxdBo5+UJQDB/8AwLP4CJ5dO4Oy5l54ujfu2f1D3K/bI+7X7RH369Z1NXRl45aN9O1hHJH1K/Bj95rddPTqyMBOA5FOSHwY9SEr9CtIrEiknXs7Xmn6CkPXD8XG1Ia+9fqyJGUJ9dvW5+iFo+zfux+AIPsgFJICC7UFBWYFRERE8Mn+TyAf8qzy6NS5ExP2TGCg/0DaebSrzVtQrTYndJ0BPCVJUgFIxhm9dYDTVxeUJMkd2AZMkmV5ZY22UhCE66vXAfy7G783GACNHoOIz+DiSVjYG4rPwR+TQV8JXm2h/BKk76rdNguCINwHlAolJpJJ9bavrS9zus3h3VbvAtDPtx9+tn4kXkzE18aXSe0nUd++Pj19ejI8eDhB9kEA7M3ey9SDUzFTmQFwLP8YfrZ+NHVuyrH8Y5TqStmauRWAI+eP8MfpP9iSueW60wZqS60FqrIsnweOAE9e3jUIyJBlOePv5SRJcgO2A5/LsryoRhspCMLN9foCmj0NoU+AJEGbl2DAHCg5B7PbQ9wS8A6Dft8ayyf9VqvNFQRBuF+1cW+Dk7kTYAxkJ7WfxCD/QcyLmIejmSOSJDEtfBrPN36eALsAAL449AXF2mI+D/scfzt/ABo5NaKxU2Oq5CpmHplJQWUB9hp7yqvK+ebwN0hI9PTpWWv9vFptvyL7PPC8JEmpGFNQjQKQJGmjJEnNL5f5BPACXpMkKe7yZ2TtNFcQhCvYeUPf/xlzsP6p0RBoNQZKLxhHUoctM2YNcAmBuKWw5iUoy6+1JguCIDwIGjg0YGLbidfNA+tr64tSUqKX9YxsOJJOXp0YHjwcgGYuzWjr3haFpGBJivF99rHNjK/3ZJdk09K1Ja4WrjXXkX9Qq3NUZVk+zrXpqJBludffvo8GRtdkuwRB+I+6Twa/rlC3HZhcTko9YLZxidYjS6DsEgxbeuPz03aCU7BxxSxBEAThtpgoTeji1QUZmddCXwPgEb9HqGNVh1DnUJQKJYt6LGJl6kpsTW3p5dOLydGTKa8qp3e93rXc+ivV9stUgiA8iJQq46pWf+faEEZuNKaqSvoV9s+CikLjSlgqU4iPhMCexoUD1rwIwX3hsSWQ+Cs4Bxs/giAIwi35quNXV2xLkkRz1+bV202cm9DEuUn1diu3VkSfjaZr3a411sZbIQJVQRBqVo8pcHIbbJ5w7bETm6lO/pG6BTKj4JeRxpHZkRtrtJmCIAgPk4/bfkxhZSFWJlb/XLgGiUBVEISaZeUKA+dC2g7jqGlhNlQUgG8X2PIenDkIjYfBge9g5dPGczKjoOQ8WDr/VU/6HjC1BPfQWumGIAjCg8ReY4+9xr62m3ENEagKglDzAnsYP1d7fAXoq0BbAofmQkkuqM1BVwbHNxozDADkp8NPjxi/d3rX+HKWR1NoOKjGuiAIgiDcfbX91r8gCMKVlCows/0rR2vPaaBQQ8o646iqtgx2TgVDFZjZw/ZPYP9MWD8W9LrabbsgCIJwR4kRVUEQ7k1dPgLP5tDkcUj+zTiv9Ut/MLEEbSnU6wSD5htHWnMOQ8wCSNsF5pcfXXk0rd32C4IgCP+ZCFQFQbg3OQUYPwCtXjDOZf0/e/cdH1d15n/886hXq1mucm/YBmwIvReDHQIJCQRCEkIoMbthQxJg0zYJacvujxAvSxolIRBIKEsJkBiM6cUBUyzABmxjI/cqq/dyfn+cO9JIHskee0Yjyd/36zUvzb333HvPfTQePzrn3HNHzYbtH8COD2HO9ZBdBIdfDOOO84nq67+H9a/7VtlrPiC9qRx2rYXCiYm9FhER2SdKVEWk/5syx79C2logObVzuWiSf6DAR890rnv7zxy9/EZYmQTXvA+N1X7sa8G4zjLO+SdqiYhIv6QxqiIy8IQnqSEzP+N/Tj8HUrPhqe+R2bwT6rb7BPbez8Edp0F7my9XtQn+exy8dVefVVtERKKjRFVEBoej5sOpP4RP/wZmfQFcO7UZo/y2J78LW0qhfidsfdev++BxaKqC137vW1ZFRKTfUaIqIoNDRh6c/O9+xoDjvwkTT+Hdqd/yDwuoXNdZruxV//PDf/ifOz6EVU/BnZ+El34J7e19XnUREYlMiaqIDD4F4+Arj1GTPR4OvdCvO/g8sGRY96qfd3Xdks6brB74MqxfAs/9Ah6+zA8PcM6PaxURkYTRzVQiMrjNugiaqmHWF6FinU9QVz4Jrg1OvBZevQV2rvRDB2q2wIpHoWAC7FwFa56DK56B4TM7j9faDLvWwLDpibsmEZEDhBJVERncUtLguG/49+NPgE1vwuIfgSXB1HmQM8I/znXOT/1DBP54BryyoHP/f1wLlz4JTTW+NXbx9T6xPfQL8KmbIL1/PRdbRGQwUaIqIgeO8SfCqzf7p1udfTNkD+069VVyCnz+LrhzHkw6FdKy/fyst8z2rbE4SMmA0UfAu/dD1Qa45O+QFIyiam+DpOREXZ2IyKCjRFVEDhyTToNP/9rfYFU0KXKZoklw7Yc+4Wyo9E+7amuGGZ+BkiNh+tmQPw6e+Ca8fTe89Sc48nKf0D79I/jUAph1od83M79vr09EZJBRoioiB46kJDj8K3tRLmgVzcyHq9+OXGbuf/r5WRdfD9tW+EQVB4993Sew616F03/sx8GKiMg+0V3/IiL7Ij0Xzv2dT37f/KMfRnDRA5CWA+v/CTnD4dmfwdI79nws5+CNP8Cuj+NfbxGRAUQtqiIi+2riKfDva2HrO/6mrLzR8K+v+ke8pmX7sa5PfhdGzoYxR/qEtOxl/1SsnGKYeJpPdDe+4W/aGnEofO15P1ZWRETUoioisl+SU2D0J3ySCpBXAoUTIGcYXHgPJKX4uVmrN8Mz18Pd58Df/gXuPQ9+fxxsex9WP+333fouLL1t93PU74IPF+oJWiJywFGiKiISL8Nn+rGslevhf2bCq/8LJUfBF+6D467201w9/UNYvRgy8qFgPDz3n7BrbdfjPP0juP8iWPFI1/UVZfDsz6Gxqq+uSESkT6l/SUQkno68AvLGwGu/88sX3uMf93rQWX56qxWP+vUHnweHXwJ//jQ8+i9+7takZN+auvwhX2bx9TD1k5CWBW2t8H9fhc3LoLkWPvn/fJn6Xf5nVmGfXqaISDyoRVVEJJ7MYNo8uORx/8rI69x27L91vp9yJkw8GY75Omx4Hf78GVh2Lyy9HVob/RywVRtg4XU+GX3+P32SmpLhb9ja/iG0t8Mfz4TbTvbTY4mIDHBKVEVEEqXkCBhzNFgyTDrdrzv9ephxrp854LGr4IX/guxi+OIDMOYYKP0L3DjBPz2r+CD48sP+cbCLfgAfvwjlq6FqPfz9WxrTKiIDnrr+RUQS6fw7/VOvcor9cmoGXHA3NFTAew/Buw/A7C/5WQQuXQhv3QWrFsH44/367KH+ca7v3g8VwfRWY4/zQwomnwGHfSlhlyYisr+UqIqIJFJeiX91l1kAR33Nv0KSkv1TsI68vGvZOdfDB4/7m7DGnwgX/Bl+fzws/HcYe0zPT+ESEenn1PUvIjLQDRnV+QSsIy7zN1J97nZoqYeHL4fW5sTWT0RkH6lFVURkMDjhGph0Kow63C9PONEnr+2tia2XiMh+UKIqIjIYJCX5Bw+EO+2HftYBEZEBSl3/IiKDlQPdi8IAACAASURBVJJUERnglKiKiIiISL+kRFVERERE+iUlqiIiIiLSLylRFREREZF+SYmqiIiIiPRLCU1UzWyKmS0xs1VmttTMZkQoc2RQpt7MHkpEPUVERESk7yW6RfU24Hbn3FTgRuCPEcpsAb4FfLsvKyYiIiIiiZWwRNXMhgGHA/cGqx4GJpjZ+PByzrmNzrmlQFOfVlBEREREEsqcc4k5sdkngHucczPC1i0FrnPOvRSh/FeBs51z5/dyzGuAa0LL2dnZox9++OGY1rs3jY2NZGRk9Nn5BjrFKzqKV/QUs+goXtFRvKKjeEXnQIrXvHnzNjnnSiJtS/QjVLtnyfv1GBXn3AJgQcfBzJrmzZu3Y3+OGaUcoLYPzzfQKV7RUbyip5hFR/GKjuIVHcUrOgdSvIp72pDIRHUDUGJmKc65VjMzYAywPlYncM6lx+pYe8PMNvb0F4HsTvGKjuIVPcUsOopXdBSv6Che0VG8vISNUXXObQeWAV8OVp0HlDnnyhJVJxERERHpPxJ91/+VwJVmtgr4HnA5gJktNLMjgveTzGwjvkv/LDPbaGZfT1iNRURERKRPJHSMqnNuJXBshPVnhb1fAwyUpu8Fey4iYRSv6Che0VPMoqN4RUfxio7iFR3FiwTe9S8iIiIi0ptEd/2LiIiIiESkRFVERERE+iUlqiIiIiLSLylRjQEzm2JmS8xslZktNbMZe97rwGJmZWb2oZmVBq8Lg/WKHWBmtwQxcmZ2cNj6HuNzIMeul3hF/JwF2w7keGWY2d+Cay81s6dCj6s2s2HB8mozW25mJ4Tt1+O2wWwP8XrBzNaGfca+HbbfARkvADN72szeDWLyspnNDtbrO6wHvcRM32PhnHN67ecLeA74avD+fOCfia5Tf3sBZcDBil2P8TkJP7tFlzj1Fp8DOXa9xCvi50zxIgM4i84baP8NeDp4fyfwk+D9kcA6IGVP2wbzaw/xegH/OO9I+x2Q8QquNz/s/bnA28F7fYdFHzN9j4Vfc6IrMNBfwDCgMuyL3YCtwPhE160/vSL9w1Pseo9Tb/FR7CJ/rnr6gle8dovHEcBHwftaoDhs21LglD1tO5Be3eLVW6KqePnrvgR4U99h0ccseK/vsbCXuv733xhgs3OuFcD5T896YGxCa9U//cXM3jOzP5hZMYrdnvQWH8WuZ90/Z6B4dXc18ISZFQFJzrkdYdvKgLG9beuzWvYfVwNPhC3/MviMPWBmEwEULzCzP5vZBuAX+MRL32F7ECFmIfoeCyhRjY3uk9FaQmrRv53knJsFHA6UA3cH6xW73vUWH8Vudz19zkDxAsDMfgBMAf4jWKXPWC8ixOti59x04FDgZeDvYcUP6Hg5577inBsD/BD4ZWh1t2L6fIXpIWb6HgujRHX/bQBKzCwFwMwM/1fP+oTWqp9xzq0PfrYANwMnotjtSW/xUewi6OFzBooXAGZ2HfA54JPOuXrnXHmwvjis2DhgfW/b+qq+idY9XgDOuQ3BT+ec+w0w0cyKFK9Ozrm7gVOBjeg7bK+EYhZ8lvQ9FkaJ6n5yzm0HlgFfDladB5Q558oSVql+xsyyzSw/bNVFwDLFrne9xUex211PnzPQv1MAM7sGH5MznHOVYZv+D7gqKHMkMAJ4ZS+2DWqR4mVmKWY2PKzMecC2UJLKARovMxtiZqPClj+LbwnUd1gPeolZo77HutIjVGPAzKYBdwFFQDVwiXNuRUIr1Y8EY7geBpLx3RRrgW8658oUO8/Mfgt8Bv8f206g1jk3ubf4HMixixQv4Ex6+JwF+xzI8SrBt8asBWqC1U3OuaODxOseYALQDHzdOfdisF+P2waznuIFnAa8CKQD7fjP3jXOuXeC/Q7UeI3B/9vLxMdlB3Cdc65U32GR9RQzfBz0PRZGiaqIiIiI9Evq+hcRERGRfkmJqoiIiIj0S0pURURERKRfUqIqIiIiIv2SElURERER6ZdSEl0BEZEDiZmVAY3BK+SLzrn3Y3iO8fjnhg+N1TFFRBJBiaqISN873zm3PNGVEBHp79T1LyLSD5iZM7OfmNmrZrbKzC4K2zbPzN42s3fN7EUzmxG27VIzKzWzd8zszaA1NbTtZ2b2lpl9ZGZn9e0ViYjsP7Woioj0vYfMLLzr/6jgp3POHR88zW2pmb2CfyLSvcCpzrn3zOxLwIPAwWZ2CvAfwInOuS1mlhUcZxj+yTVvOed+bGbzgP8FFsb/0kREYkdPphIR6UPBGNWzu3f9m5kDSpxzm4Llv+ET0hr8IxTnhJWtBKYD1wA1zrmfdTvWeGC5cy4nWM4Dyp1zapwQkQFFXf8iIv2Xwz/vO1KLwp5aGcJbbNvwzw4XERlQlKiKiPQfl0FHi+gJwCvAP4HZZjY92PYFYKNzbivwBPAVMxsRbMsK6/4XERnw1A0kItL3uo9R/Ubws8nMXgWKgW845zYAmNnFwF/MLBmoBC4AcM69ZGa/AJ4Ohg40A+f31UWIiMSbxqiKiPQDQaKZ65yrTXRdRET6C3X9i4iIiEi/pBZVEREREemX1KIqIiIiIv2SElURERER6ZeUqIqIiIhIv6REVURERET6JSWqIgcIM/uBmW03M2dmp5jZUDNbZGb1wWM9+wUzm2xm/zSzJjN7oYcyzszmRNoWx3q9EMxZGotjjQ+uYXIsjjcQmFmZmV0RvN/j9ZvZvWZ2136e8ydm9sr+HGMvztFxXSISe0pURQaBIIlyEV5fCLaPA34BzAdGAkuArwOjgUOBI2NQh1/0lFhG6QdAPTAV+FwMjrcbM7siwcn5Bvzv4eME1iGRYn79ZvaKmf2k2+qbgE/H6hwi0vf0ZCqRweNm4P91W1cZ/JyAf2b8Yy6Yk87MJgJvOec+6rsq7pWJwIvOuXWJrki8OOfagK2Jrkei9NX16+EJIgOfWlRFBo8659zWbq9GM/sq8HxQpj1oaX0BuAT/nHgX6mI1s4lm9oSZ1ZrZZjP7Tfiz480sO1i31cwazOxtMzs6OMd/ACeHteaOj1RJM5tiZk8H+283s1+aWUqwrQw4GfhxcIyf9HK9E8zsZTNrNLM3zeyQsHMcZ2bPm1mlme0ws/vMbGiw7RTgDmBcWF1PCbZNMrPHzKzazKrM7BkzKwg7Z5qZ3WZmNUGX7xd6qpx5/2Vmm4I6rjWzK4NtXbq+g2N1bw0vCzvW4UGreUNQ9qehmPVw7mwz+4OZVQS/y4fNbHjY9ruCrvVfmNmu4Hd9TS/H+18zW9ht3TAzazWzTwTLNwfXWG9mK8zswl6Ot1vXv5l9w8y2BXH/Ff4Pq/B9vm9mHwTHX21mV4dfD3A8cH147Kxb13+s49LDtR1lnUNXNpjZd8K29faZyDCzO4J/Ew1m9qGZnRvNuUUGIyWqIoPfAwTPhsd3t47Ed6k/DDwYLH/TzNKARcBq4BPAZ/BDAn4VdqzbgTnAV4CDgf/Ef488gG/R/WfYOTZ0r4j5Z9U/BjQBRxEky0DoP/MjgaXBOUfiu2578jPgFuBwfBfyo8HxAXKA3wNHAJ8ExgC/C7YtAa4FNobVdYmZpQNPB9dzKnA08AgQOibAlcCHwGHAXcCfzGxYD/X7PPBFfOynAZcD23ooe2RYXcYB7wMvA5hZEbAYWAgcAnw1OO61PUYG/gef8H8GOAk/xOOebmU+DaQCxwA/AX5lZof2cLz7gTlmVhi27nzgY+fcW8FyOfAF/Ofi18A94X889MbMTgYWANfj457J7l32TcDXgJn4P4puMLOzgm3fpOvnpqehLLGOS/fryMX/nlYAs/Gf6+vN7ItBkd4+E1fj/919EpgBfBuo3pvzigxqzjm99NJrgL+AF4BmoLbba2KwfY7/595ln3uBu8KWvwK82a3McfgEIRnfJe+AI3qowy+AF/ZQz3lAA1AYtu5fgB1hy68AP9nDcRzw32HLeUAdcHYP5Y8BWoDkYPkKoKxbmUuB7UBWLzFeGLacsodzXgs8Q/AEwG7bxgfXMDnCttuBd0L1AH4MPNStzBeBj3o4b25wrWeFrTsoON/MYPkuYEW3/VYC/9bDMQ0oA67oFo9f9PI7egr4cdhyx/7drx//h8793WK7MfzzGeH4twJ39va5wSear8QrLhGu61+ATUBK2Pb/Bt7Yi8/Er4E/9va510uvA/GlFlWRweMOfCtO+Gu3Vs1eHALMCrpEa82sFt+Sl4ZveZqJH17w5n7UcRqw2jm3K2zdP4Gh3Vrr9sbS0BvnXBU+oZgGYGYlZnZP0LVaAzyLT35G9HK8g4Glzrn6Xsq8F3bOVmAn0FOL6sP4lrEPzOx/glbDXgXdwOcB54bV4xDg091+L38ExptZpO/wifhrfS2srh/ixytPCyu3vNt+W3u6Fuecw7e+XxjUcwRwIj7BDNX9EvNDMHYGdTwd35K9N6bR9ffZCrwdXsDMPmX+hqltwfEvi+L4EIe4RDANP+67NWzdP8OO39tn4h7gfDN7y8xuCA2pEDnQKVEVGTwqnHMfdXu1RLF/DvASXRPdWcAUYAu+Vc3tZx1tz0X2Wm91uQvfhT4f3w18frA+tZd99qZu3ePp6OF71DlXho/dD/GxfcLMft3jyc2OxXdNf8E5F343fA6+6z3893IIcJBzrn0frwOiuJbAA8CpwVCHzwMrnXPvBXU/Ef+H0j3AGUEdn6H3eHevc4+/T/M3/j0CPAd8Cj/04s9RHD90jr0RbVz2+hy9fSacc0vxNz3ejP/svmpm1+3leUUGLSWqIhLyDr4rdGMPCe9yIMfMjuhh/xa6jueM5ENgSrfW02PxXf+7etinJ0eF3pjZEPx0ViuDVccAC5xzzwStZkP3oq7vAUda2M1j+8s5V+ece8g59zX8cIPLI5ULWigfwneVL+62+R1gRoTfSU+zNawBWvExCB3/ICAfH/99vZa38GOBz8OPsbw/bPPRwPvOuf91zi0D1gKTojj8Srr+PpPxyWjI4UCDc+7Hzrk3nXOr8UlduD19/uISl24+BD5hXW90Ozb8+L19Jpxzu5xz9zjnvoQf8nFZjOolMmApURUZPLLNbES3V3YU+/8FP871ATM70vzE++eY2U0Azrm1wF+Be83sDPMzBJxrZqH/+NcB08zsIPMPE4j0/fI0Ptm5y8wONrNPAj/FtyJF6xIzO9/MpuNb87bhx0WCT0ouNj/DwDz83Kzh1gHDzeyIoK6pwbXVBtf/CTObamZXWjBbQLSCrvCvmtl0M5sKnEtnIt3dQ/hE+d6w311xsO23wKTgjvBZZjbNzC4wsx9GOpBzrga4E7jZzE40s8PxLcyLnXPv78u1hHkAuAo/djk8UV2D/92fbWbT8OMtextm0d3v8d3e84P9/xefQIYff0gQz8nBtXe/YWodcIyZjbauMzUAcY9LyF+AdOD3wb+Di4BvEHy+e/tMmNm3zezzwWf2EOBMev68iBwwlKiKDB7fwnfRh7++sbc7B/+Rn4JPVhfjW/J+ERwnZD5+qqv78C2sPwJC3c8P4ccZvgHsAMZGOEc7/o7rzKDc3fgu3Bv3tp5hfgJcA5Tiu1M/FzY28ApgMj75+zm+qzXcS/hE65mgrsc755qAufjvxZeC+n0O3wq3L6rwD1VYGrwK8XfFR3J8cO7w390bAM65Dfg71McArwbrrwPW93Lua/GzBjwRXMsm4OJ9vI5w9+PHKr/rnFsVtv5vdHb9LwFqgnPvFefc8/hr+gX++lqBx8O2L8Pf6X8jfuzqeOC2boe5CSjCt+Yu6+FU8YpLqJ41wFn4oRnvAL8Efuqc+2tQpLfPRB3+39M7+BvVdgH/Gqu6iQxU5sfIi4iIiIj0L2pRFREREZF+SYmqiIiIiPRLSlRFREREpF9SoioiIiIi/ZISVRERERHpl1L2XGTgSk9Pd8XFxXsuGCNNTU2kp6f32fkGOsUrOopX9BSz6Che0VG8oqN4RedAitemTZuanXMRL3ZQJ6rFxcVs3Lixz863aNEi5s6d22fnG+gUr+goXtFTzKKjeEVH8YqO4hWdAyleZrajp23q+hcRERGRfkmJqoiIiIj0S0pURURERKRfGtRjVEVERET6s/b2dnp6nH1bW1sf1yY+zIykpH1rG1WiKiIiItLHmpubWb9+PS0tLRG3FxcXs2rVqj6uVfykpqYyduxY0tLSotpPiaqIiIhIH1u/fj25ubkUFRVhZrttr66uZsiQIQmoWew55ygvL2f9+vVMnjw5qn2VqIqIiIj0ofb2dlpaWigqKiIlJXIqlpSURHJych/XLH6KiorYtWsX7e3tUQ0D0M1UIiIiIn0oNCY1UkvqYBW61p7G4/ZEiaqIiIiI9EtxT1TNbIqZLTGzVWa21MxmRCjzPTMrDXtVm9mCsO2Xm9lqM1tjZreb2YAZstDa3kpLW+SB0iIiIiL9xZlnnsmhhx7K7NmzOfHEEyktLaWxsZFzzz2XqVOnMnv2bObNm0dZWVmf1akvWlRvA253zk0FbgT+2L2Ac+6/nXOznXOzgaOAZuAvAGY2Afg5cAIwGRgBXN4H9Y6J77z0HT77+GejbuoWERER6UsPPvgg7777LqWlpVx77bVcdtllAMyfP5+VK1dSWlrK2Wefzfz58/usTnFtmTSzYcDhwJnBqoeB35jZeOdcWQ+7nQtsdM69FSyfDzzqnNsWHPNW4Dv4BLhfW7Z9GYvXLQZgR8MOhmUNi1iuua2ZXY27GJE9oi+rJyIiIv3AFXe/wbry+i7r2trbSd7HuUe7G1eUxR8uOXKP5fLz8zveV1VVkZSUREZGBmeddVbH+mOOOYabb745JvXaG/HuQh8DbHbOtQI455yZrQfGAmU97HM5XVtdxwLrwpbLgnW7MbNrgGtCy9nZ2SxatGhf6x61xsbGjvM55/j9rt93bLv/mfuZlj4t4n5P1TzFC3Uv8J2h36EwpbBP6tofhMdL9kzxip5iFh3FKzqKV3QUr66Ki4uprq4mKSmJlpZW2trbu5VwEdbtm5aWVqqqqvaq7JVXXskrr7wCwEMPPbTbfjfddBNnnnnmXh8vpL29nYaGBp555pmo9uuLsZ7d+7x7vMXNzMbgu/gv6uUYPe7vnFsAdIxtLSkpcXPnzt37mu6nRYsWETpf6fZS1j65lkl5k1hTtYb8SfnMnRm5LgufW0hrXStVJVVcNKv7pQ9e4fGSPVO8oqeYRUfxio7iFR3Fq1NbWxurVq1iyJAhJCcnc/cVx+5Wpqqqiry8vD6v2/333w/A3Xffzc9+9jMWLlzYse2GG25g3bp13HnnnWRlZUV13La2NjIzM5kzZ05U027Fe4zqBqAkdPOT+bkJxgDreyh/KfC4c25X2Lr1wPiw5XG97N9vPFX2FAD/fuS/A/BR5Uc9lt1StwWAx9c8rrGsIiIiknCXXHIJzz//POXl5YBvSX3kkUd48skno05S90dcE1Xn3HZgGfDlYNV5QFmk8alBEvtVdr/Z6mHgs2Y2PCjzL8D98apzLLS7dhavW0xJTgnHjTqOwoxC1lSu6bH81rqtAGyo2cCy7cv6qpoiIiIigH8S1ubNmzuWH330UYqKiigsLGTBggXcd999LF68uMs41r7QF13/VwJ3mdkPgGrgEgAzWwj82Dn3ZlDuNHy3/rPhOzvn1prZ9cCr+MT6OSLMHJBoD658kA/qPiD5/VnkF25me/12Ljv4MsyMyfmTWb5zOc653Sb3bWhtoKKpgikFU1hdsZqnyp7i8OGHJ+gqRERE5EBUVVXFeeedR0NDA0lJSRQXF/P3v/+dTZs2ce211zJx4kROPfVUANLT03n99df7pF5xT1SdcyuB3QZfOOfO6rb8LDChh2PcAdwRlwrGQEt7C39+/8+sq1nH/S8sY2JxBgBnjvOTHUzOn8zSrUvZUreFUTmjuuy7rW4bACeMPoH11etZX93vRzWIiIjIIDNmzBiWLl0acVsihyXqyVQxkJqUyl8/9VfGcyQp2WtYX7+CqQVTmVHkn20wKX8SAFcuvpKLF15Mu+u8iy80PnVU9ihGZI/oGAYgIiIicqAbME946u+GpA3hoJbPs7zsDD5/xGj+8+xjO7r5pxZMBaCsuqzj58S8iUDn+NSR2SMZkT2C5TuX933lRURERPohtajGUF2Lw7UOoa4+h9Sk1I71s4pnccMJN/D1WV8HYMXOFR3bQi2qI7JHMCJrBHUtddQ01/RtxUVERET6ISWqMVTX4n+W1zZ1WW9mnDPpHD418VMArCjvTFRDLaojskd0PJlK3f8iIiIiSlRjqq7V/9xZ2xxx+5jcMQxJG8Lyncv5uOpj/rH2H2yp20JWShZD0oZ0JKqhVlYRERGRA5nGqMZQTy2qIWbGzKKZvL39ba578TpWVawiLSmNMbljMDO1qIqIiIiEUYtqrLxzP3Ob/NOoKupbaGmL/Hzeg4ceTFNbE6sqVgHQ3N7MiByfoI7IUqIqIiIiEqJENVZe+z1fbXuYJHyCWlEXuft/ZtFMALJTs/n2J74N+KmpgI4W1U21m7j0qUu5a/ldca60iIiIiHf11Vczfvx4zIzly/0sRI2NjZx77rlMnTqV2bNnM2/ePMrKyjr2efPNNzn22GM57LDDmD59OjfeeGNM66RENVYmnUa+1XKIrQVgRw/d/4cNP4zctFz+dda/cunMS/npcT/l0pmXApCTlkNuai7Pb3ieN7e9yWNrHuuz6ouIiMiB7fzzz+eVV15h3LhxXdbPnz+flStXUlpaytlnn838+fM7tn3ta1/j+9//PsuWLePVV1/lpptu4v33349ZnTRGNUaaJ5xK2isLODHpPd5pm0x5DzdUFWYU8vKFL5OclAzA56Z8rsv24dnD+ajyIwDWVK6hprmG3LTc+FZeREREEuevX4CKj7usymlrg+Tk2By/YAJ88f49FjvppJN2W5eRkcFZZ3U+TPSYY47h5ptv7lKmsrISgLq6OtLS0igsLNzPCndSi2qMVBbOptZlMCfNN5Xv7KFFFehIUiMJdf8bhsPx3o73YltRERERkX10yy23cM4553Qs/+lPf+JHP/oRY8eOZerUqfzXf/0XI0aMiNn51KIaIxVNxjvtMznNlpFLfY8tqnsSSlQ/O+WzPLL6EUp3lHLc6ONYUb6CX735K3518q8oyCiIZdVFREQkkSK0dtZWVZGXl5eAyvTshhtuYPXq1dx6660d6375y1/yy1/+kgsuuIC1a9dyyimncNRRRzFt2rSYnFMtqjFSUd/MS+2HkEw7xyWt6LVFtTenjz2do0cezbcO/xbpyemUbi8F4NHVj/LG1jdYunVpLKstIiIiskc33XQTjzzyCE8++SRZWVkA7Ny5k0cffZQLLrgAgIkTJ3L00UezZMmSmJ1XiWqMVNY380L7LABOS1rW46T/e3LC6BP4w5l/oCCjgJlFM3lv53u0tbfx1ra3AFhbuTZmdRYRERHZkwULFnDfffexePFi8vPzO9YXFBSQkZHBiy++CPjE9bXXXuPggw+O2bnV9R8jlfUtbHDDqc2bwmmVy3iypmG/jzlr2Cze3v42SzYv6bzBqmrNfh9XREREpLurrrqKxx57jK1btzJnzhxycnJ44YUXuPbaa5k4cSKnnnoqAOnp6bz++uskJyfz4IMPcs0119Da2kpLSwvXXXcdRx55ZMzqpEQ1Rirq/WOp6sadwfCq31FYtRw4Zr+OOW/8PP60/E/8/LWfd6xbU6lEVURERGLvt7/9Lb/97W93W++c63GfOXPm8NZbb8WtTur6j5HKet/V76bOBeCQ2n/u9zFnFM1gVvEsttRtAaAkp4Sy6jJa21v3+9giIiIi/Z0S1RipCBLVzAnHUJOUx/Ftr8fkuF846AsAFKQXMGfcHFrbW9lQsyEmxxYRERHpz5SoxkhlfQsG5Gamszz3BKbaBpo2L9/v45457kzG5I7h1LGnMil/EqAbqkREROTAoEQ1RirrW8hOhaQk48Nh/gkOraUP7Pdx05LTeOwzj3H9sdczMW8iAGurlKiKiIjI4KdENUaqGnyiClBZfASbXBFp7z8M7e37fezU5FSSLKkjUX2q7Cluf/d2mtr2ba5WERERkYFAiWqMLPzmiXz/E/7RqHlZ6TzedhyptZtgw2sxO0dOWg4HFR7EqopV/HrZr3l+w/MxO7aIiIhIf6NENUaSk4zMFAMgPyuVxW2f8BvW7//d/+H+ctZfuHPunQC8v/P9Hsutq17HSfefxJLNsXs6hIiIiAxe48eP56CDDmL27NnMnj2bBx7wQxivvvpqxo8fj5mxfHnn/TeNjY2ce+65TJ06ldmzZzNv3jzKyspiWiclqnGQn5XKRlfsF6q3xPTYaclpHD7scDJTMllRvqLHcovXLaaiqYLXtsSuRVdEREQGt4ceeojS0lJKS0u58MILATj//PN55ZVXGDdu3G7l58+fz8qVKyktLeXss89m/vz5Ma2PJvyPg7zMVHaSR7slk1QT20QVIDkpmemF03m//H3aXTtJtvvfG//c7Fty11Wti3iMupY6KhorKMktiXn9REREZO9949lv7Db1ZFt7G8lJyTE5/pjcMfz69F/v8/4nnXRSxPUZGRmcddZZHcvHHHMMN9988z6fJxK1qMZBXmYa7SRRmzoUqjfF5Rwzh86ktqU24pyq9S31vL39bcAPAYjk5rdu5nOPf47a5tqOdb09eUJEREQGvy996UsccsghXHHFFezYsSOqfW+55RbOOeecmNYn7i2qZjYFuBsYClQCX3XO7Ta40sxOBm4CsoBk4FLn3D/N7KvAzUBZULTCOXdqvOu9P/Kz/O3/lSlDGRLjrv+QmUUzAVixcwXjhnRtin9z25sdT69aX7M+4l9lqytX09DawAe7PuDIEUfS2NrIOX87h4sOuojLDr4sLnUWERGR3UVq7ayqqiIvL69P6/HSSy8xduxYWlpa+OEPf8gll1zCwoUL92rfG264gdWrV3PrrbfGtE590aJ6G3C7c24qcCPwx+4FzGwUoKXDaAAAIABJREFUPpn9inNuJjAb+CCsyDPOudnBq18nqeC7/gHKk4ZC7TZoa4n5OWYUzQDgz+//mYsXXsym2s6W21C3/7Ejj6WlvaXjEazhttT6de+X+78ZVlasZGvdVlbuWhnzuoqIiEj/N3bsWABSU1P51re+xcsvv7xX+91000088sgjPPnkk2RlZcW0TnFNVM1sGHA4cG+w6mFggpmN71b068C9zrkPAJxzjc65ynjWLZ5Sk5PITktmG4WA88lqjI0bMo7s1GxWlK+gdEdpx939VU1V/GPtPxiZPZLTx54O7N7939reyrZ6X6cPdvm/Bz4s/xDwwwZ688SaJzoSYRERERkc6urqqKzsTL3uu+8+DjvssD3ut2DBAu677z4WL15Mfn5+zOsV767/McBm51wrgHPOmdl6YCydXfkAM4CPzewZ/BCBl4HvOudCWdPJZlYK1AH/45x7KNLJzOwa4JrQcnZ2NosWLYrxJfWssbGx43zp1sba+kwAXlv8KFW5U2N+vnOyzmFDywZerX+VV999lSEfD+GR6keoaKrgooyL2LHKjy156vWnqM3uHIta0VZBm2sD4I11b7CofhGLqxYDsGH7hl5j9vNtP2dEygiuKrpqv+sfHi/ZM8UreopZdBSv6Che0VG8uiouLqa6upqkpMhths45qqqq+qw+ZWVlXHzxxbS1+fxg3Lhx/OY3v6GqqorrrruOhQsXsm3bNk4//XSys7NZtmwZmzZt4tprr2X8+PEdN1ylp6fz7LPP7nb89vZ2GhoaeOaZZ6KqV1/c9d/9Dh2LUCYVOAWYA9QAdwI/Ab4D/B140DlXb2bTgafNbKNzbrd5l5xzC4AFoeWSkhI3d+7cWFzDXlm0aBGh89384cvsrB8BTXDM9BKYGft6zGUudS11HPPXY8gansWUQ6fw2mOvcfiww/n+vO+zrX4btz10G5mjM5l7dOf539z6JgTfFTvadnDCaSfwp0V/ggbIyM2gp5g55/jePd/rtUw0wuMle6Z4RU8xi47iFR3FKzqKV6e2tjZWrVrFkCFDSE6OfGd/X49RnTVrFu+++27EbXfccUfE9Xl5eXt9I3ZbWxuZmZnMmTOnx2uOJN5jVDcAJWaWAmBmhm9lXd+t3DrgH865iqD19X7gKADn3M5Qy2owNGAhcHyc673f8jNTWdsUNIHH6YYqgOzUbIakDWFL3RaWbVtGu2vnkpmXYGYMyxpGZkrmbl3/oTGrhw49FIdjxc4VrK5YDUB9a89d/y3tLbS6VhpaG+J2PSIiIiIhcU1UnXPbgWXAl4NV5wFlzrmybkX/CpxqZunB8jzgHQAzGx0qZGbDgdOCY/ZreZmprGka4hfiNEVVyMjskWyp20JZdRkAk/InAZBkSYzNHUtZVVmX8ptrNwMwZ9wcAB5b8xjN7c1A72NUQ9sa2xpjWX0RERGRiPrirv8rgSvNbBXwPeByADNbaGZHADjnlgBPAKVm9h5QDPw42P8qM1sRjFFdjB+j+lwf1Hu/5Gelss0V+IU4TPofbmT2SLbVb2Nt1VpSklIYndOR2zMqZxTb6rfR7to71oVaVM8Ydwa5abk8vubxjm29JaqhltTGViWqIiIiEn9xH6PqnFsJHBth/Vndlm/ET1/VvdwPgB/ErYJxkpeVSjOptGUWkVy9Oa7nGpE9gtb2VpZtW8aY3DGkJHX+WodmDqXNtVHZVElhRiHgW1QzUzIZnTOaW+fcypWLr6S2pZbJ+ZP5uOpjnHP4URpdhYYFKFEVERHZd6H/Yw+kB+2ErjVSftEbPUI1TkJzqTZnDicz3l3/OSMBqGmp4YghR3TZVpRZBEB5Q3lnolq3mVHZozAzDi0+lLvm3UXp9lJKd5TyUeVHNLc3k56cTncdLaptjT0+ulVERER6l5SURGpqKuXl5RQVFUVM3trb2zvuwB/onHOUl5eTmpra4ywHPVGiGif5mWkA1GWNInPzC9DSCKkZcTnXyOyRHe/H543vsq0oI0hUG8uZwhTaXTtbardw1MijOspMK5zGtMJprKpY5evcUhcxUQ0fFtDY2khWamwn9RURETlQjB07lvXr17Nr166I2xsaGsjMzOzjWsVPampqxwMFoqFENU5Cj1HdmXsQQ9ufgW3LoeSIPey1b8IT1QlDJnTZNjRzqK9Hw04AdjXuorm9ucs41pDs1GzAJ6Sh1tdw4TMCNLYpURUREdlXaWlpTJ48mfb29ohDAJ555hnmzJmTgJrFnplF3ZIaokQ1TvKDrv+NGdM5CGDzsrglqiOyR3S8361FNazrH+Djqo8BGJM7ZrfjZKb6v9x6mqIqfFoqjVMVERHZf70lcNHMNzpYaZBhnIzK90nfey5o4dz0dtzOVZxZTIqfqpZxQ8Z12Rbe9Q90zJc6JX/KbsfJSvEtpD3d+d+9619EREQkntSiGiclBZmkJBkrqtIgbyxsjl+impyUzPDs4dQ011CQXtBlW/cW1Y8qPwJgSsHuiWp4138k4S2qDW2a9F9ERETiS4lqnKQkJzG2KIu1O+tgzGHw/uPQVAvpOXE532UHX0ZzW/Nudw5mpWSRmZLZkaiurlhNXnpex9jV7mWh567/8PUNLUpURUREJL7U9R9HE4dms768nraRhwEOtrwTt3NdMO0Cvjzjy7utNzMKMwopbyzHOcdHlR8xJX9KxKkwQi2qdS11Ec/RpetfT6cSERGROFOiGkcThmbT2u7YmTvDr9icmCe/FmUWsbNhJ1vqtlDbUhux2x/ouIs/1HJaur2U77/8fZrb/ONVdTOViIiI9CUlqnE0Yajv5l+ZPNmviOM41d4MzRhKRWNFxzypk/MnRywXfjNVfUs933npO/x97d/5cNeHfn1413+ruv5FREQkvpSoxtGEob4r/aOqJCiaEtc7/3tTlFlEm2vjja1vADC1YGrEcqEW1bqWOn697NdsqdsCQEVjBdCtRVVd/yIiIhJnSlTjaGKxT1Q/3lkHow6Dio+hoaLP6xG68/+ljS8Be25RrWyq5L4P7yMj2T9Jq6LJ1zl8jKpuphIREZF4U6IaR8Ny08lKS/aJ6ujD/coEjFMdmuHv8C+rLuO4UceRkxZ55oHQzVSrK1bT5to4tPhQoLNFtfuTqURERETiSYlqHJkZE4Zms3ZHLYwKEtUEdP+HWlQBrpp9VY/lMlP8QwpWV/qHAhw89GCgh65/3UwlIiIicaZENc5KCjLZWt1I2/CDwZIT06IazJl6UslJHa2kkSQnJZORnNExPdUhQw8Bunb956bmArqZSkREROJPE/7H2dCcdNodVLSkMHTY9IQkqocMPYSrD7uasyeevceyWalZHd36UwumkpqU2qVFtTCzkJqWGnX9i4iISNypRTXOhuakA7CztglGzYbqTVCzrU/rkJyUzNcO/Rojc0busWzohqokS2JkzkgKMgq6jFEtzCgE1KIqIiIi8adENc6G5gaJak1z5zjVBE38vzdCN1QNzxpOalIqBekF7GrcBfg7/fPS8kiyJI1RFRERkbhTohpnxTlpQNCi2nHnf2LmU90boblUR+eMBqAgo4DKpkpa2lpoda1kpWaRkZyhRFVERETiTolqnBWFd/0PmwnJaf26RTXU9R+eqNa21FLVXAX4mQEyUjLU9S8iIiJxp0Q1zkJjVHfUNkFKGgw/2E9R5VyCaxZZR4tqbpCophcAsKl2U8f2zJRM3UwlIiIicadENc6Ghrr+a5r9itGHQ/1OqNqQwFr1LNSiWpJTAvgWVYDNtZuBoEU1WS2qIiIiEn9KVOMsJz2F9JQk3/UP/lGq0G+7/0M3U4W6/kN3+Xe0qKYELaoaoyoiIiJxpkQ1zsyMoTnpnYnqyNn+59bliatUL2YUzWBY5jAmF0wGID89H+ja9Z+RopupREREJP404X8fGJqbztaqoKs8K3icaVN14irUi89M/gyfmfyZjuWIXf8pGRqjKiIiInEX9xZVM5tiZkvMbJWZLTWzGT2UO9nM3jCzFWb2oZkdG7bth2a2Jnj9PN51jrXinDTKa5tpb3eQ5seA0lyX2ErtpVDXfyhRDXX9N7Q24PrpDWEiIiIyOPRF1/9twO3OuanAjcAfuxcws1HA3cBXnHMzgdnAB8G2k4CLgEOBGcAnzWxuH9Q7ZobmpNPa7qhubIFgDCgt9Ymt1F4KtahurN0IdN5MBdDU1pSweomIiMjgF9dE1cyGAYcD9warHgYmmNn4bkW/DtzrnPsAwDnX6JyrDLZdCNzlnKtzzjUBd+IT1wGjy2NUk1P8XKoDpEU1Ly0Pw2htbwUgJy2HjBSfqOrOfxEREYmneI9RHQNsds61AjjnnJmtB8YCZWHlZgAfm9kzwFDgZeC7zrn6oOyLYWXLgPMjnczMrgGuCS1nZ2ezaNGimF3MnjQ2NkY8346N7QAsfO4VphUkcaqlUbN1A2/2Yd32x2nZp7GtdRvDU4az+e3NbKvZBsCi5xZRkFywz8ftKV4SmeIVPcUsOopXdBSv6Che0VG8vL64mar7QEaLUCYVOAWYA9TgW01/AnwnwjEi7e8LObcAWBBaLikpcXPn9t0ogUWLFhHpfM3vbOaB1cuYMH0Wc2eNghX5FOVmRCzbH82laz3XLlvLS+++xFHHH8XEvIn7fNye4iWRKV7RU8yio3hFR/GKjuIVHcXLi/cY1Q1AiZmlAJiZ4VtZ13crtw74h3OuImh9vR84Kti2HhgfVnZchP37tS5d/+BvqBogXf+RZKZkAmiKKhEREYmruCaqzrntwDLgy8Gq84Ay51xZt6J/BU41s/RgeR7wTvD+/4BLzCw72H4ZPpEdMIYN8Ze1tTpI7FKzoHlg3EwVSehmqufWP9cxv6qIiIhIrPXFXf9XAlea2Srge8DlAGa20MyOAHDOLQGeAErN7D2gGPhxsO0F4EHgPfxMAE87557qg3rHzKg83wK5pTJIVNNyoGXgtqgOzx4OwG3v3sZ3X/pugmsjIiIig1Xcx6g651YCx0ZYf1a35Rvx01dFOsbPgJ/FpYJ9IDMtmYKsVLaEJv1PG9gtqnPGzuHBsx/k+iXXs7ZqbaKrIyIiIoOUHqHaR0blZ7K5Mqzrv7UB2tsSW6l9ZGZML5rOQYUHUdNcQ1VTVaKrJCIiIoOQEtU+MjIvk63VjbS1O0gbWJP+96QktwTofBiAiIiISCwpUe0jo/MzaGt37Khp6kxUB3D3P0BJjk9UN9RsSHBNREREZDBSotpHRub7G6o2VTb4rn8Y0DdUQViLao1aVEVERCT2lKj2kVFBorqlqiGsRVWJqoiIiEhPlKj2kdH5fu7RzeEtqgO8678gvYCslCyNURUREZG4UKLaR0YGc6lurmwMu5lqYLeomhkluSVqURUREZG4UKLaR4blppOcZL5FdZB0/YO/oWpr3VZa2lsSXRUREREZZJSo9pGU5CSG56azpapx0HT9gx+n2uba2Fq3NdFVERERkUFGiWofGpWfyYaKeupcul8xwLv+ofOGKk1RJSIiIrGmRLUPHTQyl8r6Fq64/wO/YhC0qI7IGgHAjvodCa6JiIiIDDZKVPvQj8+eyQ2fPYSKllS/YhCMUS3MLARgV+OuBNdEREREBhslqn0oLSWJC44ooYHB0/VfmKFEVUREROJDiWofS0lOIiMrxy8Mgq7/oowiAMobyhNcExERERlslKgmQGZOnn/TMvAT1azULDJTMtWiKiIiIjGnRDUBhuQO8W+aaxNbkRgpzChUoioiIiIxp0Q1AYYOyaTBpdHaOPDHqIJPVMsb1fUvIiIisaVENQGKc9OpJ53WxsHRolqUUcSuxl045xJdlf3W0t5CRWNFoqshIiIiKFFNiOKcdOpdBm1NgyNRLcwspLW9lerm6ojbW9pbuOXtWwbE06vuef8e5j48l8rGykRXRURE5ICnRDUBQi2qrmnwdP1D5xRV/1j7Dx748IGO7cu2LeOO9+7gbx/9LSH1i8bqitU0tDZQVl2W6KqIiIgc8JSoJkBxbjoNpGOD4K5/2D1R/W3pb/ndO7/r2L6xdmOX7f1ZaJqtgdD6KyIiMtgpUU2AYbnp1Lt0klob/IrS++DWEwfsvKqhuVR3Ne6ivqWejTUbqW6u7hizuql2U8f2/i5Ux811mxNcExEREVGimgDFORnUkUFqe5CYfvwibH0XtryT2Irto9BjVMsbyvm46mMcjtb2VpramoCBlaiGZi/YUrslwTURERERJaoJMCQzhSbLIMW1QlsL1O30G7a+m9iK7aPwrv/Vlas71tc01wCwqSZIVBv6d6La7to77vjfUqdEVUREJNGUqCaAmdGaGjxGtaES6n2iumzpi7y8ekfknWq2wbM/94ltPxPe9b+mck3H+lCiurl2c8f2/qyqqYo21wao619ERKQ/UKKaIA3pQ/2buu1Q57ub03Ys59G3N0XeYfnD8PJNsPGNPqrh3stPz8ew3VpUq5uraWprYnvDdgAqmyppbW9NVDX3KHQjFcDWWt1MJSIikmhxT1TNbIqZLTGzVWa21MxmRCjzVTOrNLPS4PX83mwbyFozhwHQXr2to0V1im2ktbUp8g6hGQL64UwByUnJFGQUUN5QzkcVH3Wsr2mu6WhNBXA4Kpv67/ykoRZfw6hpqeloERYREZHE6IsW1duA251zU4EbgT/2UO4Z59zs4HVqFNsGplyfqNZtX9ORfKZZG0V1H0cuH0pge0pkE2x0zmiW71zOtvptZKdmAz5RDd1INTTTtyDva/f/2qq1/ODlH9AQmikhDkI3Uk3MmwjQJckWERGRvhfXRNXMhgGHA/cGqx4GJpjZ+HiedyBIGzISgObNywGoyRgFwMiGVZF3CCVoLfFL1PbHfxz9H6SnpAMwu3g20LVF9ZChhwDs8+NJn/z4SZ5Y+wTv7XgvBrWNLNT1f/DQgwHNpSoiIpJoKXE+/hhgs3OuFcA558xsPTAWKOtW9mQzKwXqgP9xzj20l9s6mNk1wDWh5ezsbBYtWhSzi9mTxsbGvT7fxnLfrVy/9nWKgGVM5yQ2U1y9POIxDvp4NeOA90rfYvPm3BjWOnYuy72MJ2ueZFzdOF7lVd5a8RYNzifWGRUZADz/+vNUZvru/2ji9XbV2x37V2TuW7K7J2/WvAlA0g7/99uzbzxL4/uNcTnXvogmXuIpZtFRvKKjeEVH8YqO4uXFO1EFcN2WLUKZvwMPOufqzWw68LSZbXTOvbaHbV1P5NwCYEFouaSkxM2dOzd2V7IHixYtYm/Pl7FiPfwfDGv1LY6b8w+jecsLjEqu4uhIx3jsCdgKhxw0mUOO7LtritYVXMGG6g389dG/MmL8CD81VR189tjP8uTiJxkzbQxzp/v6RxOvh59+GBpg7EFjmXtQfK5/yZIlsBouOPECHv3Ho+SPy2fuJ/pPrKOJl3iKWXQUr+goXtFRvKKjeHnxHqO6ASgxsxQAMzN8K+v68ELOuZ3Oufrg/QfAQuD4PW0byIYV5lPtMklvqQJgQ1M22ymgoC2YU/W1W2Hdks4dOsao9p8Wvp7kpvkW35rmGrbUbaE4s5gR2SOAzu71JZuW8Fztc3t9zG3124B9HzqwN3Y17CI1KZVJ+ZMA2FHfw1RhIiIi0ifimqg657YDy4AvB6vOA8qcc2Xh5cxsdNj74cBpwX69bhvIhg/JYIfL71gua8hkmytgaPtOaKyGp74LS37duUNojOoASFRz0vwcsTXNNWyt28qI7BFdHgrw0saXuOq5q1hYu5C6lrq9OmYoUY3nXKzljeUUZhSSkeyHKYSerCUiIiKJ0Rd3/V8JXGlmq4DvAZcDmNlCMzsiKHOVma0IxqEuxo9DfW4vtg1YBVmplNOZqH5Um8ZWV0C+q4Jda/3K+s55Pfv7Xf/hUpJSyErJoqKxgp0NOxmRPYIhaUNIsRTe2/ke17xwTcd8qlVNVXs8Xm1zbUdCG88W1fIGn6iaGalJqbT0w4criIiIHEjiPkbVObcSODbC+rPC3v8A+EEP+/e4bSAzM2pSC8E/CIktrTlsSykkCdc5qX+XRDVoSe2nd/13l5uWy5rKNTgcw7OGY2YUZBSwqsLPanDUiKNYunUp1c3VjGJUr8cKtaYCVDTFJ1F1zrGrcRcT8/3UVGnJaWpRFRERSTA9mSqBGtP83KIuKYVqstnqCvyG9f/0P8MT1ZYgUR0ALargE9XQvKSh8amh7v/Dhh3GnHFzAKhuqt7jsbbVhSWqjRU0tjaycO1CnOt+n96+q2+tp7GtsaOOaUlpNLc3x+z4IiIiEj0lqgnUmlUMQHNaAWBsdT5JYv3r/mdDJbQHTa6hFtU4TngfS0PShnS8756ozj90fsf26ua9SFTDWlR3Ne7i0Y8e5bsvf5c3tsbucbKhIQgF6f6PhbTkNHX9i4iIJJgS1UTKGQ5AdVIeADWpPnGlemNQwPlkFcIS1YHTohoSSlQ/P+3zXDzjYo4fdXxHoro3Y1RDE++X5JRQ1VTV8ZjWrfWxm5A/lDCH6p2WrBZVERGRROuLeVSlB6l5PoHb3ubvkk8tGAmV3QrVl0N2UVii2v/v+oduiWqWv84zxp3BGePOAGBIevQtqtOLprOxdiPv7fRPpwpNdRULNc01XeqVlqQxqiIiIommFtUEyir0j1FdW59BVloyecPG7V4oNE411JLaMrAS1RRLYWjm0N2256X5VuS9SVS31m8lIzmD8UPGA7CyYiUQ20Q1NFa2S4tqm1pURUREEkmJagLlDJtAk0thvRvGv548ieycIVS5LL8xOR2AdRs2+OWWgdWimpPqW4mLs4pJTkrebXtHi+pe3kw1PHs4BRl+/Gi7awfouFkrFkIJc2hIQmqypqcSERFJtL1OVM3sSjPLC97/1szeNLOT4le1wa9o2EjObr6BR7Iu5IoTJ5KemtR5Q9WIQwD43ZNLWVdeN+DGqIYSvtD41O5CLZdVzVU8svoRLvr7RT0mhtvqtjE8qzNRDYlpi2q3RDU9OV1jVEVERBIsmhbVq5xzVWZ2PHAw8B/ATfGp1oFhXGEWRx11HD+/4Bgy05JJT0lmW2iKqlGHAVBADTtrGiE0XnKA3PUfSkSHZw2PuD01KZV0S6e6qZqXN77M8vLlbK7bvFu5isYKalpqGJ0zmsL0wi7bYtmi2jFGNa1zjKq6/kVERBIrmkS1Nfh5GvBn59widDPWfvn/7L13mFxXff//OtPLzjbtrrQqq5VVLNmW3HHFNm4yEGwTmwAxxJAA+SahhZB8CQFCCAHiH/UbwBCC6Q5gGzA2NnJvkmy5q/eu7WV2er3n98e5d+6d2dnVrqSVtpzX8+iZmTv33jlzZ7X7nvenuVyC/3j7Si5donI4/R6Ho2oJVREnl3GI0yniqFpCdSRHFSAogsRyMTqTnQD0pHqG7bN3SE3pOq3utDJHNeAOTIijaq3b6/Yes1A9FD/EvqF9J2xtGo1Go9HMVMYjVA0hxLuAdwKPm9t8J35JMxe/x8VrcglFbw20XwZAI3FymaS90xTJUbUKqBZEFoy4T8gVKhOqfem+YfuUhGp9uVBd1byKwewgRavP7HFSCv07qv4LslDKhx0Pn1v7OT7x1CdOyLo0Go1Go5nJjEeofhh4F/ADKeV+IcQy4MmJWdbMxO91c3fxajbc+iLULyQvvDSIOPmsw1GdIlX/57acyzff9E1uXnLziPsEXUH60n0MZAaAERzVqBKqi+oWlQYGNAYaaa9tx5AG0WxlP69jI56L43f78ZtFbNbtsbiq0WyU3nTvCVmXRqPRaDQzmTGH7qWUzwM3AwghBNAppfzIRC1sJuL3uABBRnpACBKuWhpFnB6nUJ0ijqoQgmvarhl1n6AIknbk3Pamhou7fbF9+Fw+5obn4na5afA3sLh+MbOCswCVp2rdPx5i2VhZ71efWwULckaOAIFxnStXzJHMJZFSov6raDQajUajORbGU/X/QyFEvRDCB7wGdAsh/nbiljbzUEIVsgUVzo6LWupJUMil7J2miFAdCyFXqOxxT3q4o7ovuo/2uvZSi6tvX/NtPnfx55gVMIXqCcpTjefiZWNfvS4vcGyOas7IUZAFMsXp81lpNBqNRnMqGE/o/3wpZRRYDbwKzAH+ekJWNUPxe5QYyxZUXmRURGgUcQo5h+ApZEDKU7G8E05QBMseVzqqqXyKjmQHi+oWlbatal5Fe117maN6IojlRnBUj0Womsck88mj7KnRaDQajWY0xlO1b8UwrwAelFLGhBDjrzTRjIjfazqqeXVZB4mwSiSR2Xj5jsUcePwne3knHKej6nF5SnmdeSPPhx//MGFvGFAV/5WUhGoVR3X/0H5yRo5lDcvGvJZKR/V4hKrVDzaRS1SdyqXRaDQajWZsjEeodgkhvgfcAPyHEMILDB85pDlmKkP//YZy+PzmrPsS+fS0EKpBl+2ormhcwe7obqSUPH3oadZ1rCs9V1WoBkZ2VP/luX8hmo3yhz/9w5jWkSvmyBQzI+aojpe8oYSqdlQ1Go1Gozk+xhP6vw3YDrzLTAGYB3x9QlY1Q6kM/fcZagypP20KVbN10lTppXo0rNB/2BtmUd0i0oU0yXyS+3bdh1u4WdG4AqCqMzqSoyqlZHd0N53JTuQYUyQqp1KBak8FHNMYVUvcJvKJcR+r0Wg0Go3GZjxV/31CiO8Dq4QQbwA2SSl/PGErm4HYjqoSqr3FELgglDGFaqAOsrFpU1Blhf5bw62lCVYbezey9sharlpwFV9541fYNrCN0+qHO6ohT6hq0//+TD+pgio+S+aT1PhqjrqOymb/YDuq2eL4vhQUDLv3aiKnhapGo9FoNMfDeKr+LwX2AN8D/hvYLYS4ZKIWNhMJlHJUixiGpKegRFZN1hKq9ep2mghVK/TfGm6lOdQMwPc2fg+J5NZltxLyhjh/9vlVjxVC0BxqprsiLeJg7GDpvtWf1ZAGP9v6s9LjSqzxqXX+utI2y1Edb+jfmdOqHVWNRqPRaI6P8YTSJ561AAAgAElEQVT+vw68Q0p5rpTyHOAdwDcmZlkzE2foP50vMohy+GrzZtum4PQSqmGhiqVaw620BFsAeLXnVdoibVw297KjHr8gsoDD8cNl06MOxocL1Ze7X+aOF+/gnh33VD1PLDuyozreYiorPxW0UNVoNBqN5ngZj1ANSCnXWg+klOuA4Cj7a8aJM/SfyhUZkEo41eXNtk0B0/GbJjmqje5GPnnBJ3nvGe8tOaoAH1j5gVLf1NFoi7SRKWbKJlpVc1QPxQ+p5xwi1onlqFar+h9vjqpT2FYWUw1lh+hIdIzrfBqNRqPRzGTGI1RTQohrrQdCiKsAXdZ8ArEd1SLpXJGo5agWB9UOVug/n652+JRDCMHtZ95Oe107LSHlqM4Nz+VPFv/JmI5fWLsQsIUoVHdUD8cPq+di1YVq1WKqY8xRHc1R/c8N/8lfPPwX4zqfRqPRaDQzmfG0p/oocJ8QIgtIwI/qBKA5QTj7qKbyhZKj6sKsXi+F/qeHo+pkdmg2717+bq6af1VpKtTRaKttA+BA7AAXzrkQKBejgxkl8I8kjqjn4geRUvL7Pb/nsnmXlXqcVi2mOhE5qhXFVN2p7lKvWI1Go9FoNEdnzI6qlPIlYAnwp8CtwDLgfydoXTMSn7s89J/CT1Y6RFupmGp6OKpOhBB8+qJPc+m8S8d8TFtECVXLRZVScjB+kPbadsB2VC2hOpAZYG3HWj6z9jPcs9POVy2F/v2OEaruYxuh6hS2lY5qtpjFkAZFoziuc2o0Go1GM1MZT+gfKWVeSrlZSrlJSpnDnlalOQG4XAKf20W2UCSVLQKCAWyXbzo7qsfCvJp5uIW75KIOZAZI5pOsal4F2MMALKEK8PvdvwcgmomWto3mqDpD+WPBmdNamaNqpREcyxABjUaj0WhmIuMSqlU4akd1IcRSIcQ6IcROIcQGIcQZVfZ5nxAiKoR4zfz3ZMXznxFC7DH//ftxrnlS4/e4TEe1AMCgdAjVUjHV9Kj6P168bi+t4VYOxA4Adq7qkvol1HhrGMwMki6k6Uv3lYTnE4eeAGwXFVTVv0u4qPHaPVf9bjX567gc1YrQf8b83I5lLKtGo9FoNDORo+aoVhOW4zke+D7w31LKHwshbgV+CFTrv/qYlPLWKq9/BfBuYBVQANYKIZ6TUq4Zw2tPOfxeF9m8ak8FMCgdDetLxVRaqFosrF3Iy90vY0iDnYM7AVhUt4jGQCMDmYFSlf15s8/j+c7nS66mU6gO5Yao9dXiEvb3tmMtphqt6t96brwurUaj0Wg0M5WxOKp/GOXfqIpJCNECnAf83Nx0H7BICNE+jjW+E/ixlDIppcwCd6GE67TE73Gr0H/OFKpoR3U02mrtFlWb+zYDcFbTWTQEGhjMDJbC/pfMLf9uZIX7QbWNcjb7h2PPUR2t6j9T1I6qpgqv/xLu/UswjKPvq9FoNDOMozqiUspFx3H+BUCHlLJgnksKIQ4CbcD+in2vFEK8hmp59Q0p5b3m9jbgacd++1HFXMMQQnwC+IT1OBwOs2bNyTNeM5nMcb9eIVugL5/mlY1bAIgJJVQN4eb5l17jUmDX9s3sTUx9Q/lEXK9UUo1Lvffxe1kfX0+9q56Xn3mZQqxAf7afRzc8CkByb5KQCJGSav/Ogc7Sa3cPdVPnritbS39B5bfu3LOTNb1jX+PmzObS/cHkYNk5k1nlsD7x9BM0e5qHHXs0TsT1mmlMhWt27rbv0xJ9hWe8V5IOzBnzcS39G8j6GhiKLD1ha5kK12syoa/X+NDXa3zo66UYT3uqY6Uyj7VaAdaDwK+llCkhxArgESHEYSnl81XOMWIBl5Ty66gJWgDMnz9frl69+hiXPX7WrFnD8b7eN7Y9Q7ZgsPC0ebB7J3l/I+RB+MJcesXVsBGWts9n6TUn731NFCfierX2tnL/Q/cz2DxIT7SHa9quYfVVq1m/bj1bdm2B2UAcbr7qZtY/u55NfZtoDDSCm9Jrf/YXn6WtpY3V19pr6Un18OV7vsy8tnmsvmjsa5T7ZelrVY4c119/PUKoH9lP/exTIOGiSy9iacP4xcWJuF4zjSlxzQ7+J0ThiiV1cOY41vqlv4T5F8KtHz5hS5kS12sSoa/X+NDXa3zo66U43mKqo3EImC+E8AAI9Rd7AVDWeV1K2SelsrqklNuAhwBrhuZBoN2x+8LK46cTfq+bbN4O/ef9DeoJtx88AXVfV/2XWNm0krnhudy36z4MabCyaSWAEqPA672v4xIu5tTM4a2nvZXV7atZ2rC0lKOaLWZJF9LDQv9WMdW4G/6bVf813hoKslA6vmgUKRiqQO5kVP13DqW57utPs60zdvSdNaeWZJ+67do49mPyGcglIBs/+r4ajUYzhZlQoSql7AFeBd5jbroF2C+l3O/cTwgxz3F/NnC1eRzAPcDtQoiwEMIP/CXwy4lc96kkUKr6V0LVCCrBZXgCDqGqc1QthBDcsOgGilJdr7OazgJsobpvaB+rmlbhdXm5bcVtfPXKr1LrqyWRT1A0isSySsjV+SpyVM2hA+MtfLLyTxsC6guGlafqFLzjHct6LGw5EmNXT4JXD0aPvrPmxFPIwnPfhNwYhvelVJoJXZvGfn6rvdo0mVKn0Wg0IzHRjirAXwN/LYTYCXwK+CsAIcRDQogLzH3+TgixxcxRfRSVo/oEgJTyKeDXwCZgG/CIlPKPJ2HdpwS/113WnsoVmgWA4faD1xSquuq/jBvabwBAIDhjlmpSYQlFt3Dz6Ys+Xba/NSo1kU8wlB0CGOaoWlX/x9qeyhLKVuW/VUgFJ6fqP19UhTm5gh4ucErYuQYe+1fY/tDo+1nOKEDnOBzVtDlW2dlZon8P3P0uOLB+fGvVaDSaScyE56hKKXdQpR2VlPItjvufBj5duY/j+S8AX5iQBU4y/B4XGUfo3xUxharLpx3VEVjeuJzTG07H7/YT9oYBWBBZAMDtZ97Oilkryva3+qUm8gmGctWFqsflwSVc46/6N93SkqNqihDneU5G1X/ekOZrTcNKcsOARDfUth77OQ4+D7074PzbT9y6nFiOp/lFyN4+BPd9EK7+DLSuglSf/VyiCxI9UNNy9POnzfPnUva2fU/Dzodh1yPw1q/BBe8/vveg0Wg0k4CT4ahqxoHf46JgSBLZAj63C2HOoy+6A+DygHBpoVqBEIK7briLO6+7s7RtZdNK7nnbPXzsvI8N29+aQBXPxUuOquWyOvG5fOPOJ7X2b/CXh/4zjs/spAjVghKo+eJRZ3JMPbb8Br5xhnIQj5XnvgkP/j3ICbo+Vu5oZej/0Iuwaw3sVt0oSvmpNWa1v+WqbnsQhg6PfP6So+oQqpZ4dXngiS9O3HvTaDSak4gWqpMMv8cNwGAqT9DnhpAKIRddPhACPEEtVKtQ66stE5tCCJY3Li9r4m9RTahWOqqgwv/VRGW2mOXxA4/zUtdLpAvlOYLDHNUqOaono5jKCv1nC9PQUR3cB9IYXcgdjVwCZHHiChNHEqqxw+XbLUd18ZvUbdfrkOiFX90Gz36dESnlqKZsQWptO+1Kdd6Bvcf3HjQajWYSoIXqJMPvVR9JNJUj5HPj8YcYkDVk/Uqw4vHrHNXjxBKqsVys1Ph/PEL1/t338/GnPs7717yff3jqH8qeGylHtUyonszQ/3QUqlYB0fEUEllCsTBBxUiZWPnrWMTUpDSyZl5q0iykWvAGdTt0GJI96n6ie+TzW46qNGyxbTmqS69Xt4c2HNvaNRqNZhKhheokw+9RH8lgMkfQ58bv8/DO3OfYfs4/qx2C9fYfKc0xYTmvZY6qr4pQHSH0v2twFwBNwSa2DWwre25Y1X9uuKNqtamaSOzQ/3QWqqnR9xsNS0BOVNV81hKq5dPJiB0pf33LUW1apm6TfXYXgNTAyOd3/g6wrkPGzIddep26PfQ8Go1GM9XRQnWSEfSq0H8sUyDkcxP0utkl5zPkNicZ1cwe3WnRHBXLUU3kRq76h5Ed1YPxgwTcAS6ccyF96b6Sawq2ULUc1VOVo1owrKr/6SxUj0NkWuJuwoTqSKF/01E1+/iWRGnNbAg2qMclodo/8vnTjrZj1mtkouCvhYZ2qJ2nHVWNRjMt0EJ1knH9mfYIxZDPQ8AUrpm82WYo3Kz+gBUn3pWbrtT4VNV/PBcvVf1b4tWJz+2r2krqQOwAC2oX0F7bDsCh+KHSc9b+pz5HdSaE/k+Eo3oc5xgNS6hmKx1VS6iar28VU4VmQahJ/d+2tqXH6qia1yMdhUC9ur/gDdCzrVzQajQazRREC9VJxjkL6ll95mwAQj43ATNnNWP1w6xpAeTobotmVKzQfywXYyg7RMQbweMa3qnN5/INm0z12qE+OhOdLIwspK22DYCDMXtQWkmomlX/ydypyVG1BOq0bE91ItzQklCdoHzvkqM6glC1BGyqH4RbCcxwkxn6NwVqakC14qpGxiFA8w5HNWBGBhZcBEg4/NJxvxWNRqM5lWihOgn55PWn4xLQGPI5HFXzD1bY7LFoFVxoxk1l1X+tf3hrKrBD/1v6ttCTUtf7A3evwcCgrbaNtogpVOO2UB3LZKqT4aiWQv/TUqgeZ+i/WADr85gwR7VKMVUmNjx3NdmnOnu4XMpVTQ9Aslc9J4vD+7BaOB3VnCNHNWg6qvPOV7edr6LRaDRTGS1UJyFLZ0e4728u5Z9uWE7AbFe1fk8/N377ORJes/pf56keMyFPCJdwEc/FieViVXuoAnjdXlL5FO/74/v42BMfQ0pJwugCYGHtQhbWLgTKHdVcMYdLuAh6gvhcPnsylSNH9WSMUNWh/9GOd4jHsYrdwy/Br9479nZW1XJULTcVbKGa6lMhf1BCVRrQv9veb6SCKmdIP59WzmtmyHZUZ58JiPFNu9JoNJpJiBaqk5Rz2xqYUxcohf4f29bNxsND7MuoyUskek/h6qY2QggivgiJfIJYNla1kApU6D9TzJApZtjcv5kNXRsw3Oq6t0XaqPPXUeurLXdUjRw+lxq/WuOrIW4WzTjD/Sd3hOp0FKqp8tvx4pzmNNb2VK/dDdt+D307x7Z/VaF6xPG8w1ENm0LVunW+xohC1ZmjmlTFWdKwHVVfGJqWQtemsa1Xo9FoJilaqE5yrNC/RT+mqNKh/+OixlvDYHaQeD4+olD1u/1lj3+46S6ETxW6WG5qW6StPEe1mMfr9pZeo+SoFk/yZKqibk818vGO48bqqPbvMvcfQ06rYVTPUbUcVeFWAraYV3mlITUmueSsOgVttVx0w1DHubzma6Rsh9UqpgKYs0oNR8iMkD6g0Wg0UwAtVCc51gAAix7DFFUJLVSPh1pfLR0JJRyq9VAFSoIT4PzZ57O+cx2eyGaE9NNkjrZtq22jN91LyhQ/eSOP1xQQYW/41OWoOkP/hgEbfjB9KsCPt5jKKR6dojXeDYURPhtrXOtYHNh8EjCnRVUL/Teeps5j5aJWOqpOqglVyz2tbbVfzxKjTqHaukrddm0++po1Go1mkqKF6iQnWOGodhbMNkpaqB4XEV+k5Hae3nh61X2sEH69v54vX/5l2iLtuDxJXIVmhBAApcp/q0VVrpjD57ZD/9UmU52MHFWriCpXNODwi/DQJ2Hjryf8dU8Kx1tMlaviqKYG4P+dA89/t8r+SdvlHIujarmpAEZevd66b8P+Z9U2q7n/4AF168xRtbAEZ6ofNt4DPdvt56ywf+08+z1YXQCCFY4qQJfOU9VoNFMXLVQnOZWh/56MC3wRHfo/TqzK/2varuHWZbdW3ccSnCsaV9Ba08r3rv4x+eh5eFOXlvaxKv8Px9UMd2eOquWoSinJFk5hH1XLQczFRzliCnFCi6lM4RnrUOezQvxOnMVNhTEIVWt8qsWOh+CRf1FC1ROA+gVqe9QUquEqQtUSsz3b4DcfgOe+YT9nOeO1c9XtSKH/1rPVrS6o0mg0U5jhzSM1kwqv24XbJYgEPERTeaKpPNQ062Kq4+SGRTfgc/n4/KWfxyWqf1+zhOryWcsB8LvCZDr/jEjEzl1tDavwa2eyE6hwVL01FIwC2WK2LEf1ZDiqBaejaonkieoZepx0DqXZ15fk0sVVQt+VSHkCQv9OoWqey3IpU1XGE/c5xOtYhGrJURWAtIXuihthyTUQNQdEDO5Xt5ZAdYb+m5bB4Q2w5wn12JlnOsxRTVZ3VEONUDtfO6oajWZKox3VKcDSlhresrKV2oCHgWROj1E9AdzQfgN3XHkHIW9oxH2cjirY4XRngVJrTblQdeao1njVBKxEPlEK/XtcnpPkqDqq/i1xNdYK95PMtx7bxe13bbCnr41GMa/6i0K5o3rweeh4bWwvWC30bwm9dBWh6nRUxyKOrV6pNWbPY0uQXv5xOP994K8xt1c6qg6h2tiuiq4SXeaaHXm11lotoVrmqFbkW6+8BRZeqgS+RqPRTEG0ozoFePhjb8SQsG53H4OpHLQ2qz/MP3qLCvW9++5TvcRpybzwPHwuH2c3qxCqVaCUd7R8agm1IBDVHVVzVGsynyRbzOJxeQi6gyep6t8R+p/kjupAMke+KEnnisNSXYZRrWJfSvjlbRBphb957ugvmB/FUa02tvRYHdVIq/pCaQlVa1iHzxKq5nZLoHoD6rlcQo1KDjXaBVfOvNeSo2qG/vPp6sVUANd94ejr1Wg0mkmMdlSnAEII3C5BQ9inhKo1RvXAWtjxB9i/9lQvcVpyy7JbeOTWR5hbowRBvjh82pPX5aU51ExnwhSqRq6s6h9MR7WQxe/243V7T2of1Xxx8juqadNJzY6l56vT0bTuDx1SjfP7d488ctSJM/RvXZv0aI7qKELVKELn6+XbnEIVYGCfurWc00qh6gz5l1pVzSrPWbUc1YMvwDNfVfebVUrKiKF/jUajmQZooTqFaAj5GEzmkeFmtcEXAeGCZ+44tQubpnhcHmYFbbFgh/4l0hFKbQ232qH/Yr4sRxUgmVOOqt/tx+vynqTJVM7Q/+R2VFM5JVTHFPp3im3LDbWEYiEN8c6jn6Ms9G+NHzWFXmqgPEyeS6nWVJbrmc8oMWw14t/8G/j+FdDhGFVqhf4jc9Rtokv9X/UG1WMr9G+tNdhoH+tMA3BuzyZM5/jd6rVvvhOal4HbP3roX6PRaKY4WqhOIRpCPnJFg1zYDPld8Uk461bY+xQcfvmUrm0mYIXTK++3hlvpz/STLWaHVf0DxPNxssUsAXcAn9t3cqv+yxzVySlUu8SDhBffQTI3hvGk1RxVp6M5sHcM56gyQtVyUo287bgWcvDrv1Bu5oo/MbelYctv4KtL1WsNmP1VrQIpGO6ogiqAtPCZ0+WQEGwAtyMDy9mqKuQQqrmEWleqH868Gc75c/NcISW2M1HwBMFTPqRCo9FopjpaqE4hGkIqpNzb9ha49UdwyYfhkr9VT26aJj0yJzEFR8i/rKDKqvxPdFIwCmWTqcDOUfV7/PhcvmPKUU3kEmzJbBnz/nmH+2tYTuqxVslPMBlxGJdvgFh2DO2mnO+hYLqblUL11V+ofyNhCVHhsl1m5zAEK0/1+e/C7kfh3PfAFf9ovmbWTDEoQO9OO4fUWZVvCdVah1ANO4VqxL4fquh0EJkDCOWsWqH/mjlqzZkqLai8YVOoDumwv0ajmZZooTqFaAgrp24w74Wz/lQ5Ma3nQEM7Q6/cxz/88pVTvMLpjTM3NefIp5wTViHeg3E1SrWymCqRS5ApZPC7/cpRPQahet+u+/hR9Eds7d864j69qV4+8vhH6E31lgq/AFuoTlJHtSCVkxrPjmXqU4WYLaSVUA02qMd9O+GPn4LHRykiskL/wcbhoX+w3dVu84vBm+9QbiUooWzliyZ7RhCqVuh/JKEadmyvEKpX/CO8+5fKTT3jJvWv/XJAQtzsABCotff3Bu3Qf2UhlUaj0UwDtFCdQjSaQnUg5RA6QsAZN1GX76V3hy6qmkjKQ//DHdWDMSVUK9tTJfNJcsWcnaN6DMVU0awSUnuHRg5tP9/5PE8dfornO58vW59xvJOcjoGiIfn1i4fGlHdqoH6ek5U5tLmUci2dWO/Bais2sFdV1i97s2rntPV+JRQTXeVFU2XnSAJCOZCVoX+w80/jnSrn0xdWFfmgxL4ldBM9kOxT96s5qjWz7W1OoWrlqEJ5wRSoYQCn36DuL7kG/uyndgrAkBoqUZaH6gup95PoqT6CVaPRaKY4WqhOIazQ/2Cy3JGTK24C4I35dWMrSNEcE87Qf65KL9UDMdUX08pRdfZRzRQzBNyBY676T5tFRNao1mqkTHcwXUiXrc/InXxH9dldvfzTfRu5/7Ujo+4npaRoCtVEpaP6wp1w5yW2QAPbAbUE3oH16nb+BUrkDTmuj1VVX0kuaYrPkKM9VRVHNdYBETMfvMxRNQVwYgRHNRNTIXmn81nmqDqE6ljEpbV/rEPd+p2OahiGjkB2yJ5mpdFoNNMILVSnEA0hM/SfKheqscaVHJZNrHa9SG98DAUpmmMiX5ajWl5MBbaItEL/YTPEeyJyVC0Rao1qrUayoARUupAuD/0XTn6OajSlxPj+/tHzTnNFA4TaN5mv+NmNdapc0K5N9jbrPVguY7f5XMsKaDyt/HirLdSwF03ZQrXUnmpQ5ayCylGVUjmqVp6pywVun8pRtYqxykL/DqGbjSuR6hSkVvN/KN9emaNaDcuBjZmiv9JRtQYgWO2qNBqNZhqhheoUopSjWuGo9iVzPF08mzZXL4NHdlY7VHMCyDnEnzNHtdZXS9ATLDmqlcVUsVys1J7qWHNUx+KoJvO2UHWKamkJwJPoqCayBQAOD44ujtO5IsIUqunK0L8lSnu3O7Y58kvBFqORVmhcrO77TSE3UgeAfFKJVG9AvYZhKEe0vs1c1KB6nE/ZjiqAJ6ByYi1HNdZhu6+VOar+yMi5qB4fmOkhY3NUzeKraqF/q+UVQIsWqhqNZvox4UJVCLFUCLFOCLFTCLFBCHHGKPs2CyG6hRD3Ora9TwgRFUK8Zv57cqLXPFmxHdXy0HFfPMtzxlnqwd6njnqeoVSeoqFHKo6Xkar+hRDMj8ynI6lCs1bo3+f24XP5GMwoMePMUZXjHGlpCVUrD7YaztB/uVAdX45q3sjz9vvfzs+2/mxca3SSLAnV0R3VVK4ILvXznKoU0pYo7d3h2GY5qmbovyRU59iO6qo/U7eDIzmqFaH/bAyQ0LDIXNSg3ePUWbnvCaguAVaOas82+zmnUI13qfxUT8B2acMORxVsl3RcjqoZ+g9UhP4ttKOq0WimISfDUf0+8N9SymXAHcAPR9n3u8BDVbY/JqU8x/z3polY5FSg3sxR/cOmTi78j8fY26uqj/sSOdYbZ2BIQc2R0UdI9sQzXPTlx7h7w8iCR1Od/Ag5qgDnNp+LIdU2K/QPqvK/P9MPUJpMJZEUZGFcr50qKHHUn+kvCdJKLEc1lU+VpSbIwvgc1Vg2xu7objb3bR7XGsvWMkZHNVXmqFaE/vNVBGGlUI0dUQ6jNwjLVsOCi+GSv1Mi9Kih/6BKLbAKoiyhmx60RaGzct9b4aha1f1gC1WrjVTtXFXoaIX5nTmqYLuk4YpiqmpU5qhWhv5BdT2ofA2NRqOZBkyoUBVCtADnAT83N90HLBJCtFfZ9zagG3h6Itc0lfG6XdQGPAwkc/TGszy7S/2B7U9miRJhs2xnzsAGFcrs2gxfXQZ7yg3o3d0JMnmjJHInLc99Ew6sO9WrKMMZ+s9XjPu8YM4FpftW1T+opv/9aSVUA55AyW0d73SqtMMNHSn8n8irzzRdSFNwjhJ1Nvwfg5NrieJs8djznZPmtKneeHbUAr9UrlByVDMjOap9O+3RqJXFVEjVZxRg1mL4qzXQuEi5oyOG/lNm6N8Mm8dNAVg3T+Whpgccjqoz9B8sz1F1YgnVWMVxVvi/MsRvbR+Po2qtqayPqilUm5crYazRaDTTDM/RdzkuFgAdUir7SEophRAHgTZgv7WTEGIu8AngSuDWKue5UgjxGpAEviGlvLfKPgghPmGeB4BwOMyaNWtO0Fs5OplMZsJf76aFBsmCi9/sMXj0xa3Mie/ghb1KCKw1zmJV4QHW//Z7LD58Ly2JbnY/+XP27LZzItd1qj/42/ccYM2akQtzTgYjXS9XMct1G/6VzlmXsnHZx0/Byqqz+bAt/ta9sIHoLvt7XqJoC/89O/ew5oh6X0baYKCg2h0dOXiEjKHE2B8f+yNhlyNsexR6h3pL9x949gH2B/YP2+fggHLJ9x3eR754ib22aD9Wi/lHH34Qw+H4VqMzrwTR4a7DVT+f+tg2ztpzJxvO/Ddyvoaq59ixxxanv3rgUeaEq4uoHQMGwhSq+48cLHu9i3o7qQfIp3jmgV+QDrSwfN92FgJbD3Rj5RD15328VLHOc/IhWqJbyaYSPPLwH2jr+iNdTZeS9TZwfTZOz2CCbLKXNmDj2jWsArbs7WCJK0Sqcx996WdYCqzbvJ/4fnXui9M5vIUkbiNH5fynQqKfx9esoXFoMxcC2w4PcXDNGi7PQ1C4efSZDWVC8qJ0gXrgqRe3kPWPPva1Lr6Ti6FUNPXI0+uRLvWre/GhLpYAhzJhtp6A3z0n43fYdEJfr/Ghr9f40NdLMdFCFaDSwqn2F+sHwD9JKRNiuCvwIPBrKWVKCLECeEQIcVhK+fywF5Ly68DXrcfz58+Xq1evPr7Vj4M1a9Yw0a+3GtXS5+kvPkbcE2L16st46jeb4MBBnpAX8Dc8wCUHv1tq07OkJcQSx5q2P7YLtu+kbtZsVq8+f0LXejRGvF6JXtgArU0NtJ7Ez+9oHHhmD+xShT1nn3seVy+fXfb8T377Ew7EDrDqzFWsPl2t+8HHH+TIYVWtvWLJCoayQzy//Xkuv/JyWkIVeYvAKwcHeeD1Dj771jNwuez/C1+792uIgkAiaVrcxOqzhl+Xnz/0c+iFyKzyeao4oiUAACAASURBVO+RgBdMQ/a6qy4rH81Zhdd7X4eHIFwfrv75PL0RtnTxpmX1cHr1z+eB/legUwmwhWeex1WnD3+vAGw9BC+qu/VN9eWvt/cLYOr/K1a0qND+7x+ELjjjgjfCPpVFNGvhGcPXKZ6FdS/SIOK8cVEQXvgpy4eehtvugecls+cvUnmt3WtY1d4Eu+HM8y+DxFr8RoGG2WE4BJeuvsWu2D/8NdXyKlfhEEfm4ol3sPraa2DzIGyFFRddw4oVq+HQAoh7WX3DDeXHdH8XEru46i1/evSxp90LYPNn1H1vmOvf/Fb7uee2wuF7WXDetSy4+Pj/r5yM32HTCX29xoe+XuNDXy/FROeoHgLmCyE8AEKp0AVAZYLkJcAPhRD7ga8CbxZCrAGQUvZJKVPm/W2oHNbLJnjdkxohBCtaa9nRFaNoSPoSWdwuweCsc/kv3wdNkSrA5bHz2kw6okqxWFXZkxIrtDrJJinly6r+h4fQL5itwv/OHNW/v+DvCZsFL1bVPzBiL9V7XjrMj9bu50i0PLczXUjT7FY5iCOF/u32VOU5rKLouI5juKZW4daIoX9rxGiyZ8RzJB0/X1ae6hce2Mqtd5ancwxl7LVmChWvlzMb84Nd+W91BnA2yo/MGb4AszAqlOmGro1qW/QA/ORt6r5VTAV2uD5Yr3I904Nqm8tbHpovFVNVpM00LVG32ZjdQsoK/f/J1+HWu4avb9mb4Yybjy5SoXxAgLOQCuy81Dkrj34ejUajmYJMqFCVUvYArwLvMTfdAuyXUu6v2K9RStkupWwHPgk8LKVcDSCEmGftJ4SYDVxtnnNGs6I1QiZvcKA/SV8iy6ywj9a6AHdmrkW+4ydw03egboH9R9ikY0iJhnhm/E3nTxpWsUqlcDnF5Eeo+re4uPViAOr9dg7haXWn8ZU3fgWvy0tbbVspf3WkFlVDaav5ffkXiXQhTaOnkaAnyJFE9Sb6VpFV0rwN+dwACKfgHEPlv5UPOyxntLSD2ZIp2Vv9eewcVbCF6ssHB3nl4CCGo+NELGvnew4Txvk0NJ+u7u96VOWp5lNqApWz6X1NFaFqVsBHUvtUH1bhhis/paZYgV1MBY7+pPXKbU4Pqm2ROap/qoU3aHcIcArYWaZQzUTtL4aWUJ2zEhZcOHx9F/8f+LOfDN9eDWff1UC5W87Kd8B7fwcLZ/R3d41GM405GaH/vwZ+LIT4NBADbgcQQjwEfE5K+dJRjv87IcRNQB4lrL8hpXxiIhc8FVg+R/2h3tYZpy+RpanGT0skQCpXJLH4rSrc+9rd0LOl7DjLqYtPZkfVav8z6RzV0YXq6vbV1AfquXB2uTC5asFVrH33WoKeIHuie4DqQjWVTxGtIlSLRlH1YfX6afA3MJgdHHYslPdRBQj5PKRyRVxOATiGa2oVU2WKI+xrjRhNjCJUswVm1/rpjmU5ZLaoGkzmMKT62asLKsGeyNnCOVdNqLYsh7nnwet3w7NfVdu8ofIepdUc1XnngctLQ2w7xAaU4L384/Dqz5QILSumshzVBuXUyqJqidV6dvk5PQFKmUyNiyDVp4qv6uarbZkh04n1nNgKfH/Evl8pVD1+WDxjG6FoNJoZwIS3p5JS7pBSXiKlXCalvEBKucXc/pZqIlVK+WMp5a2Ox5+WUp5ptqZaJaX87kSveSqwolUJ1e1dMfriOZoifubUqTBid0z9wU8HW5Q7ZDpkUko79J+ZxEI1P1kd1eoN/y2EEFzcejFul3vYc0FzBKflqP5060/50CMfomgo53FPdA9X33M1B4sPAuWfjyU8fcJHnb+OoewQ1agUqmG/WkeZUK1sql8F6/iRHdWxhf4bQj5m1/pLjqo1qCKWtt38eNYO/Q8T71Z1/lu/BrNXwpNfgu4tSmA6G907W0hZeIMw73waYltVyH/2WWrbVZ9SzwcbHI6qI/T/hg+p54rZ8h6qYApVE6vnarjZrsLPDJlObCtU+Rk4ZtxecJspAv7a0ffVaDSaaYaeTDVFWdJSg8cleGn/IOl8kaYaH7Nr1R/S7liGdXv6+Olm8w+/GY4cTOXJ5JXAmtQ5qrnJmaPqFKfVHNWxYOWoPrTvIdZ3rue13tfIG3n++dl/JplPkpIqDB3PFuhN9fLbXb8tCUe/8I8oVPPFfCnvNeNwVAFcTgE4Qg9WJyWhOpKjaoX+EyML1US2SI3fw7z6IEcG0+QKRsnFH3II1YQjFSEvHess5sHIKzHpC8G1/wpISHSZQjVk7xspL2orsfASPIYp0q0cznPeA+/8OZx7m32OeIcSp/46mHMWvP9haF4Bi68uP5/XIVTr2wCh2k5ZLmdmSP1fqyacjxcrT7XSUdVoNJppjhaqUxSfx8V5bQ2s36t6dDbX+JljCtXXDkX5zpO76ZKqdVAuqsRPh6NAJ5Urlk1amlSUQv+Ty1F19iZ19lQdD1Yf1YKhRNsTB5/grk13sW1ANbXPSyXSE5kCP9ryIz637nNs7d+qjhU+6v31JPKJYcVYSUdvT0tghn1uQOI2xhf6LxVTjXT9rdD/KDmqqVyBsN9DSyTAQDJLf9I+l1Oophyh/3w1QW2JyfbLVR9Ta5vDUf3drmJZ3msJZ97mHHNym8sFK95mDwmwWHKdnY/asgL+7nk4/33l5/M49g/UwYKLYP4bbEc12atcZmfv1ROFTwtVjUYzM9FCdQrzTzecXrrfVOPn8qVNLJwV4luP7WLt7n5iPtVW5+VNKk/Vyk8NeNXHnsyO3Ij9lGJVVU8yRzXvqPQ/XkfV4o/7/8hdm++ivbadWYFZFIUpVLN59kZVw3qryt8K/QPDXFWr2T9A3sgBRcJ+D16KCKQ9ynMsxVSmUC3IwvDuBIahioaATLSLB17vqDwcw5CkckXCfjcttX4MCbu67fWVCdWRHFVruyVUvUE47UrzfqAUhs95Inz8tzt5elcV0bzgDUira8DsKlXxzlD+sjG0gHE6qr6QGi7w1q/a4tEa9Vo7b/ixx4uVp1pZ9a/RaDTTHC1UpzAXtDfytrOVe9MU8RHyefjaO84mbzp/H3yLcpQ2b1NuneWoLm1Rf/Ti2Ula+Z+fnI5q2QjVKjmqY8HrtqdW+d1+elI9pAopPnreR6n3NyLc6r0nMgX2DakRoB1JJQYtRxWGC9Vk5bQkV56w341fJEkIYec2jqWYKu9sGZXh/t33s6XfLMrLDoE5KjaQj/I/T+8Ydnwyp9zisM9Dc43KrdzZHS897xSqacdnXJTVHFWHi7n0OnNbSDXP94aIe1X1/d7eKtOiAnVEa5ZCQzvUVCluskSwcA0P81fDKWyrVeJbo161o6rRaDQnDC1Upzif+5Mz+OAbF5Waz1/Q3si/33QW/7j6dJYvU47rotQm5NfPoGHP7wBYNtsUqpO1oGqS5qjmjeN3VJ3jVW9ecjMAK5tWcm3btQTdkZJQjWZSJYHakRguVKPZaNl5rUp9YTqIwpUj5PPgnvtb/nzuHGRQCZxsNs5XNnylJIKrYTmqAIlcgs+u/Sw/2PgD84UGyvYtxvuGHW859WG/h5baEYTqE/8B37u8bDRswSFUn9i4X91x5qIuvd7cZorXhkV0+E8D4GB/FaEKvHb6P8DtD1R/o9Z5Flx81CEIQLlQda7LEo9dm9RtZRHWicDqcqCLqTQazQzjZLSn0kwgzRE///LWM8q2vefihepOMY+Bi2vdr0AMrkl+kwhfZUmLcmcmbUGVJVSLWTWbfpLMMM8XnDmqYxeqPbEMN31nLV97x9n4fHbo/9ZltzInPIer265GCEHAHQFXBjDoSdtN/TuTqirdGfqvFKqWo9oQaGAgMwAiR43fA75+9vm87Je1LBqE54Z28IsjD1Prq+Vvz/nbqut1CtW+dB8SSX+633xSvW7RW4M7n8Cd6sEwZNkULctRrfF7aI4oobqjq0Kodq2Hrk0U2laDWSBvSOW0dkTT/NeajVztR4XYLerb4OrP2oVRf/kwP7xnC3QNsr+/epFYztdgFj5Vob5NCb9z/rz685U43V1fFaGaiYI3DIuuHNv5xoMuptJoNDMULVSnM24vKV8jNbk+ir4IkVyULwT/l0guiBtj8raoclamF7LluYGnkLI+qlUmU43Ezu4EnUMZXjk4yLmn20J1Ye1CljcuLz32UYMQElwZ+nJ2RX1nQglVq+ofRg79zwrOYiAzYDqqbnArV/pFv5dFwIbEAaA8p7USp1DtzyiBWurdaramioZPY1Z0I/VyiKF0noaw/b6sqVQhv5uWSKB0DSyG0nlIKidW5IdKQrVoCtXtXTECwnRXvUGKhso0dbkEXPFJe6GBOrrSSiAfHDh6N4NhhBrh/x4ob+o/GiOF/r0B1T6qmIWLPqQ6AZxofFaOav3o+2k0Gs00Q4f+pznZgEoJ2HnRV9gpTuPt8nGuXXcbf+P+/eRt+p9zhHEnUfjfGfrPFcdeiGY51/FsoVRM1RpuLfVWtXCjwrvCnSJasKdPWSJx1NC/Ke6bgyoXU7hyhP0epEvlgL7oVuvdkFLnHZbT6jyXYwRrX1oJyp5kP7fftYFMTBUtdfmUa9/EEH2J8lxi6/3ONnporlHvN523r5cSqqYQN2yn1cASqnGCqHManiBX3PEkX3hwa9lrxMzJagNmb9ZDA6lj62IxVpEKI4f+QbW38kXg0o+Ofw1joeSo6tC/RqOZWWihOs3pX3A9jxfPZUfDlbyn8Gl+0PhJ8r5arnW/MnnHqJYJ1clTUJUvGFgR7vE4qpbDGM8USjmqC2sXDttPGEr8CHeKpKFcVI/LDnqMpZiqKWi6ea4cIa9AupSQe9FI0u9ysTuvjovn4oyEM2/UCvmniwme3tnJkQ6VL7sXVdneJIbojZd/Rqlskctcm7jlmTfT1PFkWeaGz+MinsyUcl2FYTutQqgvJTu64gQxhwMUvRyJprl7w8GSKP2PP2zl7H97hL29idK2giHpHJrgLzVlVf/h8ufecge840djy3U9FiJzAAE1I/SM1Wg0mmmKFqrTnNiFH+Wv8v/I3v40PfkQO1tvJDH3claJveSrFMJMCiaro1o0CJtN9MdTTGXlbCYyhVLVf3tt+7D9jKJyWF2eFBk6mRueawtPRm9P5Qz9g3JU3d4cVnemfpnj17U1w/a331uejzz+EZ48+GTV0D+A8KRIRJUTujmnCoaaxBC9FY5qMlfgRtd6ANx925llpgWEfG6aa/zIVD+lUaSGWkfIMPCKDIWiwY6uOCFTtPZm1PXOFQx+/dIh7npuHz94dh9SwpaOGIMp+8vW/hEKqk4YTge8UqiecZPdlWAiuPCD8FePQsPwLzgajUYzndFCdZrTZLYH2toRA6Cl1k++/SpcQjKrZ/2ox+7pTfA/z+4ta6Z+eDBFf2KCXc7KHNVJQt6QhMyxpOMppiqF/jN5FtUu4ryW87i+/fph+xXyylGtC+couHtYVLeIBn9D6Xmf8BHxRXAJF9FslN5UL1Gzp2nJUQ2YwlbkcLnVtWvPKTH33QblxnpcnmE5qvti+3jq8FM8fvDxYcVUFsKdIBtTwvXFpEoxOEvsp2HHr6Bop5EkM1mucb+iHsQ7Sz+DDSEftUEvbuc5Ua9VZxh4RJZEtsDungQB01HdN2Rf5689soMvPLiVSECJ1y0dMYqGZF69EpAHRiioOmGM5qhONP4aWHDhyX1NjUajmQRooTrNmWXmCG7rNIVqJIBYonpGzusfWag+vbOXm7+9li/+YRvbumKl7X/+gxf4p3s3TuCKmbyOasEg4HXjEuPro5oy2zXFMwVC3hA/efNPuHDOcNFRyCshFI50g8izqG5RKdQPSqi6hItaXy3RbJT3PvxePvWsml1fGfoXrhyGUMLtLckkn5xzJWdks7zRXc+CyAKSuXL30WqB1ZfpK3dU0w5H1Z3ESKnHm+NhkiLEJe6tXLHt3+DVn5X2C/e8SpMwf2ZiHbSYE9Mawl7qgh68GbvFlUB9vnVFA5fIs7UzRsGQ1LiUuN7Rr24vX9JEvii56Zy5/PJDFwOw8bAS6ectVGL+wMl0VCtzVDUajUYzIWihOs2p8Xvwe1ylqVSza/2Emhexx2jltKEXVPunF38I372kJBALRYMP/+KVUrFV1AyvGobk8GCKvX1qv4/876t89nebT/yiJ2uOatHA63bhdbvGFfq3HNWjtQPLZJWgy3tVj9OFte3UO6q8/S7lTNb769k+sJ0jiSNs6tuElJJkQV2z5pDZ2N6VI28ooVprGNw+/xp+1dHNd73tRLwR4vnyHNUjCVVkNZAeIFVI4RHKtXSG/t2eBO7MIIYnRA4vjy74KN8p3EhWBOGVn5T2m9f9JABSuCDeWWr63xDyURf0EszbQtXtyiKkpMYwkK4Crx9SKQ2L69Wvps296pp94aYzWf/PV7PqzFfozr+Cz+Ni02G178p5tXhcYuIdVY/fvA2Cyz2xr6XRaDQaQAvVaY8QohR6BWiOBAj53DxrrKK+0At9O+G1u6FnK+x7FlDOXzxboNYMsVpCNZ4pYEjVF1RKyePbunl658jz3o+ZstD/JHJUiwYel8DndpEvHlsxVSJb4N3//Twv7O0fvl9aud9JVAupuaGFpdC/QOBF5bfW+etKDmosF6M/008yn8Qt3KUcVuHKkZNqn1rDUOLKE4BChrA3XDo+U1CfZUdM9W3tGdxNtpilIaBe1xn6/1TgLk4r7CXrU+I5deZtfFvcxvrQldDxKnS+DkB7/zN0yQbyLasg1llq+t8YVkJ1FnZ+rVekCUiJX0oMYRDecje/9X2OpbXqmm3vV2703PogLREf33rlW3xv453Mrw+Wvki1RALMawiehNC/6aj6tJuq0Wg0JwstVGcATTV2j8uWiB8hBC96zlUbtvwWOsx8wj1PAHbrn5UNOd7tfpxoSrma0bTKG0zminTHsqRyRXrjWaQ0RVshC+v+C7Ij9+h0ImQRfvOhkkAukXMcP6kcVYnP48LncY0rR9Uqpopn8mzvjLF+bz9PVRH4iZQSohIlzpr880uOasATQJjl8850AIA90T2k8ilC3lCp5ZVw5ciZhUo1hlRuoCfAUCzGy/tSJPNJ+tP9XPGrK/jljl/SMbgLgH6zl2m9eR5n0VXSVWCWiBGVKj+zrTFEc8TP71xmEdHLP4HoIZoyB3i6eLaaeZ/opiWsvvBYjuosYaeSeFzZklDNCbi67+ec69rNouJ+9ZqGj4aQl4DXTSwXQyLZNbiL1gb7V1dj2McPb7+Quz940Rg/kWPEak91svNTpyC9qV4+8sRH6E3ZP+frjqzj4X0Pn8JVaTSaqYgWqjOAckfVnBTkX0UeD6z/Tml2O3seByCWVsLqg8a9fNn7Q/xdSsg6K6xfN/MD0/kiyZzZI3PbA/DIZ2Dr78a0rmCmBzb+CrbeX/5EznbGntxysKyYa8xICTsePqFC99hD/0XztkBfQon9aGp4a7BYGoRUn48sBvBSW3JUQx7bxbNcU4u9Q3tJ5pPUeGvs/USOjBn6jxgGeAJk8LG3s59ERgni7QPbSRfSPHv4WY7ED5eds3Goa9j6Btzq18W+pJ+I38N5C+tpqvGxNr0QZp8Fm+6B7Q8C8KyxEnddK8gi833qi4flqDaZjuqQDFNwGfilxCO8GEIwW6rXDcf2AJDGx5w6JZqtfrIFWaAm0m2vNexjSUsNsxw/5xOC5ah6jy5Udwzs4Nyfnsum3k0Tu6ZJyrNHnuWpQ0/xYteLpW1fefEr/OeG/zyFq9JoNFMRLVRnAFZBVb3pTAF4ghE2u1dA1nS3FlwE/bth8IDtqGaVQK0ZVM3Woyl7FrtVyALYfTR7d6jbhD1VaTQ8RbNoJ+vIlyzkwMiD2W/0Nxv28MyuY0gvOLge/vddsOne8R87Albo3+sR4yqmskL/hlRdE6D8WgJIKYmm8niFaiFl5JpJZIslR9U5HMASqufPPh9Qjmo0GyXsDZc5qtmiEoiRogEeP4M5t5r4VFSC7mD8IABb+rdwJFOeitCQGnSsTeCS0Oev4Yv52/hO4W3ceM5cQj41IrU/mcM473b1s/TklzEQbHCtwl03F4BlwThCwJKWGupCPmaJGHnppljfTkYIAobE8Km2WjnTNXan1M9QGj9zzNQBZ0su6T9Yut800QLVwspRHYOjuju6m4IssDu6e4IXNTmxUkayRfW7IZVPsX9of1mhnkaj0YwFLVRnANYf8paI/Qe9xu9hrVxlPpgDF/0fdX/3o8TSeeaLXmZllBioi+8Eyl3AjYeHuMPzfT7r+Rk9MTOPtE/tR2p4/mU1PEXTOc3aoWCsUHNICRe/yPPo1m7GTdx0BJNjE81joWCG/sfrqCYdRVRWr89KRzWVK1IwJH5LqGabSWQLJUc16Jgzb4X+r194PWFvmCcPPcmRxBFWNa8iYIWnXTkyZug/Ig0Mt5940UOdp4A01D4HY+rzHcgMEDOyuKTtXDe4Ha2YDB91hkHU6+d/im9lrbGSd7+hDVA/W4aEwcU3qzzY7BB7vMvIBwJ8cWgjfS4XC71DrP/UNbz5rDnKURUx+qkl3DBbCVUpMQJzAOjDfp9ZvBi4bEc1Y4vnuNxjrzXsHcOncGwUjSJv++3buPP1O+2q/zHkqFqCzDnlayZhhfxzRfWFbPvAdiSyJFw1Go1mrGihOgOYVRKqtvioCXh4Ir9SPVh0BZx2lQppPvwp5m36Dle6Xi/t25JU+YuDDhdw05EhbnBv4Fb30/TGK4RqcmyDBNzVHFUr7G9O+PGjhOq4w/8Z033LxEbfbxzkzND/uIupcrZQtQp+oulyodphdmUIe9WITCPXQiJTKIlSZ+j/3JZzaQ23cuWCK1lct5ge0328afFNuIQLN36EK0fa7AQQKRp0JCQp6SXsLuAVSnAd2l6ecrE4b6+zYa6jfZbhp7FYIOpxE/a5OWteLWfNU66ulUrSkw/AmW8H4GXPOXjqXuFX/a+wpiYE8U7m1KkcW6uYKulpwB9pImsK1WJ4AQD/z7ix9LJZ1Llb69TPrXNsbEdGufcRvwe/Z+Iq8BP5BPtj+3mp6yVwe9QXqMjcox6XMYsAZ6qDWOmobhvYBkBRFskbk3QinkajmZRooToDsIqpnI5qJODllfwCftf+GXou/KQShu97AGYtYdXO/+LTnl8ghYvttDMvtxcMo8wFzKXi1Io0dSJFpmcPGEWVOgCQGptQtUP/DjGZq3BUydMTz7LxyBDjwjpnduRRoeOlVPXvcZWF/o+WBpDM2nPuLaE6VBH6396l1tkUVMLUyDYTzxZK1ffO0P+Fcy7kkVsfYV7NPE6rPw2ABZEFnNuiCuQ8IgCuHOliHCEhLCW7BvJk8REgR8ijXNsDWbtNFMAqw3YmGxa9qXTfLz00FA0GKfKLD17MnbedX3quJFTjWbjso8i55/Oz1CV4wupn4ZDHCzHVo5XX7mbl5q8wS8QoRpqIBiJkhIuANHAH1Of9G3kJWZ96z3mXEqhzKoTqsoZlDGR7EJ4YjY5CwYnAGjVrte/irx6F1V866nGWQJ2pQrU3bTqqhvo539q/tfRcdhIVSGo0msmPFqozAKuPZXOtLVRvOnsu8+pDfHz7GXzjJVM0zTsfPvQke+svIyyyZFvOZrN3JQGZgej+srzK2cIOw3p7NsLgfjDDfGN1VO3Qv0NMmqH/oukkzjaNxEe2DC/uqeTQQIo3f+tZdnXHbUd1JKG6/zl49utjWidA0ZAYErxm6N+q+l+3u4/ln32YLR0OIX3/h+GXt5UeOvunWv1sBytC/zu71Trn1qo+qEauucxRDY9QwLO4bjEANy6+kY6hDO/94QsgfQiRJ1VIEpRuXMATu4dIyCDeQpImj8oDPexV1fhuocTe2f5ZpfOGQ7Pwmr8eWkSexmKRISPHWfNqWNBou7tzzalQHdE0tKxg39t/z5bsLLIe5a4f8nog3qmK2578Eo0b/4ewyPK/zQY3DTxDyiUICC9+M7VBiAKJmrl8YE4LfzQjACVH1ZzCdXGravjvC/XQGK4uVLuSXbzzwXfSUzi+1A+r60F3spuCUYBZiyE86yhHaaFa6ag6hWqmOHlazmk0msmPFqozgDPm1nLm3FquXNpc2nbtGbN57v++iZaIn9cP2SFVvEF+vvCLfLdwI5nLP0VnYIna3rW5FK4OeF1lQrVuYDP07bLPMeYcVfOPeGa4o5pwq9DymS1+gl43m8bgqD6/t59tnTEe3dbtEKojhP5f+D48/m9VUwOklPzbA1t4crstcgoHX+CXvn+nliRetyjlqD61sxdDUmpUD8CBdbD7cZCSfNEgV1BOLCjBC6pbQiZvO607uuK4XYIPnv0e/mLZRzFyzSSzBXxuH5+/5PO8/6z3V30bb170Zm5ZegvvOv1dPLypk2d39ZHOuhGuLKlCghqhwuK/2zzAK3IZLiPHdYWX1XsSAlfRiyfbCsDKiD1HPuQN4XMpEdhaHKLBUO/XGX4HmG8K1SOD6rPc3BHDFThCXqrHh7w+5aj27YKhQ6Xj9osiA0aWohAE3H5ClmMsCjxYX8MLwQDPhZSQbjVzVK3XXt64HICbzg/zd1ctqXpdXu15la39W9mdO75iJstRLchCKcViLFgCNTOJ+gCfLKSUZUI1XUizd2hv6Xmdp6rRaMaDFqozgPqQjz989I1cuqSpbLsQglXz69nZHS8TTYM5F3cU3kVg+XX01yxTG7s3M5jKE/S6mVsfZDZ22Lg5sQ36zIp/f53tqKYGlJM2Ap7CyDmqg0QAaPQbNEV89CdylYcPozeh/gDu6BqDo2o9nxzeUWBnd4Ifrd3Pj9fttzfufpyLXds4Lbsdn8dNvmDA1vsx9jwFwJGoo2gmPQiFNCT7SoVUs2sdxUkmMUee6p6uAe4LfonlnRt51+m3AaLkxN6y7BbObj676tuYHZ7N5y/9PPWBeraaY3KLuQaEb4CBTD/15gSllOFhR93lAFyRXlc6PlD0Eupv59JUGowFBN3KaJnT/QAAIABJREFULQ16grhd6n59McOsovr5cI5UBZjXoESk1c1gy5EhPGH1paXeX88Rjxsjdhh2P6YOOO8vAIg6fvMEvEGCXuX2e705fm5+CRrwufj8285gcbNykwezg3iEh8X1ykU+bU6Ba8+YXfW6xMwvKCnj+IqZnH1kS+H/MTCTHdV4Pl4So7lijp2DOzGkgd+tPmMd+tdoNONBC9UZztnz6ygYsiRyQAkon8dFwOsmVbuYvHRjbP092eQQDSEvLRF/yVFN46cttwt6zUKqtouUSDv0Ivx/i0t9NatRCv0Xs3a/U1MY9Bkqj7LOazAr7Kc/efQ/blabrDKhmhnBiS0J1eFpCuv3qG27e+zBAzKtrs+sYi8+tyBflMgHPsbN/T8AbEcRwwAzRE30YKnH7Oza4S2U+pM5vvTQNl7cPwDR/ZxT3AS7H6fGr5zEWGb0kauVbO1QazQy8xDCoDfdQy0uirgo4iYw90yob6O+aAvkcNHN8lgT3+/u5buv5klllDhVxVvKUQ0bBgvMQqv9sf1lrxnyeWgM+0opDZs6ovhqtxH0BLlu4XXkBPRE98Pab4HbB6u/DB95hajZhgrA7w0TMrsVRJpeoctQLmSvS/K+yxaVBh1EM1Hq/HW0hpUD3JUcOR3EGhGbNJIj7jMWnKNmOxIdYz7OqvafiUK1z5GjnilkSu7q/Jr5apsO/Ws0mnGgheoMZ+V8FWK35qYDDKXz1AZUYU24pobvFG/C1buNfxj8Io1BN7NrA8wWSoxt9J1HRCaVIA3NgibTgd35RzVI4PBLI752KfQP9jQrM/TfmVNOXcRTpKlGOaqlCVhdm+AHV0O8vG2VJVT39CYw0qZYPKqjOjycu26Pcg2PRNMlR1Sa+88q9OB1u3AXU4j0ILVGrLSvWn/cHqAQPVA63gpfO1m/p5//fmYvH/jJS8zGTKVID1Ib9BL2udnbO7YJXwCZfLEkrIuZ+aXtNVIgzVZTK+bWwbIbqDHs4q+6Iix0q/dr1LXhNpSTnc66KBaVYK4xDBa7lau5J2q3hbKY3xDk8GAaKSWbhx5BBA5z4+IbWVS3CIBDoTpIdEHbxeCvQTaeRjRnpxAEvTWEfErI5/ybcCG4JJ2mjyJ5h6iOZqM0BBqo99fjd/vpTo3ctsxyVBPG2K9hNZI5W+iOR6hOB0d139A+frHtF+M+zjl2N1fMlRxUq/+vDv1rNJrxMOFCVQixVAixTgixUwixQQhxxij7NgshuoUQ91Zs/4wQYo/5798nes0ziVXzVbHO644G/rFMntqgEin1QR/fLNxC9PQ/4w3Ga1ztesV0VFXof3P9VQDIYh4u+XCpWp+Dz6vb6IERX7tcqJrC0Qz9H8ooYRd2///snXd8XNWZ/r93epc06tVy7xUbXOjYGFgILQTYkE0jIZuQhIRUUnZDlgVS+WXTyAKbXQIkhBrAYIqxDbhhg3HvVre6RhqNps/9/XHmnJlRs2TL1Hk+H388mrl97tzznOd93+eNke+0EkvoqmMWR9ZC4zao35SxvdYkUY3GdaIBSVSHyFEdIvQfT+hsOpIKbx9OkkU9uZ3caAtmo4GSpKKcqwkioxTVYFoOp69Ohe/TQ//SfUE2TegORimRqRRBH0aDxryqXN5p8I24scCh1l5iyfzXRLBcve8JdmG02LjjylncsHgcTL04g6jmx2NcVC6u2/c/uZJJ+UKtbOxMEIuJe8CZ0BnvLENDG5Solufaae4JsbWhlkTeM9g1L7csuIVKt7Ccaph9uVhw8kpAqI2xRIwFRQuwakYqJizHYxXft65FqHKUMCkSRSdVPQ6CqOZYc9A0jWJH8bCKak9EfF8nq6j2RlNE90RC/x/kHNX/2/N/3LXlrmGv82BI/87C8bBSUGVh4Af5mnyUoes692y7h03HNh1/4SyyGEO8G4rqvcCfdF2fAvwMuH+YZX8PrEp/Q9O0s4HrgTnADOBiTdNWnqJj/cjB67RQkWfPUFR7gjGlqOY6zIDGoelfAWBpdCNFbhvFWhdhq5ea0ou5Jvxj2r+0E876JjiTebCNomCHrpES1aTyGRHE4GhAhJ2N8bDqrNUuw/99SVLnzxxA2/0ppWZYRVXXFVE9WltDZyCV/7qnqYeeUIwJybzIgy3ieLTkdlJEVRyDR+tjSqEgatF4QuSnSvjq0hRVQVTthLjXcBcLtAO8k7zmRoOmiK9cf+E4L6FoItNNYBjIsH91vgM97sYaE2TYHU+gTbuET54xjhy7GSacx9uz74KkOF0Sj7DA3gxmB8Vl45mQOw5dN9LcZSAUEfmtLjRs+ZOpcFcMqajqOvzf9hfQjGFWln0Ol8WliGp9wXj49DNw+heAlHH/7ILZvHLtOq5Z8GWKXG61vaneaRQnfWolSYon4nSHu1UDhGJn8fCK6rtEVLe1bOP8R8/nWO+xjPc/DD6qsnisNzI6Vbq/oiqvQVZR/WCjJ9LD/bvu54kDT7zXh5LFRwynlKhqmlYELAD+knzrcWC8pmnVgyz7SaAFWNfvo2uBP+u6HtB1PQw8gCCuWYwR5lbkcqitF3+ydapQVNOJKhwIe9mdGMecvo2smOql2uLHmFNGocfBm/o0ntztY1djNziSRDU5OOm+2lTIvh+M8bRCF0koo+K9I34jMYwQC6uGBaqgSpJBfyY5aPOHyU/aFZmSRIV4BKL9FJxIAHSRO7r+7T3c91qqInlDMj/1XxaLCviDrZKoiu3lRFqwmAyUklJdl08Q3Zmau0Op/FTIIKqFbisGDWZotcwPv8ly41scbRck6s6rZnN2STLEnVx/YbUgZNtq04hvEq09IX6xej/hWBx8dfDov3CkTnSZumS2UETLw+I6uObdAJf/LrWyptEz6XL0hLim5YkgWvMOKJ4FBiM3L7iJvqNfYdvRkAr9O5d8FS78KRNzJlLbUzvAsL08Wfm/qU4UUV00WfislruEslvvbyBatUS1IJWtUHNtueRYczBoBizGlM3UlMJZlCy7FUgRVX/Ej46uyE6JowR/xE9fdPBiqTEjqkmSVmAvGDT0/0bjG7QF2zjoO5jx/och9C+JaiA2umsou1KBIKWSmCpF9QOWo7qmbs2o0j4+rJDpNB/kezqLDyZMp3j7lUCTrusxAF3XdU3T6oAqoEYupGlaGfBN4Bzg4/22UUUmea0ZZBm5nW8mtwOA0+lk9erVJ30SI0UoFHpX9zdWcAQT6Dr86YmXmZ6n0ReJ0+drZ/Xq1RzqSNowbd1Fc3wR3zQ8Rtu6+6iMt9MZKaG1Ttj//OeqfRTZ4Tczj7A4bdtaXwcr73iCby5yYUhEMCSi7OixM96jsTytveTtj7zKwnm9zD/0BqXA0T4bUZsZX3MDTdF9ALzy+ma+90iCu9jD6UDj/m3siovrHYnr+MNxFng0/H0xzInUYPjq6n8QMeeov23hds5Jvs7Xenhh3xFWa0L5XbNHEFhP134MGmzcfYTVhlrO8LViA1yhZpobapmupZnlN+8HinnqxfWcldjEvOTbvY172BTbDsCBPTuwGaEgmdOahyDmNiN42nczPiGsm+K97by8ejWhmI4GPLdlHyW9B9B1SETDrF69mvU1vcTrNvDLlvO5OvYcU+ueJmEqx2E6E6vvKADTwxGOOKGpOTjgnjzYmUBP2NCMYcYneiDYR100l72rV6PrOvZEGa8daMNUJiYph4/FWO3bicFvIKbHePj5h7FpNnKNgni0tot7xB9vxQx07DnC6n1Cfcwx5PBSzUu8UPMCn839LDNtM9kfFg4RjYcaWd0kju1o5Kg6vsDRADVJx4H129djPGhUfqhdTV2sXr2agF+Qp8dffJwiUxH90dgu9h9IBE7qN3nAJ4oE8+P5HAweZNULqzBqqU5Yb/reBGDj1o0Ed6cG8I4eMZHp6u0a0f51XWdd3zpmWmdSaCo87vKnCunPsAZfAwDrN66n2Try8P9O304ArJqV5vZmdvXsAqClVijgW7dvRduvDbn++wnBRJAft/6YM+xn8PGcgcPOB/WZfyKoj4pnVH1r/Qmf80fpeo0FstdL4FQTVVBBRoXBnlD/DXxH1/VeTRv0AZa+jSGfcLqu/wpQLu4VFRX6ypXvXpbA6tWreTf3N1aoaOrm7795nZh3PIvPmgDrXmLK+EpWrpxNcb2P/9rxBriLWJ1YyDd5jEXabtCjFE6Yzc3nnEfzU7s40tZLTUeABWdfDLt+CICuGdD0BIa+VlasuArD018mdvAVvtD5S754zmQuTiOTnf4+Js1dROn2XQQL59JZ70E32SjIdXP+0kXcv2czuZVTOLJ7HyanIATlbgPlyetd39kH619l4fTxsOcQpEUrz1tymjBql2jZDW+JlwVaD47cQlauFC1D/3j4DQpcQa66dDm/P7CO7niClSvPI7pdqIgmYsypzKGgNUVUz5pRzu9rY5RNnsWkUD0cgLBuwhxsx1M2AfYd4qzFp/N0/Tt4/YKoejVBVCcUebjoorOg7m7oAqMeZeX5Z4PZzr2HXuNAR4Dvb9JYGt3Azyz/TeJfN2FqvI8LzPfxsHEGUz1AHdhDbSyeVMQ1F83i19vXcF64g+dwcc5p57KyOvOeLG/s5k/PCXVT2k5VnX4pVaeJ5R6o3ciWo52YdEFUly5cytkVZxM5HGHN62t4Xn+e3e27efiSh5ldOJuqYz38fudrGMxdWDUPH7s41QZ19drVvFr3qvgFl8PKhSuJHonCa7BswTKWj1sOwO723fzuOaH8Xr9cBEx++9hvySnPYeUZK9neuh2ehwXTF7By5kp8+3ys2byGSfMnsaRsyYB7+p7H74FeiBLl7AvOzujsNRo8u+ZZTA0mFk1cxP69+5mzbA4V7lSx2gPPPgAhmDxzMisnp67zXY/eBUHQTfqIngl1PXV8+8lv4630csOiG07oWMcC8hkWTUT51oPfAmD63OnqexoJ/r7677g6XLgtbpw2JxUlFbAbTp99Oqs2rmLK9CmsnPrBeE429jaiP67jLnKz8tyBx/xBfeafCDY0boCXwe62n/A5f5Su12Doi/bxX2//FzfNuYlcW+5xl/+oXy+JU52jWg9UaJpmAtAEC60E6vottwS4X9O0GuAXiDxUOY2oA6rTlh03yPpZnASmlXjw2ExsOtKhvD1VjmoyBaC2o4/9eiXd7smwO5mj5C6jJMfGfZ9eyIUzS0jo0BhJdS3y5c0BoJw2/H1h2P8cpr4WxmvH2HK0A1OsD5IEwqUF6dmzBsI91BdfIDZgskEspHJU36wR5NAWS4b103JUW/2C9Ba6rEzLFeRLl8qXLKhqeht2P5lhWVVAt2pkIM+zOl+cw+QiF3WdfYSicQyRVK5rfrxV5agClFoFcW70BXl9pwgBN1mqsRLm6TfeEednNeG2mcgnk6hWFzgGnIssyDp9vJdAJE6Ow8xlOUfJIUDL3k04emsAiDe9A+1C8avUWrhyfjklHhtlph5Whjp5yL2A5VUDCUaO3QxxkTPrTRJVSueozycXCWswPdlSVXbFku1ad3fsBuD5mueBlJeqZu6k1FmWsa+7z76b9detx6gZqe0RqrUM/cswPqBC/x6Lh2JHMQX2AgyaQeWhyrxWGT4udgr/1KEKfdJtpeT+TgS9kV5cFhclzhIgs1BI13Xqe4TKlO63CqMP/ctmBuk5se8l0i2m+mKj86JtD7ZTYC/AarQSToQH5Kh+kIqp5Pfa//v9KEKm04z2fsgihfWN6/nL3r/wSt0r7/WhfKBwSomqruutwNuAlAiuBmp0Xa/pt5xX1/VqXdergW8Bz+u6LqcRfwc+rWmaU9M0K/A54K+n8rg/ajAaNE4fn8/Ohm6ae8Qgoqr+kzmqNR0BQOPQgh+kVnSXqJeS3B3pMYBBrLNRE0HwSq2Vvrq3FEGcrtWxv7ETox4FjyA2boLYDwvi87ZjGQAGs03kqDqF+rc1SVQ9Ui5Ny1GV1lSFbivVTpEXmnAlj0/mv778E3j8xozOWflaD13J1rDdwSidgQjj8gUxm1TkIqHD0RYfxniIqC6IrzfaSmkaUS00igf3C7uaqWkQuWxVM5eK6xcRx+iwGnFZTeRrST9WgziHKq8TEnHoTSsMSuapfu2Cydxz7Txe/uY5nJYn9hHrOIw7KPaR5z+A3ibC6OONbayYUYzBoLHCK8jUnPKlGA2pMLVEjsOMnpBENQEGExSlzDgkUTVp4rq7zOLvCTkT0NAwG8x4bV5eqX0FXdfx2Mx4HGAw+5mcPy5jX2aDGbfFTZmrjDq/mF9K0ikLowBlBj/VOxVN0zAZTBTaCxURlUQuzybWkcRxsIKqhJ5QHaXS9yexp2NPhu3VcAhEAzjNTvLtws0ivVDIF/YpQpyeK6vruiJn0URUtF49DuTxjrZw6VShNZiybRstSeuJ9JBjzcFitAh7qvips6cKxoKsq183ZB78yUJ+r++XCcR7iSxRPXnI/G15LbMYGd6Nqv+bgJs0TTsAfA/4PICmaas0TVt4vJV1XV8LPArsBPYCL+q6/sKpO9yPJhZP8BJL6Ly6XwxQUlF128xoGoSTNkmmSefBrKvFSp6UDZIkdzWdQVX5/1C7aG9ZqbWhH0mlGU8z1GGOJwe/JFH1aAEq216Fgim8ExZqmclqh1iIPIc4BmmAL/M7CXUrO6t0olpuF8Szz16aXC75UOiqgURMqZARzUKe1ktvQJCKug6xLUm65Tk1tQgydEQX28uNtVKqdYhiL8AR78FhMbLnWA+FJrENY5lQKK80vo6TIC6riYo8O+VmMeB5k+cwLt8Bva2iuEsqwMliMa/TwhXzy7GZjbjD4hiMvhryIoKoLtF2qSKvSaY2bL318OCVfKck6bhQPIvB4Laa0KMFmGM2chIJKJymCp0AphSLCvwCSzW51lxKXeK87SY7X53/Ve448w4uGX8JTYEm9nbuBeDqReJaVbrLGQxVnirq/fUk9IQinemhrxxrDhaDhXmF89R7Jc4SRUTVOlJRdQytqPZF+0joCbRkllBzoJl7tt1De7Cdg10HufbZa3ny0JODHmd/+CN+3BY3BXZxT6cTVUm8IXPwjiQiJPSUBdhIVFVV/PU+Ue7SC6JGe0y90V5cZhdWo5VQLKQU1FNRTPXM4We4ec3N7OncM2bbTIdSVCPvj+/lvYQiqkMUMGZxfMiIzLtNVEcyWX4/45QTVV3X9+u6vkTX9Sm6ri/UdX138v1LdF0f4Aav6/qfdV3/eL/3btd1fULy322n+pg/ilg8QShGL+wSA7+s+jcaNP5l8ThWzizml9fMZXZ5DvzTL+Hin8PE89T64wsEUantCICzgLjJyabwOGIYqdTasNWvB5ONmGZiulaHS0u6AiSJ6gytFk+sEyYtp66jD4fFiNEiFFWT0UCeQ4SGLURxammKTK843nSiWmYVr32WZHvNsF+olt2iOIRWQa4aNEHADMEOEgk9qRpDVZKoSsLa2ibI+35d2C15Q3Xka37arFUAaCEfM8s8lOfaWTnBCmgw43J6bGV8xvQiD1nuwGk18ZOPzeKcJI9z04uBBOO8DvAnK4plHm3QJyy4YinbLFtQnKfNX0thXJC3fC2lGubE2mH7w3B4Dc4DT4k3iwe3LNY0DVvPxyg+8gnxACiZk/H55CRRnZe3gteuew2PxaM++8KcL3Dx+ItVzuLLtaI16gVzhAIvK/37o8pdRTgeprWvNRX6t6RC/znWHJ66/ClumnuTeq/YUUxHsINoPEq9X4TYpZIqTf8Hq8aWg4DsYPXEoSe4f9f9PHv4WdVZq6G3YdDj7I9ANIDL7KLANghR7anLWE4iGM0kpiMJdcuK6rEiqo29jXxy1SdVasJoka5Uj4aYxBIxgrEgLosgqpF4hGA8iEkzqRSSsWyh2hFKFq2FBrpjjAXk95FVVNPaEmcV1ROGTKlJj/icamxo2sCihxaxp+PUTObeDWQ7U2UBwPRSD1VeB7VJVdFjS9XZ/eTyWdz7qYVcfVoFBoMG9jw444tgNKtlitxWbGaDsFw653u8NfN7xDHiM5cw3VBLTts2qDyDRlMVMwy15BrE4B12CDIx33AouaEZ1HQEGJfvREvmqALKdionGfZPyFs3mdvZ1htWx1FoFuu0asnq6XCPWE7aKrUKF4EDcUF6vHTjD8eo65SKqhhQJWFt7xCD4aFEOToGynxCsYzkJ4lgsIsHP38Gr9x6Dq6EH2wecBVRc91aVscXMs9wBEeojRyHGWtYpAwYSeAhIPbRk0xhKJqePKdj8F8L4KUfib9jEYxJhaukZycWYsTTfrpt7uR67/xVpF2YnZA3XnxPQyDP4eRQfAptE66E0z6T8Vmh28ofbziNW1dMHXL9eYXzyLXmsrFpIwCNflFlL9tk9sc4j0gJqO2ppSvchcvswpx2/wBUeipVCgAIoqmj0xRo4pDvEDnWHArt4jvVNI25hXPZ1rJtwENf/l3pEROLzcc2A9Dc10xLQBCwkeSt6rqOP+rHZXZR6BD77Qim0kYkeYZM1bS/YjgSRVWmEIwVIXqt4TV2tO1gQ9OGE1r/RBVVuaxUVMPxMOFYGJvJpr7bsVRU5aTgVFkmSVL2flG630t0R8RvJpaIEYlHjrN0FoNBptT0DNWI5hTgkb2PEEvE2NW+613b51gjS1SzAIRyeuuFU9TfUlEdKQwGjXFepyC60y9lg/tiAAK5k6nQ2jEmwjB5BQeoplTrZIlXDMi9plwiuhGPJgaELucEmnxBoTSarJBUX2RB1WyvCKl2WpOEKJmn2toTxmzUyLGbyTMmQ/kJoRKv23kks0NWu8jrPJQkqvlaD76+CDVJX1NJVAtdVhwWIz1dgpz4cNJetITcgLBSKpo0X2wv2IXNbMRmNor80iRBnFFZyCbjAnF9jiVtBtI6Yc3Lj4vWqjLXtmim+L/pLRH+3/MP0ZzA34SWNL6wJ1uCHrWlwvoF8y8TL3y1UH4a3LQerh8+jdtjNxPFRPdFv4WqMwZ8ftGsEkXUB4PRYGSadxqHuw+T0BM0BgRRLR8q9O8W6nNtTy3d4e6MQqqhMNUriPKejj0c6jrEpNxJpLuCXDT+IiKJCK/Wv5qxnlRUx7kFOZYkpjnQPKA4azhEEhFiiRguiwuPxYPJYMpQVGVxmIaWoTpKciNTD0YU+h9jRVUe23BNEYZDetHYiRBVp9mpclRD8RA2kw2bSeRFj2WOqrzWpyocnV5MlZ7O8VFEOrl6t8L/0USUr675qnAO+RBAKqrvVui/I9jB642vA0MXnn4QkCWqWShcNqeM6aUizJszSqIKooK9oauPSCzBsW4xOLdecA//HLmNv8/6I5x+E9ujgmCebRMKamOfmV5S1kGvd3tJ6MncTamoNu9ivE0QtMWl4patMVWLFZKKanNPiEKXFU3TcCRN3g+HBWHcV9NIpKMmdaBJNeBoQqi5BXTT1ReltqOPXIeZnGQBmaZpVHkd9CY9Mf26g72n/yckcyvtJVPEMaZ3owr61Ocmo4GS6aIwjMZtIv0grZDrz9dOxGjQ0ohqUhltSOaY+ptEPm23IIERPaVyH/CeLV6YHWjVy1L7H7cECiZB0TSGg3RzKHRbh11uOEzJm0IwFqTB30CjvxENTYXb+0MqqnU9dfjCvoxCqqEwq0CQ8ZdrX8Yf9TMpd1LG5yuqVmDSTKw6mtHMTg2oVZ6qjPebAylFVea8DgepzDrNTjRNo8BekEFU6/31FNoLcZqdmaH/JDGVhV+jyVEdSlFtD7bz7JFnj7ud9GODEyeqrX2tytJrNKFeefxSUY3pMXqjvViNVqWojmXoXxKmU6aoJrevo3/kje7TydVw90RPpGfMJlwtgRbW1q/90FTJywnguxX6f/7o88SEjf0JPwveD8gS1SwUDAaNX31iLl89fxITkjmno0F1vpOEDg1dfTT6gtjNRqrKStmQmMUu82z6EgbejgiiOjUkTMEPdGv06mJAPKZ7eemwCAuOy3cKRTUehvtXcPOxHwA6E11ikNsbF2Fd/MfY3dTN7qYe5lYKgigLjF5rTVat00dX46GMY03oGjW6yGFVimoy5SAd4/IdRAKC1PhxkHCXwVV/ElXyFYuEetqfqKaF3G/6+KVgdkDD1uRyOlhEFb0irSr0n0wlaNuX2t7hV6FHENW9WsoLtqPgDEGIC6eBd0Jq+aqljARnTS5gyYT8jBSP0WJy3mQADnQdoLG3kUJHYUaHqXSUucowaSZq/bX4Qj5ybMdXVMd5xuE2u5ViOjl3csbnubZclpYvZVPTJjpDKRcGOaCm+53C6BVVOdi6LcniMttAolrprsRhcmQM3DIcLcn4aIhqX7Rv0Ar2R/Y9wvdf+z5Huo8M+GwwjFZRjcQjfH3N1zkUFr+T1r5Wih3F2E32k1JUQUwc7CY7JoMJk2Ya09C/vO6nikSmn/v7xZHhvUIGUR1GUf386s/zvde+Nyb7lBOfkUws3+8IxULqGr5biupzR57DZXbhMDloDjQTTUR5dP+jquvcBwVZoppFBqaXerj1wqkM0XhhWKjK/44ATb4gZbk2ZW/lC0Zp7g6xIzGBuGaiyC+8OHd3JOhFhJgPJsqV64BSVAGifZQH93O+4W0qbIKovh1KWk/5m/nNK8K79BsLTPDHs+DoawAcjgii4NKC9LWKcH04aWLvx04bgtgWaT6afCFa/WHG5dkzipjG5TtxIQZBv+7AbDTAlJXw5Y3CsSCdqMajEPGDPc3I2WiC0nnCw1VaUBUkUywkUe1tEXmlOZJYJYmKZoAjr6oisN2mVHGU5q0W4f2P/ZdwXzCYAQ0qTz/OtyRw41kTeOSLi0/oe5aQRHV763YOdB0YoHimw2QwUeYq42DXQULxkKoAHw4GzcDMgpmqZavcXzqWVy0nrsfZ2pyqy5SDgNfmxa4lfXrNLjpCHTT4xbUcycAniYksAiqwF9AR6iChJwjGgvjCPkpdpTjM/YhqkjR57d6Mv4eDVFjienxQIidzY9NzR4dyz8G1AAAgAElEQVRCLBFTxWIjHZBqempYU7+Gt0Jvqf0UOYpwmp2jCvPKa+a2uJWC2hPpwWYUv2WryTqmoX9JJE9VgU/6dj/qeaoZof8hrreu6xz2HWZ3++4x2af8XZyqYrl3E+mT3HeDqCb0BAe6DjCvaB5lrjKaA828VPMSP930U6555hq2HNtyyo9hrJAlqlmMGSYl/Tf3N/fS5AtRlmvHajLisBjp6hNEtRcHLd6UK9mudh1/MvTfbq/GH4phNGjCyzPNMknXjPxH7nNMdCXD9mEPcWsujXWHWb27hRUzipnSuwWad0DbXqKahV4chHQzbvrAV4sfO8dMIoeyR3dyTM8ngYFKrZVtteJBeH3ob/CLScIyylfPyp7H8SAGKD92QVTTkU5UZSOB/kVMFaeJgq7aZGFLYTIsn05UXUVgtqkGCNhyoHKxIN1dNQAcsgqi6tOduHPzRZi/ZBYYjMKwv2pxJkk+xZiYMxGDZuDxg48TTUQ5r/K8YZefWTCTxl6hDo8k9A+p8D/AxNyJAz6f5hXX8qDvoHpPDm4eiwenQZDMsytEqoQqZoj0EE/Eh9zvCzUvsL9L5DK7zUJRzbfnE0vE6An3KAJY7CgWRDU6CFG1ZRLVP7zzB77x6jcGzXVMH7gGI0SSWI9kwD4WOKbsaJoDzRkKbW+kl++s/84AZVYWl7XF2uiL9uGP+hVRDcRGTtCkAuY0OxVRDcaCWJO/ZWlZNVZQimr0XVBUP+KV/7KYCoYmqoFogGgiSluwbUzyWBVRDX/4iOqp8v5N318kEaHcVU6xs5iWvhb1TJPPgVN9DGOFLFHNYswws8yDpsFrB9sIRuOU5wrSlWs3090XUc0EfFUXqnW64jb8ydD/lRcuZ/23z2PV186iyGNLKaq5VWiLPk9Z315Mh14U6+HiQNBNtKuRApeVb6+cmhEyj5oEae4zOPEYglj8DTQkCoknPUHjVg8Gk4Wou5xqrYWttZ246WNh018E4dz9JLz4A07b93POMoo0hR4cmIz9FEhJVBPxFGHtT1TLTxP/73tO/F84iKIqmydIoumdAFMuhGgAdj4GBhPHbEKxrNOL8Dr7hdhveBz++W9DfTWnBDaTjSp3lRrAz608d9jlbzv9NmYXzAZS+ZvHgySqRY6iQQuwJuROwKgZOdiVIqqS9HksHgqMBVR7qplTmGnB1b8pQDrag+18e923uXPznQA4LSlFVX4uc12LHcUi9D8CovrYgcd4ue5l1tavHbDP9GMZLMQsCaq0YxoO0jbLqBkJxoIZBOuNpjd4/ujzvFIrcv6aA83EEjFFhNvibSptoNxVPuDcjof0HNX0NBBZSGUz2sa2mCp5bKdMUU07949y6D+WiGWQ9qHuifSJ1Egt4IaDvJ9Gq6jW99Tzry//6/tKiZWTW4vBomzcTiWkKFDhqqDEUUI4HmZry1ZMBhNXT7majlAHh32HB133tYbXVCTr/YAsUc1izOC0mphU6GLzUZEvWCaJqsNCV1+UY92CqGpTL1br9Op2VUylFU2jKt/B1BKhYClFddJymH2NeF37BgA+3UXQUUaVsZMN3z1XmNS37QeLG1bcTs3kTwOgW9wUmoIU6e3U64XY80WBTXlJCY//61LwTmCc1kJtR4AbjC9jjiUHo60PwD5RpDNPE3l7Lo+XafLYJOy5oCfg/hXw50vFe/17OFefLULzR5KVq0pR7YR4DALtQlGFFMn1ToTTPiuU1WgA3GXYrHYejp3PY/Gzla9s6jjyxLLvMmQ4flb+LOVxOhRybbn894X/zdfmf42PTfzYiLYvie1QaQVWo5UqT1UGUZWkz21x88+5/8z/XPQ/lDhSx2YSHZ2HVGnkA16G4KWiqohqqF3lfhY7haI6XDFVKBaiJdCiBqp7d9w7QMnIUFQHUTCl4jmSgVcSzZkFwkUiPfy/t0N4CHeEOmjsbeTixy/miYNPqO33Jnp5q1WE/6d6pw4oFDsepDG+0+JU4X4Au1H8xq2msVVUR9uqdrT4sCqq975z76jySOVvSk6+hpoYdIZTueIn6uE72H57o71E4hH2d+4fkbXcmvo1vN74Om+1vDWq/T175Fn+Y9N/nNCxHg+ykGp8znjg1BdUyTSnCneFejbvat9FtaeaxaWLAdjWsm3AehubNvLlV77M3VvuPqXHNxpkiWoWY4rZFTnEE2IQThFVM76+CM1JoppfPgFK5wLQi51jxjKwelJV7xJSUZ20HMoWgFLUNDb++1UsmD0Hgx7FIts9tu2Hwqmw7OsUXPw95lflYnfnUR5vwqzFadQLyS8T1edmZx6zynMwFUzEoYUpp53PmZ5H95TDjCuEOpucUVo0ESK+74vn4bD0Kz6SxLJxm2o+MEBRdebDtEtSf+dVg9EiFNVAK6CDbPcqSW7+REGCl35N/J1TjtsCt8Vu5P/iKwcqqu8RJFG9YNwFI1reaXbyhTlfOC6plShyFPGdRd/hS3O/NPQx5E6m3l+vVJ6eSA8WgwWbyYbD4KDAXpCxvwm5ovhsqDzVY4FjGX+n56hCUlFNEtUSRwlOk5NIIqIUCEnEvNaUoio9DIscRezp2MPf9v9NkVVd1zMGrcG6IElSnV40NhRkx6zTS0S+slR/AfZ1iqhDZ7CTmu4aYnqMI91HMq7FizUiajE1b/REdSSK6pgWUx2n6v8P2//Ab976zQlvP33S8GHKUV1bv5ZVR1aNWC2XEyn5OxpqPV8odR+l+wyPFM2BZra3bld/p/8uZBOLkXyfMpc7PV1hJHh478P8bf/fTon9lgz9T8gRz59TnacqJ9zlrnL1vSX0BJNzJ7OgSNgmbm3J7LnUHe7mh2/8EIfJwWdmfuaUHt9okCWqWYwp5pSnVL2yXDE45Tks9IRi1HQEsBgNFDitsPzfecJ2FQHsPOm8Dr62faAiOPMqOP2LMPECUZQ0/izxvi0Hu80CucnKf1+9UCcDrUqtLHLbePLLy3AUjsOEyNfrzZ2C1VuptgFgTHaDusy4kUKtG23+p2Du9WIZRz44BDnRzU4q8lMdmhQkKTWY4dJfQ9n8wQua5v9L6rWzSGy7ryNVYDWYogpwxpeEeX/VEtxpjmEDFNX3CBdVX8SysmUjVkhPBJ+a8SnmF80f8vPJeZPR0TnSfQRd1/GFfKpSXyKdqMq81vRBNR3HejOJqqr6TxLVjmBHKvSfVFRhoLqXXky1o30HAD9d+lOKHcXcsfkOpdxIj06zQXzB/ZU7XdePq6huaNzAO23vACL0bzaYmVMg0h0kqdZ1XbW87Qh1KIWnM9iZUSjzduvbOEwOKtwVOMwOooko0fjIwoD9Df8l5GvZBGCsoIqphiAWTx9+micOPnHC288I/X+IFNXuSDc6Okd7jo5oeXl/SPu5oSYG6ROp9BbDErqu82LNi0M2DLhj0x3c+OKNatKXnm6xq30X4Xh4RMcsowgjUV8lYokYB7oODDiPsYI8JjlRPhVENT1SIxXVcnd5xvNvYu5Ecm25TM6bzLaWbei6jq7r/GXPX7hh1Q209rXyvdO/N8A15b1ElqhmMaaYXZEKe5enKaoAm450MKciR3S3mng+672fAMDjcgrVsT+KZ8AlPxdFRpBq2SrJXE6SdHbXCzUVhKKajiv+yNsrHuXi8J30TLtWVOpDihQnrZ0+blwv/q5eBhPPh6olcNatgngCmrVfyF9CHsvc62Dh5+CLawcegzx2TwVoRrGOJKr+JFEdLEcVwOqCr74Fy/8Nt0Xkx9rNRuwW4+DH8y5jfM54/rjijxQ5it6zY5Cq7qqjq7jy6SvZ0b5DdZKS8Nq8WAyC3E/JEznCx1NUTysWucUyNzbfLu5RqaiaNJNwFpB+o/3UvfQc1Z3tO7Gb7JxRegZ/v+zvzCucx6MHhE2MVI3kYNJfufNH/cR1oeoPNoBG41FuWXuLCtXV++upcFdQmszHlkS1Ldim1u8MdSqFpyPUkXEtdHQm503GoBlwmByDHtNQkOfismQqqvIaWU3WMfNRjSaiRBKC8AxHnDpDncQSMf5n1//wszd/Nqp99EX7hpxAfJAhCdxQOYr9MVJFNT2dZjCiejBykFvX3crzR58f8Fk8EWdry1bC8bCaRMqObQC7O4STwEiM6+U9PxoyWNNdoyZRI8kFHy3ag+3YTXZF9v0RP7vad6nCx9EgEo9w/bPX88CuB9R7L9a8yLK/LlPXp7G3EbfFjcfiyUh9mpQn0qgWFi+kLdhGnb+ON5re4O4378YX9nHTnJu4YtIVJ3OqY44sUc1iTDGj1CNM7IGSHEEwJVGNxnXOmOBVyxYlC9xl16njYuL54n9JDnOThu6+OtVtagBJtLqYvXgFFy9fwRfOngT5k4XtU65IAZCEcJKhibhmgvKFYLLA516AJV8R1fQg2qIOekwXwLRL4dzj5HsZjPCx3wjibTCAw9tPURWerpTNF2kA6Yb9BvEz9SQv0/sl7P9+wZRcQTwf3PMgtf5abph+A78651cZy2iaRrFTXGPZ8UoOqg3+BtW9BYSiajKY+PnZP+eX5/xSDc5SUW0LttHS10KRowiDZlCpAf0Le2SOaiAaYHf7bmbmz8RoMJJny+MTU8UkbdOxTWowLXOKSVR/QtQdSqlCgxHVd9reIRgLZhDSYkcxxQ5xvvJ9GfYHoQrL8GhnqJPucDcGLTUcTM0T10ie20gr/wPRABoadpM9Q1E9FaH/dHI6GFENxoIEY0F0dDpDnTx56EkeO/DYqCqdA7GAmoQNlpLxQUR6IeFIiaoktpLwDJWjKhV/t9mtFL10+OKCgA5mm3bQd1Dd+/I+Tw/9S8ur5kDzcbuEnYiiKqMNkNkqeaTY37mfH73xoyFJfEtAPDM8FjGWrGtYx/XPXc8zh58Z9b5eqXuFXR27eOzAY+q9P+/+M/6In53tovi3sbdRtbSWzz5I+VHLifjGpo2sbxBCzf9e9L/cPP/mk7ItPBXIEtUsxhR2i5EZpR7Kk9ZUkBmmPn18SjkttIsfg9c5wu5I3gkweWWKsI5EUUV0iPraBZOFk0DeOPjKm0L9BMirJpFsddmdOxMs/dqGliSJqnUIopo3Dq57KM0DdRhMugAWfV68dhYJd4HO5EAhierpX4Bv7YdBFFwZ+s8S1UyUu8uVYvfjxT/mu6d/l0pP5YDlxueMp8BeQLlLWJRJ1ebuLXfzry//K2+3vg0IRbXYUUyho5ALq1MOFXaTnWJHMXs69qhBB1Ch/75YH72RXkWa8m3iXt/dsZu+WB+zC2erbclihk1NmwaoVf3Vy3S1M52oHuw6SEughc3Nm8VnwU4C0QCBaIACewG51lwsBotKU5CFVMWOYnxhnyKwHUGhqOZac/EYxH0uyXx/En489EZ7cZqdGDTDkKH/aCI6rDXYSDFY29p0pKdJtAXbaA40E4wFR6WyBaIpovphUVT9ET960qt51Iqqa3hFVd6fswtncyxwTKWMyFB/b2JoA//0wh75eXroX1orRRPRYUPzuq4rojrcd90Z6uSml25iZ5sgdukTuZGE/n0hn1JgeyI9fP3Vr/PUoad4o+mNAcuG42FqemqYlDtJpRJJcij3u7Nt54gr7R8/8Dggoid1PXUc7DqoCGq9v55oIkpLX4t61tlNdnKtuViNVvXemeVnYjfZefbIs7ze+DplzjJV6PV+Q5aoZjHm+M3187nv0ymvVNmO1WjQOG1cqtCo3AlXzCvjsrmDt90cFJ98FC74kXjtLASjVeSotu0THqQ5VcOvD6LFqClJ9sw2fCYRJo5XLB64bLLoa0hF9UQht7s/GQJzFQ+9bBIWo0ZZjo3qE+ga9mGGQTNw05yb+MZp3+DKyVcOudyPF/+Y+1ferzxcu8JdRBNRtjQL4+s7N99JQk/QFGiizFU26DaWlC3haPdROkIdSqWQ4fE1dWtY+shSNh8TxNFtcWPUjCrvTRYwABQ6CpmUOylDUZWh+v5ENT2c2hPpIZqIous6n1v9OT7/4ud5o1EMjDE9ptwPCu2FaJpGkaOII91H6Ax1sqFpAwbNwOLSxejoHPIJNwtf2EdnqFPZeUEqPUIpqiMM/QeiAbXOYKF/qayORZ7qYJZg6UgnG0d8R9Qyo+nWFUvEKLSL58Noiqki8QhPHnzyhMK6pxrp+cjyHjjuOpKojkBRtRgsTM2bSkJP0NjbyM62nSz8y0K2t24flqimV+jLSUa6opr+HacXCAI8tPchRfy6w90qJWQ4RXV1zWo2NG3gri13oev6gIjDcIjGo1z+9OX84s1fAHD7xttV8ZIkjOk45DtEXI8z1TsVT1L0kGS6tqeWvR17+edV/8x/bv7PAev2RnozogC1PbVsbt6sIjxvNL3Bk4eeVJ/X++tp7hWqsySlAItKFnFm+ZkYDUJAcpqdLK9azjtt71Dvr2dZ+bL3nZIqkSWqWYw5xhc4mV6aInZSUZ1VnoPLmqqaNxo07rluPksnFpzYjgwGoWS2HxAtSktmqTD5aBB0CXLrmXr2wA/zqkWr1BG2Jh0xZMFV+wGRiuAc2TV44svL+I8rZh1/wY8YPj/783xu1ueGXabYWcyEnAk4zU5MBhO+sI/d7ULtzLPmsbdzL4/sewR/xK/yyPpjWdmy1PaSoXWpqG5s2oiOTktfCzajDYNmSBE0o02pqBKLSxfTFmxTVc5yn/39OuVgK/fnC/lUXmltT23GwCgHWzmIXVB1AY29jVzw9wt4q/UtVo5bSblbDF7Sb1VHp8HfQK41l/GW8eRZ8xRRVWrxKBRVl1l4GGeE/mVnquR7Y0JU08jSYMeXTlSl6wKMvFuX3KbH6sFmtI1KUV11dBU/3vBj1f73ZBGOhzMs2E4G6ZXwTb1NI/puJbktdBRi1IzD+qjm2fJURKPOX8eO9h3o6Ozu2D0kUdV1nbdb31bpJ/K76432qlzvdKQ7c8QSMX7+5s/51VaR7pM+EUlXVLe1bGND4wb1t0z32dG+g/UN69nbuVcRu+PlqB7uPkxnqJNd7bsIxUK8WPMip5ecjt1kH7Qr174O8bucljdNhf4lanpq1G/4sQOPqaJIgKORo5z76Lnc9vpt6LpONBHlF1sFOf7hGT/EpJl4+tDTPHXoKao91ThMDur99crDNr0g6lfn/op7zrsnY9/puahnlp857Dm/l8gS1SxOObzJHNQzxg984Jw0cqvAVys6P8286oQ2UT5jCbrZgXXCIGRU0+DGl+Gcb5/kgfZD6bxk21OEMmwYWXFUSY5NKdRZnBg0TSPPmocv5FPq50+X/RSHycG979wLMCRRXVy6GC2ZKiKJo9MkFMT07liSoMr/l5QtUWqixJKyJYDogpW+zwGKalJdkrY2naFORTIl5GeSqMpislsX3sp3F30Xu8nOTXNu4s6z7lQpCTL8C0KNzbXmcqHrQp6/+nlFUOW5yRzVX2/7Na/UvTLotQGRx+myCKI6mD3VmBLVfopq/9zT9NB/OlHtr8YNBXnOTpNz1DZdR3yi89eJWDT1RzwR56uvfJVrnrlmRC10j4f0ic9IK//bQ6LwLt+WL5pADKWohrvw2ryK8DX1NikXjZa+lhRR7ee40djbSFuwTbl7yCiCP+JXeZYgWjFDZkFVa18rcT3O4e7DtPW1ZUxE5LnGEjG+ufabfPmVL7OhcQPheJg3m99kUu4kLAYL31n/HfwRP0vLxBhwvNC//J3V+euo99ejozOvaB4z8mewu2P3gNQWufw070Ci2tTbpIiqQTNwx6Y71Dn+n+//CMfDPHvkWX666afc8uotrK1fy8XjL+b8qvOZUzhHpBZF+/jWwm9R6a6kwd+gng/piupgWFiykHJXOSaDiTNKzxh22fcSWaKaxSnHvIpcbrtkGl84a8LYb1xaVGlGmP3xE9vGeT9A+8oWUeD0bsFsS4X/RxD2z2JskWPNwRf2saV5CzajjaVlS1kxboUaIIciqrm2XNWEQIb+7WZBRtPJV/9Q92DtZc8oPYNCe6EadAvtQq0aKkdV2tp0hjqVqf+1U69lYs5Erp16LQB7OvYAKUVV0zRumHEDb1z3BjfPvxmjwaiIKqBINwjl0KgZVegeUopqIBqgK9TFA7se4NH9jw56bUBUaUtFNd3wX7ZQlddjLEz/JVkyaAZ09AHkN51spBfKjDT0L78Hp9mJy+IalaJa01MDCBJysvjTjj+x8dhG4npcbfdkIFXGBcUiFWUkeartfe24zW5sJht2s33Y0H+eLU+Ry6beJqV+tva1DqmoHu0WZFmSpfTQf441R5G7mfmiiUW6opr+enPzZmW7pqGpc91ybAudoU7iepxvrfsWD+19iGAsyKUTLuWW026h0l3JnMI5XDHpCjwWz3FD//s7Rb5sT6RHKaDVnmpmF8wmEA2o72nTsU1sbd7K/q79ovreWYLdZFdNR2xGGzo6a+vXkmPN4bIJl7G3cy9tfW389u3f4k/4+Y9l/8GUvCn8/cDfWd+wnhXjVnDHmXegaRorxq3AqBm566y7OKfyHCrdlRwLHFPpTLLpx1AwaAbuPvtufnXOrzJ+9+83ZIlqFqccBoPGF8+eSKF7hEVTo4HMSZ14XsqLdLQw21OE992EDP9nieq7jjxbHscCx3i79W0WFC/AbDRneMHKfNHBcFaF8POtcot7T+aoAkz3TifPmqcsrewmOxoaZ1cMTCuxGq18Yc4X1N8eq2dQ5U4R1TRFVSp110+7nqeueEqRZ5lzKPMqJdJzz6S/K8A4zzj1Otfar6MamTmqR7qFSjiUPVA8EScYCw6eoyo7U41CUU3oiWGXk4qqzDnuT57SFdX0IpWRElW5fYfZIb6XUVT9y4nEyRLV7nA39+64V018BqukP5FtQqrrW6O/8bjrtAXbKEh6Sg/VVjccD4s0GlseJc4SNDQaehvU/ZJOVPt3hZP5nZKIdoW6iMQjRBIR3Ba3Cv/PL5qPhpZxD2YQ1WOb1fdb5anCH/ETT8RZdVR0GfzhGT8kFA/x622/BkS4+1MzPsVjH3uMhy55iDmFc8i356tJjq7rrDqyShFpifR81tcaXgPEb0kSw13tu4gn4krF3de5j2neaWiahqZpKk9VPkt8YR/T8qap72Rv5162t22nwFjA5ZMu578v/G/+33n/j39c8Q9+ec4vlWXaJ6d/kvXXreei8RcBItSf0BOsa1jHxJyJg6ZN9MfcwrmcVzVwIv1+QpaoZvHBhrRxmvfJ9/Y4TgQVi8T/7ixRfbcxp2AO4XiYaCLKheNEZf/CkoXKImooRRXgs7M+y73L72V6vuiklq5ETMqdxJ8u/BM/XfZTAC6bcBmfmfkZ5cHaH1dPvlrty2Px4DIPVO4kUa32VANiEK/z16GhqRw0GeqXhKy/j2w60hVVmYsKKb/YdKTnqErlrTnQjK7rPLjnwYw0ABkql6H/oeypgBFZVP1+++85/9Hzh2xyIPcnr23/giqZZygVXhDtc0ca+pdkzGl24ja7Mzw9h0M8EVceoidLVOv99cT1uFLkZe7hyUASVdmWuDV4/JzdtmCbmvw4zIMTVfk95VnzsBgtFDmKaOptoikgrkFLIBX690f8GYVmkqiO84zDY/HQFe7KaIUsrd6qPFUU2AsyiWoytcBkMLH52GYV+pfn1xHq4JW6V5junc61067lzxf9mSJ7EdWe6oz7XyLflq/unScOPsF3X/su1z57rbKR0nWd/Z37VTRi47GN6tgl0dzZvpM9HXvwR/zKJk06achzAlhZvVK9N8U7RT1TthzbQm1PLZVmIaB4bV7Orzqf8TnjMyadmqZlpBJUusXy4XhY2U99GGA6/iJZZPE+xtR/Eib7pfPe6yMZParPBIv7g3nsH3DcctotfHHOF9HRFdE0aAZunHMjTx96OiMvrj+sRitLy1P5zJLMAVR6KlXnK4DPzPrMsMdhMVr42dk/Y1/nPmwmG06Lk/ZgO5987pNcPulyPjH1E/jCPlGRn1S0ZI5qsbNYkcF05cRusg8bxksnzVO9U3mxVrRMHVRRNaUUVaky9cX66Ax18outv8BqtPKPK/5BibNEKY5y34PaUyVTAI5n+h+MBXl438P4I37W1K3h6ilXD1hGkqUCewEHug4MIE9doS6sRitVnir2dOzBbXbjtXtHHvpPEmGHyaGUbl3Xj1sZ3RRoUiSsKdA0onWGglRQF5cuZtXRVWOiqMpw+DjPOEwG03HzXkOxEP6IX6WTOEwOmmIDCbi8PySpLHeVc7DroCL4Db0NJEj5n3aHu9W92NjbiIZGqbOUPFseXaEUUXWZXereLHOWUeIsGVRRPa/yPF6qfYktx7ZgNpip8oiIx0u1L9Eb7eWS8aKN9ZzCOay6ehWReGTQ78Vr89Id7uZo91F+9ubPlD3Zba/fhtlgZnbhbPxRPwuKFvBW61sEY0EVRfFYPBTYC9jQtEER+2neaezr3Md0b6pFeKG9kEA0oNocg/Aulo02/nH4HwCKqI4U6cVTHyaimlVUs/hgw2AQJvnvU1uNYeEqgm8fgkU3vtdH8pGEDOmm45op1/CXS/6C2TjygrX00L9MBxgN5hXN47pp1wGCGLb0tbCjfQe/efs3BKIB5XHqtQoy2hnqpM5fxzh3KmxvMVqUIioJxXDHK4mjNPaHwRVV5aMa61OhfxA9whN6gmAsqLo9SSXYbXarY5JIdz+A4yuqzx99XhGVl2pfGnQZGeqX59tfUZX5kpIwFDuLKXIUnXCOqjzf46GmuwYAs8FMMBYcsgPaSCAV1Im5E8m35Y+popprzaXIXnRcFwSZ8zliRTWNqKar0P1N+tOvS2NvI0WOIixGC3lWQVTV/ZQW+i91lVLiLKEt2KY8WpsCTViNVj4989OYDWbq/HUUOYoUud1yTORrypxcEBOn/m2WJSR5vn3j7fTF+rjjzDt46JKHKLAX8KM3fsSDex4EYMW4FWqd6pxqQCicH5v4Mer99Ty490HsJjv3XXgf/7bk3zLU058s/Qn3rriXPFsqTWiad+UgPE8AABrASURBVBp2k50JORNUakSVeXTPE6mo9j/fDzqyRDWLLN5LmG0fTJKdhUI62U3P+TyhbVlS2+oOd/PXfX+lO9QtzPitHuwmOxubNhKIBgY0NZBEon9+an9omqbC/9U51aq17GBE1W1xo6HR1NuUQVTl4O+2uHmp9iW+s+47HO4WqQGDKaqjqfrXdZ2/7vsrdpOd04pPY/OxzYP6YQajmY0V+pPIzlAnedY8lQZR7BTduvwR/4gsmSRRTZ/QDFVQdd/O+1RoWOanSqIgQ98nAqmgVrgrqHBXjE2OaqQbk8GE3WSn0FGoiOhQkK125XV0mBxEEpEB5vRSUZUTqnQv4vTwtGq40Y+oyvfzbHn4wj5lieW2uLlq8lV8ZuZnqPZUU+osRUdXSmpzbzOlzlLmFs7lnvPuwWKwUOmuJMci7uftbcL+baRm9vJ+2tqylTmFc1hcupgSZwn3nHcPCT3BQ3sfAmB+8Xw1SUr/3V8z5Ro0NLrD3SwoXkCONYePT/l4xsStylOl0g6qPdWYDCaVgy6VV6NmpMw8uJ/zUCh1lmLSTJS7ylUDkQ8DskQ1iyyyyOIkYDaYVRVvuqJxIpD5lNdOvZYSZwn377qfjlAHubZcDJqB66ddr4hPf/VWKkHHU1QhlSpQaC9UxVWDhf5tJhtLypbwRuMbNAeaVeGSrCq+fentnFNxDs/XPM/3139fnMNgOarSRzUZ+h+u6r/OX8fezr1cMv4Srph0BTE9xpq6NQOWk0RSnveA0H+4C6/dq4h7iaNkQFvZ4ZCeoyqvaXrI+cmDT3I0cpSEnuB323/HH975A5Cq+F9SKuzHTiZPtbG3EbvJTp41jwp3BZ2hzhF72g6FnnAPHotHNYToCHYM25hApgao0H8y1aX/xEASR0lo062R5hTOUa8lYZQWVb2RXrrD3Sps7bV5ietxdZ+7zC7mFM7h1oW3YtAMKo/znbZ30HVBWGWe99kVZ/P4xx7njjPvUAVLnaFOiuxFQyqo/ZFebLiiKqWazi2cyzNXPsMPzvgBtyy4hRneGeo3mE5UK9wVqnhycckgTWT64VsLv8VdZ92lojgydWhy3mQs2ui6EJoMJr4y/yvcPP/mUa33fkeWqGaRRRZZnAQ0TcNutpNjzRlUlRwNihxFGDUj/zLjX7jt9NswakbielwNhDfOvlERyv5EVSmqwxRSSUzxTqHCVYHD7FAK0mBEFeDyiZcT0wWRWVwmBl5JxuYVzeO3F/yWnyz9iVpGqo8mg0kVnPQvphpOUZUNEBaXLea8yvMwaAbVdSgdMvQvierezr18+eUv09rXSl+0j2AsiNfqVQSrxFmi8g3TieqGpg38edefh9y+w+RQ/dFll7GOYAc/3vBjXuh9gba+NmKJGPX+etqD7dT21GI32ZlXJHLP04lqTXcNZ/31LOXf2x89kZ4M5brB30CFuwJN01TetCw8OlH0RHrUfVpoL0RHV3ZMB7oO8MzhZ4glYtR01/Bm85sDQ/+mwZtA7GrfhUkzKaUwnajOLZyrXk/MmQikFFV5PumKKqAK0uTER2Jhseh6uK1lGz2RHvpifRkuHdU51RQ5ipSiCjA+d+StQdOLDZePW57xWZmrjOumXcfnZ38eTdPUxFQWOkrcNOcmZuTPyAj3D4V5RfMylpNEXDogjBY3zr6RSydcekLrvl+RLabKIosssjhJlLvKldp4MvjS3C9x5aQrqfJUUeWpYl3lOhp7G5US6La4+cZp3+Bnb/5sgEeiJGQjUVR/cMYPVOhWqqv9jcglzq86X7kRLCldwvNHRdtfl9mlBvWrJl9Fa18rv9v+OzV4a5qGzWQjGAsqoipVLZnHORhkqHZe4TxyrDlM807jrda3BhQl9UX7MGpGRbCfPPQkzYFmHtn3CB+fIjyV82x5ikSMzxk/wDBe13Xu3HwnNT01XDLhEkVkdV3nUJew+nJZXEzxCvIlu0NtOrYJgI5YR0Zof1vLNg50HaDaU62IV7p90rqGdfjCPtbUrRnUYP32jbezpm4Nq65aparbz6wQHYPk9hr8DUzOmzzk9TseusPdalvyfNuCbRzuPswtr95CMBbk3h330uhvJK7H+acJ/wSgivmkUlnvr1fhZV3X2dm+k8l5k9V3nR76Tyeq0g9Y5mHKvFu5vPw+63uEBVv/+7LEWUK5q5xtLdvUtR0szC2PE1LWbiOBnPhM907PKE4aDLMLZvPckecyCigBZhfO5m+X/m3E+0zHvKJ5fHrGp7l80uUc2XLk+Ct8BJBVVLPIIossThJ/XP5Hfn7Oz096Ox6LJ4OEGDQDle7KjPy2qyZfxYbrNwwYnCVBPV6OKohCJ6l8Xj3laj4141Oq4Kk/bCabqpheULxApSdUeaoyiOOX5n6JDddvYEb+jIz9aGgqD3Zm/kwm5kzk7wf+LgpmIgNzPre3bqfYUazO77Ti0+gMdQ7ooNQX68NhciiFT5LPpw49pcLVXpuXRSWLePDiB1kxbgUzvDPQ0FhbvxYQfphSHX6z+U1AFP3c/ebdrG1Yy7LyZeTb8ilzluE0O5WiurFJWBJ1J7ozSPd9O++jM9TJuZXnUmgvxKSZMhRQaQ6/qyPVKUudT7SPtfVriSaiPLzvYVr6WojpMaWkStLUv6AqGo/yzbXf5IFdDwzYZn/ouk53uFspqpKovtP2Dje/cjNmg5lPTPkEjf5Gcm256OjKgkzeVzIc/vjBx2kJtPD4gcdpCjTRHmxnVkGqvXOJswSjZsRusmfc05I07mzbyTl/O4cHdorjluRZTpyUomrOVFRB3BM1PTXsaNsBoGzl0pGuqI6GqFZ7qvFYPHxi6ieOu+zHp3yc1R9ffVxCOxqYDWa+tehbJzUZ+bDhlBNVTdMma5q2QdO0A5qmbdE0bcYgy1ypadoOTdO2a5q2W9O0O7TkE1DTtM9omuZLfrZd07SxaZ6cRRZZZDFGyLfnn3TYfzSQPdHTMbdwLnaTXXk5jhQXVF3AdxZ9Z1gLpVsX3soDKx9gnGecIpCDFY71zwO0GqzYTDa1baPByJfmfom+WB+ffeGzLH1kKb/f/nu1vD/i57DvsAqbQ8pmZ1vLNkD0aP/sC5/lWOAYDrMjwx4MRPGPrMz22rxomsa8onkYNAOlrlKWlC1hXf062oPtSh0GQVR7Ij18fc3XeWjvQywsXsivz/21MmmfnDuZg76D6LquiKqOrvJ1IWUEf+mESzEajBQ7i1UBlOxnD6KzUVtfGzeuvlER5NcaXyMcD6Oh8dj+x5R6K0mQVKr7t2X9+daf81LtS8N2DJMIxUNEEhGlUso0kacPPU00EeWHi3/Ij5b8iNeue00pgsFYEJvRpgjjpLxJnFZ8Gi/WvMgXX/oi/77x37lz850AGUTVZDBR4a6gyl2F1+ZVarbMUV1Tv4bOUCc72gXZlIRchv7lefYP/UMq/P/X/X8FBvc9PlFFNceaw+vXva5U+eFgNBgV2c/i1OHdUFTvBf6k6/oU4GfA/YMs8zIwT9f1ecB8YAVwWfrnuq7PS/57f7dQyCKLLLJ4DzCvaB5bPrlFhVbHEg6zg0UlokGFbB07EocDi9GS0UoVhK3PxJyJHO4+jN1k594d96q81J1tO0Xf9MIUUV1QJKrnt7Vsoy/ax79t+De2tmyltqcWh9mRoQTPKZiDQTPwYu2LmA3mjJCzxFWTryKmx/jrvr/yQs0LlDpLKXeV82bzm3z/te+ztmEtl024jN8v/33GtqfkTaE73M3GYxtpDbaqiYl0QJAThLmFc5WH55S8KRztPkooFqKht4H2YDtGzUg4HuYXW3/B5ubN3L7xdqLxKC/XvgzATXNvwh/1c8+2e4AUgSu0F2I32VWXJF3X+fOuP/PIvkcAkevZGerkN2/9ZtCcW0BV0itF1S5IlmwxK309XRYXRY4iRfAK7AUZE5lrp15LNBHlSPcRDJqBdQ3rgEyiCvDLc37JnWfdiUEzUGgvxKE58Fg8irTmWnNxmV1YDBZF+CRRDcaCmA3mQXOn5eTlYNdBJuRMGLBfELm0sshxtL+JE/W9zeLU4JTmqGqaVgQsAC5MvvU48FtN06p1Xa+Ry+m6nt7ywwZYgUzTtSyyyCKLLN5zlDiGVlT7w2q0EtfjGe8ZDUb+sPwPNPc1ixDrM5/gttdv42+X/o2tLVsBMhTVPFsek3In8Wbzm/xi6y9o7WvFbDATTURxmDKJ6rLyZZxfdT5twTY+NeNTGQU9EudXno/X5uXeHfcC8LlZn6Mz1MlTh56izl/HJeMvUb3U0yFDsb/b/jsArph4Bf+7539pDbbiMDk4t/Jcdrbv5LIJKY1ldsFsXq1/lX2d+5RCeOG4C/n/7d19cFX1ncfx95ckBIg8E1RIIDxJoddAweITKFqkkLHoKi61at1d2lmVrTvS9h/qdKTjdNpxuq07u9udjrWBpaNtYDZqaQ277AKKrEor0vCMGBJg3USUZ0MS8t0/zkm4JPfecCEPJ+Tzmrlz7z2/c+793Q8nx6/n6feHij80D+tZcaKCH7/7YzYd2sSU3Cksji1m06FN7Dy6Ezh/SDyjVwYTBk1g76d7cXeWb1nOmn1rKBhQwNyCufxi+y/YfHgzvyz/JRmWwYLxC1oNoXm8LrjNV9PexvgL78YOHNtqBLUbr72RA8cPtLpAb86oOYwfNJ7JQycztO9QflX+q+Z7gMaLH41pzug57PlwD2bG4OzB1HxWw/wx87l3/L3UnKkho1cGEBx6HzNwDPn981kydUnzOa/x8vvn8+DnHmRA7wF84/pvJJynaajS+sb6Cy6Qku6noy+mygeOuAeXg7q7m1klMAqoiJ/RzG4B/hW4DvgXYG1c8+1mtg04DfzU3Vcn+jIzWwosbXqfk5NDWVlZ+/2aNtTW1nbq93V3yis9yit9yiw9F5NX7eng1lLVO6sp25d63pH1I6nzuqSfWU01RTlFlJ4s5aHVD1FZX8mgXoOo3FrJYTt/bufwuuHsP7Ofkr0ljMgcwYy+Myg9WUrtyVq2bNzSPN/pg6cZ12cceeRRvrmcclqfCwpQlF3ELnYxMmskY6rH8NnZ4FZLva03009NZ926da2WOVYXXKW+vWY7IzJHcE31+XOE+9Of3MO5FF1VRE5FDmUHg99bezbIquTNEmoagvNmx58Y37zcDX1uYF/dPn6zJzjM/vm6z7Nx/UYeznyYkj4lVNRXsPvt3XxgwT1q+53uxye1n/Di2hdZc3QNY7LGsLjPYqoPBTft/9mW4F6fjd7Ij9b+iL69+tLgDdza71YyLINdZ4M9p0c+OELZR2W4O1mWRb3XM7xueKt/p961wbnF546fa9X2WJ/H8FPOyRMnySSTEb1GsP4/1pPMFKYwMXsiZWVlZNQHRemQ6iFUHQsK+LKd5z9/Sd8l0ABVW6uooirh500n2Ku6cf3GpN95rV9LVq+shP+e3YG2XyF377AHMB3Y0WLau8BtKZbJBd5omgcYBvQLX08CqoCbLub7R44c6Z3p9ddf79Tv6+6UV3qUV/qUWXouJq/Tdad9y5Et7fadjY2N/u0N3/ZYccxnrJrhu4/ubjVP9elqX7ljpZfsKfFPP/vUz9Sf8bklc335W8u94VyDx4pjHiuO+YfHPrykPtScqfGZL830VTtXJZ3nxNkTXrii0G9/+XY/cvKIN5xr8CnFUzxWHPMn/vOJhMscP3vcY8Uxf+q/n/K5JXN9Tskcb2xs9Dt/e6fHimO+rXqbl39c7qt2rvLK45UJs4n38q6XPVYc82VvLPNYccxL95W6u/vZhrM+deXU5hxmvjTTp62c1vz+gVcf8PKPy/2+V+7zqSum+v5P9zd/ZtGaIo8Vx3ztB2sT9n/WS7P8he0vpMyvvKbcq05UpZzH/fz69d2N3/UF/77AzzWea3OZnqwnbb+AQ56kluvoPapVQJ6ZZbp7Q3iBVD5QmWwBd68xs7XAA8Amd/84rm2Xmf0euBX4nw7uu4iItNAvqx83Xdv2jcwvlpmx/JblDMoexJcLvnzB4eImuf1yeWTyIxdMe+XeV+id0Zte1ovsjGwMu+QBF4b1HcamRZtSnpvYv3d/nr/jeUYPGN18384hGUOoOVeT8KpzCO7iUDCggPWV62n0Rh6f8jhmxj3j7mHXJ7soHFaImSW9Z2bL/jRl03TawA3XBBcV9c7ozcTBE9lxdAeThkxidv5sfv7+zynMLWT61dNZsWMFX/1dMEzv41MeZ9ygcc2fObzfcCpPVjZ/Vsv+r39gffM5pcm0vFVaW34484fUN9YnvChQpKUOLVTdvdrM3gMeBoqB+4EKjzs/FcDMJgL73L3RzPoDdwMrwraR7n44fH01cCdwaTcoExGRyMnJyuHpm55Oa5n48xIH9h4Y3A4pPM/xUlzMBTSz82df8H5oxlBqztUkPBe2SWFuIRUnKoJbP4W3PHpy2pOX1McJgydgGA2NDc0XgTWJDYux4+gOZuXNYvH1i8nrn8eXRn2JnKwc5o6eyzNvPUN2RjbfvP6bF3zmos8tojC3MOnV600jJrWnzF6ZbRa/Ik06Y035W6DYzJYBJ4BHAcI9o993960Ee0+/Zmb1QAawGnghXH6Jmd0D1BPcpeCn7t56PD0REemRnrv9uYseIrM9Dc0cCnVcMDJSS7FhMV794FXmj5l/UYMxpJKTlcOoAaM4eOJg8y2amszOn03p/lLmFcwjOyObBeMWXNCH1QtWtxo0AWBewTzmFcy7rH6JdKQOL1TdfQ9wc4LpRXGvnwWeTbL8MmBZh3VQRES6tWlXT+uS7x2dNZq37e1WIxPFu2v0XWz9aCuPTXmsXb7zusHXBYVqi0P1M0fO5N2H3k25Z1i3XZLuSCeIiIiIXIIv9PkCGxZtSHmrrmF9h/GT2T+55PNnW7p5xM30zezLLSNuadWmQlSuRDpJRERE5BKYWaeOSAawcMJCvjL2KwnvHSpyJdIeVRERkW7CzFSkSo+iQlVEREREIkmFqoiIiIhEkgpVEREREYkkFaoiIiIiEkkqVEVEREQkklSoioiIiEgkqVAVERERkUhSoSoiIiIikaRCVUREREQiSYWqiIiIiESSuXtX96HDmNlZoKYTv/Iq4FQnfl93p7zSo7zSp8zSo7zSo7zSo7zS05PyynX37EQNV3Sh2tnM7JC753V1P7oL5ZUe5ZU+ZZYe5ZUe5ZUe5ZUe5RXQoX8RERERiSQVqiIiIiISSSpU29c/dHUHuhnllR7llT5llh7llR7llR7llR7lhc5RFREREZGI0h5VEREREYkkFaoiIiIiEkkqVNuBmU0ws7fMbK+ZvWNmk7u6T1FjZhVmttvMtoWPReF0ZQeY2T+GGbmZxeKmJ82nJ2eXIq+E61nY1pPz6mNmpeFv32Zmr5tZQdg2PHy/z8zKzWxm3HJJ265kbeS1wcwOxK1jT8Ut1yPzAjCzdWa2PczkDTObGk7XNiyJFJlpOxbP3fW4zAfwX8Bfha8XAlu6uk9RewAVQEzZJc3nNiCvZU6p8unJ2aXIK+F6przoAxRx/rqEvwPWha9fBJ4JX38ROAhkttV2JT/ayGsDcHeS5XpkXuHvHRT3+l7gT+FrbcPSz0zbsfjf3NUd6O4PYDhwLG7DbsBHQEFX9y1Kj0R/eMoudU6p8lF2iderZBt45dUqjxuA/eHrUwSjwjS1vQPMbqutJz1a5JWqUFVewe9+FNiqbVj6mYWvtR2Le+jQ/+XLB464ewOAB2tPJTCqS3sVTb82sz+b2Qtmlouya0uqfJRdci3XM1BeLT0JvGZmQ4Fe7h4/1HQFMCpVW6f1MjqeBF6Le/9cuI79xszGAigvMLOVZlYFPEtQeGkb1oYEmTXRdiykQrV9tLzHl3VJL6LtNnefAkwDjgIrwunKLrVU+Si71pKtZ6C8ADCzZcAE4HvhJK1jKSTI6xF3nwQUAm8Av4ubvUfn5e5fd/d84GnguabJLWbT+hUnSWbajsVRoXr5qoA8M8sEMDMj+L+eyi7tVcS4e2X4XA/8DJiFsmtLqnyUXQJJ1jNQXgCY2XeA+4D57n7G3Y+G03PjZhsNVKZq66z+drWWeQG4e1X47O7+T8BYMxuqvM5z9xXAHcAhtA27KE2ZheuStmNxVKheJnevBt4DHg4n3Q9UuHtFl3UqYswsx8wGxU16EHhP2aWWKh9l11qy9Qz0dwpgZksJMrnL3Y/FNZUAS8J5vghcA7x5EW1XtER5mVmmmV0dN8/9wP81Fan00LzMbICZjYh7/xcEewK1DUsiRWa12o5dSCNTtQMzmwgUA0OBE8Cj7r6jSzsVIeE5XGuADILDFAeAv3f3CmUXMLN/Bu4h+A/bx8Apdx+fKp+enF2ivIC5JFnPwmV6cl55BHtjDgAnw8ln3f3GsPD6N2AMUAc84e4bw+WStl3JkuUF3AlsBLKBRoJ1b6m7vx8u11Pzyif42+tLkEsN8B1336ZtWGLJMiPIQduxOCpURURERCSSdOhfRERERCJJhaqIiIiIRJIKVRERERGJJBWqIiIiIhJJKlRFREREJJJUqIqIiIhIJGV2dQdERHoSM6sAasNHk6+5+852/I4CYKu7D2uvzxQR6QoqVEVEOt9Cdy/v6k6IiESdDv2LiESAmbmZPWNmm81sr5k9GNc2z8z+ZGbbzWyjmU2Oa/trM9tmZu+b2dZwb2pT2w/M7I9mtt/Mijr3F4mIXD7tURUR6XyrzSz+0P+M8Nnd/dZw2OF3zOxNgqE7VwF3uPufzewh4LdAzMxmA98DZrn7/5pZv/BzhhMMsfhHd/++mc0Dngd+3/E/TUSk/WgIVRGRThSeo3p3y0P/ZuZAnrsfDt+XEhSkJwnG+p4TN+8xYBKwFDjp7j9o8VkFQLm7XxW+HwgcdXftnBCRbkWH/kVEossBC58TtaUSv8f2HJDRXp0SEeksKlRFRKLjb6B5j+hM4E1gCzDVzCaFbV8FDrn7R8BrwNfN7JqwrV/c4X8RkW5Ph4FERDpfy3NUvxU+nzWzzUAu8C13rwIws0eAX5tZBnAM+EsAd99kZs8C68JTB+qAhZ31I0REOprOURURiYCw0Ozv7qe6ui8iIlGhQ/8iIiIiEknaoyoiIiIikaQ9qiIiIiISSSpURURERCSSVKiKiIiISCSpUBURERGRSFKhKiIiIiKRpEJVRERERCLp/wFwo9YEOHhEFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml_utils.plot_loss_by_param(model_state_by_batch_size_trial_1, 'batch size', 'batch_size_loss_trial_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml_utils.plot_accuracies_by_param(model_state_by_batch_size_trial_1, 'batch size', 'batch_size_accuracy_trial_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.save_model_state(model_state_by_batch_size_trial_1, 'model_state_by_batch_size_trial_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, batch sizes 32 and 128 performed worse than 512. Could this be because the learning rate is too high? Let's try a lower learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/10000\n",
      "    582/Unknown - 25s 43ms/step - loss: 0.6926 - accuracy: 0.5135\n",
      "Saving weights for epoch 0\n",
      "582/582 [==============================] - 31s 54ms/step - loss: 0.6926 - accuracy: 0.5135 - val_loss: 0.6919 - val_accuracy: 0.4963\n",
      "Epoch 2/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6869 - accuracy: 0.5651 - val_loss: 0.6902 - val_accuracy: 0.5045\n",
      "Epoch 3/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6785 - accuracy: 0.5801 - val_loss: 0.6920 - val_accuracy: 0.5103\n",
      "Epoch 4/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6716 - accuracy: 0.5861 - val_loss: 0.6793 - val_accuracy: 0.5638\n",
      "Epoch 5/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6651 - accuracy: 0.5974 - val_loss: 0.6783 - val_accuracy: 0.5686\n",
      "Epoch 6/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6597 - accuracy: 0.6048\n",
      "Saving weights for epoch 5\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6596 - accuracy: 0.6048 - val_loss: 0.6695 - val_accuracy: 0.5862\n",
      "Epoch 7/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6558 - accuracy: 0.6100 - val_loss: 0.6592 - val_accuracy: 0.5982\n",
      "Epoch 8/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6508 - accuracy: 0.6131 - val_loss: 0.6547 - val_accuracy: 0.6058\n",
      "Epoch 9/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6456 - accuracy: 0.6183 - val_loss: 0.6435 - val_accuracy: 0.6255\n",
      "Epoch 10/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6436 - accuracy: 0.6227 - val_loss: 0.6423 - val_accuracy: 0.6249\n",
      "Epoch 11/10000\n",
      "580/582 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.6256\n",
      "Saving weights for epoch 10\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6402 - accuracy: 0.6255 - val_loss: 0.6375 - val_accuracy: 0.6320\n",
      "Epoch 12/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6367 - accuracy: 0.6298 - val_loss: 0.6327 - val_accuracy: 0.6419\n",
      "Epoch 13/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6331 - accuracy: 0.6386 - val_loss: 0.6302 - val_accuracy: 0.6462\n",
      "Epoch 14/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6301 - accuracy: 0.6389 - val_loss: 0.6263 - val_accuracy: 0.6498\n",
      "Epoch 15/10000\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6263 - accuracy: 0.6431 - val_loss: 0.6212 - val_accuracy: 0.6569\n",
      "Epoch 16/10000\n",
      "409/582 [====================>.........] - ETA: 3s - loss: 0.6219 - accuracy: 0.6465\n",
      "Saving weights for epoch 15\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7c02cf9ad300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mextra_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, validation, epochs, extra_callbacks)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_callback\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 128, 512]\n",
    "model_state_by_batch_size_trial_2 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model(optimizer=keras.optimizers.SGD(learning_rate=0.01))\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    model_state_by_batch_size_trial_2[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=10000,\n",
    "        extra_callbacks=[es],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size 8 performed poorly, the learning rate may be too high for small batch sizes. On the other hand, 32 and 128 didn't converge, so we may need to up the number of epochs. So, let's try a lower learning rate and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/100\n",
      "2327/2327 [==============================] - 55s 24ms/step - loss: 0.6840 - accuracy: 0.5558 - val_loss: 0.6565 - val_accuracy: 0.6187\n",
      "Epoch 2/100\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.6485 - accuracy: 0.6257 - val_loss: 0.6115 - val_accuracy: 0.6829\n",
      "Epoch 3/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.6004 - accuracy: 0.6779 - val_loss: 0.5513 - val_accuracy: 0.7291\n",
      "Epoch 4/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5560 - accuracy: 0.7195 - val_loss: 0.5134 - val_accuracy: 0.7517\n",
      "Epoch 5/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5310 - accuracy: 0.7338 - val_loss: 0.4981 - val_accuracy: 0.7612\n",
      "Epoch 6/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5105 - accuracy: 0.7502 - val_loss: 0.4941 - val_accuracy: 0.7711\n",
      "Epoch 7/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5004 - accuracy: 0.7546 - val_loss: 0.5188 - val_accuracy: 0.7543\n",
      "Epoch 8/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4850 - accuracy: 0.7666 - val_loss: 0.4716 - val_accuracy: 0.7771\n",
      "Epoch 9/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4805 - accuracy: 0.7681 - val_loss: 0.4788 - val_accuracy: 0.7797\n",
      "Epoch 10/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4711 - accuracy: 0.7760 - val_loss: 0.4518 - val_accuracy: 0.7861\n",
      "Epoch 11/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4630 - accuracy: 0.7797 - val_loss: 0.4369 - val_accuracy: 0.7964\n",
      "Epoch 12/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4579 - accuracy: 0.7847 - val_loss: 0.4378 - val_accuracy: 0.7936\n",
      "Epoch 13/100\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.4492 - accuracy: 0.7874 - val_loss: 0.4383 - val_accuracy: 0.7887\n",
      "Epoch 14/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4470 - accuracy: 0.7912 - val_loss: 0.4485 - val_accuracy: 0.7876\n",
      "Epoch 15/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4447 - accuracy: 0.7919 - val_loss: 0.4284 - val_accuracy: 0.8037\n",
      "Epoch 16/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4422 - accuracy: 0.7922 - val_loss: 0.4293 - val_accuracy: 0.7984\n",
      "Epoch 17/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4342 - accuracy: 0.7969 - val_loss: 0.4124 - val_accuracy: 0.8091\n",
      "Epoch 18/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4295 - accuracy: 0.8013 - val_loss: 0.4305 - val_accuracy: 0.7964\n",
      "Epoch 19/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4296 - accuracy: 0.8034 - val_loss: 0.4147 - val_accuracy: 0.8085\n",
      "Epoch 20/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4260 - accuracy: 0.8041 - val_loss: 0.3989 - val_accuracy: 0.8179\n",
      "Epoch 21/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4270 - accuracy: 0.8028 - val_loss: 0.4389 - val_accuracy: 0.8048\n",
      "Epoch 22/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4205 - accuracy: 0.8061 - val_loss: 0.4251 - val_accuracy: 0.8067\n",
      "Epoch 23/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4165 - accuracy: 0.8075 - val_loss: 0.4157 - val_accuracy: 0.8102\n",
      "Epoch 24/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4137 - accuracy: 0.8102 - val_loss: 0.4087 - val_accuracy: 0.8231\n",
      "Epoch 25/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4178 - accuracy: 0.8083 - val_loss: 0.4324 - val_accuracy: 0.7986\n",
      "Epoch 26/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4154 - accuracy: 0.8094 - val_loss: 0.4025 - val_accuracy: 0.8242\n",
      "Epoch 27/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4097 - accuracy: 0.8147 - val_loss: 0.4102 - val_accuracy: 0.8134\n",
      "Epoch 28/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4122 - accuracy: 0.8080 - val_loss: 0.4121 - val_accuracy: 0.8126\n",
      "Epoch 29/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4122 - accuracy: 0.8114 - val_loss: 0.4167 - val_accuracy: 0.8184\n",
      "Epoch 30/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4033 - accuracy: 0.8153 - val_loss: 0.3918 - val_accuracy: 0.8270\n",
      "Epoch 31/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4099 - accuracy: 0.8114 - val_loss: 0.4315 - val_accuracy: 0.8063\n",
      "Epoch 32/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4056 - accuracy: 0.8141 - val_loss: 0.4617 - val_accuracy: 0.7713\n",
      "Epoch 33/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4060 - accuracy: 0.8143 - val_loss: 0.4137 - val_accuracy: 0.8067\n",
      "Epoch 34/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4060 - accuracy: 0.8144 - val_loss: 0.3976 - val_accuracy: 0.8231\n",
      "Epoch 35/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4096 - accuracy: 0.8167 - val_loss: 0.4134 - val_accuracy: 0.8080\n",
      "Epoch 36/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4035 - accuracy: 0.8183 - val_loss: 0.4192 - val_accuracy: 0.8031\n",
      "Epoch 37/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4021 - accuracy: 0.8148 - val_loss: 0.4183 - val_accuracy: 0.8134\n",
      "Epoch 38/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4061 - accuracy: 0.8160 - val_loss: 0.4156 - val_accuracy: 0.8074\n",
      "Epoch 39/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3960 - accuracy: 0.8202 - val_loss: 0.4056 - val_accuracy: 0.8130\n",
      "Epoch 40/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4048 - accuracy: 0.8122 - val_loss: 0.4024 - val_accuracy: 0.8212\n",
      "Epoch 41/100\n",
      "2327/2327 [==============================] - 27s 11ms/step - loss: 0.3918 - accuracy: 0.8238 - val_loss: 0.4117 - val_accuracy: 0.8033\n",
      "Epoch 42/100\n",
      "2327/2327 [==============================] - 27s 11ms/step - loss: 0.4001 - accuracy: 0.8200 - val_loss: 0.4084 - val_accuracy: 0.8134\n",
      "Epoch 43/100\n",
      "2327/2327 [==============================] - 27s 11ms/step - loss: 0.3993 - accuracy: 0.8222 - val_loss: 0.4088 - val_accuracy: 0.8102\n",
      "Epoch 44/100\n",
      "2327/2327 [==============================] - 27s 11ms/step - loss: 0.3997 - accuracy: 0.8195 - val_loss: 0.4136 - val_accuracy: 0.8134\n",
      "Epoch 45/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3965 - accuracy: 0.8197 - val_loss: 0.4198 - val_accuracy: 0.8020\n",
      "Epoch 46/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3947 - accuracy: 0.8224 - val_loss: 0.4118 - val_accuracy: 0.8134\n",
      "Epoch 47/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3982 - accuracy: 0.8214 - val_loss: 0.4005 - val_accuracy: 0.8132\n",
      "Epoch 48/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3934 - accuracy: 0.8204 - val_loss: 0.3977 - val_accuracy: 0.8119\n",
      "Epoch 49/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3964 - accuracy: 0.8219 - val_loss: 0.3988 - val_accuracy: 0.8203\n",
      "Epoch 50/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3993 - accuracy: 0.8219 - val_loss: 0.4390 - val_accuracy: 0.7966\n",
      "Epoch 51/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3893 - accuracy: 0.8238 - val_loss: 0.4020 - val_accuracy: 0.8166\n",
      "Epoch 52/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3913 - accuracy: 0.8219 - val_loss: 0.3865 - val_accuracy: 0.8244\n",
      "Epoch 53/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3961 - accuracy: 0.8217 - val_loss: 0.4256 - val_accuracy: 0.7969\n",
      "Epoch 54/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3965 - accuracy: 0.8222 - val_loss: 0.4195 - val_accuracy: 0.8147\n",
      "Epoch 55/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3979 - accuracy: 0.8229 - val_loss: 0.4444 - val_accuracy: 0.7831\n",
      "Epoch 56/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3977 - accuracy: 0.8207 - val_loss: 0.4117 - val_accuracy: 0.8089\n",
      "Epoch 57/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3844 - accuracy: 0.8287 - val_loss: 0.4332 - val_accuracy: 0.7977\n",
      "Epoch 58/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3948 - accuracy: 0.8260 - val_loss: 0.4008 - val_accuracy: 0.8126\n",
      "Epoch 59/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3965 - accuracy: 0.8241 - val_loss: 0.4138 - val_accuracy: 0.8123\n",
      "Epoch 60/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3984 - accuracy: 0.8178 - val_loss: 0.4067 - val_accuracy: 0.8190\n",
      "Epoch 61/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3974 - accuracy: 0.8238 - val_loss: 0.4242 - val_accuracy: 0.8083\n",
      "Epoch 62/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3986 - accuracy: 0.8197 - val_loss: 0.4130 - val_accuracy: 0.8070\n",
      "Epoch 63/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3934 - accuracy: 0.8283 - val_loss: 0.4029 - val_accuracy: 0.8080\n",
      "Epoch 66/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3982 - accuracy: 0.8179 - val_loss: 0.4317 - val_accuracy: 0.8059\n",
      "Epoch 67/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4013 - accuracy: 0.8198 - val_loss: 0.3864 - val_accuracy: 0.8224\n",
      "Epoch 68/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4114 - accuracy: 0.8192 - val_loss: 0.3931 - val_accuracy: 0.8235\n",
      "Epoch 69/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.3998 - accuracy: 0.8180 - val_loss: 0.4100 - val_accuracy: 0.8175\n",
      "Epoch 70/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4073 - accuracy: 0.8174 - val_loss: 0.4231 - val_accuracy: 0.8046\n",
      "Epoch 71/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4066 - accuracy: 0.8163 - val_loss: 0.4010 - val_accuracy: 0.8173\n",
      "Epoch 72/100\n",
      " 981/2327 [===========>..................] - ETA: 14s - loss: 0.3942 - accuracy: 0.8202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4089 - accuracy: 0.8139 - val_loss: 0.4056 - val_accuracy: 0.8123\n",
      "Epoch 78/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4135 - accuracy: 0.8120 - val_loss: 0.4086 - val_accuracy: 0.8113\n",
      "Epoch 79/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4138 - accuracy: 0.8142 - val_loss: 0.4367 - val_accuracy: 0.7904\n",
      "Epoch 80/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4150 - accuracy: 0.8098 - val_loss: 0.4156 - val_accuracy: 0.8087\n",
      "Epoch 81/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4067 - accuracy: 0.8132 - val_loss: 0.3997 - val_accuracy: 0.8199\n",
      "Epoch 82/100\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.4170 - accuracy: 0.8093 - val_loss: 0.4154 - val_accuracy: 0.8091\n",
      "Epoch 83/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4151 - accuracy: 0.8108 - val_loss: 0.4222 - val_accuracy: 0.8076\n",
      "Epoch 84/100\n",
      " 811/2327 [=========>....................] - ETA: 16s - loss: 0.4063 - accuracy: 0.8163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4341 - accuracy: 0.8047 - val_loss: 0.4436 - val_accuracy: 0.7954\n",
      "Epoch 90/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4243 - accuracy: 0.8070 - val_loss: 0.4081 - val_accuracy: 0.8113\n",
      "Epoch 91/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4140 - accuracy: 0.8110 - val_loss: 0.4288 - val_accuracy: 0.8076\n",
      "Epoch 92/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4260 - accuracy: 0.8085 - val_loss: 0.4102 - val_accuracy: 0.8205\n",
      "Epoch 93/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4138 - accuracy: 0.8118 - val_loss: 0.4240 - val_accuracy: 0.8134\n",
      "Epoch 94/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4352 - accuracy: 0.7985 - val_loss: 0.4065 - val_accuracy: 0.8147\n",
      "Epoch 95/100\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.4286 - accuracy: 0.8045 - val_loss: 0.4016 - val_accuracy: 0.8169\n",
      "Epoch 96/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4237 - accuracy: 0.8070 - val_loss: 0.4057 - val_accuracy: 0.8188\n",
      "Epoch 97/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4700 - accuracy: 0.7565 - val_loss: 0.5018 - val_accuracy: 0.7472\n",
      "Epoch 98/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4345 - accuracy: 0.8039 - val_loss: 0.4407 - val_accuracy: 0.7975\n",
      "Epoch 99/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4299 - accuracy: 0.8068 - val_loss: 0.4154 - val_accuracy: 0.8085\n",
      "Epoch 100/100\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4254 - accuracy: 0.8051 - val_loss: 0.4466 - val_accuracy: 0.7971\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/100\n",
      "582/582 [==============================] - 31s 53ms/step - loss: 0.6876 - accuracy: 0.5428 - val_loss: 0.6733 - val_accuracy: 0.6139\n",
      "Epoch 2/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6826 - accuracy: 0.5662 - val_loss: 0.6663 - val_accuracy: 0.6169\n",
      "Epoch 3/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6639 - accuracy: 0.6012 - val_loss: 0.6586 - val_accuracy: 0.6034\n",
      "Epoch 4/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6435 - accuracy: 0.6319 - val_loss: 0.6503 - val_accuracy: 0.6165\n",
      "Epoch 5/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.6146 - accuracy: 0.6629 - val_loss: 0.5848 - val_accuracy: 0.6988\n",
      "Epoch 6/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5893 - accuracy: 0.6870 - val_loss: 0.5965 - val_accuracy: 0.6851\n",
      "Epoch 7/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5632 - accuracy: 0.7058 - val_loss: 0.5350 - val_accuracy: 0.7251\n",
      "Epoch 8/100\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.5361 - accuracy: 0.7303 - val_loss: 0.5074 - val_accuracy: 0.7474\n",
      "Epoch 9/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5156 - accuracy: 0.7434 - val_loss: 0.4946 - val_accuracy: 0.7603\n",
      "Epoch 10/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.5057 - accuracy: 0.7509 - val_loss: 0.4861 - val_accuracy: 0.7603\n",
      "Epoch 11/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4928 - accuracy: 0.7596 - val_loss: 0.4621 - val_accuracy: 0.7833\n",
      "Epoch 12/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4847 - accuracy: 0.7652 - val_loss: 0.4661 - val_accuracy: 0.7706\n",
      "Epoch 13/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4749 - accuracy: 0.7715 - val_loss: 0.4513 - val_accuracy: 0.7878\n",
      "Epoch 14/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4681 - accuracy: 0.7747 - val_loss: 0.4502 - val_accuracy: 0.7846\n",
      "Epoch 15/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4573 - accuracy: 0.7823 - val_loss: 0.4505 - val_accuracy: 0.7857\n",
      "Epoch 16/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4502 - accuracy: 0.7874 - val_loss: 0.4602 - val_accuracy: 0.7775\n",
      "Epoch 17/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4452 - accuracy: 0.7873 - val_loss: 0.4429 - val_accuracy: 0.7900\n",
      "Epoch 18/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4376 - accuracy: 0.7932 - val_loss: 0.4037 - val_accuracy: 0.8143\n",
      "Epoch 19/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4333 - accuracy: 0.7965 - val_loss: 0.4554 - val_accuracy: 0.7749\n",
      "Epoch 20/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4279 - accuracy: 0.7992 - val_loss: 0.4240 - val_accuracy: 0.7951\n",
      "Epoch 21/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4216 - accuracy: 0.8039 - val_loss: 0.4061 - val_accuracy: 0.8123\n",
      "Epoch 22/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4131 - accuracy: 0.8039 - val_loss: 0.4076 - val_accuracy: 0.8123\n",
      "Epoch 23/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4123 - accuracy: 0.8074 - val_loss: 0.4230 - val_accuracy: 0.8042\n",
      "Epoch 24/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.4038 - accuracy: 0.8132 - val_loss: 0.3956 - val_accuracy: 0.8287\n",
      "Epoch 25/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3991 - accuracy: 0.8177 - val_loss: 0.4122 - val_accuracy: 0.8065\n",
      "Epoch 26/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3951 - accuracy: 0.8180 - val_loss: 0.4074 - val_accuracy: 0.8104\n",
      "Epoch 27/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3916 - accuracy: 0.8190 - val_loss: 0.4174 - val_accuracy: 0.8065\n",
      "Epoch 28/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3926 - accuracy: 0.8205 - val_loss: 0.3893 - val_accuracy: 0.8250\n",
      "Epoch 29/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3856 - accuracy: 0.8232 - val_loss: 0.4410 - val_accuracy: 0.7902\n",
      "Epoch 30/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3781 - accuracy: 0.8245 - val_loss: 0.3854 - val_accuracy: 0.8259\n",
      "Epoch 31/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3758 - accuracy: 0.8285 - val_loss: 0.3825 - val_accuracy: 0.8270\n",
      "Epoch 32/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3768 - accuracy: 0.8278 - val_loss: 0.4026 - val_accuracy: 0.8113\n",
      "Epoch 33/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3694 - accuracy: 0.8330 - val_loss: 0.3753 - val_accuracy: 0.8272\n",
      "Epoch 34/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3643 - accuracy: 0.8343 - val_loss: 0.3821 - val_accuracy: 0.8235\n",
      "Epoch 35/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3606 - accuracy: 0.8391 - val_loss: 0.3936 - val_accuracy: 0.8145\n",
      "Epoch 36/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3592 - accuracy: 0.8390 - val_loss: 0.3824 - val_accuracy: 0.8222\n",
      "Epoch 37/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3586 - accuracy: 0.8412 - val_loss: 0.4049 - val_accuracy: 0.8117\n",
      "Epoch 38/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3532 - accuracy: 0.8416 - val_loss: 0.3815 - val_accuracy: 0.8224\n",
      "Epoch 39/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3454 - accuracy: 0.8471 - val_loss: 0.3720 - val_accuracy: 0.8280\n",
      "Epoch 40/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3470 - accuracy: 0.8448 - val_loss: 0.3635 - val_accuracy: 0.8343\n",
      "Epoch 41/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3405 - accuracy: 0.8486 - val_loss: 0.3613 - val_accuracy: 0.8366\n",
      "Epoch 42/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3347 - accuracy: 0.8485 - val_loss: 0.3843 - val_accuracy: 0.8216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3329 - accuracy: 0.8528 - val_loss: 0.3802 - val_accuracy: 0.8282\n",
      "Epoch 44/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3314 - accuracy: 0.8499 - val_loss: 0.3676 - val_accuracy: 0.8300\n",
      "Epoch 45/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3312 - accuracy: 0.8523 - val_loss: 0.3679 - val_accuracy: 0.8272\n",
      "Epoch 46/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3298 - accuracy: 0.8523 - val_loss: 0.3598 - val_accuracy: 0.8313\n",
      "Epoch 47/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3248 - accuracy: 0.8544 - val_loss: 0.3730 - val_accuracy: 0.8319\n",
      "Epoch 48/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3236 - accuracy: 0.8559 - val_loss: 0.3497 - val_accuracy: 0.8424\n",
      "Epoch 49/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3210 - accuracy: 0.8556 - val_loss: 0.3747 - val_accuracy: 0.8345\n",
      "Epoch 50/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3151 - accuracy: 0.8631 - val_loss: 0.3674 - val_accuracy: 0.8315\n",
      "Epoch 51/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3098 - accuracy: 0.8671 - val_loss: 0.3839 - val_accuracy: 0.8289\n",
      "Epoch 52/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3147 - accuracy: 0.8626 - val_loss: 0.3993 - val_accuracy: 0.8102\n",
      "Epoch 53/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3085 - accuracy: 0.8633 - val_loss: 0.3759 - val_accuracy: 0.8267\n",
      "Epoch 54/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3040 - accuracy: 0.8645 - val_loss: 0.3650 - val_accuracy: 0.8293\n",
      "Epoch 55/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3049 - accuracy: 0.8676 - val_loss: 0.4036 - val_accuracy: 0.8136\n",
      "Epoch 56/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.3019 - accuracy: 0.8686 - val_loss: 0.3578 - val_accuracy: 0.8386\n",
      "Epoch 57/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2984 - accuracy: 0.8699 - val_loss: 0.3499 - val_accuracy: 0.8386\n",
      "Epoch 58/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2960 - accuracy: 0.8712 - val_loss: 0.3620 - val_accuracy: 0.8353\n",
      "Epoch 59/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2927 - accuracy: 0.8752 - val_loss: 0.3496 - val_accuracy: 0.8422\n",
      "Epoch 60/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2880 - accuracy: 0.8765 - val_loss: 0.3568 - val_accuracy: 0.8401\n",
      "Epoch 61/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2862 - accuracy: 0.8761 - val_loss: 0.3750 - val_accuracy: 0.8263\n",
      "Epoch 62/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2879 - accuracy: 0.8738 - val_loss: 0.3605 - val_accuracy: 0.8371\n",
      "Epoch 63/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2794 - accuracy: 0.8775 - val_loss: 0.3677 - val_accuracy: 0.8298\n",
      "Epoch 64/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2790 - accuracy: 0.8803 - val_loss: 0.3518 - val_accuracy: 0.8368\n",
      "Epoch 65/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2820 - accuracy: 0.8767 - val_loss: 0.3564 - val_accuracy: 0.8349\n",
      "Epoch 66/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2763 - accuracy: 0.8797 - val_loss: 0.3651 - val_accuracy: 0.8295\n",
      "Epoch 67/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2749 - accuracy: 0.8805 - val_loss: 0.3643 - val_accuracy: 0.8300\n",
      "Epoch 68/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2708 - accuracy: 0.8825 - val_loss: 0.3711 - val_accuracy: 0.8317\n",
      "Epoch 69/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2733 - accuracy: 0.8806 - val_loss: 0.3767 - val_accuracy: 0.8263\n",
      "Epoch 70/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2700 - accuracy: 0.8818 - val_loss: 0.3654 - val_accuracy: 0.8334\n",
      "Epoch 71/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2666 - accuracy: 0.8831 - val_loss: 0.3899 - val_accuracy: 0.8212\n",
      "Epoch 72/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2577 - accuracy: 0.8893 - val_loss: 0.3534 - val_accuracy: 0.8396\n",
      "Epoch 73/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2596 - accuracy: 0.8896 - val_loss: 0.3619 - val_accuracy: 0.8368\n",
      "Epoch 74/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2563 - accuracy: 0.8912 - val_loss: 0.3674 - val_accuracy: 0.8317\n",
      "Epoch 75/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2570 - accuracy: 0.8905 - val_loss: 0.3626 - val_accuracy: 0.8313\n",
      "Epoch 76/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2562 - accuracy: 0.8904 - val_loss: 0.3672 - val_accuracy: 0.8319\n",
      "Epoch 77/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2514 - accuracy: 0.8944 - val_loss: 0.3732 - val_accuracy: 0.8255\n",
      "Epoch 78/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2480 - accuracy: 0.8964 - val_loss: 0.3761 - val_accuracy: 0.8287\n",
      "Epoch 79/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2431 - accuracy: 0.8953 - val_loss: 0.3739 - val_accuracy: 0.8293\n",
      "Epoch 80/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2496 - accuracy: 0.8940 - val_loss: 0.3617 - val_accuracy: 0.8371\n",
      "Epoch 81/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2400 - accuracy: 0.8969 - val_loss: 0.3945 - val_accuracy: 0.8291\n",
      "Epoch 82/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2450 - accuracy: 0.8963 - val_loss: 0.3708 - val_accuracy: 0.8319\n",
      "Epoch 83/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2386 - accuracy: 0.9005 - val_loss: 0.3547 - val_accuracy: 0.8478\n",
      "Epoch 84/100\n",
      "582/582 [==============================] - 13s 22ms/step - loss: 0.2400 - accuracy: 0.8973 - val_loss: 0.3542 - val_accuracy: 0.8454\n",
      "Epoch 85/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2345 - accuracy: 0.9013 - val_loss: 0.3656 - val_accuracy: 0.8338\n",
      "Epoch 86/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2312 - accuracy: 0.9023 - val_loss: 0.3763 - val_accuracy: 0.8362\n",
      "Epoch 87/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2338 - accuracy: 0.9008 - val_loss: 0.3740 - val_accuracy: 0.8291\n",
      "Epoch 88/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2311 - accuracy: 0.9039 - val_loss: 0.3726 - val_accuracy: 0.8325\n",
      "Epoch 89/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2258 - accuracy: 0.9066 - val_loss: 0.3496 - val_accuracy: 0.8457\n",
      "Epoch 90/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2227 - accuracy: 0.9070 - val_loss: 0.3606 - val_accuracy: 0.8426\n",
      "Epoch 91/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2243 - accuracy: 0.9051 - val_loss: 0.3646 - val_accuracy: 0.8405\n",
      "Epoch 92/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2249 - accuracy: 0.9052 - val_loss: 0.3886 - val_accuracy: 0.8250\n",
      "Epoch 93/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2218 - accuracy: 0.9067 - val_loss: 0.3572 - val_accuracy: 0.8472\n",
      "Epoch 94/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2239 - accuracy: 0.9057 - val_loss: 0.3574 - val_accuracy: 0.8487\n",
      "Epoch 95/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2164 - accuracy: 0.9096 - val_loss: 0.3618 - val_accuracy: 0.8450\n",
      "Epoch 96/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2172 - accuracy: 0.9106 - val_loss: 0.3790 - val_accuracy: 0.8332\n",
      "Epoch 97/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2119 - accuracy: 0.9128 - val_loss: 0.3605 - val_accuracy: 0.8399\n",
      "Epoch 98/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2069 - accuracy: 0.9154 - val_loss: 0.3755 - val_accuracy: 0.8332\n",
      "Epoch 99/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2119 - accuracy: 0.9128 - val_loss: 0.3712 - val_accuracy: 0.8373\n",
      "Epoch 100/100\n",
      "582/582 [==============================] - 12s 21ms/step - loss: 0.2064 - accuracy: 0.9142 - val_loss: 0.4125 - val_accuracy: 0.8194\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 30s 205ms/step - loss: 0.6915 - accuracy: 0.5252 - val_loss: 0.6907 - val_accuracy: 0.5077\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6838 - accuracy: 0.5698 - val_loss: 0.6812 - val_accuracy: 0.5800\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6777 - accuracy: 0.5737 - val_loss: 0.6787 - val_accuracy: 0.5626\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6730 - accuracy: 0.5861 - val_loss: 0.6710 - val_accuracy: 0.5875\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6695 - accuracy: 0.5927 - val_loss: 0.6565 - val_accuracy: 0.6285\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6632 - accuracy: 0.5953 - val_loss: 0.6540 - val_accuracy: 0.6206\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.6577 - accuracy: 0.6061 - val_loss: 0.6432 - val_accuracy: 0.6445\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6509 - accuracy: 0.6124 - val_loss: 0.6403 - val_accuracy: 0.6335\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6452 - accuracy: 0.6221 - val_loss: 0.6332 - val_accuracy: 0.6436\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6368 - accuracy: 0.6329 - val_loss: 0.6181 - val_accuracy: 0.6614\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6269 - accuracy: 0.6464 - val_loss: 0.6162 - val_accuracy: 0.6621\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6177 - accuracy: 0.6555 - val_loss: 0.6117 - val_accuracy: 0.6653\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.6067 - accuracy: 0.6672 - val_loss: 0.6129 - val_accuracy: 0.6606\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5998 - accuracy: 0.6739 - val_loss: 0.5702 - val_accuracy: 0.7111\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5970 - accuracy: 0.6742 - val_loss: 0.5994 - val_accuracy: 0.6722\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5828 - accuracy: 0.6925 - val_loss: 0.5702 - val_accuracy: 0.7055\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5769 - accuracy: 0.6958 - val_loss: 0.5760 - val_accuracy: 0.6933\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5708 - accuracy: 0.6987 - val_loss: 0.5488 - val_accuracy: 0.7380\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5632 - accuracy: 0.7069 - val_loss: 0.5410 - val_accuracy: 0.7319\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5548 - accuracy: 0.7143 - val_loss: 0.5286 - val_accuracy: 0.7511\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.5453 - accuracy: 0.7193 - val_loss: 0.5155 - val_accuracy: 0.7569\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5383 - accuracy: 0.7282 - val_loss: 0.4993 - val_accuracy: 0.7631\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5340 - accuracy: 0.7276 - val_loss: 0.5075 - val_accuracy: 0.7592\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5250 - accuracy: 0.7351 - val_loss: 0.5259 - val_accuracy: 0.7556\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5162 - accuracy: 0.7420 - val_loss: 0.5295 - val_accuracy: 0.7470\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5113 - accuracy: 0.7437 - val_loss: 0.4896 - val_accuracy: 0.7674\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5020 - accuracy: 0.7528 - val_loss: 0.4870 - val_accuracy: 0.7721\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.5001 - accuracy: 0.7539 - val_loss: 0.4822 - val_accuracy: 0.7706\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4940 - accuracy: 0.7563 - val_loss: 0.5163 - val_accuracy: 0.7601\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4873 - accuracy: 0.7618 - val_loss: 0.4716 - val_accuracy: 0.7850\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4852 - accuracy: 0.7669 - val_loss: 0.4575 - val_accuracy: 0.7915\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4789 - accuracy: 0.7656 - val_loss: 0.4657 - val_accuracy: 0.7842\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4773 - accuracy: 0.7714 - val_loss: 0.4634 - val_accuracy: 0.7859\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4711 - accuracy: 0.7732 - val_loss: 0.4732 - val_accuracy: 0.7848\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4695 - accuracy: 0.7716 - val_loss: 0.4605 - val_accuracy: 0.7859\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4655 - accuracy: 0.7772 - val_loss: 0.4515 - val_accuracy: 0.7850\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4617 - accuracy: 0.7824 - val_loss: 0.4451 - val_accuracy: 0.7908\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4583 - accuracy: 0.7829 - val_loss: 0.4401 - val_accuracy: 0.7962\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4541 - accuracy: 0.7841 - val_loss: 0.4443 - val_accuracy: 0.7896\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4544 - accuracy: 0.7876 - val_loss: 0.4417 - val_accuracy: 0.7945\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4498 - accuracy: 0.7869 - val_loss: 0.4505 - val_accuracy: 0.7848\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4498 - accuracy: 0.7901 - val_loss: 0.4454 - val_accuracy: 0.7936\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4455 - accuracy: 0.7892 - val_loss: 0.4312 - val_accuracy: 0.7997\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4460 - accuracy: 0.7874 - val_loss: 0.4322 - val_accuracy: 0.7984\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4375 - accuracy: 0.7937 - val_loss: 0.4535 - val_accuracy: 0.7911\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.4363 - accuracy: 0.7961 - val_loss: 0.4277 - val_accuracy: 0.8025\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4353 - accuracy: 0.7960 - val_loss: 0.4313 - val_accuracy: 0.8042\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4361 - accuracy: 0.7962 - val_loss: 0.4289 - val_accuracy: 0.7979\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4315 - accuracy: 0.7961 - val_loss: 0.4236 - val_accuracy: 0.8078\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4297 - accuracy: 0.7995 - val_loss: 0.4274 - val_accuracy: 0.8052\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4277 - accuracy: 0.7990 - val_loss: 0.4197 - val_accuracy: 0.8076\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4258 - accuracy: 0.8042 - val_loss: 0.4228 - val_accuracy: 0.8016\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4229 - accuracy: 0.8023 - val_loss: 0.4406 - val_accuracy: 0.7945\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4198 - accuracy: 0.8051 - val_loss: 0.4195 - val_accuracy: 0.8067\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4168 - accuracy: 0.8069 - val_loss: 0.4276 - val_accuracy: 0.8029\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4193 - accuracy: 0.8045 - val_loss: 0.4210 - val_accuracy: 0.8031\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4179 - accuracy: 0.8069 - val_loss: 0.4249 - val_accuracy: 0.8016\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4152 - accuracy: 0.8072 - val_loss: 0.4118 - val_accuracy: 0.8119\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4114 - accuracy: 0.8071 - val_loss: 0.4280 - val_accuracy: 0.7994\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4102 - accuracy: 0.8106 - val_loss: 0.4178 - val_accuracy: 0.8065\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4070 - accuracy: 0.8113 - val_loss: 0.4128 - val_accuracy: 0.8106\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4056 - accuracy: 0.8097 - val_loss: 0.4222 - val_accuracy: 0.8009\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4021 - accuracy: 0.8138 - val_loss: 0.4396 - val_accuracy: 0.7966\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.4002 - accuracy: 0.8169 - val_loss: 0.4051 - val_accuracy: 0.8151\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3996 - accuracy: 0.8168 - val_loss: 0.4106 - val_accuracy: 0.8130\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3977 - accuracy: 0.8168 - val_loss: 0.4060 - val_accuracy: 0.8143\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3989 - accuracy: 0.8181 - val_loss: 0.4020 - val_accuracy: 0.8207\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3942 - accuracy: 0.8190 - val_loss: 0.4025 - val_accuracy: 0.8156\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3945 - accuracy: 0.8188 - val_loss: 0.4052 - val_accuracy: 0.8130\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3898 - accuracy: 0.8220 - val_loss: 0.4000 - val_accuracy: 0.8188\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3874 - accuracy: 0.8254 - val_loss: 0.4124 - val_accuracy: 0.8095\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3915 - accuracy: 0.8212 - val_loss: 0.4085 - val_accuracy: 0.8089\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3883 - accuracy: 0.8235 - val_loss: 0.3922 - val_accuracy: 0.8248\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3836 - accuracy: 0.8253 - val_loss: 0.3930 - val_accuracy: 0.8175\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3845 - accuracy: 0.8222 - val_loss: 0.3898 - val_accuracy: 0.8227\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3780 - accuracy: 0.8303 - val_loss: 0.3947 - val_accuracy: 0.8186\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3794 - accuracy: 0.8254 - val_loss: 0.4294 - val_accuracy: 0.7947\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3817 - accuracy: 0.8243 - val_loss: 0.4057 - val_accuracy: 0.8091\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3779 - accuracy: 0.8286 - val_loss: 0.4160 - val_accuracy: 0.8095\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3764 - accuracy: 0.8294 - val_loss: 0.3962 - val_accuracy: 0.8209\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3717 - accuracy: 0.8295 - val_loss: 0.3925 - val_accuracy: 0.8212\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3755 - accuracy: 0.8267 - val_loss: 0.3849 - val_accuracy: 0.8293\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3701 - accuracy: 0.8310 - val_loss: 0.3851 - val_accuracy: 0.8242\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3722 - accuracy: 0.8326 - val_loss: 0.3957 - val_accuracy: 0.8192\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3677 - accuracy: 0.8347 - val_loss: 0.4058 - val_accuracy: 0.8141\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3604 - accuracy: 0.8379 - val_loss: 0.4268 - val_accuracy: 0.8016\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3612 - accuracy: 0.8372 - val_loss: 0.4023 - val_accuracy: 0.8134\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3663 - accuracy: 0.8347 - val_loss: 0.3800 - val_accuracy: 0.8306\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3613 - accuracy: 0.8368 - val_loss: 0.3819 - val_accuracy: 0.8257\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3600 - accuracy: 0.8380 - val_loss: 0.3846 - val_accuracy: 0.8274\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3561 - accuracy: 0.8394 - val_loss: 0.3855 - val_accuracy: 0.8298\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3554 - accuracy: 0.8395 - val_loss: 0.3921 - val_accuracy: 0.8259\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3538 - accuracy: 0.8402 - val_loss: 0.4131 - val_accuracy: 0.8117\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3589 - accuracy: 0.8381 - val_loss: 0.3893 - val_accuracy: 0.8242\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3498 - accuracy: 0.8414 - val_loss: 0.3811 - val_accuracy: 0.8274\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3544 - accuracy: 0.8386 - val_loss: 0.3886 - val_accuracy: 0.8194\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3438 - accuracy: 0.8464 - val_loss: 0.4067 - val_accuracy: 0.8156\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.3476 - accuracy: 0.8414 - val_loss: 0.4119 - val_accuracy: 0.8113\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3450 - accuracy: 0.8464 - val_loss: 0.3776 - val_accuracy: 0.8308\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.3459 - accuracy: 0.8452 - val_loss: 0.3784 - val_accuracy: 0.8298\n",
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 29s 797ms/step - loss: 0.6940 - accuracy: 0.4939 - val_loss: 0.6929 - val_accuracy: 0.5254\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 8s 203ms/step - loss: 0.6923 - accuracy: 0.5210 - val_loss: 0.6924 - val_accuracy: 0.4966\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6911 - accuracy: 0.5333 - val_loss: 0.6927 - val_accuracy: 0.4914\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6896 - accuracy: 0.5518 - val_loss: 0.6924 - val_accuracy: 0.4916\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6873 - accuracy: 0.5692 - val_loss: 0.6923 - val_accuracy: 0.4931\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6849 - accuracy: 0.5733 - val_loss: 0.6923 - val_accuracy: 0.4940\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 7s 203ms/step - loss: 0.6818 - accuracy: 0.5806 - val_loss: 0.6917 - val_accuracy: 0.4966\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6790 - accuracy: 0.5845 - val_loss: 0.6961 - val_accuracy: 0.4953\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6776 - accuracy: 0.5835 - val_loss: 0.6988 - val_accuracy: 0.4955\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6767 - accuracy: 0.5824 - val_loss: 0.6920 - val_accuracy: 0.5077\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6763 - accuracy: 0.5760 - val_loss: 0.6943 - val_accuracy: 0.5058\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6748 - accuracy: 0.5839 - val_loss: 0.7099 - val_accuracy: 0.4927\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6723 - accuracy: 0.5851 - val_loss: 0.6777 - val_accuracy: 0.5574\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6736 - accuracy: 0.5812 - val_loss: 0.6990 - val_accuracy: 0.5034\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6713 - accuracy: 0.5872 - val_loss: 0.6924 - val_accuracy: 0.5146\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6705 - accuracy: 0.5901 - val_loss: 0.6854 - val_accuracy: 0.5333\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6671 - accuracy: 0.5953 - val_loss: 0.6882 - val_accuracy: 0.5318\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6657 - accuracy: 0.5946 - val_loss: 0.6970 - val_accuracy: 0.5159\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6644 - accuracy: 0.5986 - val_loss: 0.6640 - val_accuracy: 0.5961\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6646 - accuracy: 0.5992 - val_loss: 0.6670 - val_accuracy: 0.5858\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6610 - accuracy: 0.6011 - val_loss: 0.6691 - val_accuracy: 0.5789\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6596 - accuracy: 0.6030 - val_loss: 0.6718 - val_accuracy: 0.5718\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6611 - accuracy: 0.5986 - val_loss: 0.6544 - val_accuracy: 0.6133\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6569 - accuracy: 0.6054 - val_loss: 0.6583 - val_accuracy: 0.5997\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6548 - accuracy: 0.6102 - val_loss: 0.6647 - val_accuracy: 0.5813\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6540 - accuracy: 0.6078 - val_loss: 0.6508 - val_accuracy: 0.6122\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6524 - accuracy: 0.6098 - val_loss: 0.6501 - val_accuracy: 0.6139\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6502 - accuracy: 0.6174 - val_loss: 0.6577 - val_accuracy: 0.5972\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6485 - accuracy: 0.6160 - val_loss: 0.6452 - val_accuracy: 0.6163\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6479 - accuracy: 0.6150 - val_loss: 0.6505 - val_accuracy: 0.6116\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6465 - accuracy: 0.6167 - val_loss: 0.6479 - val_accuracy: 0.6133\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6449 - accuracy: 0.6190 - val_loss: 0.6444 - val_accuracy: 0.6174\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6440 - accuracy: 0.6204 - val_loss: 0.6366 - val_accuracy: 0.6309\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6414 - accuracy: 0.6224 - val_loss: 0.6374 - val_accuracy: 0.6270\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6403 - accuracy: 0.6267 - val_loss: 0.6369 - val_accuracy: 0.6285\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6386 - accuracy: 0.6298 - val_loss: 0.6390 - val_accuracy: 0.6232\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6367 - accuracy: 0.6303 - val_loss: 0.6371 - val_accuracy: 0.6270\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6350 - accuracy: 0.6328 - val_loss: 0.6283 - val_accuracy: 0.6395\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6293 - accuracy: 0.6417 - val_loss: 0.6428 - val_accuracy: 0.6180\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6321 - accuracy: 0.6390 - val_loss: 0.6265 - val_accuracy: 0.6395\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6260 - accuracy: 0.6446 - val_loss: 0.6242 - val_accuracy: 0.6453\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6217 - accuracy: 0.6515 - val_loss: 0.6294 - val_accuracy: 0.6367\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 7s 203ms/step - loss: 0.6188 - accuracy: 0.6549 - val_loss: 0.6320 - val_accuracy: 0.6350\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6144 - accuracy: 0.6583 - val_loss: 0.6350 - val_accuracy: 0.6292\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.6083 - accuracy: 0.6608 - val_loss: 0.6230 - val_accuracy: 0.6414\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6065 - accuracy: 0.6667 - val_loss: 0.6375 - val_accuracy: 0.6270\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6024 - accuracy: 0.6708 - val_loss: 0.6210 - val_accuracy: 0.6421\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.6008 - accuracy: 0.6745 - val_loss: 0.6189 - val_accuracy: 0.6429\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5971 - accuracy: 0.6758 - val_loss: 0.6153 - val_accuracy: 0.6442\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5952 - accuracy: 0.6772 - val_loss: 0.6073 - val_accuracy: 0.6548\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5935 - accuracy: 0.6814 - val_loss: 0.6174 - val_accuracy: 0.6408\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5894 - accuracy: 0.6863 - val_loss: 0.5973 - val_accuracy: 0.6642\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 7s 203ms/step - loss: 0.5866 - accuracy: 0.6866 - val_loss: 0.6043 - val_accuracy: 0.6554\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5873 - accuracy: 0.6868 - val_loss: 0.6094 - val_accuracy: 0.6479\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 8s 203ms/step - loss: 0.5837 - accuracy: 0.6860 - val_loss: 0.5997 - val_accuracy: 0.6612\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5837 - accuracy: 0.6905 - val_loss: 0.6110 - val_accuracy: 0.6546\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5790 - accuracy: 0.6919 - val_loss: 0.5906 - val_accuracy: 0.6750\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 8s 203ms/step - loss: 0.5757 - accuracy: 0.6961 - val_loss: 0.5860 - val_accuracy: 0.6844\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5754 - accuracy: 0.6968 - val_loss: 0.5920 - val_accuracy: 0.6666\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5752 - accuracy: 0.6956 - val_loss: 0.5947 - val_accuracy: 0.6649\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5702 - accuracy: 0.6984 - val_loss: 0.5922 - val_accuracy: 0.6726\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5785 - accuracy: 0.6947 - val_loss: 0.5771 - val_accuracy: 0.6844\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 0.5659 - accuracy: 0.7024 - val_loss: 0.5616 - val_accuracy: 0.7128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5743 - accuracy: 0.6960 - val_loss: 0.5574 - val_accuracy: 0.6971\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5672 - accuracy: 0.7048 - val_loss: 0.5594 - val_accuracy: 0.7008\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5602 - accuracy: 0.7085 - val_loss: 0.5525 - val_accuracy: 0.7038\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5565 - accuracy: 0.7088 - val_loss: 0.5733 - val_accuracy: 0.6866\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5588 - accuracy: 0.7088 - val_loss: 0.5341 - val_accuracy: 0.7264\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5549 - accuracy: 0.7135 - val_loss: 0.5684 - val_accuracy: 0.6926\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5509 - accuracy: 0.7167 - val_loss: 0.5791 - val_accuracy: 0.6808\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5491 - accuracy: 0.7213 - val_loss: 0.5434 - val_accuracy: 0.7089\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5476 - accuracy: 0.7179 - val_loss: 0.5543 - val_accuracy: 0.7025\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5449 - accuracy: 0.7191 - val_loss: 0.5380 - val_accuracy: 0.7229\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5401 - accuracy: 0.7242 - val_loss: 0.5657 - val_accuracy: 0.6911\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5450 - accuracy: 0.7208 - val_loss: 0.5615 - val_accuracy: 0.6945\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5427 - accuracy: 0.7210 - val_loss: 0.5700 - val_accuracy: 0.6933\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5422 - accuracy: 0.7230 - val_loss: 0.5503 - val_accuracy: 0.7055\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 0.5375 - accuracy: 0.7280 - val_loss: 0.5533 - val_accuracy: 0.7031\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5321 - accuracy: 0.7286 - val_loss: 0.5564 - val_accuracy: 0.6993\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5372 - accuracy: 0.7254 - val_loss: 0.5515 - val_accuracy: 0.7083\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5285 - accuracy: 0.7334 - val_loss: 0.5555 - val_accuracy: 0.7036\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5308 - accuracy: 0.7302 - val_loss: 0.5272 - val_accuracy: 0.7354\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5283 - accuracy: 0.7336 - val_loss: 0.5517 - val_accuracy: 0.7068\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5246 - accuracy: 0.7362 - val_loss: 0.5372 - val_accuracy: 0.7231\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 8s 203ms/step - loss: 0.5221 - accuracy: 0.7379 - val_loss: 0.5473 - val_accuracy: 0.7152\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5238 - accuracy: 0.7391 - val_loss: 0.5287 - val_accuracy: 0.7248\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5168 - accuracy: 0.7430 - val_loss: 0.5499 - val_accuracy: 0.7143\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5178 - accuracy: 0.7399 - val_loss: 0.5259 - val_accuracy: 0.7279\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5144 - accuracy: 0.7448 - val_loss: 0.4998 - val_accuracy: 0.7470\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5144 - accuracy: 0.7422 - val_loss: 0.5156 - val_accuracy: 0.7412\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5120 - accuracy: 0.7499 - val_loss: 0.5387 - val_accuracy: 0.7225\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5079 - accuracy: 0.7479 - val_loss: 0.5103 - val_accuracy: 0.7418\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5043 - accuracy: 0.7502 - val_loss: 0.5833 - val_accuracy: 0.6797\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.5085 - accuracy: 0.7486 - val_loss: 0.5282 - val_accuracy: 0.7311\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.5014 - accuracy: 0.7532 - val_loss: 0.5140 - val_accuracy: 0.7408\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.4998 - accuracy: 0.7529 - val_loss: 0.4892 - val_accuracy: 0.7582\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.4984 - accuracy: 0.7548 - val_loss: 0.4979 - val_accuracy: 0.7496\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.4995 - accuracy: 0.7548 - val_loss: 0.4994 - val_accuracy: 0.7580\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.4983 - accuracy: 0.7548 - val_loss: 0.4811 - val_accuracy: 0.7635\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 8s 203ms/step - loss: 0.4966 - accuracy: 0.7568 - val_loss: 0.5152 - val_accuracy: 0.7397\n",
      "Training Data Summary\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-95cac6de3852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cats_and_dogs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36mload_batched_and_resized_dataset\u001b[0;34m(dataset_name, batch_size, img_size, shuffle_buffer_size, shuffle_seed)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mraw_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cats_vs_dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Resize images and normalize (divide by 255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dataset_name, shuffle_seed)\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Data Summary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msummarize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nValidation Data Summary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msummarize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36msummarize_dataset\u001b[0;34m(tf_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mtf_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrefetchDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mclass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mclass_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_freq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mtf_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrefetchDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mclass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mclass_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_freq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    226\u001b[0m   return _from_tensor_list_helper(\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       element_spec, tensor_list)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sizes = [8, 32, 128, 512, 2048]\n",
    "model_state_by_type_trial_2 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model(optimizer=keras.optimizers.SGD(learning_rate=0.05))\n",
    "    model_state_by_type_trial_2[batch_size] = ml_utils.train_model(model, train, validation, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml_utils.plot_loss_by_param(model_state_by_type, 'batch size', 'batch_size_loss_2nd_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.save_model_state(model_state_by_type, 'model_state_by_batch_size_2nd_try')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it may take many more iterations for larger batch sizes to converge. Let's try early stopping so we don't have to manually specify the number of iterations to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Summary\n",
      "No. of examples: 18610\n",
      "Class: 0 :::: Count: 9378 :::: Percentage: 50.39226222461043\n",
      "Class: 1 :::: Count: 9232 :::: Percentage: 49.60773777538957\n",
      "\n",
      "Validation Data Summary\n",
      "No. of examples: 4652\n",
      "Class: 0 :::: Count: 2280 :::: Percentage: 49.011177987962164\n",
      "Class: 1 :::: Count: 2372 :::: Percentage: 50.988822012037836\n",
      "Epoch 1/20000\n",
      "2327/2327 [==============================] - 57s 25ms/step - loss: 0.6873 - accuracy: 0.5503 - val_loss: 0.6778 - val_accuracy: 0.5929\n",
      "Epoch 2/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.6740 - accuracy: 0.5848 - val_loss: 0.6626 - val_accuracy: 0.6193\n",
      "Epoch 3/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.6580 - accuracy: 0.6114 - val_loss: 0.6377 - val_accuracy: 0.6599\n",
      "Epoch 4/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.6353 - accuracy: 0.6403 - val_loss: 0.6135 - val_accuracy: 0.6750\n",
      "Epoch 5/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.6089 - accuracy: 0.6663 - val_loss: 0.5910 - val_accuracy: 0.6840\n",
      "Epoch 6/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5820 - accuracy: 0.6908 - val_loss: 0.5594 - val_accuracy: 0.7324\n",
      "Epoch 7/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5507 - accuracy: 0.7187 - val_loss: 0.5218 - val_accuracy: 0.7530\n",
      "Epoch 8/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5282 - accuracy: 0.7327 - val_loss: 0.4943 - val_accuracy: 0.7689\n",
      "Epoch 9/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5113 - accuracy: 0.7477 - val_loss: 0.4855 - val_accuracy: 0.7717\n",
      "Epoch 10/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.5008 - accuracy: 0.7536 - val_loss: 0.4730 - val_accuracy: 0.7803\n",
      "Epoch 11/20000\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.4907 - accuracy: 0.7589 - val_loss: 0.4734 - val_accuracy: 0.7842\n",
      "Epoch 12/20000\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 0.4820 - accuracy: 0.7682 - val_loss: 0.4631 - val_accuracy: 0.7874\n",
      "Epoch 13/20000\n",
      "2326/2327 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.7691WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2327/2327 [==============================] - 27s 11ms/step - loss: 0.4727 - accuracy: 0.7692\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3004bd5c0e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mextra_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;32m~/ml-experiments/utils/ml_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, validation, epochs, extra_callbacks)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_callback\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     )\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sizes = [8, 32, 128, 512]\n",
    "model_state_by_type_trial_3 = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = ml_utils.load_batched_and_resized_dataset(\n",
    "        dataset_name='cats_and_dogs',   \n",
    "        batch_size=batch_size,\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    model = ml_utils.build_model(optimizer=keras.optimizers.SGD(learning_rate=0.01))\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    model_state_by_type_trial_3[batch_size] = ml_utils.train_model(\n",
    "        model,\n",
    "        train,\n",
    "        validation,\n",
    "        epochs=20000,\n",
    "        extra_callbacks=[es]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Norm:  27.4336502727029\n",
      "Epoch 1\n",
      "Norm:  27.449913808055737\n",
      "Epoch 2\n",
      "Norm:  27.466143766527928\n",
      "Epoch 3\n",
      "Norm:  27.4870729318222\n"
     ]
    }
   ],
   "source": [
    "for batch_size, model_state in model_state_by_type.items():\n",
    "    print(\"Batch size {}\".format(batch_size))\n",
    "    for i in range(4):\n",
    "        print(\"\\tEpoch {}\".format(i))\n",
    "        flattened_weights = np.array([])\n",
    "        curr_weights = model_state.weights_by_epoch[i]\n",
    "        for j in range(len(curr_weights)):\n",
    "            flattened_weights = np.concatenate((flattened_weights, curr_weights[j].flatten()))\n",
    "        print(\"\\tNorm: \", np.linalg.norm(flattened_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
